[{"title":"Cephx 实战演练","url":"/2017/11/30/Cephx-practice/","content":"\n<center>![cephx][2]</center>\n\n本文就阅读完[徐小胖的大话Cephx][1]后，针对一些猜测和疑惑进行了实战演练，对原文的一些说法和结论进行了验证，并进行了一系列的扩展的思考猜想和总结。最后收获满满，不仅对原文的一些结论进行了验证，也发现了其中的一些问题，更多的是自己动手后一些奇妙的场景和发现。\n\n<!--more-->\n\n本文实战任务和完成情况如下：\n\n- [x] 删除`client.admin.keyring`\n- [x] 修改`cephx`配置\n- [x] 修改`Monitor keyring`\n- [x] 修改`OSD keyring`\n- [x] 修改`client.admin.keyring`，通过`Mon`找回正确的`keyring`\n- [x] `Mon Cap`\n- [x] `OSD Cap`\n- [x] 删除所有`keyring`文件再恢复\n- [x] 删除`ceph.conf`再恢复\n- [ ] 关闭`CephX`后不重启`OSD`\n- [x] 通过`osd.keyring`访问集群\n- [ ] 配置只能访问一个`RBD`的用户权限\n\n## 删除 client.admin.keyring\n\n主节点开始存在`keyring`，可以正常访问集群\n\n```\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n \n  services:\n    mon:     3 daemons, quorum node1,node2,node3\n    mgr:     no daemons active\n    osd:     6 osds: 5 up, 5 in\n    rgw:     1 daemon active\n    rgw-nfs: 1 daemon active\n \n  data:\n    pools:   10 pools, 444 pgs\n    objects: 257 objects, 36140 kB\n    usage:   6256 MB used, 40645 MB / 46901 MB avail\n    pgs:     63.288% pgs not active\n             311/771 objects degraded (40.337%)\n             158 undersized+degraded+peered\n             158 active+undersized+degraded\n             65  down\n             58  incomplete\n             5   active+clean+remapped\n```\n\n将`keyring`文件移动到其他地方，相当于删除了`keyring`，这时访问集群报错\n\n```\n[root@node1 ceph]# mv ceph.client.admin.keyring /tmp/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-mgr.keyring  ceph.bootstrap-osd.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring  rbdmap\n[root@node1 ceph]# ceph -s\n2017-11-23 18:07:48.685028 7f63f6935700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 18:07:48.685094 7f63f6935700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 18:07:48.685098 7f63f6935700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n再拷贝回来又可以访问集群了\n\n```\n[root@node1 ceph]# mv /tmp/ceph.client.admin.keyring ./\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n```\n \n`node3`由于`/etc/ceph/`目录下没有`keyring`文件，所以也无法连接集群\n\n```\n[root@node3 ceph]# ls\nceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# ceph -s\n2017-11-23 17:59:16.659034 7fbe34678700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 17:59:16.659085 7fbe34678700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 17:59:16.659089 7fbe34678700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n**结论：**\n\n> 当`ceph.conf`中的`auth`配置为`cephx`的时候，访问集群是需要秘钥文件的\n\n## 修改 cephx 配置\n\n在`node3`节点上的`/etc/ceph/`目录下操作，首先将`ceph.client.admin.keyring`文件删除，然后将`auth`配置从`cephx`改为`none`，然后先重启`monitor`，再重启`osd`，这时候依然不可以访问集群，因为`cephx`是面向整个集群的，而不是某个节点，接下来需要在其他节点做一样的操作，更改`cephx`为`none`，然后重启`monitor`和`osd`，这时候便可以在没有`keyring`文件的情况下访问集群了。\n\n```\n# 删除keyring文件\n[root@node3 ~]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# mv ceph.client.admin.keyring /tmp/\n# 更改cephx配置\n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# vim ceph.conf \n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = none\nauth_service_required = none\nauth_client_required = none\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon.target\n[root@node3 ceph]# systemctl restart ceph-osd.target\n# 更改单个节点配置后依然不可以访问集群\n[root@node3 ceph]# ceph -s\n2017-11-27 23:05:23.022571 7f5200c2f700  0 librados: client.admin authentication error (95) Operation not supported\n[errno 95] error connecting to the cluster\n# 相应的更改其他几个节点并重启，便又可以正常访问集群了\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n**结论：**\n\n> 当`auth`配置为`cephx`的时候访问集群必须要借助秘钥文件，而当`auth`配置为`none`的时候，不再需要秘钥文件就可以访问集群了。（**更改配置需要集群所有节点都做才可以生效，而不是单一节点**）\n\n## 删除monitor秘钥\n\n`/etc/ceph`和`/var/lib//ceph/mon/ceph-node1`各有一个`mon keyring`\n```\n[root@node1 ceph-node1]# cd /etc/ceph/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  keyring  kv_backend  store.db  systemd\n```\n\n先删除`/etc/ceph/ceph-mon.keyring`，还是可以访问集群\n\n```\n[root@node1 ceph]# rm ceph.mon.keyring \nrm: remove regular file ‘ceph.mon.keyring’? y\n[root@node1 ceph]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2\n...\n...\n```\n\n再删除`/var/lib/ceph/mon/ceph-node1/keyring`\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph-node1]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph-node1]# ceph -s\n\n```\n\n访问集群一直`timeount`，查看`log`文件发现`Mon`初始化失败\n\n```\n2017-11-24 00:33:55.812955 7fa16f995e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:33:55.812991 7fa16f995e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:33:55.812999 7fa16f995e40 -1 failed to initialize\n```\n\nok，那我们再试试将`/var/lib/ceph/mon/ceph-node1/keyring`删除，将`etc/ceph/ceph.mon.keyring`拷贝回来，这时候意外发生了，居然`mon`初始化失败\n\n**结论：**\n\n> `Monitor`启动是需要`keyring`文件进行秘钥认证的，并且这个文件必须是`/var/lib/ceph/mon/ceph-node1/`目录下的，`/etc/ceph/`目录下的`ceph.mon.keyring`并不起作用\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring  \n[root@node1 ceph]# ceph -s\n// timeout\n...\n```\n\n`mon.log`中的现象：\n\n```\n2017-11-24 00:44:26.534865 7ffaf5117e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:44:26.534901 7ffaf5117e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:44:26.534916 7ffaf5117e40 -1 failed to initialize\n```\n\n至此，我们可以得出结论`monitor`初始化的时候依赖的文件是`/var/lib/ceph/mon/ceph-node1/keyring`而不是`/etc/ceph/ceph.mon.keyring`\n\n## 修改 Mon keyring\n\n\n### 原始的 keyring\n\n```\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 将中间的五个A替换成了五个C\n\n```\n[root@node1 ceph-node1]# vim keyring \n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 重启查看 Mon keyring\n\n理想结果：\n\n```\n[root@node1 ceph-node1]# systemctl restart ceph-mon.target\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n令人疑惑的现实：\n\n```\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n可以看到一会是修改之前的`keyring`，一会是修改之后的`keyring`，那遇到这种问题，我们就通过`log`观察如何获取`keyring`的\n\n`node1`的`mon.log`中日志：\n\n```\n2017-11-24 09:30:08.697047 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:08.697106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1169357136' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:10.020571 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:10.020641 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2455152702' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:11.393391 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:11.393452 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1704778092' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:12.669987 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:12.670049 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/275069695' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:14.113077 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:14.113147 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/3800873459' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:15.742038 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:15.742106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1908944728' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:17.629681 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:17.629729 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2193002591' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n`node2`的`mon.log`中日志：\n\n```\n2017-11-24 09:29:23.799402 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4284881078' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:26.030516 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:26.030588 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4157525590' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:38.637677 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:38.637748 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4028820259' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n**结论：**\n\n* `Monitor`的秘钥哪怕被修改过了，也不会影响`Monitor`的启动，也就是说`Monitor`启动时只要存在秘钥文件就好，内容忽略并不重要\n* `Monitor`启动的时候读取秘钥文件是随机的，并不一定是当前节点的，具体选择机制需要后期去看源代码了\n\n\n## 修改OSD keyring和修复\n\n`OSD`启动的时候需要秘钥才可以登录集群，这个秘钥会存在`Monitor`的数据库中，所以登录的时候就会拿本地的`keyring`和存在`Monitor`中的`keyring`相匹配，正确的话才可以启动成功。\n\n下面我们将本地的`OSD keyring`故意改错，然后重启`OSD`查看效果\n\n```\n# 更改秘钥文件\n[root@node3 ceph]# cd /var/lib/ceph/osd/ceph-2\n[root@node3 ceph-2]# ls\nactivate.monmap  active  block  bluefs  ceph_fsid  fsid  keyring  kv_backend  magic  mkfs_done  ready  systemd  type  whoami\n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = BBBp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service\n# 重启后发现OSD的状态时down\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2    down  1.00000 1.00000 \n```\n\n查看日志，发现`init`失败，原因是`auth`认证出错\n\n```\n2017-11-27 23:52:18.069207 7fae1e8d2d00 -1 auth: error parsing file /var/lib/ceph/osd/ceph-2/keyring\n2017-11-27 23:52:18.069285 7fae1e8d2d00 -1 auth: failed to load /var/lib/ceph/osd/ceph-2/keyring: (5) Input/output error\n...\n2017-11-27 23:52:41.232803 7f58d15ded00 -1  ** ERROR: osd init failed: (5) Input/output error\n```\n\n我们可以通过查询`Monitor`数据库获取正确的`keyring`，将错误的`keyring`修正过来再重启`OSD`\n\n```\n# 查询Monitor数据库中的osd keyring\n[root@node3 ceph-2]# ceph auth get osd.2\nexported keyring for osd.2\n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n\tcaps mgr = \"allow profile osd\"\n\tcaps mon = \"allow profile osd\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service \n# 重启OSD后可以发现osd.2状态已经变为up\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2      up  1.00000 1.00000 \n```\n\n**结论：**\n\n> `OSD`启动需要正确的`keyring`，错误的话则无法启动成功，正确的`keyring`会被存在`Monitor`的数据库中\n\n\n## 修改Client keyring和修复\n\n之前我们通过删除`client keyring`验证了当`auth=cephx`的时候，客户端需要`keyring`才可以访问集群，那么它是像`Monitor`一样内容不被`care`还是和`OSD`一样需要精确匹配`keyring`呢？\n\n```\n# 修改ceph.client.admin.keyring\n[root@node3 ceph-2]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = BBBB7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群出错\n[root@node3 ceph]# ceph -s\n2017-11-28 00:06:05.771604 7f3a69ccf700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:06:05.771622 7f3a69ccf700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:06:05.771634 7f3a69ccf700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n```\n\n可以看出访问集群需要正确的`keyring`，这时候如何修复呢？大家应该能够猜到，它和`OSD`的原理是一样的，正确的`keyring`也存在与`Monitor`的数据库\n\n```\n# 直接获取client.admin出错\n[root@node3 ceph]# ceph auth get client.admin\n2017-11-28 00:08:19.159073 7fcabb297700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:08:19.159079 7fcabb297700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:08:19.159090 7fcabb297700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n# 需要加上monitor的keyring文件才可以获取client.admin.keyring\n[root@node3 ceph]# ceph auth get client.admin --name mon. --keyring /var/lib/ceph/mon/ceph-node3/keyring\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph]# vim ceph\nceph.client.admin.keyring  ceph.conf                  ceph-deploy-ceph.log       \n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群成功\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n出现了令人惊奇的一幕，就是上面通过`ceph auth`获取`OSD`的`keyring`可以正常获取，而获取`client.admin.keyring`却要加上`monitor.keyring`，原因可以从报错信息看出，`ceph auth`需要以客户端连接集群为前提。\n\n结论：\n\n> `Client`访问集群和`OSD`一样，需要正确的`keyring`与存在`Monitor`数据库中对应的`keyring`相匹配，并且当`client.admin.keyring`\n不正确时，通过`ceph auth`读取`keyring`的时候需要加上`monitor keyring`的选项\n\n## Mon Caps\n\n\n### r 权限\n\n`Monior`的`r`权限就是拥有读权限，对应的读权限都有哪些操作？在这里的读权限其实就是拥有读取`Monitor`数据库中信息的权限，`MON`作为集群的状态维护者，其数据库(`/var/lib/ceph/mon/ceph-$hostname/store.db`)内保存着集群这一系列状态图(`Cluster Map`)，这些`Map`包含但不限于：\n\n* `CRUSH Map`\n* `OSD Map`\n* `MON Map`\n* `MDS Map`\n* `PG Map`\n\n所以接下来我们可以创建一个新的只拥有读权限的用户，进行相关操作验证读权限具体拥有哪些权限\n\n```\nceph auth get-or-create client.mon_r mon 'allow r' >> /root/key\n[root@node3 ceph]# ceph auth get client.mon_r\nexported keyring for client.mon_r\n[client.mon_r]\n\tkey = AQABvRxaBS6BBhAAz9uwjYCT4xKavJhobIK3ig==\n\tcaps mon = \"allow r\"\n\t\nceph --name client.mon_r --keyring /root/key -s      // ok\n\nceph --name client.mon_r --keyring /root/key osd crush dump     // ok\nceph --name client.mon_r --keyring /root/key osd getcrushmap -o crushmap.map        // ok\n\nceph --name client.mon_r --keyring /root/key osd dump       // ok\nceph --name client.mon_r --keyring /root/key osd tree       // ok\nceph --name client.mon_r --keyring /root/key osd stat       // ok\n\nceph --name client.mon_r --keyring /root/key pg dump        // ok\nceph --name client.mon_r --keyring /root/key pg stat        // ok\n```\n\n尝试了下两个写操作，都显示报错权限拒绝\n\n```\n[root@node3 ceph]# rados --name client.mon_r --keyring /root/key -p testpool put crush crushmap.map\nerror putting testpool/crush: (1) Operation not permitted\n\n[root@node3 ceph]# ceph --name client.mon_r --keyring /root/key osd out osd.0\nError EACCES: access denied\n```\n\n**注意：**\n\n虽然上面有`osd`和`pg`等信息，但是这些都隶属于`crush map`的范畴中，所以这些状态数据都是从`Monitor`获取的\n\n**结论：**\n\n> `Monitor`的读权限对应的是从`Monitor`数据库获取一系列的`Map`信息，具体的上面也都讲的很详细了，并且该权限只能读取状态信息，不能获取具体数据信息，且不能进行`OSD`等守护进程写操作\n\n### w 权限\n\n`w`权限必须配合`r`权限才会有效果，否则，单独`w`权限执行指令时，是会一直`access denied`的。所以我们在测试`w`权限时，需要附加上`r`权限才行：\n\n```\nceph auth get-or-create client.mon_rw mon 'allow rw' >> /root/key\n```\n\n而`w`权限就可以做一些对组件的非读操作了，比如：\n\n```\n# 踢出OSD\nceph osd out\n# 删除OSD\nceph osd rm \n# 修复PG\nceph pg repair\n# 替换CRUSH\nceph osd setcrushmap\n# 删除MON\nceph mon rm\n...\n# 还有很多操作，就不一一赘述\n```\n\n**结论：**\n\n> `Mon`的`r`权限可以读取集群各个组件的状态，但是不能修改状态，而`w`权限是可以做到的\n\n**注意：**\n\n> 这里的`w`权限能做到的写权限也只是修改组件的状态，但是并不包括对集群对象的读写权限，因为这些组件状态信息是存在`Mon`，而对象信息是存在`OSD`里面的，而这里的`w`权限也只是`Mon`的写权限，所以也很好理解了。\n\n### x 权限\n\n`MON`的`x`权限很局限，因为这个权限仅仅和`auth`相关，比如`ceph auth list`，`ceph auth get` 之类的指令，和`w`权限类似，`x`权限也需要`r`权限组合在一起才能有效力：\n\n```\n# 用上面创建拥有rw权限的用户访问auth list后auth报错\n[root@node3 ~]# ceph --name client.mon_rw --keyring /root/key auth list\n2017-11-28 21:28:10.620537 7f0d15967700  0 librados: client.mon_rw authentication error (22) Invalid argument\nInvalidArgumentError does not take keyword arguments\n# 创建rw权限的用户访问auth list成功\n[root@node3 ~]# ceph --name client.mon_rx --keyring /root/key auth list\ninstalled auth entries:\n\nosd.0\n\tkey: AQDaTgBav2MgDBAALE1GEEfbQN73xh8V7ISvFA==\n\tcaps: [mgr] allow profile osd\n\tcaps: [mon] allow profile osd\n\tcaps: [osd] allow *\n...\n...\n```\n\n这边需要注意的是徐小胖的原文应该是笔误，他是用的`client.mon.rw`访问的，所以说实践可以发现很多光看发现不了的东西\n\n**结论：**\n\n> `x`权限也需要和`r`权限搭配才有效果，该权限只能处理与`auth`相关的操作\n\n### * 权限\n\n这没什么好说的，猜也能猜到了，就是拥有`rwx`所有权限\n\n## OSD Caps\n\n这一章需要研究一波再发出来\n\n## 丢失所有秘钥的再恢复\n\n如果所有秘钥全部删除，是否真的能恢复？所有秘钥包括\n\n* `MON` ： `/var/lib/ceph/mon/ceph-$hostname/keyring`\n* `OSD` ： `/var/lib/ceph/osd/ceph-$hostname/keyring`\n* `Client` ：`/etc/ceph/ceph.client.admin.keyring`\n\n```\n# 删除 mon keyring\n[root@node1 ceph-node1]# mv keyring /root/\n# 删除 ceph.conf\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.conf /root/\n# 删除 client.admin.keyring\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.client.admin.keyring /root\n# 尝试访问集群报错\n[root@node1 ceph-node1]# ceph -s\n2017-11-29 23:57:14.195467 7f25dc4cc700 -1 Errors while parsing config file!\n2017-11-29 23:57:14.195571 7f25dc4cc700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195579 7f25dc4cc700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195580 7f25dc4cc700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n# 尝试获取auth list报错\n[root@node1 ceph-node1]# ceph auth list\n2017-11-29 23:57:27.037435 7f162c5a7700 -1 Errors while parsing config file!\n2017-11-29 23:57:27.037450 7f162c5a7700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037452 7f162c5a7700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037453 7f162c5a7700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n```\n\nok，下面开始修复：\n\n### 伪造 Mon keyring\n\n在`ceph`中除了`mon.`用户以外的的账户密码都保存在`Mon`的数据库`leveldb`中，但是`mon.` 用户的信息并没有保存在数据库里，而是在`MON`启动时读取`Mon`目录下的`keyring` 文件得到的，这也是我们之前验证后得到的结论。所以，我们可以随便伪造一个`keyring`，放到`Mon` 目录下去。然后同步到各个`Mon`节点，然后重启三个`Mon`。\n\n```\n[root@node1 ceph-node1]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  kv_backend  store.db  systemd\n[root@node1 ceph-node1]# vim keyring\n# 伪造 keyring，可以看到里面还有tony的字样，可以看出明显是伪造的\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCtonyZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n# 重启 mon\n[root@node1 ceph-node1]# service ceph-mon@node1 restart\nRedirecting to /bin/systemctl restart  ceph-mon@node1.service\n```\n\n可以看到效果：\n\n```\n# monitor log显示mon.node1@0初始化成功，并被选举成了monitor leader\n2017-11-30 00:15:04.042157 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.042299 7f8c4e28a700  1 mon.node1@0(electing).elector(934) init, last seen epoch 934\n2017-11-30 00:15:04.048498 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.048605 7f8c4e28a700  1 mon.node1@0(electing).elector(937) init, last seen epoch 937, mid-election, bumping\n2017-11-30 00:15:04.078454 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1@0 won leader election with quorum 0,1,2\n```\n\n**注意（很重要）：**\n\n> 虽然说`mon`在启动的时候读取对应的`keyring`，不在乎内容的正确性，但是不代表这个`keyring`可以胡乱修改。也就是说这个`keyring`是要**符合某种规范和格式的**，在实践过程我发现`keyring`前三位必须为大写的`AQC`，当然还有其他的格式要求，比如结尾是否必须要是`==`？长度是否是固定的？这个格式要求可能很多，我没有时间一个一个手动无脑验证，这个可以日后查看源码了解实现思路，有兴趣的童鞋可以试试，说不定可以发现很有趣的现象。当然说了这么多是否意味着很难伪造呢？这个我们也不必担心，最好的做法是从别的集群的`Mon keyring`拷贝一份过来就可以了，自己胡乱伪造启动会报错如下：\n\n```\n2017-11-29 23:49:50.134137 7fcab3e23700 -1 cephx: cephx_build_service_ticket_blob failed with error invalid key\n2017-11-29 23:49:50.134140 7fcab3e23700  0 mon.node1@0(probing) e1 ms_get_authorizer failed to build service ticket\n2017-11-29 23:49:50.134393 7fcab3e23700  0 -- 192.168.1.58:6789/0 >> 192.168.1.61:6789/0 conn(0x7fcacd15d800 :-1 s=STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH pgs=0 cs=0 l=0).handle_connect_reply connect got BADAUTHORIZER\n```\n\n\n###　还原 ceph.conf\n\n没有`/etc/ceph/ceph.conf`这个文件，我们是没法执行`ceph`相关指令的，所以我们需要尽可能的还原它。首先`fsid`可以通过去任意`osd`目录（`/var/lib/ceph/osd/ceph-$num/`）读取`ceph-fsid`文件获得，然后`mon_initial_members`和`mon_host`代表着集群每个节点的`hostname`和`ip`，这些都是我们知道的。\n\n```\n# 还原 ceph.conf\n[root@node1 ceph-node1]# cat /var/lib/ceph/osd/ceph-0/ceph_fsid \n99480db2-f92f-481f-b958-c03c261918c6\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.conf\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.conf\n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\n\n# 通过 mon keyring 访问集群状态成功\n[root@node1 ceph-node1]# ceph -s --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n### 恢复 ceph.client.keyring\n\n有了`Mon keyring`，并且可以执行`ceph`指令，那么我们就可以通过`ceph auth get`去`Monitor leveldb`获取任意`keyring`\n\n```\n# 通过 Mon 获取 client.admin.keyring\n[root@node1 ceph-node1]# ceph --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring auth get client.admin\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 创建 /etc/ceph/ceph.client.admin.keyring，并将上面内容更新到该文件\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.client.admin.keyring\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.client.admin.keyring\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n\n# 用默认 ceph -s 测试一下，发现可以正常访问了\n\n[root@node1 ceph-node1]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n## 总结\n\n首先感谢徐小胖给我提供了`cephx`方面的思路，希望日后多出好文，我也在不断地拜读这些优质文章。这篇文章花了我很长时间，大家从日志的时间可以看出来，跨度已经有好几天了，很多实践真的不是一蹴而就的，需要反复的尝试和思考才能得到最后的成功。`Ceph`还是要多动手，看别人文章是好事，但是记得要加以实践，否则再好的文章也只是想当然，作者说什么你就跟着他的思路走，你永远不知道别人一句简短的话语和结论的背后花了多少时间去推敲和实践，你看起来一条命令执行成功或者在某一步执行某个命令那也许是别人失败了无数次总结出来的。所以我们要自己实践去验证，除了可以验证原文的观点正确与否，往往可以发现一些其他有用的知识。\n\n经历这次总结，收获满满，我对`cephx`的理解又上了一个层次。本文就`cephx`在不同组件中的角色扮演和依赖关系进行梳理，然后再对各组件的`cap`进行了研究，最后针对各个`keyring`的恢复给出了详细的指南和步骤。然后还剩两项任务没有完成，等有空进行完善！\n\n[1]: http://www.xuxiaopang.com/2017/08/23/easy-ceph-CephX/\n[2]: http://ow0mgad6r.bkt.clouddn.com/cephx-600x450.jpg\n","tags":["Cephx"],"categories":["tech"]},{"title":"阅读感悟：《Ceph Cookbook》","url":"/2017/11/26/read-ceph-cookbook/","content":"\n<center>![ceph cookbook][1]</center>\n\n阅读周期：2017/10/15 ～ 2017/11/26\n阅读人群：拥有`ceph`基础知识想进阶的同学\n阅读评分：4.8\n\n这本书和之前一本《ceph分布式存储学习指南》是同一个作者：卡兰.辛格。是`ceph`界最先出版的学习书籍了，对他表示崇高的敬意和感谢。这本书绝不是上一本书的复制品，我觉得我的运气很好，先阅读了上一本然后才看的这一本，没错，在我看来这本书是上一本的进阶版，内容更加饱满和细致。很多方面还是讲解的比较全面的，而不是笼统的说那些道理，基本上每个章节都可以让读者进行实战代码演练。这本书让我对`cookbook`系列的书籍有着很好的印象。\n\n<!--more-->\n\n## 第1章 Ceph介绍和其他\n\n第1章没什么好说的，就是传统的介绍和与其他存储做比较。然后第一章还把`ceph`的部署也插入了进来，估计是不想浪费过多的章节讲解这么基础的 东西吧，嗯，很棒。\n\n## 第2章 使用Ceph块存储\n\n哇，我只想说讲的太全面了，第一本讲解的方式永远是三个存储方式放在一起介绍一下，而这本书是每一个存储方式分了一章。从讲解块设备的创建、映射、调整、快照、克隆到和`openstack`的集成，面面俱到，为了阐述`rbd`是`openstack`的最佳匹配，分别就`openstack`的`glance`、`cinder`和`nova`讲解了详细的对接挂载方式，过程是可以自己实操的。\n\n## 第3章 使用Ceph对象存储\n\n这一章除了讲解老一套的`rados`网关配置与`S3`和`Swift`对接之外，还讲了`rados`网关和`openstack keystone`的集成。最大的特色是讲解了`radosgw`多区域网关配置和测试，无限逼近线上大规模生产环境的场景。最后就`RGW`创建文件同步和共享服务进行了介绍，干货多多。\n\n## 第4章 使用Ceph文件系统\n\n借着`cephfs`介绍了`mds`一番，然后讲解了多种访问`cephfs`的方式，然后就是熟悉的将`cephfs`导出为`nfs`，这算是很常见的场景了，但是据我所知目前`ceph`版本的`cephfs`的性能堪忧，以至于用的比较多的还是`rbd`，希望`ceph`可以在之后的版本中将`cephfs`做的越开越好。最后介绍了`ceph-dokan`的`cephfs`的`windows`客户端，很牛逼，还在持续迭代中。还介绍了如何使得`cephfs`替换`hdfs`，这个我之前在`ceph`线下沙龙南京站听那位中兴的工程师讲的`cephfs`和`hadoop`的恋爱史印象非常深刻，讲的蛮好的。\n\n## 第5章 用Calamari监控Ceph集群\n\n先科普了一下各大监控命令和手段，然后介绍了`Calamari`这个工具的编译和搭建，这个工具主要就是一个`dashboard`，并不是那种能在`UI`上操作和管理集群的工具，并且已经很久没再更新了，用于生产环境的请注意，自己玩玩倒是还可以。\n\n## 第6章 操作和管理Ceph集群\n\n先是介绍了两种方式管理各种守护进程和服务。然后讲解了横向扩展和纵向扩展。最后介绍了集群的升级。\n\n## 第7章 深入Ceph\n\n这一章蛮重要的，首先讲解了`crush`的机制和算法，然后介绍了`ceph`的身份验证，也就是`cephx`，最后结合`crush`和`pg`创建了定制化的`osd`和`pool`。\n\n## 第8章 Ceph生产计划和性能优化\n\n这本书在讲解这一部分明显要比上本书要全面的多，分别针对操作系统、`osd`、`filestore`、`cilent`等性能优化给出参数配置建议。然后介绍了纠删码的各种插件，配了结构图很好理解，最后还是缓存分层，做了测试，更能清楚了了解分层的原理。\n\n## 第9章 Ceph虚拟存储管理器（VSM）\n\n这个工具就要比上面提到的`calamari`之类的工具要强得多了，不仅有`dashboard`可以监控集群，还可以操作和管理集群，还有牛逼的是升级`ceph`方面，你也只需要修改配置要升级版本的网址，便可以实现自动升级，这是`intel`开发并提供开源的一款产品，值得去尝试一番。\n\n## 第10章\n\n同样的基准测试，不一样的味道。这一章提供了`n`多种的方式。有`dd`，有`rados bench`，有`rados load-gen`，`rdb bench-write`和`fio`等等。从测试网络，池，块设备和模拟负载应有尽有，关键这些都是`ceph`自带的除了`fio`。然后介绍了`ceph daemon`和`ceph tell`两个高校操作手段，最后介绍了使用`Ansible`部署`Ceph`，它可以通过配置文件配置集群信息而搭建集群，也就是说不需要手动的一个一个创建了，这样就显得很高效了。\n\n## 总结\n\n`ceph`已经看完两本书了，按理来说不应该再是那种一无所知的小白了，我觉得我缺少的还是大量的实践，所以之后我会对着这两本书照着例子再把代码敲一遍加深印象，然后对`ceph`应该会有另一番理解吧。接下来的进阶路线应该是中兴出版的《ceph原理和源码分析》了把，这本今年才出版的书我已经入手了，看了第一章，总体觉得还是蛮深入的，大量的源码，需要一定的知识铺垫，正好之后我也有看`ceph`源码和提供贡献的想法，所以正好那这本书过渡一下，加油！\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/cookbook-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Ceph分布式存储学习指南》","url":"/2017/11/26/read-ceph-guide/","content":"\n<center>![ceph guide][1]</center>\n\n阅读周期：2017/9/5 ～ 2017/10/7\n阅读人群：入门ceph的研发或者运维\n阅读评分：4.7\n\n\n这本书不到两百页，但是也讲解了`ceph`的方方面面。对于入门`ceph`来说，看官网文档是必要的，但是很多时候官方文档太基础，太零散了，而这样一本书籍就可以弥补这样的缺陷，它把所有组件进行总结和概括，并且进行了实战例子的演示。\n\n<!--more-->\n\n## 目录结构\n\n* 第1章 Ceph存储介绍\n* 第2章 Ceph实战部署\n* 第3章 Ceph架构和组件\n* 第4章 Ceph内部构件\n* 第5章 Ceph部署\n* 第6章 Ceph存储配置\n* 第7章 Ceph操作及管理\n* 第8章 监控Ceph集群\n* 第9章 Ceph与Openstack的集成\n* 第10章 Ceph性能调优和基准测试\n\n## 具体分析\n\n### 第1章\n\n第1章主要介绍了一下`ceph`的特点和历史发展，通过与`raid`，其他存储相比较得出自身的优势和特色，然后着眼于未来的一些设计和解决方案。最后针对`ceph`三大存储接口：块存储、文件系统存储和对象存储进行了介绍。\n\n### 第2章\n\n这一章其实就是利用`ceph-deploy`手动搭建集群的一个演示，后续章节还会有相应的补充\n\n### 第3章\n\n第3章重点讲解了`ceph`的几大组件，`rados`存储网关，`osd` 讲的比较多，也是应该的，毕竟是存储的核心，首先对比了目前主流三个文件系统的优劣，然后对日志进行了科普，顺带讲解了一下不推荐在`ceph`集群中采用`raid`的原因。之后就是一些`osd`，`monitor`和`mds`的基础命令了。\n\n### 第4章\n\n如果说上一章主要讲的是`osd`，`monitor`，`mds`的话，那么这一章很符合标题内部构件，对应讲解的是`pg`，`crush`，对象和`pool`。`pg`是对象的载体，`pg`的状态往往决定着集群的状态，徐小胖有一篇文章叫大话`pg`讲的挺简单明了的，其实写一个数据然后查看一下存储目录结构就知道七七八八了。`ceph`池是存储对象的逻辑分区，用户可以根据自己的需求定制池的类型和规则。比如副本池还是`EC`池，也可以为`pool`定制`crush rule`。\n\n### 第5章\n\n这一章也是部署，跟第2章不同的是这一章不用`ceph-deploy`工具而采用了手工部署的方式。这样做可能要做的事情多一点，繁琐一些，但是会更灵活一些，因为`ceph-deploy`这种自动化的工具往往存在局限性，比较适用于测试环境，并不适用于生产环境。往往生产环境需要做很多的定制化的改变。最后还讲解了一些如何进行`ceph`升级。\n\n### 第6章\n\n这一章干货满满，分别就块设备，文件系统和对象存储进行了讲解，还是挺全面的，建议一边看书一边实践，这些命令还是要多用，否则很快也会忘记。目前`rbd`还是`ceph`最稳定的存储方式，相比`cephfs`而言他的性能、稳定和快照复制等功能都做的很全面了。`cephfs`虽然越来越被更多的应用内嵌，但是还是不能应用与生产环境之上。最后介绍了一下`rados`网关配置分别和`s3`，`swift`对接的流程。\n\n### 第7章\n\n这一章主要介绍了如何运行和管理集群里面的守护进程和相关服务。然后就集群扩容和所容等扩展操作进行了讲解。最后重点讲解了如何管理`crushmap`，通过配置确定集群存储的结构，定制`pool`的规则等等。\n\n### 第8章\n\n这一章对应标题讲解了一系列的监控命令，有`osd`，有`pg`，还有`mon`等等。最后引入了一些开源的监控软件，比如`kraken`，`ceph-dash`和`Calamari`，讲的比较简洁，想要具体了解还需要自己动手去搭建，不过貌似有一些已经长时间不维护了，建议动手前看一下。\n\n### 第9章\n\n这一章主要讲`ceph`和`openstack`的完美集成，不过讲的也很简洁，其实就是了解歌大概情况，我觉得这对刚入门的小白来说挺好的，讲得太细也没什么用，反而云里雾里的，其实这本书对我最大的帮助就是了解了存储和`ceph`大概是个什么情况，然后知道了一些命令的用法和应用场景。第一遍熟悉一下就可以了，然后之后可以在深入，否则第一次接触就搞`openstack`这种高级玩意估计吃不消呢。\n\n### 第10章\n\n最后一章先分别对软件和硬件的性能调优提出了建议，这个东西我觉得还是得到生产环境中不断尝试，虚拟机的话没有任何参考建议。然后集群优化分别介绍了`osd`，`filestore`和客户端的相关参数，这个我们可以熟悉一下，自己没事动手改改，看看效果。然后针对`ec`和缓存分层进行了一些介绍，也是局限于基本够用的层面上。最后介绍了`rados bench`这个基准测试工具。\n\n## 总结\n\n这本书专为入门而生，就是在官网文档的基础之上进行规整和总结，如果看不下去官网文档的可以尝试这个，书很薄，很快就可以看完了，看完大概就对`ceph`没那么陌生了，还是一个比较不错的入门书籍。如果已经了解`ceph`一段时间的朋友我就不是很推荐了。\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/book-rocket-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Python编程快速上手---让繁琐工作自动化》","url":"/2017/11/26/read-python-automation/","content":"\n<center>![python automation][1]</center>\n\n> 阅读感悟这个系列是针对每次阅读一本书后，先对全本书内容进行整体的总结和评价，然后针对每一章节进行具体分析，分享一下自己从中的收获，这样不但可以在看完整本书后对全书有个完整的加深回顾，还可以对没有阅读过本书的人提供经验和建议或者跟同样阅读过的伙伴相互交流，交换感悟和经验。\n\n- 阅读周期：2017/7/15 ～ 2017/10/7\n- 阅读人群：无语言基础的小白，有其他语言基础让入门`python`的，熟练`python`想深入学习的请勿下手\n- 阅读评分：4.7\n\n<!--more-->\n\n大家都知道其实一本书可能并不是很贵，一般都是几十元，就算是本烂书对大家的经济损失来说其实并不是很严重，最关键的就是时间了，在这个压力山大和节奏飞快的时代，时间无疑是最宝贵的，所以很多时候我买书之前都会很仔细的看下评论，我个人是比较喜欢买书的，一般会选择在亚马逊上面购买，对比京东而言的话我觉得他的优惠策略更加直接，不需要抢什么券什么的，还有他的物流非常快，最多隔天就到了，包装也很精致，很少出现盗版书这种情况，最后也是最重要的是相比京东而言亚马逊书籍评论更加贴切一些，往往京东一本书下面会有上千条评论，你很少会看到负面的评价，而亚马逊就不一样了，一般评论的都是骨灰级读者了，一旦评论都是很认真并且很全面的，所以也很难得在亚马逊上面看到全五星的书籍。\n\n这本书是我入门`python`的第一本书，在此之前我有`C`，`Java`和`PHP`的语言经验。无论是在阅读这本书的过程中还是看完了之后，我都有一种庆幸感，那就是我当初选择这本书真的没错。在阅读前三章的时候，我一度觉得这本书太浅了，这主要可能因为我有其他语言基础，而这本书面对的读者不仅仅有语言基础的，而他这么做对那种小白来说就非常`nice`了。\n\n这本书给我最深的印象可以概括为四个字：深入浅出。就是在讲的很细致的同时，同时也会配有一些深入的内容。全本书的最大特色莫过于插图和`demo`了。一般的书一般是以代码围绕着讲解，而这本书的话往往是通过一些小项目或者小例子讲解一些概念和用法。第一部分主要讲解了语言的基础概念和`python`重要的数据结构：列表和字典。第二部分主要讲解了自动化知识，就比较深入了，具体比如正则表达式、对文件的操作、调试技巧、获取`web`信息、处理`excel`文件、处理`pdf`和`word`文档、处理`CSV`文件和`JSON`数据、计划任务、邮件和短信、操作图像和操作鼠标和键盘。掌握了列表、字典和字符串这些数据结构就可以进行基础的`python`编程了，然后学习了正则表达式和文件操作后，就可以很轻松了写一些自动化脚本了。从`web`抓取信息这一章是专门为喜爱爬虫的小伙伴准备的，提供了很多有用的工具和手段。之后还有对各种类型文件的操作，操作图像这一章我也很喜欢，准备之后做一个小项目，因为我个人对好看的图片很有兴趣。\n\n## 目录结构\n* 第一部分 Python编程基础\n    * 第1章 python基础\n    * 第2章 控制流\n    * 第3章 函数\n    * 第4章 列表\n    * 第5章 字典和结构化数据\n    * 第6章 字符串操作\n* 第二部分\n    * 第7章 模式匹配与正则表达式\n    * 第8章 读写文件\n    * 第9章 组织文件\n    * 第10章 调试\n    * 第11章 从Web抓取信息\n    * 第12章 处理Excel电子表格\n    * 第13章 处理PDF和Word文档\n    * 第14章 处理CSV文件和JSON数据\n    * 第15章 保持时间、计划任务和启动程序\n    * 第16章 发送电子邮件和短信\n    * 第17章 操作图像\n    * 第18章 用GUI自动化控制键盘和鼠标\n\n\n## 章节分析\n\n### 第1章、第2章和第3章\n\n这3章对我来说没什么好说的，就是一些语言基础，比如字符串拼接啊，控制流啊，函数什么的，对没有语言基础的小伙伴来说非常的贴心和容易理解，老司机的话可以快速阅读或者略过。\n\n### 第4章\n\n列表是`python`比较有特色的数据结构了，操作起来非常的方便。这一章分别阐述了获取列表、操作列表（merge、copy、delete）、列表循环和多重赋值技巧。然后就是一些深入的操作技巧了，比如排序、插入和删除。最后引入了字符串和元组和列表加以比较，讲解了可变类型、转换类型、传递引用和深拷贝等，总的来说还是很实用。\n\n### 第5章、第6章\n\n首先围绕字典讲解了一些基本的`API`，然后通过很好的棋盘建模模拟了字典的数据结构，很形象。字符串的话也差不多一样。\n\n### 第7章\n\n相比其他语言的正则来说，我觉得`python`的正则表达式无论是理解还是使用上面还算是比较简单的。先讲解了单个匹配和全局匹配，然后针对邮件，电话等常见场景给出了例子。\n\n\n### 第8章\n\n先讲解了`os`模块很多有用的`API`，大多数都是跟文件和目录有关，然后讲解了文件的读写操作，其中提到了`shelve`模块，这个在需要确保文件保密的场景非常有用。最后通过一个小例子讲解了`pyperclip`模块如何进行剪切板操作。\n\n### 第9章\n\n讲解了通过`shutil`模块复制文件和文件夹、移动、重命名、永久删除和安全删除。然后讲解了围绕`zipfile`模块相应的操作。\n\n### 第10章\n\n断言给我的印象比较深，还有`python`自带的`IDLE`的调试器的使用，我本人平时基本上是在`linux`下`vim`工作，所以这个工具平时也用不到，以后可以在`windows`下尝试一下。\n\n### 第11章\n\n这一章学习到了很多通过`python`取获取`web`信息的手段，这会在爬虫的时候很受用。比如通过`webbrowser`模块启动浏览器；通过`request`模块下载网页；通过`BeautifulSoup`模块解析`html`，这个模块非常强，简直和`js`操作`html`相媲美。\n\n### 第12章、第13章、第14章\n\n这三章分别针对`excel`、`pdf`、`word`、`csv`和`json`类型的文件或数据格式进行操作。对`excel`的支持还蛮强的，但是`pdf`和`word`的话就或多或少可能有问题，这个就不赘述了，用到的时候再查就好了。\n\n### 第15章、第16章、第18章\n\n这几章没什么好说的\n第15章主要讲解了保持时间、计划任务和启动程序等功能\n第16章主要讲解了消息通知，包括了邮件和短信接口\n第18章主要讲解了如何通过`python`控制键盘和鼠标，其中比较有意思的就是控制键盘原生不存在的一些特殊字符\n\n\n### 第17章\n\n`python`操作图像还是蛮好用的，本章主要讲解了对图片的裁剪，旋转，改变像素等，后续我会做一个小项目，通过`python`对图片进行一些操作，顺便深入了解一下这部分的知识。\n\n\n## 总结\n\n第一篇阅读总结感觉写的并不是很好，篇幅虽然还算长，但是感觉内容并不是很饱满，很多地方略显啰嗦。想了一下，之后可以把每一章的具体分析放在平时，每阅读完一章节就可以进行总结了，然后读完全本书就进行一个整体总结就可以了，这样每一章的总结就更加细致了，就不用读完全本书的时候再去回忆，这样效率很低。最后我会在之后有空的情况下分别做一个`python`的爬虫项目和一个操作图像的项目。\n\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/python-automation-600x450.jpg\n","tags":["Read"],"categories":["read"]},{"title":"博客主题 shadow","url":"/2017/11/19/hexo-theme-shadow/","content":"\n<center>![blog theme shadow][4]</center>\n\n使用了[black-blue][1]主题一段时间后，有些地方不符合我个人的审美和习惯，然后自己偶尔改了一些，久而久之还改了不少东西，为了方便存档和以后持续更新，就打造一个自己的专属主题：[hexo-theme-shadow][2]，大家可以去[我的博客][3]看下实际效果\n\n如果大家有任何批评和建议，随时可以在`github`上面提出`PR`，要是觉得主题还不错的话，欢迎留下你的`star`哦 ^_^\n\n<!--more-->\n\n具体如何配置主题和搭建博客这些基础通用的东西我就不讲了，这边我主要罗列一下我已经更新和以后可能要做的功能点\n\n## 已经更新的功能\n\n* 添加左侧栏`segmentfault`小图标\n* 更新`requirejs`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`jquery`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`baidu`分享源到本地，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`markdown code`的风格，具体参见博客\n    * 更改了全局的代码颜色样式\n    * 原来项目编号中的代码样式是白色的，很丑陋，也进行了相应的更新\n* 将头像从懒加载改为了直接加载，懒加载头像需要很长时间，用户体验很差\n\n## 待更新的功能\n\n* 网站背景音乐\n* 代码块样式修改，每个代码块上面有个蓝色的条条很不好看\n* 三级、四级和五级标题样式\n* 链接样式\n* 评论\n* 赞和打赏\n\n如果大家有什么建议或者问题，可以在`github`上面提`issue`或者`PR`，也可以通过邮箱联系到我\n\n`github`地址： `https://github.com/tony-yin/hexo-theme-shadow`\n邮箱地址： `1241484989@qq.com`\n\n[1]: http://geeksblog.cc/\n[2]: https://github.com/tony-yin/hexo-theme-shadow\n[3]: http://www.tony-yin.top/\n[4]: http://ow0mgad6r.bkt.clouddn.com/do_blog_600x450.png\n","tags":["博客"],"categories":["tech"]},{"title":"LVM动态扩展","url":"/2017/11/14/LVM-Space-Expansion/","content":"\n<center>![LVM Space Expansion][1]</center>\n\n在平时的开发工作中，经常会创建磁盘不足够大的虚拟机，然后往集群里面写一些数据导致磁盘满了。手动编辑虚拟机的磁盘大小是不会文件系统识别的，大多数同学只能无奈的重新装`OS`，这里我介绍一种基于`LVM`实现动态的方式。\n\n<!--more-->\n\n## LVM了解\n\n`LVM`是逻辑盘卷管理（`LogicalVolumeManager`）的简称，它是`Linux`环境下对磁盘分区进行管理的一种机制，`LVM`是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。通过LVM系统管理员可以轻松管理磁盘分区，如：将若干个磁盘分区连接为一个整块的卷组（`volumegroup`），形成一个存储池。管理员可以在卷组上随意创建逻辑卷组（`logicalvolumes`），并进一步在逻辑卷组上创建文件系统。管理员通过`LVM`可以方便的调整存储卷组的大小，并且可以对磁盘存储按照组的方式进行命名、管理和分配。\n\n\n## 查看分区\n\n当前默认只有一个采用`lvm`的分区，一开始`sda`磁盘容量为`16G`，后来发现不够用了，编辑磁盘大小为`50G`，但是可以发现这`50G`并没有起到扩展分区容量的效果。\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       14G  3.4G  9.6G  26% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n└─sda2                           8:2    0 15.5G  0 part \n  ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n  └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n```\n\n## 分区\n\n可以通过新增一块其他磁盘来扩容，我这边采取的是增大当前磁盘的容量实现扩容。\n\n有时候因为系统设备处于繁忙状态，所以分区需要重启后才会生效。\n\n```\n[root@tony-play ~]# fdisk /dev/sda\nWARNING: DOS-compatible mode is deprecated. It's strongly recommended to\n         switch off the mode (command 'c') and change display units to\n         sectors (command 'u').\nCommand (m for help): n\nCommand action\n    e   extended\n    p   primary partition (1-4)\np\nPartition number (1-4): 3\nFirst cylinder (2089-6527, default 2089):   // 直接回车，用默认值就可以了\nUsing default value 2089\nLast cylinder, +cylinders or +size{K,M,G} (2089-6527, default 6527):    // 直接回车，用默认值就可以了\nUsing default value 6527\nCommand (m for help): w\nThe partition table has been altered!\n\n# 可以看到新建的分区sda3已结被创建出来了，采取默认值会将剩余所有空间都分到分区中\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n├─sda2                           8:2    0 15.5G  0 part \n│ ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n│ └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n└─sda3                           8:3    0   34G  0 part \n```\n\n## 查看当前文件系统\n\n当前文件系统为`ext4`\n\n```\n[root@tony-play ~]# mount\n/dev/mapper/vg_tonyplay-lv_root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext4 (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n```\n\n## 为新分区创建文件系统\n\n```\n[root@tony-play ~]# sudo mkfs.ext4 /dev/sda3\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n2228224 inodes, 8912727 blocks\n445636 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=4294967296\n272 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks: \n    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n    4096000, 7962624\n\nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 39 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n\n## 查看卷组信息\n\n```\n[root@tony-play ~]# vgdisplay\n  --- Volume group ---\n  VG Name               vg_tonyplay     // 卷组名在下面扩展中会用到\n  System ID             \n  Format                lvm2\n  Metadata Areas        1\n  Metadata Sequence No  3\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                2\n  Open LV               2\n  Max PV                0\n  Cur PV                1\n  Act PV                1\n  VG Size               15.51 GiB\n  PE Size               4.00 MiB\n  Total PE              3970\n  Alloc PE / Size       3970 / 15.51 GiB\n  Free  PE / Size       0 / 0   \n  VG UUID               Y9usSM-nDU5-ZAUd-Y3Te-u5Pd-uFBr-gcYHf0\n```\n\n## 创建新物理卷\n\n```\n[root@tony-play ~]# pvcreate /dev/sda3\n  Physical volume \"/dev/sda3\" successfully created\n```\n\n## 扩展到卷组\n\n```\nvgextend vg_tonyplay /dev/sda3  // 卷组名在查看卷组信息中\n  Volume group \"vg_tonyplay\" successfully extended\n```\n\n## 查看逻辑分区\n\n`/dev/vg_tonyplay/lv_root`就是根分区，也是我们要扩展的分区。\n\n```\n[root@tony-play ~]# lvdisplay \n  --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_root   // 根分区\n    LV Name                lv_root\n    VG Name                vg_tonyplay\n    LV UUID                IPd7lm-Sx8g-pe7k-llNL-j1wc-mbA2-2cAdsy\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:58:53 -0400\n    LV Status              available\n    # open                 1\n    LV Size                13.91 GiB\n    Current LE             3561\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:0\n\n    --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_swap\n    LV Name                lv_swap\n    VG Name                vg_tonyplay\n    LV UUID                qX637q-iD6i-8blp-hmmS-MvLy-xZ0y-b4D0BF\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:59:07 -0400\n    LV Status              available\n    # open                 1\n    LV Size                1.60 GiB\n    Current LE             409\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:1\n```\n\n## 扩展容量到逻辑分区\n\n```\n[root@tony-play ~]# lvextend /dev/vg_tonyplay/lv_root /dev/sda3\n  Size of logical volume vg_tonyplay/lv_root changed from 13.91 GiB (3561 extents) to 47.91 GiB (12264 extents).\n  Logical volume lv_root successfully resized\n```\n\n## 刷新逻辑分区容量使扩展生效\n\n`ext4`用`resize2fs`，`xfs`用`xfs_growfs`\n\n```\n[root@tony-play ~]# resize2fs /dev/vg_tonyplay/lv_root\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vg_tonyplay/lv_root is mounted on /; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 3\nPerforming an on-line resize of /dev/vg_tonyplay/lv_root to 12558336 (4k) blocks.\nThe filesystem on /dev/vg_tonyplay/lv_root is now 12558336 blocks long.\n```\n\n## 查看逻辑分区容量\n\n可以发现`/dev/mapper/vg_tonyplay-lv_root`已经从开始的`14G`扩展到了`48G`。ok，这就说明大功告成了，再也不用通过重装系统这种蹩脚的方式扩容了\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       48G  3.4G   42G   8% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n```\n\n## 小结\n\n至此，`lvm`扩容工作的过程应该是比较清楚了，之后有机会的话我会再补充一下`LVM`的压缩、删除等操作过程。\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/work-for-love-600x450.png\n","tags":["LVM"],"categories":["tech"]},{"title":"Ceph 编译（Giant版本）","url":"/2017/11/14/Ceph-Compile/","content":"\n<center>![Ceph Compile][1]</center>\n\n如今入门`Ceph`的时候，大家一般用`ceph-deploy`工具比较多，这个工具的确很强大，很方便，对应的也就是很无脑。如果之后想深入`Ceph`或者想在生产环境中部署`Ceph`的话，就得熟悉`Ceph`源码编译了。这对我们熟悉`Ceph`的`feature`的变化，`component`的相互关系以及围绕`Ceph`各种定制化扩展都大有裨益。本文就笔者`Ceph`源码编译过程和遇到的问题作出分享，希望有人能够得益于此。\n\n<!--more-->\n\n## 环境声明\n\n* OS： Centos 6.8\n* Ceph： Giant\n\n## 源码下载\n\n### 指定 giant 分支\n\n```\n git clone -b giant git://github.com/ceph/ceph.git\n ```\n\n### 下载子模块\n\n```\ngit submodule update --init --recursive\n```\n\n这一步我始终更新不了，尝试了一些办法未果，所以就去直接手动下载了源码，这个问题以后有时间看下\n\n## 预检\n\n```\ncd ceph \n./autogen.sh \n./configure\n```\n\n### Autogen\n\n这一步会频繁的报错一些`m4`文件没有，这需要我们手动创建这些`m4`文件夹即可，\n\n一开始我在`ceph`根目录创建了`m4`文件夹，并生成了包括`acx_pthread.m4`在内的文件，但是还是报错：`acx_pthread.m4 not exist`，后来发现这时候已经**切换目录**了，不止一个地方需要`m4`文件夹，一共有这几个地方需要手动创建目录：`mkdir m4`\n\n```\n./src/rocksdb/m4\n./src/gtest/m4\n./src/erasure-code/jerasure/jerasure/m4\n./src/erasure-code/jerasure/gf-complete/m4\n./m4\n```\n\n如果还是报错一些文件不存在，并且通过上述方法不能自行初始化生成的话，可以从网上或者已经编译过的`ceph`环境拷贝过来\n\n```\n报错：umdefined macro\n```\n\n下载地址：https://www.gnu.org/software/autoconf-archive/ax_check_classpath.html#ax_check_classpath\n\n### Configure\n\n这一步是编译过程中可能出错的次数最多的，因为可能会因为你的环境缺少相应的包不断报错。不过数量虽多，解决起来还是比较容易的，就根据报错的缺包对应下载安装就好了，下面我先给出一个所有包的安装步骤，然后再针对每个报错环节给出具体的解决方案\n\n#### 总体解决方案\n\n```\n# 1. 通过yum安装所有可以安装的包\nyum install -y yasm libuuid-devel libblkid-devel libudev-devel cryptopp-devel fuse-devel libunwind-devel libedit-devel libatomic_ops-devel snappy-devel leveldb-devel libaio-devel xfsprogs-devel boost*\n\n# 2. 部分yum不能安装的可以通过rpm安装\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n#### 具体解决方案\n\n1.yasm\n\n报错：\n\n```\nyasm command not found\n```\n\n解决：\n\n```\nyum install yasm -y\n```\n\n2.libuuid\n\n报错：\n\n```\nconfigure: error: libuuid not found\n```\n\n解决：\n\n```\nyum install libuuid-devel -y\n```\n\n3.libblkid\n\n报错：\n\n```\nconfigure: error: blkid/blkid.h not found (libblkid-dev, libblkid-devel)\n```\n\n解决：\n\n```\nyum install libblkid-devel -y\n```\n\n4.libudev\n\n报错\n\n```\nconfigure: error: libudev.h not found (libudev-dev, libudev-devel)\n```\n\n解决：\n\n```\nyum install libudev-devel -y\n```\n\n5.crypto\n\n报错：\n\n```\n configure: error: no suitable crypto library found\n ```\n\n 解决：\n\n ```\n yum install cryptopp-devel -y\n ```\n\n 6.fuse\n\n 报错：\n\n ```\n configure: error: no FUSE found (use --without-fuse to disable)\n ```\n\n 解决：\n\n ```\n yum install fuse-devel -y\n ```\n\n 7.tcmalloc\n\n 报错：\n\n ```\n configure: error: no tcmalloc found (use --without-tcmalloc to disable)\n ```\n\n 解决：\n\n ```\n# 1. 需要先安装libunwind-devel，被gperftools依赖\n yum install libunwind-devel -y\n\n# 2. 安装tcmalloc（yum无法安装，需要通过rpm的方式）\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n8.libedit\n\n报错：\n\n```\nconfigure: error: No usable version of libedit found.\n```\n\n解决：\n\n```\nyum install  libedit-devel -y\n```\n\n9.libatomic-ops\n\n报错：\n\n```\nconfigure: error: no libatomic-ops found (use --without-libatomic-ops to disable)\n```\n\n解决：\n\n```\nyum install libatomic_ops-devel  -y\n```\n\n10.libsnappy\n\n报错：\n\n```\nconfigure: error: libsnappy not found\n```\n\n解决：\n\n```\nyum install snappy-devel -y\n\nor \n\nwget ftp://195.220.108.108/linux/centos/6.9/os/x86_64/Packages/snappy-devel-1.1.0-1.el6.x86_64.rpm\nrpm -ivh snappy-devel-1.1.0-1.el6.x86_64.rpm\n```\n\n11.libleveldb\n\n报错：\n\n```\nconfigure: error: libleveldb not found\n```\n\n解决：\n\n```\nyum install leveldb-devel\n```\n\n12.libaio\n\n报错：\n\n```\nconfigure: error: libaio not found\n```\n\n解决：\n\n```\nyum install libaio-devel -y\n```\n\n13.libxfs\n\n报错：\n\n```\nconfigure: error: xfs/xfs.h not found (--without-libxfs to disable)\n```\n\n解决：\n\n```\nyum -y install xfsprogs-devel\n```\n\n14.boost\n\n报错：\n\n```\nCan't find boost spirit headers\n```\n\n解决：\n\n```\nyum install boost* -y\n```\n\n## 编译安装\n\n机器配置不好的话，编译需要时间比较长。可以使用`make -j`增加并发度，`4`表示同时执行的`make`方法数。\n\n```\nmake -j4\nmake install（可选）\n```\n\n## 小结\n\n`ceph`手动源码编译遇到的问题还是蛮多的，如果不自己动手经历一下的话很多东西都不知道，当然这都是经验的积累，不断地锻炼自己解决问题的能力，要学会见招拆招，通过问题发现原理和本质。\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/ceph-600x450.png\n","tags":["编译"],"categories":["tech"]},{"title":"通过ganesha-nfs导出Ceph为NFS（Luminous）","url":"/2017/11/08/Ceph-NFS-Ganesha/","content":"\n<center>![ganesha][8]</center>\n\n自从`Jewel`版本，`nfs-ganesha`开始支持`ceph`，并且把对接点选择了`rados`。`Ganesha`支持两种方式将`Ceph`导出为`NFS`，一种通过`RGW`，一种通过`CephFS`，通过`FSAL`模块 连接到`RGW`或者`CephFS`， 其中，`FSAL_RGW`调用`librgw2`将`NFS`协议转义为`S3`协议再通过`RGW`存入到`Ceph`中，`FSAL_CEPH` 调用`libcephfs1`将`NFS`转义为`Cephfs`协议再存入到`Ceph` 中。所以需要额外安装这两个包。\n\n本文就`Luminous`版本的`ceph`基于`ganesha`导出`nfs`部署，并且测试一下`rgw`和`cephfs`的性能。[@徐小胖][1]已经就`jewel`版本的过程进行了大致的讲解，我这边主要分享一下我遇到他文章没提到的和`Luminous`场景导致的问题。\n\n<!--more-->\n\n## 参考链接：\n\n* [通过ganesha-nfs将 Ceph 导出为 NFS][1]\n* [RGW+Ganesha环境部署][2]\n\n## 环境声明\n\n* os： centos7\n* ceph： luminous\n* nfs-gnesha： **v2.5 stable**（important）\n\n## 安装依赖\n\n一些编译需要的公共库\n\n```\nyum install gcc git cmake autoconf libtool bison flex doxygen openssl-devel gcc-c++ krb5-libs krb5-devel libuuid-devel nfs-utils -y\n```\n\n`ubuntu`的我也试了一下，主要有以下几个包不同：\n\n* `gcc-c++`   ->   `g++`\n* `libuuid-devel` ->  `uuid-dev`\n* `nfs-utils`   ->  `nfs-kernel-server` \n\n如果要生成`FSAL_RGW`模块，需要安装`librgw2-devel`（我装的`librgw-devel`也可以`work`，看了`so`文件发现就是`2`，只是建立了映射关系，这个看了源码知道了是因为`K`版本对`librgw`编译这块做了升级，加了`2`标识一下）\n\n```\nyum install librgw2-devel -y \n```\n\n如果要生成`FSAL_CEPH`模块，需要安装`libcephfs1-devel`\n\n```\nyum install libcephfs1-devel -y\n```\n\n## 源码下载\n\n上面两篇文章一个下载的是`v2.3 stable`，一个是`v2.4 stable`，两个我都试过，都会在`make`到`80%`左右的时候报错，应该是源码版本和库的版本有冲突导致的，这个问题耽误我挺长时间的，后来猜想可能是版本问题，尝试了一下`v2.5 stable`的源码就可以了\n\n```\ngit clone -b V2.5-stable https://github.com/nfs-ganesha/nfs-ganesha.git --recursive\n```\n\n**注意：（重要）**\n\n最近我同事根据这篇文档部署`nfs-ganesha`的时候，发现之后`cmake`的操作后`USE_FSAL_RGW`始终是`OFF`的状态，一开始检查了一下环境发现`ganesha`是`v2.5`，然后`ceph`也是`L`版本，`centos`也是`7`。\n\n报错如下：\n\n```\nlocal RGW version is 1.1.4 not suitable match, but required latest RGW version is 1.1.6\n```\n\n很好理解，就是我们安装的`RGW`达不到`nfs-ganesha`的要求，首先我们尝试去安装新的版本，但是`yum`的源已经配置了`luminous`，并且网上搜索了一圈并没有找到更新的。\n\nok，那就第二步，那就找原因同样的环境为啥我可以成功？而这位同事却不行？莫非因为我帅？不能这么逆天吧^_^ （明明可以靠脸吃饭，偏偏靠才华，哎……）\n\n言归正传，通过报错信息查看源代码？定位到了报错文件`src/CMakeLists.txt`，然后代码中对`RGW`的版本要求也的确是`1.1.6`，回过头再看我之前下载下来的源代码，怪异的现象发生了，我代码中对`RGW`的版本要求只有`1.1.3`，此时我的第一反应就是应该这个文件被修改过导致两份代码不一样，出于本能直接上了`github`，果不其然，发现了`mattbenjamin`同学在`10`月`17`号在`v2.5`的分支上提交了一个`commit`针对`RGW`版本做出了修改！具体参看[commit详情][3]，而这位贡献者几乎同时也在`ceph`提交了相关代码。这样想想就说得通了，我在搭建`nfs-ganesha`的时候恰好是在他提交之前的，所以我本地`RGW`本地版本是`1.1.4`是比要求版本`1.1.3`要高的，虽然不是完全`match`，但是也基本符合了要求，而我这位同事脸就比较黑了。。。\n\n那该怎么解决这个问题呢？\n\n有两种解决方案：\n\n1. 手动编译`Luminous Ceph`把最新的`RGW`包编译出来，因为现在这个资源还没被公开分享出来；\n2. 克隆这个改动之前的代码\n\n这么看来第二种更为简单一些，我们也是采用的这种方式，`git clone v2.5 stable`后，然后`git checkout`恰当的版本号即可。\n\n**总结：**\n\n其实直接`clone`实时的代码这是不合理的，因为你不知道他什么时候会更新。正确的方式是我们应该找到稳定的`release`版本，`nfs-ganesha`也有提供[一系列的release][4]，而我们通过查看`2.5 tree`上面的`commit`信息，可以知道`RGW`的改动是介于`2.5.3`和`2.5.4`之间的，所以我们下载`2.5.3`较为合适，下载地址在[这里][5]。\n\n这边还需要注意一个问题，这边`release`包下载下来你会发现`libntrirpc`这个文件夹没有内容，出现这个问题是因为`libntrirpc`对应的也是一个`git`仓库，我们需要去[ntrirpc][6]手动克隆，然后通过`nfs-ganesha`的目录发现对`ntrirpc`的版本要求是`fadcbde`（这些都是上`github`可以看到的），我们需要做的就是在下载下来的`ntrirpc`目录下切到`fadcbde`这个`commit`上，并把当前的内容拷贝到`nfs-ganesha`的`libntrirpc`目录下。\n\n实在搞不定的童鞋，我提供一个我克隆的`nfs-ganesha`的代码包，你们可以对比一下，这个我是可以成功部署的\n\n附：我的`nfs-ganesha`[下载地址][7]\n\n## 编译\n\n编译`nfs-ganesha`， **注意打开对应的模块：**\n\n* 如果需要生成`FSAL_RGW`模块，则在编译选项中添加： `-DUSE_FSAL_RGW=ON`\n* 如果需要生成`FSAL_CEPH`模块，则在编译选项中添加： `-DUSE_FSAL_CEPH=ON`\n\n```\ncd src/\nmkdir build\ncd /build/\ncmake -DUSE_FSAL_RGW=ON -DUSE_FSAL_CEPH=ON ../\n```\n\n`cmake`的过程中会有以下输出：\n\n```\n-- Looking for ceph_ll_lookup_root in cephfs - found\n-- Found cephfs libraries: /usr/lib64/libcephfs.so\n-- Found CEPHFS: /usr/include  \n-- Looking for rgw_mount in rgw\n-- Looking for rgw_mount in rgw - found\n-- Found rgw libraries: /usr/lib64/librgw.so\n-- Found RGW: /usr (found suitable version \"1.1\", minimum required is \"1.1\") \n...\n-- USE_FSAL_CEPH = ON\n-- USE_FSAL_CEPH_MKNOD = OFF\n-- USE_FSAL_CEPH_SETLK = OFF\n-- USE_FSAL_CEPH_LL_LOOKUP_ROOT = ON\n-- USE_FSAL_RGW = ON\n```\n\n这一步，很重要，很多时候会因为没有装好的`librgw2-devel`或者`libcephfs1-devel`导致这边的`USE_FSAL_RGW`或者`USE_FSAL_CEPH`状态为`OFF`\n\n**确保，确保，确保：** `-- USE_FSAL_CEPH`为`ON`，以及`-- USE_FSAL_RGW`为`ON`。\n\n如果是`OFF`，请检查下`librgw2-devel`或者`libcephfs1-devel`是否有安装，如果这两个包都已经安装了，还显示为`OFF`， 可以尝试下清空编译目录：`rm -rf build/*`，再进行编译，如果依旧为 `OFF`，可以尝试下删除所有的`Ceph`包，再重新`yum install ceph librgw2-devel libcephfs1-devel -y`。\n\n编译和安装，在`build`目录下\n\n```\nmake\nmake install\n```\n\n> PS:\n> 在`make install`生成的输出中，可以看到:\n>\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/rgw.conf\n> ...\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/ceph.conf\n>\n> 这两个文件就是配置将`RGW`和`CephFS`配置为`ganesha-nfs`的配置模板。\n \n## 编辑配置文件\n\n注意`Path`后面的路径需要加引号，`rgw`替换成创建`S3`用户生成的用户信息\n\n```\ncat /etc/ganesha/ganesha.conf\nEXPORT\n{\n        Export_ID=1;\n        Path = \"/\";\n        Pseudo = /cephfs;\n        Access_Type = RW;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = CEPH;\n        }\n}\nEXPORT\n{\n        Export_ID=2;\n        Path = \"/\";\n        Pseudo = /rgw;\n        Access_Type = RW;\n        Squash = No_root_squash;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = RGW;\n                User_Id = \"admin\";\n                Access_Key_Id =\"1MWH3LWM1BS4ZF4HN5IH\";\n                Secret_Access_Key = \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\";\n        }\n}\nRGW {\n    ceph_conf = \"/etc/ceph/ceph.conf\";\n}\n```\n\n## 启动 Ganesha\n\n```\nganesha.nfsd -f /etc/ganesha/ganesha.conf -L /var/log/nfs-ganesha.log -N NIV_DEBUG\n```\n如果一切顺利，你应该可以看到`ganesha.nfsd` 进程在那，如果进程不在，那么查看`Log`，记得在启动进程前，关闭所有`CephX`配置。\n\n**重要**：`librgw init failed (-5)` 解决方法\n\n报错内容如下\n\n```\nRGW-1 : nfs-ganesha-2232083[main] create_export :FSAL :CRIT :RGW module: librgw init failed (-5)\nRGW-1 : nfs-ganesha-2232083[main] mdcache_fsal_create_export :FSAL :MAJ :Failed to call create_export on underlying FSAL\n\n```\n\n经过多次尝试，包括在`ganesha.conf`内添加`init_args`指定秘钥和`Ceph`的用户，`ganesha-nfs` 均无法启动，报的错如标题，解决方法就是关闭`CephX`，将`/etc/ceph/ceph.conf`内的三个 `cephx`改为`none`，然后重启`ceph-mon`，`ceph-osd`，`ceph-radosgw`，`ceph-mds` 进程，再启动`ganesha-nfs`，即可正常运行。\n\n**需要注意的是：**\n\n当你在当前节点上，关闭`cephx`后，你用`ceph -s`查看集群状态时，这时候会报错说明由于没有`auth`认证导致无法连接集群，所以我当时试了很久，尽管`nfs-ganesha`已经运行了，但是`mount`都没有成功`export`，查看`log`才发现原来在`init`配置的时候就失败了，导致这个问题的原因是我只关闭了一个节点的`cephx`，所以需要做的就是将集群内所有节点的`cephx`全部关闭，然后集群就可以正常访问了，并且`nfs-ganesha`也不会因为`cephx`导致加载配置出错\n\n## Check Export\n\n```\n[root@node1 build]# showmount -e  \nExport list for node1:  \n/ (everyone)  \n/ (everyone)  \n```\n\n## 挂载 NFS\n\n```\n[root@node1 mnt]# mount -t nfs4 192.168.1.1:/  /mnt/ceph/  \nroot@node1 mnt]# ls ceph/*  \nceph/cephfs:  \ntest  \n   \nceph/rgw:  \nmy-new-bucket \n```\n\n说明`CephFS`和`RGW`都已经正常对接。\n如果，你所使用的`admin`用户名下有很多的桶，那么这些桶都会以`/mnt/rgw/xxbucket`的结构显示出来，如果你在`/mnt/rgw/`下建立的一个目录，那么就相当于通过`RGW`建立了一个桶，所以，你执行`touch /mnt/rgw/123`是会报错的，因为不符合`S3`的对象必须位于桶内的规定，简单点说，就是把`/mnt/rgw/`和`S3`的根目录一一对应即可。\n\n同样，`CephFS`内的内容都会显示在`/mnt/cephfs/`目录下。可以开始愉快的玩耍了！！\n \n[1]: http://www.xuxiaopang.com/2017/03/27/ganesha-nfs-deploy/#more\n[2]: http://blog.csdn.net/younger_china/article/details/73432726\n[3]: https://github.com/nfs-ganesha/nfs-ganesha/commit/8d039cd139ea1dcfbe316b7e93e96efe755669ef\n[4]: https://github.com/nfs-ganesha/nfs-ganesha/releases\n[5]: https://github.com/nfs-ganesha/nfs-ganesha/releases/tag/V2.5.3\n[6]: https://github.com/nfs-ganesha/nfs-ganesha/tree/next/src\n[7]: http://ovv1r40we.bkt.clouddn.com/nfs-ganesha.tar.gz\n[8]: http://ow0mgad6r.bkt.clouddn.com/ganesha-600x450.png\n","tags":["Luminous"],"categories":["tech"]},{"title":"RGW 安装和创建","url":"/2017/11/08/Ceph-RGW/","content":"\n<center>![Ceph RGW][2]</center>\n\n本文通过`ceph-deploy`安装和创建`RGW`，然后分别创建`S3`和`Swift`接口并提供了相应的方案。\n\n`Ceph RGW`基于`librados`，是为应用提供`RESTful`类型的对象存储接口。`RGW`提供两种类型的接口：\n\n* S3：兼容`Amazon S3 RESTful API`\n* Swift：兼容`OpenStack Swift API`\n\t\n`S3`和`Swift API`共享同一个命名空间，所以可以使用两种`API`访问相同的数据。\n\n<!--more-->\n\n参考链接: [Ceph：创建RGW][1]\n## 部署 RGW\n\n### 进入 ceph 目录\n\n```\ncd /etc/ceph\n```\n\n### 安装 ceph object gateway\n\n我这边是`node1`，根据`hostname`对号入座就行\n\n```\nceph-deploy install --rgw node1 \n```\n\n### Gather keys\n\n```\nceph-deploy gatherkeys node1\n```\n\n### 创建 rgw 实例\n\n```\nceph-deploy rgw create node1\n```\n\n### Ceph CLI\n\n`Ceph CLI`工具需要在管理员模式下运行，因此需要执行以下命令\n\n```\nceph-deploy admin node1\n```\n\n### 测试是否安装成功\n\n一旦`RGW`开始运行，就可以通过端口`7480`（如果没有修改的话）来访问。如： \n`http://node1:7480`，如果`RGW`运行正常，它应该返回类似的信息： \n\n```\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\n<ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<Owner>\n<ID>anonymous</ID>\n<DisplayName/>\n</Owner>\n<Buckets/>\n</ListAllMyBucketsResult>\n```\n\n## 创建 S3 用户\n\n想正常的访问`RGW`，需要创建相应的`RGW`用户，并赋予相应的权限，`radosgw-admin`命令实现了这些功能。\n\n其中`keys`中的`user`，`access_key`和`secret_key`用于之后的`S3`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin user create --uid=\"admin\" --display-name=\"First user\"\n2017-11-08 16:51:39.883217 7fb6868fac40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 创建 Swift 用户\n\n`Swift`用户是作为子用户`subuser`被创建的，执行以下命令：\n\n其中`swift_keys`中的`user`和`secret_key`用于之后的`swift`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin subuser create --uid=admin --subuser=admin:swift --access=full\n2017-11-08 16:55:05.371174 7fb4cbfc2c40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [\n        {\n            \"id\": \"admin:swift\",\n            \"permissions\": \"full-control\"\n        }\n    ],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [\n        {\n            \"user\": \"admin:swift\",\n            \"secret_key\": \"PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P\"\n        }\n    ],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 测试 S3 接口\n\n需要创建一个`python`测试脚本来测试`S3`访问。该脚本会连接`RGW`，创建一个`bucket`并打印输出所有的`bucket`。其中，变量`access_key`和`secret_access`的值，来自于创建`S3`用户命令时，`radosgw-admin`命令返回的`keys->access_key`和`keys->secret_key`。\n\n### 安装 python-boto库\n\n执行以下步骤，首先安装`python-boto`库，该库用于连接`S3`：\n\n```\nyum install -y python-boto \n```\n\n### 创建脚本\n\n```\n#!/usr/bin/python  \n# -*- coding:utf-8 -*-  \n\nimport boto.s3.connection  \n   \naccess_key = '1MWH3LWM1BS4ZF4HN5IH'     \nsecret_key ='cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O'  \nconn = boto.connect_s3(  \n        aws_access_key_id=access_key,  \n        aws_secret_access_key=secret_key,  \n        host='{hostname}',port={port},  \n        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),  \n        )  \n   \nbucket = conn.create_bucket('my-new-bucket')  \nfor bucket in conn.get_all_buckets():  \n    print\"{name} {created}\".format(  \n            name=bucket.name,  \n            created=bucket.creation_date,  \n            )  \n   \n```\n\n需要将上面的`{hostname}`替换成对应的`hostname`或者`IP`,例如`192.168.1.1`；将`{port}`替换成`RGW`运行的端口，默认为`7480`；前者为字符串，后者为数字\n\n### 执行脚本测试\n\n```\n[root@node1 ceph]# python s3test.py \nmy-new-bucket 2017-11-01T10:58:53.670Z\n```\n\n## 测试 Swift 接口\n\n### 安装相关软件包\n\n```\npip installpython-setuptools    \npip installpython-swiftclient  \n```\n\n### 命令行访问\n\n替换`{ip}`，`{port}`和`{swift_secret_key}`，其中`{swift_secret_key}`为创建`Swift`用户时，`radosgw-admin`命令返回的`swift_keys->secret_key`的值\n\n```\nswift -A http://{ip}:{port}/auth/1.0 -Utestuser:swift -K '{swift_secret_key}' list \n```\n\n正常输出应该如下：\n\n```\n[root@node1 ceph]# swift -Ahttp://192.168.1.1:7480/auth/1.0 -U admin:swift -K 'PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P' list  \nmy-new-bucket \n```\n\n## 小结\n\n`ceph-deploy`命令，大大地简化了对`Ceph`集群和`RGW的`安装和配置，可以很快速的搭建测试环境，达到测试效果，但如果要在生产环境中应用的话，可能需要手动做一些配置就，不能完全依赖`ceph-deploy`了。\n\n[1]: http://blog.csdn.net/younger_china/article/details/73410918\n[2]: http://ow0mgad6r.bkt.clouddn.com/gateway-600x450.png\n","tags":["RGW"],"categories":["tech"]},{"title":"使用NFS挂载RBD","url":"/2017/10/31/RBD-Mount-NFS/","content":"\n<center>![RBD][3]</center>\n\n**具体场景**\n\n- 在`Server`端创建`RBD`块设备并挂载到某个目录，然后在`Client`端通过`NFS`将`Server`端之前说的目录再挂载到本地，就可以实现在客户端写文件通过`NFS`到`Server`端的块设备上\n- 这波操作的目的是因为测试`EC`性能时，`Ceph`版本还是`Hammer`，`CephFS`不是很稳定，性能不行，所以想用`RBD`的方式取代，但是`Hammer`版本中`EC`是只支持`RGW`的，并不支持`CephFS`和`RBD`，至于这个场景具体流程之后会在其他文章分享\n- 这波操作在之前`Ceph`的`Hammer`版本做过，这次是要在新版本`Luminous`上实践，由于遇到了很多问题，大部分都是新版本缘故引入的，故与大家分享\n\n> PS：`Luminous`新版本提供了新的`NFS`的处理方式，不再需要用这种老式的`RBD`的手段，之后也会在其他文章分享出来\n\n<!--more-->\n\n## Hammer版本\n\n操作步骤\n\n```\n1. 使用RBD方式导出NFS，步骤如下：\n\n\t1.1 create RBD block device image (size unit: MB)\n\t\t# rbd create --size 1024 ec-pool/test_image\n\t\t\n\t1.2 map RBD device\n\t\t# rbd map ec-pool/test_image\n\t\t\n\t\t[Optional] check mapped device\n\t\t# rbd showmapped\n\t\t\n\t1.3 create filesystem on the device\n\t\t# mkfs.ext4 /dev/rbd0\n\t\t\n\t1.4 mount it,\n\t\t# mkdir -p /vol/test_image\n\t\t# mount /dev/rbd0 /vol/test_image/\n\t\t\n\t1.5 export it, modify the /etc/exports\n\t\t# cat /etc/exports\n\t\t/vol/test_image *(rw,async,no_subtree_check,no_root_squash)\n\t\t# service nfs-kernel-server restart\n\t\t# exportfs -r\n\t\t\n\t1.6 check mount info for NFS server\n\t\t# showmount -e 192.168.1.167\n\t\n2. 在客户端以nfs方式mount上述导出的文件夹\n\t# mount -t nfs 192.168.1.167:/vol/test_image /mnt\n```\n\n## Luminous版本\n\n### Create RBD image\n\n之前`EC Pool`只能为`RGW`服务 ，直到`Luminous`版本，`EC Pool`也可以应用在`RBD`和`CephFS`，并且增加了`data-pool`这样的新特性，具体请参考官网：http://docs.ceph.com/docs/luminous/rados/operations/erasure-code/#erasure-coding-with-overwrites\n\n所以创建`image`的方式也有所改变\n\n#### Enable ec overwrites\n\n```\nceph osd pool set ec_pool allow_ec_overwrites true\n```\n\n#### Create pool and image\n\n由于`EC`不支持`omap`，所以在`CephFS`或者`RBD`场景使用`EC`时，需要将`data`存在`EC Pool`中，将`metadata`存在`Replicated Pool`中\n\n```\n// 创建pool\nceph osd pool create ec-pool 12 12 erasure  // ec pool, store data\nceph osd pool create md-pool 12 12 replicated   // replicated pool, store metadata\nceph osd pool create ssd-pool 12 12 replicated  // replicated pool, used as cache pool\n// 创建image\nrbd create --size 1024 --data-pool ec-pool md-pool/test_image\n```\n\n### Map RBD\n\n这一步之前的操作是`rbd map ec-pool/test_image`，所以当前需要执行\n\n```\n rbd map md-pool/test_image\n```\n\n但是一直`timeout`，通过查看`dmesg`和网上资料发现是因为`linux kernel`版本太低，所以升级了一下内核到`4.3`版本之后就可以`work`了\n\n升级内核版本请参考：\n> * [内核版本过低导致RBD Feature不支持][1]\n> * [RBD Feature][2]\n\n### Check map\n\n```\n rbd showmapped\n```\n\n### Create filesystem on RBD\n\n```\nmkfs.ext4 /dev/rbd0\n```\n\n### Mount\n\n```\n mkdir -p /vol/test_image\n mount /dev/rbd0 /vol/test_image/\n```\n\n### Modify export for nfs on server\n\n```\nvim /etc/exports\n/vol/test_image *(rw,async,no_subtree_check,no_root_squash) // Modify it to /etc/exports\n```\n\n#### Server\n\n##### Install NFS on server and client\n\n`Server`端执行一下操作\n\n##### Install\n\n```\nyum -y install nfs-utils rpcbind\n```\n\n##### Start service\n\n`nfs`依赖`rpcbind`，所以必须先启动`rpcbind`，这很重要\n\n```\nservice rpcbind start\nservice nfs start\nexportfs -r\n```\n\n#### Client\n\n客户端也执行以上操作，假设客户端`ip`为`192.168.1.1`，服务端为`192.168.1.2`\n\n##### Check mount\n\n查看是否可以挂载\n\n```\nshowmount -e 192.168.1.2\n```\n\n##### Mount server to client\n\n```\nmount -t nfs 192.168.1.2:/vol/test_image /mnt\n```\n\n## 总结\n\n遇到的主要问题\n\n* `EC Pool`创建`image`因为新版本改动\n* `rbd map`因为`linux kernel`版本低\n* `nfs`因为`nfs`在`rpcbind`前启动\n\n希望帮助大家尽量少些踩坑 ~~~\n\n作者： Tony\n日期： 2017-10-31 22:33\n\n[1]: http://www.jianshu.com/p/66a724a1f3af\n[2]: http://xiaqunfeng.cc/2017/06/06/ceph-rbd-map-failed/\n[3]: http://ovv1r40we.bkt.clouddn.com/rbd-london-christmas-600x450.jpg\n","tags":["RBD"],"categories":["tech"]},{"title":"Python Profiler","url":"/2017/10/10/Python-Profiler/","content":"\n<center>![Python Profiler][10]</center>\n\n`OSD`启用过程耗时较长，需要进行性能优化。期间通过`python profilers`对代码进行性能分析和数据统计，有坑，有收获，总而言之，这是一个不错的工具\n\n<!--more-->\n\n## Profilers简介\n\n`python profilers`内置的主要有三种`cprofile`, `profile`和`hotshot`,`cprofile`是基于`profile`之上做的扩展，性能要比后者好很多，所以我用的就是`cprofile`\n\n更详细的介绍可以查看官网，`python profilers`的好处在于不用看教程，只要看着官网简短的概述，就能掌握其使用方法\n\n* [Python Profile][1]\n* [关于Python Profilers性能分析器][2]\n\n\n## Cprofile快速使用\n\n### 官网例子\n\n#### 代码\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")')\n```\n\n#### 分析结果\n\n```\n197 function calls (192 primitive calls) in 0.002 seconds\nOrdered by: standard name\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1    0.000    0.000    0.001    0.001 <string>:1(<module>)\n     1    0.000    0.000    0.001    0.001 re.py:212(compile)\n     1    0.000    0.000    0.001    0.001 re.py:268(_compile)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)\n     4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)\n   3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)\n```\n\n#### 图解\n\n<center>![cprofile][3]</center>\n\n## Cprofile深入\n\n上面的基本用法可以在脚本中测试某个语句或者函数，然后打印到控制台。`cprofile`也可以将结果输出到文件中，这是比较常见的做法，因为打印到控制台，第一不能保存结果，第二如果数据量多没法全部浏览，并且影响阅读效果。而放在文件中还可以对结果进行按需排序、筛选等操作\n\n### 输出文件\n\n1. 参数方式\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")', 'restats')\n```\n\n2. CLI方式\n\n```shell\npython -m cProfile [-o output_file] [-s sort_order] myscript.py\n```\n\n## Cprofile优雅使用\n\n上述方式可以使得`cprofile`在一些测试环境中受用，但是在一些复杂的环境中不能很好的`work`。由于`cprofile`是根据`python`在每个事件中存放的`hook`进行性能分析，所以在`cprofile.run()`的时候，要保证他就是最上层，他是调用的源头。\n\n但是实际场景中，我们经常会对一些`API`中的某个方法进行性能分析，如果在被调用处使用`cprofile`，会出现变量或者模块`undefined`的现象，模块不能识别还可以在`run`方法中引入，然后通过分号分隔，例如`cprofile.run(import re, re.compile(\"foo|bar\"))`，具体可以参考这篇文章：[Python Profile 工具性能分析][5]\n\n变量无法识别更是让人头疼，所以为了达到测试效果，你会不得不修改一些并不是很少量的源代码，并且测一个方法就要搞一次很麻烦。还有一些多进程或者跨机器的场景导致代码异步执行，这样`cprofile`更不能达到用户的需求\n\n还好我们可以通过`python`装饰器的机制来做，这样既不用改动源代码，也可以很方便的切换函数分析\n\n### 装饰器接口\n\n这里要注意设置全局变量\n\n```shell\nexport PROFILING=y\n```\n\n接口定义：\n\n```python\nimport cProfile\nimport pstats\nimport os\n# 性能分析装饰器定义\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"tottime\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n### 分析使用\n\n这时候只需要在调用的函数上面加一个装饰器即可\n\n```python\n@do_cprofile('filename')\ndef run():\n    print 'hello world'\n```\n## pstats分析工具\n\n`pstats`可以根据`cprofile`生成的文件进行排序、筛选等处理，呈现更主要的结果\n\n```python\nimport pstats\n \n# 创建Stats对象\np = pstats.Stats(\"result.out\")\n \n# strip_dirs(): 去掉无关的路径信息\n# sort_stats(): 排序，支持的方式和上述的一致\n# print_stats(): 打印分析结果，可以指定打印前几行\n \n# 和直接运行cProfile.run(\"test()\")的结果是一样的\np.strip_dirs().sort_stats(-1).print_stats()\n \n# 按照函数名排序，只打印前3行函数的信息, 参数还可为小数,表示前百分之几的函数信息 \np.strip_dirs().sort_stats(\"name\").print_stats(3)\n \n# 按照运行时间和函数名进行排序\np.strip_dirs().sort_stats(\"cumulative\", \"name\").print_stats(0.5)\n \n# 如果想知道有哪些函数调用了sum_num\np.print_callers(0.5, \"sum_num\")\n \n# 查看test()函数中调用了哪些函数\np.print_callees(\"test\")\n```\n\n上述代码摘自：[使用cProfile分析Python程序性能][4]，原文还提供了`pstats`命令行交互工具方式\n\n## 图形可视化\n\n上面的命令行界面的确是有点反人类，不易一下子清晰地分析性能瓶颈，有很多图形可视化工具可以帮助我们生成简洁明了的图片\n\n工具有：\n\n* gprof2dot\n* vprof\n* RunSnakeRun\n* KCacheGrind & pyprof2calltree\n\n最终我选择了`gprof2dot`，比较符合我的口味\n\n### 安装\n\n我的机器是`ubuntu`，其他类型机器找对应方式，具体参考：[Github gprof2dot][6]\n\n```\napt-get install python graphviz\npip install gprof2dot\n```\n\n注意：\n\n如果`pip`安装软件包报错：'Cannot fetch index base URL  http://pypi.python.org/simple/'\n\n解决办法\n\n```\n1. windows下创建/%user%/pip/pop.ini，并添加以下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n2. linux创建文件~/.pip/pip.conf，并添加一下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n3. 再次使用pip安装相应的包即可。\n```\n\n### 使用\n\n根据`cpofile`输出的文件生成图片，这边输出的文件名为`osd.out`，生成的图片名为`osd.png`\n\n```shell\ngprof2dot -f pstats osd.out | dot -Tpng -o osd.png\n```\n\n具体参考这篇文章：[Python优化第一步: 性能分析实践][7]，写的很好，也很具体\n\n### 效果图\n\n这是我进行性能分析产生的两张图\n\nPicture 1：\n<center>![enable_osd1][8]</center>\n\nPicture 2：\n<center>![enable_osd2][9]</center>\n\n## 继续深入\n\n* `pstats`深入了解\n* `gprof2dot`深入了解\n* 其他的可视化工具\n\n不过最终的目的都是通过性能分析找到性能瓶颈，然后进行优化，适合自己的就好\n\n[1]: https://docs.python.org/2/library/profile.html\n[2]: http://www.cnblogs.com/btchenguang/archive/2012/02/03/2337112.html\n[3]: http://images.cnblogs.com/cnblogs_com/btchenguang/201202/201202031502512968.png\n[4]: http://xianglong.me/article/analysis-python-application-performance-using-cProfile/\n[5]: http://ju.outofmemory.cn/entry/46805\n[6]: https://github.com/jrfonseca/gprof2dot\n[7]: https://zhuanlan.zhihu.com/p/24495603/\n[8]: http://ovv1r40we.bkt.clouddn.com/enable_osd3.png\n[9]: http://ovv1r40we.bkt.clouddn.com/enable_osd3-2.png\n[10]: http://ovv1r40we.bkt.clouddn.com/publicenemyno1-600x400.png\n","tags":["性能优化"],"categories":["tech"]},{"title":"启用OSD性能剖析脚本","url":"/2017/10/10/Enable-OSD-Profile-Script/","content":"\n\n<center>![Profile][1]</center>\n\n针对`OSD`启用过程缓慢进行性能剖析，用到了一系列的脚本，例如批量创建`OSD`,批量启用`OSD`,`Cprofile`装饰器接口等\n\n<!--more-->\n\n`docprofile.py`：提供`cprofiler`的装饰器接口\n\n```python\nimport cProfile\nimport pstats\nimport os\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            print filename\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"cum\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n`pstat.py`：用于处理生成的`cprofiler`文件，做一些排序，截取和打印等工作,默认按`culativetime`排序，打印前`100`条记录，有其他需求的可以适当修改参数\n\n```python\nimport pstats, sys\nfile = sys.argv[1]\np = pstats.Stats(file)\np.strip_dirs().sort_stats('cum').print_stats(100)\n```\n\n[1]: http://ovv1r40we.bkt.clouddn.com/pozza-600x450.png\n","tags":["Profile"],"categories":["tech"]},{"title":"批量创建和删除磁盘分区脚本","url":"/2017/10/02/Batch-Create-And-Delete-Disk-Partition-Script/","content":"\n<center>![Disk Partition][1]</center>\n\n最近在做批量启用`OSD`的性能优化，需要反复的删除磁盘分区，以保证彻底的删除数据，然后就要批量创建分区，保证足够量的分区数量创建足够多的`OSD`，由于数量较多，每次手动搞都要二十几次，所以针对创建和删除写了两个脚本，支持单一、批量和选择大小等功能\n\n<!--more-->\n\n## 清空分区\n\n### 脚本名称 \n\n`clean_disks.py`\n\n### 使用说明\n\n* `python clean_disks.py [disk_name1, disk_name2 ...]` \n* 支持单磁盘和多磁盘清空\n* 支持脚本参数，如果没有参数，默认清空所有磁盘分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会清空系统磁盘，脚本会做过滤工作\n\n### 使用范例\n\n* 清空所有磁盘分区\n\n```\npython clean_disks.py\n```\n\n* 清空`sdb`的分区\n\n```\npython clean_disks.py sdb\n```\n\n* 清空`sdb`和`sdc`的分区\n\n```\npython clean_disks.py sdb sdc\n```\n\n### 脚本代码\n\n```\nimport sys\nfrom mcs3.command import do_cmd\n\ndef clean_disk(disk_name):\n    print 'disk: {} clean start ...'.format(disk_name)\n    do_cmd('sgdisk -Zog /dev/{}'.format(disk_name))\n    print 'disk: {} clean done ...'.format(disk_name)\n\nmount_info = do_cmd('mount')\nsys_disk_name = mount_info[5:8]\n\nif len(sys.argv) > 1:\n    disks = sys.argv[1:]\n    for disk_name in disks:\n        if disk_name == sys_disk_name:\n            print '{} is system disk, can\\'t be clean!'.format(disk_name)\n        else:\n            clean_disk(disk_name)\nelse:\n    all_disks = do_cmd('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                clean_disk(disk_name)\n```\n\n## 创建分区\n\n### 脚本名称\n\n`create_partitions.py`\n\n### 使用说明\n\n* `python create_partitions.py size num [disk_name1, disk_name2 ...]`, `size`为分区大小，`num`为分区数\n* 支持单磁盘和多磁盘创建分区\n* 支持脚本参数，如果没有参数，默认为所有磁盘创建分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会为系统磁盘创建磁盘分区，脚本会做过滤工作\n\n### 使用范例\n\n* 为`sdb`和`sdc`分别都创建两个大小为`100G`的分区\n\n```\npython create_partitions.py 100G 2 sdb sdc \n```\n\n* 为所有磁盘创建三个大小为`10G`的分区\n\n```\npython create_partitions.py 10G 3\n```\n\n* 为`sdc`创建四个大小为`1T`的分区\n\n```\npython create_partitions.py 1024G 4 sdc\n```\n\n### 脚本代码\n\n```\nimport sys\nfrom mcs3.command import do_cmd\n\nsize = sys.argv[1]\nnum = sys.argv[2]\nmount_info = do_cmd('mount')\nsys_disk_name = mount_info[5:8]\n\ndef parted_disks(num, size, disk):\n    for i in range(int(num)):\n        do_cmd('sgdisk -n {}:0:+{} /dev/{}'.format(i+1, size, disk))\n        print 'disk {} partition {} done ...'.format(disk, i+1)\n\nif len(sys.argv) > 3:\n    disks = sys.argv[3:]\n    for disk in disks:\n        if disk == sys_disk_name:\n            print '{} is system disk, can\\'t be parted!'.format(disk)\n        else:\n            parted_disks(num, size, disk)\nelse:\n    all_disks = do_cmd('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                parted_disks(num, size, disk)\n```\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/floppy_disk_600-450.png\n","tags":["Script"],"categories":["tech"]},{"title":"JS判断对象是否为空","url":"/2017/09/28/JS-Object-Null/","content":"\n<center>![JS][3]</center>\n\n今天在改代码的时候中发现一个`Bug`，通过`debug`发现最终的原因便是判断对象是否为空导致的错误，一个小小的错误反映出`Javascript`这门语言的不严谨性，也提醒我们得熟悉它的一些技巧和原理，避免经常在一些问题上踩坑\n\n<!--more-->\n\n场景还原：通过`if(object)`判断对象内容不为空，\n\n```\nobj = {};   // obj其实是通过其他方式获得，但是它的内容为空\nif (obj) {\n        xxx;    // 判断当obj不为空的时候，执行判断内的代码\n}\n```\n\n那么问题就出现了，即使对象内容为空，这个判断代码块里面的代码还是会被执行\n用`chrome`简单的验证了一下：\n\n<center>![object empty][1]</center>\n\n这和一些其他语言不一样，比如`php`,`python`是可以通过`if(object)`来检查对象是否为空的，但是`JS`不可以，这个问题要不是遇到了肯定想不到\n\n那么`JS`如何判断对象内容是否为空呢？网上搜了下，大概有以下方式：\n\n### 通过JSON.stringify将对象转换成字符串进行判断\n\n```\nvar tt = {};\nif (JSON.stringify(tt) == \"{}\") {\n        console.log('tt is not empty!');\n}\n```\n\n### 通过读取对象元素\n\n```\nvar tt = {};\nvar t = {id: 1};\nif (tt.id) {\n        console.log('tt is not empty!');\n} \nif (t.id) {\n        console.log('t is not empty');\n}\n```\n\n### 总结\n\n方法1相对于方法2来说更加严谨，阅读性高，而方法2的效率却比方法1高\n\n### jQuery提供了判断对象是否为空的API\n\n如果用的是`jquery`的话，可以使用其提供的现有方法：`isEmptyObject`,这是`jQuery1.4`版本后加入的功能，具体用法：\n\n```\nvar tt = {}; \nif ($.isEmptyObject(tt)) {  // jQuery.isEmptyObject(tt)这样调用也可以\n        console.log('tt is empty!');\n}\n```\n\n### 判断对象是否存在\n\n借助上面的问题，顺便了解了一下：[如何判断Javascript对象是否存在][2]\n\n这篇文章里面讲的还是比较全面的，针对不同场景给出了不同的方案\n\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/object_empty.png\n[2]: http://www.ruanyifeng.com/blog/2011/05/how_to_judge_the_existence_of_a_global_object_in_javascript.html\n[3]: http://ow0mgad6r.bkt.clouddn.com/jsbanana-600-450.png\n","tags":["Javascript"],"categories":["tech"]},{"title":"OSD创建和删除全过程","url":"/2017/09/27/OSD-Create-And-Delete/","content":"\n<center>![create and delete process][1]</center>\n\n`OSD`是`Ceph`中最基本也是最常用的功能，所以经常的创建和删除操作少不了，然后这个两个过程并不是一两个命令那么简单，本文就`OSD`的创建和删除进行内容进行步骤分离和讲解，结尾还有一份一键删除指定`OSD`的脚本\n\n<!--more-->\n\n## 创建过程\n\n### 创建一个OSD\n\n```\nceph osd create [uuid]   #若没有uuid参数，则该命令自动生成一个uuid。该命令产生一个新的osd-number\n```\n\n### 为新的OSD创建默认的工作目录\n\n```\nmkdir -p /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 为新的OSD准备一块磁盘\n\n```\nmkfs.xfs -f /dev/vde \nmount /dev/vde /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 初始化新的OSD工作目录\n\n```\nceph-osd -i {osd-number} --mkfs --mkkey\n```\n\n>  注意：在执行上述命令前要求新的OSD工作目录必须为空\n\n### 注册新的OSD认证密钥\n\n```\nceph auth add osd.{osd-number} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-number}/keyring\n```\n\n### 将新的OSD添加到Crush Map中\n\n```\nceph osd crush add osd.{osd-number} {weight} [{bucketype}={bucket-name}......}\n```\n\n此步骤也可以添加buckets后再添加osd，即：\n\n```\nceph osd crush add-bucket node5 host     #创建一个名字为node5的bucket\nceph osd crush move node5 root=default    #将创建出来的bucket放到root下\nceph osd crush create-or-move osd.{osd-number} 1.0 root=default host=node5     #将新的OSD添加到node5下\n```\n\n### 将新的OSD信息填写到ceph.conf中\n\n```\n[osd.{osd-number}]\nhost = {hostname}\ndevs = /dev/vde\n```\n\n### 启动新的OSD\n\n```\n/etc/init.d/ceph start osd.{osd-number}\n```\n\n> 此时通过集群状态查看命令#ceph -s可以看到OSD数量以及up和in的数量都发生了变化，此时再通过命令#ceph -w可以看到ceph经过peering状态后，最终达到active+clean状态\n\n## 删除过程\n\n### 将需要删除的OSD在集群中的状态修改为out状态\n\n假定`osd`的`id`为`1`\n\n```\nceph osd out osd.1\n```\n\n### 停止OSD进程\n\n```\n/etc/init.d/ceph stop osd.1\n```\n\n### 将OSD从crush map中删除\n\n```\nceph osd crush remove osd.1    #删除指定的OSD\nceph osd crush remove node1    #删除OSD所在的bucket（此步骤可以不做）\n```\n\n### 清除到OSD的认证密钥\n\n```\nceph auth del osd.1\n```\n\n### 在OSD Map中清除OSD\n\n```\nceph osd rm 1\n```\n\n### 从ceph.conf中清除OSD\n\n```\n[osd.1]\n    host = {hostname}\n    ...\n```\n\n## 一键删除OSD脚本\n\n```\n#! /usr/bin/bash\n\nosd_id=$1\nceph osd out osd.$osd_id\n/etc/init.d/ceph stop osd.$osd_id\nceph osd crush remove osd.$osd_id\nceph auth del osd.$osd_id\nceph osd rm $osd_id\n# 清空 ceph.conf\n```\n\n参考链接：\n> https://my.oschina.net/linuxhunter/blog/539312\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/5ways-facebook-600-450.png\n","tags":["OSD"],"categories":["tech"]},{"title":"如何打造一个好的产品","url":"/2017/09/21/How-To-Forge-Product/","content":"\n<center>![product][1]</center>\n\n今天`BOSS`带来一个曾经的得力干将。目前是腾讯一家游戏公司的老总，职位为`Producer`，在产品方面分享很多观点，受益匪浅。下面是分享过程中我总结的一些自己觉得正确的观点和心得\n\n> 虽热目前我是一个只负责敲代码的程序猿，产品这种事情好像跟我没多大关系，应该是产品经理的事情，但是其实一个`coder`想要进步，不仅仅是技术水平越来越强，对产品的理解也非常重要。这能理清自己逻辑，能够发散思维，能够帮你做出更好的产品，你开发出一个没人用的产品和一个上线客户量很大的产品对你自己来说无论是成就感还是技术提升都是天壤之别。也许未来我们可能也会创业，那到时候我们的职责可能是老板，可能是产品，可能是研发，更可能是各种职能集一身，对产品拥有良好理解和恰当的打磨技巧对我们来说都是宝贵的财富。\n\n<!--more-->\n\n## 技能心态\n\n### 工作强度 \n\n`BAT`加班不比任何一家中小公司少，腾讯一直把自己当做创业型公司，时刻保持压力和紧张感，这或许是其成功的一个很重要的原因（意料之外，之前没想到`BAT`工作强度这么大）\n\n### 不用过多的Design \n\n因为没有一个产品可以凭借设计文档一步到位的，几乎每个成功的产品都和原来的设计有着天壤之别（仁者见仁，智者见智，把握一个度吧，没必要死扣设计为了最完美化而浪费大量的时间）\n\n### 没有具体的Deadline \n\n每天都是`Deadline`，每天规定的任务做好了再回去\n\n### 谁的声音大听谁的 \n\n不要在内部为了争执一个方案纠结和浪费太多时间，谁的声音大听谁的，因为这绝不是最后一次修改（有性格）\n\n### 产品是每个人的\n\n一个人不应该被他的职位所局限，产品团队中的每个人都应该为这个产品在设计、宣传和包装等方面提意见\n\n### Just Do It \n\n很多事情自己不做一下是不知道的，很多行为看起来很low，不符合你的职位，但是只有真正去做了，才代表走过这条路，你才知道这条路可不可行，还有哪些其他路可以尝试（那位大佬制作某款卡牌游戏时，自己找了同类`50`多种游戏都玩了一遍，而且都玩到了很`6`的地步，用他的话说最后玩到吐。。。）\n\n### 相信自己的潜力 \n\n永远不要觉得自己是做技术的，所以理所当然`UI`不擅长，宣传不擅长，包装不擅长，做一行就要把自己的头埋进去，逼着自己不断尝试，慢慢可能会发现自己做的不错（他们经常会亲自去`QQ`群、贴吧和其他社交平台宣传，我去，托这种事原来真的存在）\n\n### 学会成熟 \n\n对很多事情都要慢慢做到心里有底，比如清楚上线必有`Bug`，设计必会被颠覆（他说上线没问题他会很慌。。。）\n\n### 明确最终目的 \n\n要明确做产品，开发等工作的目的不是按时完成任务，而是要把产品做起来，最终目的是赚钱，这样你做其他类型的事情就可以想明白很多，有时候开发并不一定是最优先最重要的，植物，但是作为一个开发经历了这些事情你的思维会不一样，做事方向也会不一样\n\n### 观察用户习惯\n\n可以尝试花钱请人来使用产品，内部开发人员在背后观察它的使用习惯，不轻易提示\n\n### 用数据说话\n\n当产品上线后，通过数据采集和分析，哪个页面或者操作用户比较感兴趣，哪一步开始用户量变少，出现这种情况，可以尝试做几个版本，然后让用户使用，选择最优的方案（在这个环节不以职位高低说话，完全靠数据评断）\n\n### 要相信自己的技术\n\n相信自己可以实现用户提的所有需求，当你全身心投入一件事的时候，手段很多，解决的方案也很多，比如这位大佬述说他收集一款游戏的同类游戏的经历，收集渠道除了iOS的还有android的，还有韩国市场的，还有模拟器的，还有页游等等（只有想不到，没有做不到）\n\n### 抄袭不丢脸\n\n互联网产品从`0`到`1`很少很少，大家在做一个产品之前都会看看同类产品怎么做的，然后学习每个产品中的精华部分，这个说直接一点就是抄袭，但是不要因为这个觉得丢脸，因为抄袭容易，抄袭得好并不容易（谈到腾讯，大家都会因为**抄袭**而骂声不绝，但是静下想想这的确是个正常套路，一切为了快速出产品）\n\n### 不要害怕被告\n\n等哪天你某些方面抄袭别人的，导致被告，那么说明你已经做大了，产品在某个方面来说已经相当成功了\n\n### 被骂是极好的\n\n很多时候，用户会因为产品一些蹩脚的地方而骂声不断，这时候不要觉得丢脸或者懊恼，相反应该开心，因为很多时候别人愿意骂你说明很大程度上还是很喜欢这个产品的，而真正不喜欢的懒得留下评论\n\n## 产品本身\n\n### 快速出Demo\n\n以上说的不用过多`Design`也好，还是不要过分争执也好，都是为了尽快做出一个产品`Demo`，就是最起码可以一个`work`的东西，这很重要，一旦`work`之后很多想法就会不一样了，他们往往两个星期出一个`Demo`，这对他们`UI`来说很有挑战，因为一般设计就要一个星期，然后原型图又是一个星期，但是两个星期就得弄出来，不管是借鉴别人的也好，还是网上买模板也好\n\n### 不用太完善 \n\n快速做出来不是说实现所有功能和面面俱到，而是可以做到静态页面或者是设计图去找用户，询问他们的观点和建议，这样既可以缩短时间，也可以通过询问客户的意见走向正确的道路，避免不必要的返工\n\n### 尽快让产品上线\n\n产品上线和不上线是天壤之别，因为自己内部永远是在想当然，而客户的反馈才是最重要的，这样才会往正确的方向不断迭代\n\n### 永远不要给用户看到错误信息\n\n能解决的让系统自己解决，不能解决的人工处理，不要太**实诚**，否则用户会感觉到惊恐\n\n### 不要给用户太多选择 \n\n如果一个页面存在两个按钮让用户点击，`80%`的用户会点`A`按钮，`20%`的用户会点`B`按钮，那么页面就显示一个`A`按钮就好了，只要保证`B`按钮能够在页面其他地方能找到就可以了，太多选择会让用户心里没底自己选择的对不对（相信大家脑海里都会浮现某个用户做完一个操作询问对不对的场景吧？）\n\n### 数字必有颜色\n\n这是一个约定俗成的习惯，保证页面的数字有合适的颜色，让用户一眼就能看到最想呈现的元素\n\n### 科技感很重要\n\n要让页面充满科技感，让别人觉得这是一家充满互联网气息的新公司，第一印象好\n\n### 恰当的图标\n\n图标选择要有意义，凸出按钮的作用\n\n### 恰当的名词\n\n用词准备具体，不要让用户去猜，比如下载就是下载，不要说保存\n\n### 易懂高大上的名词\n\n用词不要太专业，这样会让用户不明白，可以高大上一些，比如远程数据复制可以叫云复制\n\n### 抓住用户的期望\n\n产品介绍要突出用户最期望的东西，而不是过分强调技术上面的高档，这不是客户最期望的东西，比如存储你与其说内部实现机制怎么分片，怎么自动控制，不如和用户说我能保证多少数据量的存储，能保证永远不会因为故障丢失数据，能存储多长时间等等\n\n### 调节气氛的话语\n多用网络诙谐的话语进行提示，比如亲，朕等等，这样会舒畅用户的心情\n\n\n作者： Tony\n日期： 2017/09/19\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/product-600-450.jpg\n","tags":["产品"],"categories":["summary"]},{"title":"PHP-Excel 开发解决方案","url":"/2017/09/18/PHP-Excel/","content":"\n<center>![office][1]</center>\n\n 过去工作中使用`PHPExcel`较多，碰到并解决了各种大大小小的问题，总结出这样一篇文章，一方面记录自己踩过的坑，一方面与大家分享，让大家少走弯路，并不断完善之，欢迎大家去`github`上面`star`和提交`pull request`，不断补充和优化，打造最全面的`PHPExcel`开发解决方案\n\nGithub 地址：https://github.com/tony-yin/PHPExcel_Road\n原文地址：https://tony-yin.github.io/2017/09/18/PHP-Excel/\n\n<!--more-->\n\n## 基础：小试牛刀\n\n### 1. 引用文件\n\n```\nyourpath . /phpexcel/PHPExcel.php\n```\n\n### 2. 实例化phpexcel类\n\n```\n$excel = new PHPExcel();\n```\n\n### 3. 获取当前单sheet（多sheet会在下面讲）\n\n```\n$objexcel = $excel->getActiveSheet();\n```\n\n### 4. 合并单元格\n\n```\n$objexcel->mergeCells('A1:M1');\n```\n\n### 5. 获取一个cell的样式\n\n```\n$objexcel->getStyle('A1');\n```\n\n+ 获取一个cell的字体样式\n\n```\n$cellFont = $objexcel->getStyle('A1')->getFont();\n```\n+ 设置字体大小\n\n```\n$fontStyle->setSize(15);\n```\n+ 设置字体是否加粗\n\n```\n$fontStyle->setBold(true);\n```\n+ 设置字体颜色\n\n```\n$fontStyle->getColor()->setARGB(PHPExcel_Style_Color::COLOR_RED);\n```\n+ 获取一行样式\n\n```\n$rowStyle = $objexcel->getStyle(1)->getRowDimension();\n```\n+ 设置行高度\n\n```\n$rowStyle->setRowHeight(2);\n```\n+ 获取一列样式\n\n```\n$columnStyle = $objexcel->getStyle('A')->getColumnDimension();\n```\n+ 设置列宽度\n\n```\n$columnStyle->setWidth(10);\n```\n+ 获取一列对齐样式\n\n```\n$alignStyle = $objexcel->getStyle('A')->getAlignment();\n```\n+ 设置水平居中：同一水平线上居中，即为左右的中间\n\n```\n$alignStyle>setHorizontal(PHPExcel_Style_Alignment::HORIZONTAL_CENTER);\n```\n+ 设置垂直居中：同一垂直线居中，即为上下的中间\n\n```\n$alignStyle->setVertical(PHPExcel_Style_Alignment::VERTICAL_CENTER);\n```\n+ 自动换行\n\n```\n$$alignStyle->setWrapText(true);\n```\n\n### 6. 获取指定版本excel写对象\n如需更早的版本可将`Excel2007`换成`Excel5`\n```\n$write = PHPExcel_IOFactory::createWriter(\"xcel, 'Excel2007');\n```\n\n## 进阶：一些有用的小知识\n\n### 1.行列数字索引方法\n> `phpexcel`一般获取`cell`或者获取列都是通过`ABC`这样的英文字母获取的，它也可以通过`0、1、2、3`这样的数字表示sheet中的列，从`0`开始，`0`对应`A`，`1`对应`B`，基本上大多数方法都是数字行列索引，例如`getStyleByColumnAndRow($col,$row)`,默认列参数在前，行参数在后，更多的可以参加`phpexcel`源码；\n\n### 2. 单行或单列参数格式\n> 有的时候一个方法需要行列两个参数，例如只需要某一行参数可写成`(null, $row)`,例如只需要获得某一列参数可写成`($col, null)`\n\n### 3. 列的数字索引格式和字母索引格式互转\n\n+ 数字转字符串\n\n```php\nPHPExcel_Cell::columnIndexFromString('A');  // Return 1 not 0;\n```\n+ 字符串转数字\n\n```php\nPHPExcel_Cell::stringFromColumnIndex(0);    // Return 'A';\n```\n\n### 4.PHPExcel读取数字类型 \n> `PHPExcel`读取的`cell`数字，类型都是`double`型，可用`gettyle()`方法检测类型，当初我一直使用`is_int()`方法无果，搞得焦头烂额。。。\n\n### 5. 多cell边框线设置\n`PHPExcel`生成的表格如果你不加处理，是不会帮你生成边框线的，生成边框线的方法如下：\n```php\n$borderArray = array(\n    'borders' => array(\n        'allborders' => array(\n            'style' => PHPExcel_Style_Border::BORDER_THIN\n        )\n    )\n);\n$objexcel->getStyle($col1, $row1,$col2, $row2)->applyFromArray($borderArray);\n```\n>注：\n> 1: `getStyle()`可以看需求改为`getStyleByColumnAndRow()`方法通过数字行列索引读取`style`\n> 2: array中`PHPExcel_Style_Border::`后面有三种格式分别是`BORDER_THIN`和`BORDR_MEDIUM`，表示边框线的粗细；\n> 3: `getStyle()`中的索引可以是静态的，也可以是动态的，一般是在导出`excel`的数据`set`完毕后填写左上角的单元格行列索引和右下角的单元格行列索引；\n\n> 参考资料\n>\n> http://phpexcel.codeplex.com/workitem/22160\n> http://phpexcel.codeplex.com/workitem/20150\n\n### 6. 多cell字体加粗处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(  \n        'bold' => true,                                              \n    ),                                                               \n));\n```\n\n### 7. 多cell字体颜色处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(\n        'color' => array(\n            'rgb' => 'ff0000',\n            ),\n        ),\n    )\n);\n```\n\n### 8. 多sheet导入\n\n动态为当前`sheet`设置索引，然后获取当前`sheet`，便可循环读取每一个`sheet`内容\n\n```php\n$objexcel->setActiveSheetIndex($index);   //$index = 0 1 2 3\n$objexcel->getActiveSheet();    //return sheet1 sheet2 sheet 3\n```\n\n### 9. 固定格式excel读取在写入\n\n> 当需求是给定一个一个模板`excel`，需要往里面塞数据，我们不一定要通过代码给它设定样式，如果这个模板变化不大，我们完全可以存放一个格式相同的静态文件，然后通过`PHPExcel`读取，再往里面塞数据，最后进行保存操作，可以达到一样的效果，并且可以节省大量的资源。\n\n### 10. 合并单元格导入问题\n\n> 在特殊的表格中，合并单元格普遍存在，而多个单元格合并成的一个单元格，只能`setValue()`一次，而我们如何判断合并单元格的具体行列呢？\n\n```php\n$range = $start_cell->getMergeRange();  // 通过合并单元格的开始单元格比如‘A1’，获取合并范围‘A1:A4’\n$cell->isInRange($range);    // 遍历之后每一个单元格便可通过isInRange()方法判断当前单元格是否在合并范围内\n```\n## 高级：特殊场景特殊手段\n\n### 1. 单元格文本格式数据处理 \n> 一般excel单元格中数据的格式为数据类型，而`PHPExcel`中的`getValue()`方法读取的也是数据类型，当把数据从数据类型改为文本类型后，在`PHPExcel`中读出来的是`PHPExcel_RichText`类型，`getValue()`读取返回`PHPExcel_RichText`是一个`object`类型（`PHPExcel_RichText`数据保存格式）；那如何读取这一类的数据呢？仔细查看读取出来的对象，不难发现有`getPlainText()`这样的方法可以读取文本类型数据，所以我们只要判断当当前数据为文本数据时用`getPlainText()`读取，一般数据用`getValue()`读取\n\n```php\nif ($cell->getValue() instanceof PHPExcel_RichText) {\n    $value = $cell->getValue();\n} else {\n    $value = $cell->getValue();\n}\n```\n> 参考资料\n>\n> http://www.cnblogs.com/DS-CzY/p/4955655.html\n> http://phpexcel.codeplex.com/discussions/34513\n\n### 2. 单元格数据算法处理\n> `excel`拥有强大的算法功能，一般算法格式为`=A3+A4`这类的，复杂的更多，如果使用`PHPExcel`提供的默认读取方法`getValue()`读取出来的结果则为字符串`=A3+A4`,好在`PHPExcel`也足够强大，提供了相应的接口：`getCalculatedValue()`，这个方法专门读取算法数据，但是我们不能将这个方法作为默认读取方法，因为这样可能会将一些本来要读成字符串的读成算法数据，而且`PHPExcel`没有将它作为默认读取方法的另一个重要原因就是算法方式读取很耗时间和性能，一般数据读取根本没有必要这样浪费资源，所以我们可以采用以下这种方式\n\n```php\nif (strstr($cell->getValue(), '=')) {   \n    // 判断如果cell内容以=号开头便默认为算法数据\n    $value = $cell->getCalculatedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 3. 日期数据处理\n\n> 除了以上所说的文本数据和算法数据外，我还遇到过日期类型数据，比如`2016-12-28`输入到`excel`中，它会默认转换成`2016/12/28`，如果采用一般的`getValue()`方式读取也会读取到错误的数据，`PHPExcel`也提供了相应的接口`getFormattedValue()`,并提供了适配的识别方式`PHPExcel_Shared_Date::isDateTime($cell)`,所以代码就很好实现了\n\n```php\nif (PHPExcel_Shared_Date::isDateTime($cell)) {\n    $value = $cell->getFormattedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 4. 读取方法封装\n> 针对`excel`各种数据类型，我们可以写一个函数，将原有的`getValue()`封装一下，这样以后就不用每次都判别一下数据类型了，目前我只遇到上面三种特殊格式，如果有新的，欢迎大家补充，封装函数如下\n\n```php\nfunction get_value_of_cell($cell) {\n    if (strstr($cell->getValue(), '=')) {   \n        $value = $cell->getCalculatedValue(); \n    } else if ($cell->getValue() instanceof PHPExcel_RichText) {\n        $value = $cell->getValue();\n    } else if (PHPExcel_Shared_Date::isDateTime($cell)) {\n        $value = $cell->getFormattedValue(); \n    } else {\n        $value = $cell->getValue();\n    }\n}\n```\n\n### 5. 导出文件在IE、360等浏览器中文件名中文乱码问题\n\n```php\n$filename = 'xxx导出表';\n// 判断如果是IE内核形式的浏览器采用urlencode处理文件名\nif (!preg_match(\"/Firefox/\", $_SERVER[\"HTTP_USER_AGENT\"])) {\n    $filename = urlencode($filename);\n}\n```\n\n> 未完待续。。。\n> 期待你的补充和优化\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/office-card-game.png\n","tags":["PHPExcel"],"categories":["tech"]},{"title":"磁盘反复拔插记录","url":"/2017/09/10/Disk-pull-and-plug-record/","content":"\n<center>![pull disk][10]</center>\n\n最近在做磁盘拔插后的`OSD`热恢复功能。在反复拔插盘的实践过程中，遇到了一个难题，就是时常会由于拔盘，然后再插盘后盘符发生了改变，`kernel`和`udev`接收不到任何`device`增加或删除的事件消息，\n由于我是基于`udev`监测磁盘的插拔，所以一旦感知不到的话，就无法进行相应的恢复操作，所以在此对拔盘的记录进行一些记录，并在大量操作中得出总结。\n\n<!--more-->\n\n## Operation Record\n\n`normal`: 表示正常情况，没有被拔\n\nenv | udev monitor | UI data | disk alias | device | lsblk | mount info | mount point \n:--:|:----:|:----:|:---:|:---:|:---:|:---:|:---:|\nnormal | - | /dev/sdg1 | osd4-data | /dev/sdg | exist | exist | /data/osd.1\nfirst pull | remove sdg | empty | remove | remove | remove | exist | exist, readerror\nfirst plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\nsecond pull | no remove | empty | osd4-data | /dev/sdd | not change | exist | exist, readerror\nsecond plug | All Not Change | - | - | - | - | - | - \nclear mount | - | - | exist | - | exist | remove | change style\nthird pull | remove sdd | empty | remove | remove | remove | remove | change style\nthird plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\n\n## Before pull\n<center>![before pull][1]</center>\n\n## First pull\n<center>![First pull][2]</center>\n\n## Read error\n<center>![Read error][9]</center>\n\n## First plug\n<center>![First plug][3]</center>\n\n## Second pull\n<center>![Second pull][4]</center>\n\n## Second plug\n<center>![Second plug][5]</center>\n\n## Clear mount info\n<center>![Clear mount info][6]</center>\n\n## Third plug udev monitor\n<center>![Third pull][7]</center>\n\n## Third plug \n<center>![Third plug][8]</center>\n\n## 注意：\n通过上面的表格和图片，如果拔盘前没有卸载`mount`链接，那么可以发现：\n\n* 下次插盘，磁盘名称不再是原来的名称，在当前排序最后的一个磁盘名往后追加，比如当前最后一块盘是`sdf`，那么就改名为`sdg`\n* `lsblk`显示`device`列表不会变化，`alias name`也是不会变化\n* 再次拔盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的删除的事件信息\n* 再次插盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的添加的事件信息\n\n如果清除了`mount`信息，插拔变得正常起来\n\n* `udev`可以正常接收设备添加和删除的信息\n* `lsblk`可以正常显示设备列表，会随着磁盘的拔盘而更新显示，`alias name`也是如此\n* 磁盘名称变回正常，即开机最原始的名称，例如上文的`sdd`\n\n所以针对一块硬盘的拔插只要做好即时清除`mount`就够了，那么这个时候拔另外插槽的一块盘呢？\n结果是：\n\n* 第一次拔盘： `udev`感知到（`remove` 当前磁盘名），`device`列表和`alias name`消失 \n* 第一次插盘： `udev`感知到（`add` **sdg**），`device`增加了`sdg`，`alias name`增加，没有变化\n* 第二次拔盘： 一切都没了反应\n* 清除`mount`信息后，插盘：无反应\n* 删除`alias name`： 无反应\n* 无论怎么样都没反应，也许除了开机或者这快盘插在别的插槽内\n\n## 总结\n\n### 问题1：\n\n目前来看，只要拔盘超过两块，一旦第二块盘的磁盘名称变了跟之前名称不一样，那么之后再怎么清理`mount`信息或者反复拔插都不会被`udev`感知到\n理论上来说，要实现硬盘热替换，那就得保证`udev`每次都能检测到硬盘的插入，拔盘检测倒不是那么重要，\n针对这种情况，目前没发现什么比较好的解决方案，唯一一种也是我当前采用的方案，在检测拔盘事件时会触发一个脚本，做拔的盘的`mount`信息的检查，如果存在`mount`信息，那么就`umount`掉，这是一种比较完美的场景，也就是说要保证每次插盘前，当前盘在系统中都不存在任何`mount`信息\n\n这种方案还是太完美化了，还需要大量的测试，期望能找出更能经得起推敲的方案\n\n### 问题2\n\n测试过程中偶然发现每次硬盘插入如果被`udev`感知到，也会执行大量的`udev rules`，其中包括创建设备，建立连接，建立连接这个事情就包括硬盘分区的`alias`，而我们感知到磁盘设备插入执行脚本在这之前，找了一系列的方案，都没能够使得让脚本在它`alias`之后再执行\n\n这里有一份官方的解释：http://www.reactivated.net/writing_udev_rules.html#external-run\n\n至于为什么脚本依赖这个别名，那是因为系统存的`data path`还是`journal path`都是以别名的形式，如果我们能够在别名之后执行脚本，那么理想情况下系统`db`之前存的别名会自动和插入的盘匹配建立连接，不需要我们再做过多的事情，否则我们得在每次创建`osd`的时候再存原始路径，记录分区信息等等，还有`/etc/fstab`存的都是别名，这样就导致有些操作无法进行，还有一些其他的矛盾暂时还没想到\n[1]: http://ovv1r40we.bkt.clouddn.com/before_pull.png\n[2]: http://ovv1r40we.bkt.clouddn.com/first_pull.png\n[3]: http://ovv1r40we.bkt.clouddn.com/first_plug.png\n[4]: http://ovv1r40we.bkt.clouddn.com/second_pull.png\n[5]: http://ovv1r40we.bkt.clouddn.com/second_pull.png\n[6]: http://ovv1r40we.bkt.clouddn.com/clear_mount.png\n[7]: http://ovv1r40we.bkt.clouddn.com/third_plug.png\n[8]: http://ovv1r40we.bkt.clouddn.com/third_plug_alias.png\n[9]: http://ovv1r40we.bkt.clouddn.com/readerror.png\n[10]: http://ow0mgad6r.bkt.clouddn.com/open-uri20150611-11-s7rw56.png\n","tags":["Linux"],"categories":["ceph"]},{"title":"Linux Udev","url":"/2017/09/10/Linux-Udev/","content":"<center>![udev][21]</center>\n\n在我入门`udev`，看着各种各样的资料觉得这个东西很高深的时候，突然瞄到一篇文章里面说道： 要树立一个信念：`udev`很简单，当时不以为然，现在深表赞同。看起来很高端很遥远的`Title`，比如`Linux Kernel`，比如软硬件交互，比如热插拔等等，其实掌握几个`rules`，然后再动手实践实践，对比着讲解，就能知道七七八八了，想要再深入，就多看看文档实现和内核源码，其实很多东西都是这样，了解了就很简单，没事了解一点就比别人多知道一些，共勉 \n\n<!--more-->\n## Udev 是什么\n`udev`是`linux kernel`的设备管理器，处理硬件的添加和删除，但它的功能远远不仅于此。比如`U`盘插入，电脑是如何即时的做出响应？耳机插入又是如何做到自动关闭外部音量？这些都跟`udev`有关。虽然`udev`很强大，功能很多，本来就针对`SCSI`硬盘设备的插拔做一些经验的分享。至于为什么我会研究到这个东西，是因为最近在做`ceph`相关的项目，需要实现一个类似热插拔的`feature`，懂的人应该很清楚，就是拔插硬盘后，`osd`自动恢复，主要应用于硬盘寿命即将到期，替换新盘的场景，不明白的同学也不要紧，因为本文主要讨论的是`udev`\n\n## Udev Rules\n`udev`管理各种各样的设备，比如`usb`,`scsi`,`cd`等等，为了区别设备类型，精确匹配，`udev`制定了一套特色的`rules`，为什么说特色呢？因为我觉得它用最简洁的语法满足了方方面面的需求\n\n这里推荐一些有用的`link`：\n\n* [UDEV Primer][3]  `udev`入门必读，可以快速将`udev`了解一番\n* [Writing udev rules][4]  经典文章，看完这个基本就能掌握`udev rules`\n* [Writing udev rules 修订版][5]  国内论坛对格式样式做了一些修改，有洁癖的可以看看\n* [Writing udev rules 译][6]  翻译版本，英文不好的可以中英文对比着看\n* [udev 中文手册][7]  这位金步国先生为开源事业做出了杰出的贡献，早前就看过他翻译的`Apache`的手册\n* [Writing udev rules and kernel examples][8]  看够了上面各种规则，来看看具体的例子吧\n* [Udev 详解][11]  文章如名，真的很详细\n* [Udev 使用方法][20]  一些特殊的技巧  \n\n大家看完上面的文章应该知道`rules`文件的位置和规则,下面我贴一些我实践中的代码和例子\n\n### Rules Example\n这是两条分别针对`SCSI`硬盘设备的添加和删除的`rules`，设备事件分别会触发相应的脚本：\n```\nKERNEL==\"sd[a-z]\", ACTION==\"add\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k add\"\nKERNEL==\"sd[a-z]\", ACTION==\"remove\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k remove\"\n```\n\n### Notice\n* 执行外部命令必须加上环境变量路径, 上面的这个也可以不加`/usr/bin/python`，但是必须在`py`脚本头部加上`shebang`：`#! /usr/bin/python`\n* `RUN`可以运行程序外的程序，而`PROGRAM`不是做这个事情的，而是为了生成之后需要的变量\n\n### Link\n* [hashtag and shebang][1]\n* [shebang][2]\n\n## Udevadm\n`udevadm`是一个`udev`的管理工具，可以用来获取设备信息(`info`)，监视和控制udev运行时的行为(`control`)，请求内核事件(`trigger`)，管理事件队列(`settle`)，以及提供简单的调试机制(`test`)。很实用，很好用！！！\n\n### 推荐 Link\n\n* [udevadm 中文手册][9]  很全，很基础 ~~\n* [udevadm command examples][10]  很多`udevadm`的命令可以借鉴，说不定以后可以用到呢？\n\n介绍几个我具体使用过的例子：\n### udevadm info\n可以从`udev`数据库中提取设备信息,此外，还可以从`sysfs`中提取设备的属性， 以帮助创建与此设备匹配的`udev`规则\n\n获取`/dev/sdb`的`path`信息\n```\nudevadm info -q path -n /dev/sdb\n```\n### udevadm test\n\n模拟向`udev`发送设备信息，并不会真的触发，**而是将触发后的过程信息打印出来**，对调试很有帮助\n\n``` \nudevadm test --action='add' `udevadm info -q path -n /dev/sdb`\n```\n\n**注意：** `test`不会真正触发事件，而`trigger`可以\n\n### udevadm trigger\n\n可以真正触发`udev`事件，但是不会真正改变硬件，只是触发`kernel`和`udev`的事件，会触发`udev rules`\n\n```\nudevadm trigger -v --action=add --sysname-match=sda\n```\n\n**注意：** \n\n* 不加`sysname-match`那么会触发所有设备，如果是`action=remove`，那么则会移除很多设备，甚至导致`sshd`服务停掉\n* 后面是`sda`而不是`/dev/sda`因为它是根据`pci`路径来定位`name`的，比如`sda`的路径是：\n```\n/sys/devices/pci0000:00/0000:00:10.0/host2/target2:0:0/2:0:0:0/block/sda\n```\n\n### udevadm control\n\n* 重载`udev rules`，对之后触发的新设备有效，对之前已经触发的无效\n```\nudevadm control --relaod-rules\n```\n* 更改`udev`日志等级为`info`，`udev`默认等级为`err`，可以在`/etc/udev/udev.conf`中查看，日志具体信息可以在`syslog`中查看\n```\nudevadm control --log-priority=info\n```\n\n## More about udev\n`udev`还有很多有意思的地方，如果想要更加深入的学习，那么除了熟练地掌握`rules`等技巧，还要对其实现原理有了解，这就得多看文档，多看源码了，建议大家有条件的情况下多看源码，多看原始的东西，因为经过别人的总结就可能会变味，相互对比，不要一味依赖别人总结的产物\n\n* [Linux 自动挂载usb设备 小例子][12]  如何在`usb`设备插入后自动挂载？\n* [udev使用笔记][13]  这篇文章里面那张图片不错\n* [写个程序检测我们的设备插拔][14]  模仿内核源码简单地实现一个设备插拔的检测\n* [linux下热插拔事件的产生是怎样通知到用户空间][15]  事件传递过程，`udev`源码解析\n* [热插拔的工作原理 hotplug机制和udev][16]  `udev`原理讲解\n* [Uevent 上报event事件给上层的详细讲解][17]  `uevent`源码讲解\n* [linux内核空间和用户空间的是怎样区别的，如何交互，如何从用户空间进入内核空间][18]  内核空间和用户空间的一些剖析\n* [Netlink实现热拔插监控 ][19]  消息传输用的是什么？`Netlink`是如何做到的\n\n[1]: http://www.cnblogs.com/daishuguang/p/4192703.html\n[2]: http://smilejay.com/2012/03/linux_shebang/\n[3]: http://webpages.charter.net/decibelshelp/LinuxHelp_UDEVPrimer.html\n[4]: http://www.reactivated.net/writing_udev_rules.html\n[5]: http://www.lai18.com/content/1612267.html\n[6]: http://blog.csdn.net/nhczp/article/details/4099647\n[7]: http://www.jinbuguo.com/systemd/udev.html#\n[8]: http://wiki.dreamrunner.org/public_html/Embedded-System/kernel/udev-rules.html\n[9]: http://www.jinbuguo.com/systemd/udevadm.html\n[10]: http://fibrevillage.com/sysadmin/93-udevadm-usage-examples\n[11]: http://blog.csdn.net/lqrensn/article/details/8097288\n[12]: https://my.oschina.net/bwhite/blog/808995\n[13]: http://www.jianshu.com/p/dd6cecd7755a\n[14]: http://blog.csdn.net/fjb2080/article/details/5009791\n[15]: http://www.itnose.net/detail/6691523.html\n[16]: http://blog.csdn.net/bingqingsuimeng/article/details/7950639\n[17]: http://blog.csdn.net/sunweizhong1024/article/details/7928530\n[18]: http://blog.csdn.net/bingqingsuimeng/article/details/7924756\n[19]: http://blog.chinaunix.net/uid-24943863-id-3223000.html\n[20]: http://www.cnblogs.com/qigaohua/p/6673930.html\n[21]: http://ow0mgad6r.bkt.clouddn.com/pingu.png\n","tags":["Linux"],"categories":["tech"]},{"title":"Network Down Notification","url":"/2017/09/10/network-down-notification/","content":"\n<center>![network][3]</center>\n\n项目中要做一个`network` `down`了之后给用户发送邮件提醒的功能，这里面涉及到的问题有：\n* 如何监测 network 状态\n* network down之后，如何发送邮件\n* public network 和 storage network的用处和场景\n\n<!--more-->\n\n## get role interfaces\n\n## get interfaces\n\n## get interface info\n\n### 获取 MAC 和 busid \n\n```\nsyspath = os.path.join(\"/sys/class/net/\", interface)  // interface such as 'eth0'\nmac = linecache.getline(os.path.join(syspath, \"address\"), 1)[:-1].upper()\nbuspath = os.path.realpath(syspath)\nbus_id = os.path.basename(os.path.dirname(os.path.dirname(buspath)))\n```\n\n* MAC: 直接在`syspath`目录下读取`address`文件\n* 假设为`eth0`,查看`eth0`目录可见该目录被建立了软连接，并不是真实地址，所以需要取出它的真实地址，这个需要使用`python`中的`os.path.realpath()`处理获得`0000:03:00.0`,作为下一步`lspci`的参数\n\n```\nroot@ceph133:/sys/class/net# ll eth0\nlrwxrwxrwx 1 root root 0  8月  8 01:26 eth0 -> ../../devices/pci0000:00/0000:00:15.0/0000:03:00.0/net/eth0/\n```\n\n### 获取网卡信息\n\n`lspci -s`后面的参数为`pci`设备的地址，也叫`pci寻址`，这个地址由总线编号、插槽编号和功能块编号信息组成\n```\nlspci -m -s xxx // 这边的参数就是上面的bus_id\n```\n\n> 具体`lspci`信息请参照：\n> [lspci命令详解][1]\n> [LSPCI命令详解分析][2]\n\n### 获取网卡状态\n\n通过`ethtool`工具获取网卡状态，就是看`Link detected`是否为`yes`，`yes`表示已激活\n\n```\n/sbin/ethtool eth0 | grep 'Link detected: yes' | wc -l\n```\n\n## bond filter\n\n## Network down notification\n**1.** 目前无论是`storage`还是`public`网卡只要`down`其中一个都无法发送`email`:\n\n* public: `visit foreign network and connect smtp server`\n* storage: `connect ceph rados to get notification info, such as send account and smtp server etc`\n所以只能`monitor`除这两个以外的网卡\n\n**2.** `Python`\n\n* `log`打印的文件取决于当前文件进程的初始化`log level`\n* `logger`: `handler`, `formatter`, `filter`\n* `dictdefault`\n\n\n  [1]: http://coolnull.com/2246.html\n  [2]: http://blog.csdn.net/xiadidi/article/details/18408169\n  [3]: http://ow0mgad6r.bkt.clouddn.com/it_specialist_kit8-net.png\n","tags":["Notification"],"categories":["tech"]},{"title":"Python操作Oracle","url":"/2017/09/10/Python-Oracle/","content":"<center>![Python][3]</center>\n\n近期在做`Intel`的测试，需要监测各种故障情况下的热恢复或者热迁移等功能，比如拔线测试，拔盘测试，断电测试等，来检验分布式的稳定性和灵活性，以`Oracle`为例，需要一直向数据库中写数据，然后在发生故障时，通过观察插入数据的情况来判断是否能够处理异常情况，所以我需要写一个脚本，能够做到通过`python`自动连接`oracle`数据库并自动插入数据。\n\n<!--more-->\n\n> 当前环境：Linux Centos 7\n\n### 1. 下载安装包cx_Oracle\n\n由于我本地`Python`版本是`2.7`,所以选择是`2.7`版本\n\n```\nwget https://pypi.python.org/packages/e1/18/00987c6a9af9568ee87d1fcba877407684a3f1b87515e5eb82d5d5acb9ff/cx_Oracle-6.0rc1-py27-1.x86_64.rpm#md5=ff8f5cce07bd5ec8ed3ce5c79d451474\n```\n\n参考:\n\n> - [python-cx_Oracle][1]\n> - [安装包列表][2]\n\n\n[1]: https://oracle.github.io/python-cx_Oracle/\n[2]: https://pypi.python.org/pypi/cx_Oracle/5.3\n\n### 2. 安装cx_Oracle\n\n- 安装：\n\n```\nrpm -ivh cx_Oracle-6.0rc1-py27-1.x86_64.rpm\n```\n\n- 查看当前`python`引入模块库的路径\n\n```\n[root ~]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys\n>>> print sys.path\n['', '/usr/lib/python2.7/site-packages/AliyunUtil-0.0.1-py2.7.egg', '/usr/lib/python2.7/site-packages/cloud_init-0.7.6-py2.7.egg', '/usr/lib64/python27.zip', '/usr/lib64/python2.7', '/usr/lib64/python2.7/plat-linux2', '/usr/lib64/python2.7/lib-tk', '/usr/lib64/python2.7/lib-old', '/usr/lib64/python2.7/lib-dynload', '/usr/lib64/python2.7/site-packages', '/usr/lib/python2.7/site-packages']\n```\n\n- 检查安装路径，如果`cx_Oracle.so`文件不在以上`python`引入路径内需要将该文件拷贝到以上`python`引入路径,所以我这里不需要进行拷贝操作\n\n```\n[root ~]# rpm -qa | grep cx_Oracle | xargs rpm -ql\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/PKG-INFO\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/SOURCES.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/dependency_links.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/top_level.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle.so\n```\n\n### 3. 下载 Oracle Library\n\n由于`Oracle`官网下载东西存在`Auth`认证，所以简单点就点击选中的`rpm`包保存在`windows`上，然后上传到`linux`上即可\n\n**注意：**\n> - 下载的版本要和连接的`Oracle`基本一致，我需要连接的`oracle`版本是`11.2`，所以我下载的是`oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm`\n> - **操作系统要和之前下载的`cx_Oracle`要一致**，这很重要！！！我在这个上面栽了跟头，花了比较多的时间。。。所以我需要下载`64`位的\n> - 在`Oracle`官网下载东西你得先勾选`Accept License Agreement`，否则你下载不了\n> - 其次你得有`Oracle`的账号\n\n### 4. 安装 Oracle Library\n\n```\nrpm -ivh oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm\n```\n\n### 5. 配置环境变量\n\n- **修改`Oracle`安装路径**\n\n将`client64`改为`client`，至于为什么要改暂时还不清楚，只知道不改的话会报错`Oracle Client library cannot be loaded: libclntsh.so: cannot open shared object file: No such file or directory`\n\n```\ncd /usr/lib/oracle/11.2/\nmv client64 client\n```\n\n- 打开`/etc/profile`,追加环境变量路径\n\n```\nvim /etc/profile\n```\n\n- 配置环境变量\n\n```\n// 查看Oracle安装路径\n[root@iZbp180vortqapb7yf8217Z ~]# rpm -qa | grep oracle | xargs rpm -ql\n/usr/lib/oracle/11.2/client64/bin/adrci\n/usr/lib/oracle/11.2/client64/bin/genezi\n/usr/lib/oracle/11.2/client64/lib/libclntsh.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libnnz11.so\n/usr/lib/oracle/11.2/client64/lib/libocci.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libociei.so\n/usr/lib/oracle/11.2/client64/lib/libocijdbc11.so\n/usr/lib/oracle/11.2/client64/lib/ojdbc5.jar\n/usr/lib/oracle/11.2/client64/lib/ojdbc6.jar\n/usr/lib/oracle/11.2/client64/lib/xstreams.jar\n// 追加ORACLE_HOME环境变量路径\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/lib\n```\n\n- 生效配置\n\n```\nsource ~/.bashrc\n```\n\n### 6. 检查是否可以引入`cx_Oracle`\n\n`import`没有报错，说明之前的辛苦没有白费\n\n```\n[root client64]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import cx_Oracle\n>>>\n```\n\n### 7. 连接 Oracle\n\n一个插入10条数据的Python脚本demo：\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport cx_Oracle\ncon = cx_Oracle.connect('test/test@xxx.xxx.xxx.xxx/xe')\ncur = con.cursor()\nfor n in range(1, 10):\n    cur.execute('insert into test(id, name) values (1, n);')\ncur.close()\ncon.close()\n```\n[3]: http://ow0mgad6r.bkt.clouddn.com/we-love-python-6000-450.png\n","tags":["Oracle"],"categories":["tech"]},{"title":"Oracle Linux And Oracle Database 11g R2 Intsallation","url":"/2017/09/10/oracle_install/","content":"\n<center>![oralce][9]</center>\n\n最近工作中用到了`Oracle`作为`Intel`测试的数据库，在装`Oracle Linux`和`Oracle Database 11g R2`折腾了很久，这篇文章就这两点对大家分享一些我安装过程中遇到的问题和注意点，`rac`暂时还未配置，稍后找时间补上\n\n<!--more-->\n\n> 友情提醒： 本文参照`link`较多，请细心并且耐心地进行每一步操作\n### Resource Link\n> * [Oracle Linux ISO][1]\n> * [Linux Oracle Database 11g R2][2] \n> * [Windows Oracle Database 11g R2][3] \n\n### Oracle Linux Installation\n**参考列表：**\n\n  * [oracle linux 6.5 安装 oracle 11gR2图文详解][4]\n  * [Oracle 11g R2（自己整理的Word，带详细RAC配置）][5]\n\n> 以下操作步骤基本是按上面两条`link`走的，有一些地方不一样或者是没必要的我会在下面详细讲解，我装的时候以`link1`为主线，然后参照`link2`的`word`文档进行，没有特殊标记的一般都是根据`link1`操作，大家也可以这样操作，可以少遇一些坑，节省一些时间~~~\n\n**注意点：**\n  \n  * `Base server`这一步很关键\n    - 勾选`Oracle Linux Server`, `Resilient Storage`和 `Scalable Filesystem Support`（参照`link2`）\n    - 勾选`Customize Now`，这一步依然非常关键，只有勾选了才可以装一些`Oracle Linux`需要的套件\n    - 需要手动添加安装的套件（系统默认勾选的不要删掉，这边的操作只添加勾选）：\n      * `Base System`：`Compatibility libraries`\n      * `Servers`: `System administration tools`\n      * `Desktops`: `Desktop`、`Desktop Platform`、`Fonts`、`General Purpose Desktop`、`Graphical Administration Tools`、`Input Methods`、`X Window System`\n      * `Development`: `Additional Development`、`Development Tools`\n      * `Appilication`: `Internet Browser`\n  * 安装应该是`1390`个包，装好直接`reboot`\n  * 这时候创建普通用户的时候，千万不要创建`oracle`用户，否则之后会有很多麻烦\n\n### Oracle Linux Configuration（important and complicated）\n**下面讲的主要是和两个文档不同的地方（依然以`link1`为主线）：**\n\n**1.** `selinux`需要`dsiable`\n\n**2.** `/etc/sysctl.conf`参照`link2`，`kernel.shmmax = 536870912`（这行参照`link2`）\n\n**3.** `/etc/security/limits.conf`\n```\n  oracle soft nproc 16384\n  oracle hard nproc 16384\n  oracle soft nofile 4096\n  oracle hard nofile 65536\n  oracle soft stack 10240\n```\n\n**4.** `oracle linux 6.5`光盘下载包（我是通过`yum`安装,没有安装的大部分都是系统已经安装了，少部分是没必要安装）\n* 需要安装：\n  * `ksh`\n  * `libaio-devel.i686`\n  * `libaio-devel.x86_64`\n  * `libaio.i686`\n  * `libstdc++.i686`\n  * `libstdc++-4.4.7-18.el6.i686`\n  * `compat-libstdc++-33.i686`\n* 不需要安装：\n  * `numactl-devel-2*x86_64*`\n\n> 由于我本地的是`libstdc++-4.4.7-4.el6.x86_64`，而安装的是`libstdc++-4.4.7-18.el6.i686`，版本不匹配会造成`yum`报错，而这个库作为众多库的依赖还不能删除，所以只能强行升级：`yum install libstdc++-4.4.7-18.el6.i686`，然后再安装`i686`文件\n\n**5.** 路径、权限与环境变量配置\n这一步参照: [Automating Database Startup and Shutdown on Linux][6]\n\n**附上脚本和`~/.bash_profile`:**\n\n`start_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbstart $ORACLE_HOME\n```\n\n`stop_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbshut $ORACLE_HOME\n```\n\n`setenv.sh`：\n\n```\n# Oracle Settings\nexport TMP=/tmp\nexport TMPDIR=$TMP\n\nexport ORACLE_HOSTNAME=tonydb.localdomain\nexport ORACLE_UNQNAME=sampledb\nexport ORACLE_BASE=/home/oracle/app/oracle\nexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1\nexport ORACLE_SID=sampledb\n\nexport PATH=/usr/sbin:/usr/local/bin:$PATH\nexport PATH=$ORACLE_HOME/bin:$PATH\n\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib\nexport CLASSPATH=$ORACLE_HOME/jlib:$ORACLE_HOME/rdbms/jlib\n```\n\n`~/.bash_profile`最后一行追加： \n\n```\n. /home/oracle/scripts/setenv.sh\n```\n\n**6.** 安装`FTP`并上传`zip`文件（这一步有需求就执行，我是通过`windows`传输到`linux`上）\n> 安装包在页面最上方`Resource Link`的第二个`link`，有两个`zip`包，都是需要的，这就是`Oracle Database 11g R2`的包，下面就开始安装数据库了，离成功也就不远了\n\n### Oracle Database 11g R2 Installation\n**参照：** [Oracle Database 11g Release 2 Installation on RHEL/CentOS 6.x/5.x/4.x][7]，以这个`link`的步骤为主，`link1`和`link2`为辅\n\n1. 如果出现`next button`闪退的现象，进入`preference`里面调节屏幕分辨率即可\n\n2. `check`时候报错，如果你在上面安装了那么就`ignore`吧，因为这些包都安装了，只不过版本比它`check`的高：\n```\nlibaio-0.3.105\ncompat-libstdc++-33-3.2.3\nlibaio-devel-0.3.105\nlibgcc-3.4.6\nlibstdc++-3.4.6\nunixODBC-2.2.11\nunixODBC-2.2.11\nunixODBC-devel-2.2.11\nunixODBC-devel-2.2.11\npdksh-5.2.14\n```\n\n### Oracle Listener Configuration\n**参照：** [Oracle Listener][8]\n> 最后`test`不成功也没关系，先保存设置进行下一步，这边把`lsnrcetl`起起来就可以了，先不要进入`oracle`命令行，需要等下一步创建了数据库再进行`startup`等操作\n\n### 创建数据库\n> `dbca`进入创建向导，填写正确的`sid`，`sid`不用填`ORCL`,就用同步生成的就好，不同的数据库可以拥有不同的`sid`，按照默认的配置不断地`next`就好\n\n* 安装后的默认数据库为orcl，如需创建新的数据库\n  - 使用辅助工具(Linux UI mode)\n    > root@oracledb: xhost local:oracledb\n    > root@oracledb: su - oracle\n    > oracle@oracledb: dbca\n  - 使用em管理WebUI\n* 创建表空间\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create tablespace test_space datafile '/opt/oracle/oradata/test_data.dbf' size 500M\n  - 使用em管理WebUI\n* 创建用户    \n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create user test identified by test default tablespace test_space;\n  - 使用em管理WebUI\n* 授权用户权限\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant connect, resource to test;\n  - 使用em管理WebUI\n* 授权DBA权限（RMAN需要dba权限来运行）\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant sysdba to test;\n  - 使用em管理WebUI\n* 数据库启动/关闭\n  * 切换用户\n    > root@oracledb: su - oracle\n  * 启动监听\n    > lsnrctl start\n  * 启动实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > startup\n  * 关闭实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > shutdown immediate\n    > exit\n  * 关闭监听\n    > lsnrctl stop\n* 归档日志模式启动/关闭\n  * 启动归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database archivelog\n    > alter database open\n  * 关闭归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database noarchivelog\n    > alter database open\n\n作者： Tony\n\n日期： 2017/06/26\n\n  [1]: https://pan.baidu.com/s/1pL7ENn9\n  [2]: https://pan.baidu.com/s/1qYufAQo\n  [3]: https://pan.baidu.com/s/1kV3JpCr\n  [4]: https://wenku.baidu.com/view/afd5ca9e5022aaea998f0fa2.html\n  [5]: https://pan.baidu.com/s/1miDQEoC\n  [6]: https://oracle-base.com/articles/linux/automating-database-startup-and-shutdown-on-linux\n  [7]: https://www.tecmint.com/oracle-database-11g-release-2-installation-in-linux/\n  [8]: http://blog.csdn.net/haiross/article/details/13613931\n  [9]: http://ow0mgad6r.bkt.clouddn.com/oracle-eloqua-campaign-manager.png\n","tags":["Oracle 11g R2"],"categories":["tech"]},{"title":"Moodle Local插件","url":"/2017/09/09/Moodle-Local-Plugin/","content":"\n<center>![moodle][8]</center>\n\n`Local`插件在`Moodle`中相比于其他插件算是比较容易上手的一个插件，`Local`插件中前后台分离，不用遨游在杂糅前后端的代码里。学会`Local`插件是上手其他插件的前提，其实一旦学会`Local`插件也会发现其他插件不过是它的一种变种形式罢了，只要举一反三就能很快掌握。\n\n<!--more-->\n\n## Moodle Local 插件架构\n<center>![image][2]</center>\n\n## 前端\n### [Template][3]\n服务器配置了相应的`url`，所以每个插件的加载首页都是`index.php`，但是并不意味着页面的代码就在`index.php`中，首先在`index.php`中调用`renderer.php` 中相应的页面渲染方法，例如  `render_index_page($page) ` 方法：\n\n```\npublic function render_index_page($page) {\n$data = $page->export_for_template($this);\n    render_from_template('local_xxx/index', $data)\n}\n```\n\n* 在这个方法中，首先调用了`index_page`这个类中的 `export_for_template` 方法，获得页面静态加载需要的数据，例如`mustache`模板需要的数据，`data`数组的索引与模板名称一一对应，例如`mustache`文件中若有 &#123;&#123;`#user`&#125;&#125;`hello!`&#123;&#123;`/user`&#125;&#125;，那么对应 class 一定存在 `$data['user'] = xxxx`；\n* 以 `render_from_template('local_xxx/index', $data)` 为例， `local_xxx/index`，`template`目录下必定有文件名为`index.mustache`的文件，这是`moodle` 封装好的命名规范，而`mustache`文件就是页面展示的代码所在了；\n* 通过&#123;&#123;`#js`&#125;&#125;`require xxx`&#123;&#123;`/js`&#125;&#125;引入对应的`JavaScript`文件；\n\n\n### [CSS][4]\n每个 local 插件中都存在一个`style.css`这样的一个`css`样式文件，系统运行时，会将每个模块的`css`文件内容合并到一起，所以如果有两个名称一样的`css`，则会发生覆盖的现象，所以建议起名字统一在最前面加上当前的插件名称，要注意的是，每次新增、修改或者删除`css`文件内容后，如若需要立即查看效果，必要以`admin`角色进入系统，然后**清空缓存**；\n\n### [JavaScript][5] \n目前系统中的`js`是基于`RequireJS`框架，遵循`AMD`开发规范，利用`Grunt`构建的。`amd`目录下有`src`目录和`build`目录，`src`下的`xxx.js`是供开发时编写调试使用，而`build`目录下的`min.js`则是网站实际运行时所使用的`js`文件，经过压缩可以大幅度提升运行速度。所以每次修改`js`完毕后，提交代码切勿忘记生成对应的`min.js`文件并提供，生成命令为 `grunt uglify`；\n\n## 后端\n### [db][6]\n1. access.php\n主要负责记录权限的配置；\n\n2. service.php\n负责接收`ajax`提交，配置对应的后台`API`的类名、路径 和方法名等信息，并将对应的`ajax`方法存入数据库中；\n\n3. install.xml\n用于安装系统时表结构的对应生成；\n\n4. install.php\n用于安装系统时数据库的缺省数据的提供；\n\n5.  upgrade.php [开发规范][7]\n用于系统表结构变化等版本升级事项的实现，由于生产环境中客户已有大量数据存在于数据库中，所以这时候不能采取重装系统的方式更改表结构或者更改数据库等操作，这时候就得借助`upgrade.php`平滑升级。\n\n### 后台 API\n1. externallib.php\n负责编写`service.php`中`ajax`提交方法对应的后台方法，并调用`lib.php`中的`API`实现；\n\n2. lib.php\n负责方法的具体实现，代码与数据库的数据交互\n\n3. lang/en/xxx.php 和 lang/zh_cn/xxx.php\n\n`en`目录下的表示英文翻译语言文件，`zh_cn`目录下的表示中文翻译语言文件，具体调用方法为： \n\n* `php`中： `get_string(字符串名, 插件名)`\n* `mustache`中： &#123;&#123;`#str`&#125;&#125;`字符串名, 插件名`&#123;&#123;`/str`&#125;&#125;\n\n作者：Tony\n2017年02月28日\n\n[1]: https://www.processon.com/view/link/58b50108e4b02ba7e0885137\n[2]: http://on-img.com/chart_image/58b5173ce4b01d80b64d335e.png \n[3]: https://docs.moodle.org/dev/Templates\n[4]: https://docs.moodle.org/dev/CSS\n[5]: https://docs.moodle.org/dev/Javascript_Modules\n[6]: https://docs.moodle.org/dev/Data_definition_API\n[7]: https://note.youdao.com/share/?token=18D5479D0AFA4286A46EACE741345DF5&gid=29389535\n[8]: http://ow0mgad6r.bkt.clouddn.com/moodle-logo-concept.png\n","tags":["PHP"],"categories":["tech"]}]