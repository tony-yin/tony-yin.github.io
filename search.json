[{"title":"为什么 crontab 不执行","url":"/2018/10/29/Why-Crontab-Not-Work/","content":"\n<center>![Why Crontab Not Work][5]</center>\n\n作为`linux`中的定时任务工具，`crontab`被广大开发者所热爱和使用。该技术由来已久，相当成熟，但是在真正使用的时候会时不时地发现为什么`crontab`脚本没有按照预期那样执行？本文以本周笔者遇到一个`crontab`不能运行的问题为引子，详细地介绍为什么`crontab`不运行的各种原因。\n\n<!--more-->\n\n## 引子\n\n本周遇到一个`crontab`不能执行的问题，发现原因后觉得甚是有趣。\n\n笔者通过一个`python`脚本向`/etc/cron.d`目录下的一个文件写入定时任务命令，每分钟调用一个脚本，调用的这个脚本是个`python`文件，然后发现`cron`并没有按照预期每分钟执行一次。然后笔者就将原定时任务脚本`aaa`拷贝了一份，并重新命名为`bbb`，然后将定时任务中调用脚本改成了执行一个简单的`echo`命令，然后保存退出，发现`bbb`是可以正常定时运行的，这时候，笔者就通过`file`命令想比较一下这两个文件有何不同：\n\n```\n[root@tony cron.d]# file *\naaa:     ASCII text, with no line terminators\nbbb:     ASCII text\n```\n\n这个时候我们可以发现`aaa`文件出现了比较奇怪的标识：\n\n```\nwith no line terminators\n```\n\n显而易见，这是在说`cron`脚本中定时命令没有行终止符，导致这个问题是因为该`cron`脚本由`python`代码生成时没有添加换行符：\n\n```\nwith open('/etc/cron.d/aaa', 'w') as f:\n    f.write('xxx')\n```\n\n然后笔者尝试性地在`aaa`文件中在定时命令下新增一行后，发现定时任务可以正常运行了。不得不说，这是一个很有意思的问题，`crontab`居然会因为一个换行符导致定时任务的不运行，后来`google`了一下发现，`crontab`的确存在这个机制，具体解释下面会提到。\n\n在`google`的同时，在`ask unbuntu`上发现了这篇文章：[《Why crontab scripts are not working?》][3]，里面很多开发者罗列了他们遇到`cron`不能正常运行的各种因素，笔者大致浏览了下，发现有遇到过，也有很多并不知道的，所以想把这些因素和解决方案一一罗列下来。\n\n## 因素\n\n### 因素1：环境变量\n\n#### 场景及原因\n\n`cron`中的环境变量和系统的环境变量是不一样的，我们可以通过设置定时脚本将`cron`中的环境变量打印出来：\n\n```\n* * * * * env > /tmp/env.output\n```\n\n可以看到`cron`中的环境变量：\n\n```\nXDG_SESSION_ID=12952\nSHELL=/bin/sh\nUSER=root\nPATH=/usr/bin:/bin\nPWD=/root\nLANG=en_US.UTF-8\nSHLVL=1\nHOME=/root\nLOGNAME=root\nXDG_RUNTIME_DIR=/run/user/0\n_=/usr/bin/env\n```\n\n查看系统的环境变量：\n\n```\n[root@tony cron.d]# env\nXDG_SESSION_ID=1140\nHOSTNAME=tony\nTERM=xterm-256color\nSHELL=/bin/bash\nHISTSIZE=1000\nUSER=root\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin\nMAIL=/var/spool/mail/root\nPWD=/etc/cron.d\nLANG=en_US.UTF-8\nTMUX_PANE=%18\nHISTCONTROL=ignoredups\nSHLVL=2\nHOME=/root\nLOGNAME=root\n_=/usr/bin/env\nOLDPWD=/root\n```\n\n我们可以看到`cron`中的环境变量很多都和系统环境变量不一样（`cron`会忽略`/etc/environment`文件），尤其是`PATH`，只有`/usr/bin:/bin`，也就是说在`cron`中运行`shell`命令，如果不是全路径，只能运行`/usr/bin`或`/bin`这两个目录中的标准命令，而像`/usr/sbin`、`/usr/local/bin`等目录中的非标准命令是不能运行的。\n\n这个问题笔者也遇到很多次，所以很多非标准命令都选择了全路径，但是这个方法也有问题，因为不同环境的命令所存在的目录是不一样的。\n\n#### 解决方案\n\n**方案1：**\n\n在`cron`脚本文件头部声明`PATH`\n\n```\n#!/bin/bash\nPATH=/opt/someApp/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n# rest of script follows\n```\n\n**方案2：**\n\n在定时脚本调用的脚本头部声明`PATH`\n\n```\nPATH=/opt/someApp/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n15 1 * * * backupscript --incremental /home /root\n```\n\n\n### 因素2：换行符\n\n#### 场景及原因\n\n这个因素就是笔者引子中提到的，官方解释（`man crontab`）如下：\n\n```\nAlthough cron requires that each entry in a crontab end in a newline character, neither the crontab command nor the cron daemon will detect this error. Instead, the crontab will appear to load normally. However, the command will never run. The best choice is to ensure that your crontab has a blank line at the end.\n\n4th Berkeley Distribution      29 December 1993     CRONTAB(1)\n```\n\n简单翻译一下就是：\n\n尽管`crontab`要求`cron`中的每个条目都要以换行符结尾，但`crontab`命令和`cron`守护进程都不会检测到这个错误。相反，`crontab`将正常加载。然而，命令永远不会运行。最好的选择是确保您的`crontab`在末尾有一个空白行。\n\n#### 解决方案\n\n给`cron`中每个条目下面添加一个空行\n\n**注意：**\n\n除了没了换行符会导致`cron`中的命令不会运行，即引子中所标识：\n\n```\nwith no line terminators\n```\n\n但是因为非`linux`操作系统导致的非`\\n`换行符同样会导致该问题，比如`windows`的`^M`、`mac`的`\\r`等\n\n```\nwith CR line terminators\n```\n\n**解决方案：**\n\n`windows`的话就通过`dos2unix`命令转换；而`mac`则可以通过`mac2unix`来转换，`mac2unix`也是`dos2unix`软件中的一部分\n\n#### Refer\n\n* [remove CR line terminators][4]\n\n### 因素3：crond 服务\n\n#### 场景及原因\n\n很多时候`crond`服务未开启，也会导致定时任务不会正常执行。\n\n#### 解决方案\n\n查看服务是否运行，如果未运行，启动`crond`服务即可。\n\n查看方式有两种：\n\n1.通过进程查看\n\n`pgrep`相当于`ps -ef | grep`\n\n```\npgrep cron\n```\n\n2.通过`service`查看\n\n```\nservice crond status\n```\n\n启动服务：\n\n```\nservice crond start\n```\n\n### 因素4：shell 解释器\n\n#### 场景及原因\n\n从因素`1`就知道`cron`环境变量中的`SHELL`是`sh`而不是`bash`，我们知道很多`shell`命令是可以在`bash`中正常运行，但是不能在`sh`中运行的，所以这个因素也会影响定时任务的正常运行。\n\n#### 解决方案：\n\n**方案1：**\n\n将`cron`中需要执行的命令在`sh`中执行确认\n\n**方案2：**\n\n将`cron`中需要执行的命令外面加一个`bash shell`的封装：\n\n```\nbash -c \"mybashcommand\"\n```\n\n**方案3：**\n\n修改`cron`中的`SHELL`环境变量的值，让所有命令都用`bash`解释器：\n\n```\nSHELL=/bin/bash\n```\n\n**方案4：**\n\n如果定时任务执行的命令是`shell`脚本，只要在脚本内添加`bash`解释器：\n\n```\n#!/bin/bash\n```\n\n### 因素5：时区\n\n#### 场景及原因\n\n当修改系统时区后，无论是之前已经存在的`cron`还是之后新创建的`cron`，脚本中设置的定时时间都以旧时区为准，比如原来时区是`Asia/Shanghai`，时间为`10:00`，然后修改时区为`Europe/Paris`，时间变为`3:00`，此时你设置`11:00`的定时时间，`cron`会在`Asia/Shanghai`时区的`11:00`执行。\n\n#### 解决方案：\n\n**方案1：**\n\n重启`crond`服务\n\n```\nservice crond restart\n```\n\n**方案2：**\n\n`kill crond`进程，因为`crond`进程是可重生的\n\n### 因素6：百分号%\n\n#### 场景及原因\n\n当`cron`定时执行命令中，有百分号并且没有转义的时候，`cron`执行会出错，比如执行以下`cron`：\n\n```\n0 * * * * echo hello >> ~/cron-logs/hourly/test`date \"+%d\"`.log\n```\n\n会有如下报错：\n\n```\n/bin/sh: -c: line 0: unexpected EOF while looking for matching ``'\n/bin/sh: -c: line 1: syntax error: unexpected end of file\n```\n\n有的日志也会有如下报错：\n\n```\n(echo) ERROR (getpwnam() failed)\n```\n\n`crontab manpage`中解释：\n\n```\nThe \"sixth\" field (the rest of the line) specifies the command to be run. The entire command portion of the line, up to a newline or % character, will be executed by /bin/sh or by the shell specified in the SHELL variable of the cronfile. Percent-signs (%) in the command, unless escaped with backslash (\\), will be changed into newline characters, and all data after the first % will be sent to the command as standard input.\n```\n\n即`cron`中换行符或`%`前的命令会被`shell`解释器执行，但是`%`会被认为新一行的字符，并且`%`后所有的数据都会以标准输出的形式发送给命令。\n\n#### 解决方案\n\n为百分号做转义，即在`%`前添加反斜杠`\\`\n\n#### Refer\n\n* [Cron and Crontab usage and examples][1]\n* [How can I execute date inside of a cron tab job?][2]\n\n\n### 因素7：密码过期\n\n#### 场景及原因\n\n`Linux`下新建用户密码过期时间是从`/etc/login.defs`文件中`PASS_MAX_DAYS`提取的，普通系统默认就是`99999`，而有些安全操作系统是`90`。更改此处，只是让新建的用户默认密码过期时间变化，已有用户密码过期时间仍然不变。\n\n当用户密码过期也会导致`cron`脚本执行失败。\n\n#### 解决方案\n\n将用户密码有效期设置成永久有效期或者延长有效期\n\n**方案1：**\n\n```\nchage -M <expire> <username>\n```\n\n**方案2：**\n\n```\npasswd -x -1 <username>\n```\n\n**方案3：**\n\n手动修改`/etc/login.defs`文件中`PASS_MAX_DAYS`的值\n\n### 因素8：权限\n\n#### 场景及原因\n\n很多时候解决方案都是采用`root`用户执行`cron`，但是有时候这并不是一个很好的方式。如果采用非`root`用户执行`cron`，需要注意很多权限问题，比如`cron`用户对操作的文件或目录是否存在权限等。\n\n如果权限不够，`cron`会拒绝执行：\n\n```\nsudo service cron restart\ngrep -i cron /var/log/syslog|tail -2\n2013-02-05T03:47:49.283841+01:00 ubuntu cron[49906]: (user) INSECURE MODE (mode 0600 expected) (crontabs/user)\n```\n\n#### 解决方案\n\n```\n# correct permission\nsudo chmod 600 /var/spool/cron/crontabs/user\n# signal crond to reload the file\nsudo touch /var/spool/cron/crontabs\n```\n\n### 因素9：不同平台\n\n#### 场景及原因\n\n一些特殊选项各个平台支持不一样，有的支持，有的不支持，例如`2/3`、`1-5`、`1,3,5`\n\n#### 解决方案\n\n需要针对不同平台做兼容性测试\n\n### 因素10：不同 cron\n\n#### 场景及原因\n\n将之前运行的`Crontab Spec`在从一个`Crontab`文件移动到另一个`Crontab`文件时可能会崩溃。有时候，原因是你已经将`Spec`从系统`crontab`文件转移到用户`crontab`文件，反之亦然。\n\n`cron`分为系统`cron`和用户`cron`，用户`cron`指`/var/spool/cron/username`或`/var/spool/crontabs/crontabs/username`，系统`cron`指\n`/etc/crontab`以及`/etc/crontab`，这两者是存在部分差异的。\n\n系统`crontab`在命令行运行之前有一个额外的字段`user`。这会导致一些错误，比如你将`/etc/crontab`中的命令或者`/etc/cron.d`中的文件移动至用户`crontab`会报错如下：\n```\ngeorge; command not found\n```\n相反，当发生相反的情况时，`cron`将显示`/usr/bin/restartxyz is not a valid username`之类的错误。\n\n#### 解决方案\n\n当共享系统`cron`或用户`cron`时，注意用户的添加和删除。\n\n### 因素11：crontable 变量\n\n#### 场景及原因\n\n虽然你可以在`crontable`里面声明环境变量，但是在下面这种情况定时任务是不会执行的：\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=${SOME_LOG}/some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=${BIN_DIR}/some_executable_file\n\n0 10 * * * ${MY_EXE} some_param >> ${MY_LOG_FILE}\n```\n\n这是因为在`crontable`里面只能声明变量，不能对变量进行操作或者执行其他任何`shell`命令的，所以上述的`shell`字符串拼接是不会成功的，所以只能声明变量，然后在命令中引用变量。\n\n#### 解决方案：\n\n**方案1：** \n\n直接声明变量\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=/var/log/some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=/usr/local/bin/some_executable_file\n\n0 10 * * * ${MY_EXE} some_param >> ${MY_LOG_FILE}\n```\n\n**方案2：**\n\n声明多个变量，在命令中引用拼接\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=some_executable_file\n\n0 10 * * * ${BIN_DIR}/${MY_EXE} some_param >> ${SOME_DIR}/${MY_LOG_FILE}\n```\n\n### 因素12：GUI\n\n#### 场景及原因\n\n如果你的`cronjob`调用了相关`GUI`应用时，你需要告诉它们应该使用什么`DISPLAY`环境变量，从因素`1`我们可以知道`cron`中的环境变量是和系统环境变量不一样的，`DISPLAY`同样如此，比如\n\n```\nFirefox launch with cron.\n```\n\n#### 解决方案\n\n声明`DISPLAY=:0`\n\n```\n* * * * * export DISPLAY=:0 && <command>\n```\n\n## 总结\n\n目前主要总结了影响`cron`运行的`12`种因素，当然肯定还存在其他影响因素，本文将持续更新，希望这些坑能够被广大开发者所熟知。\n\n大家如果有上述以外导致`cron`不能正常运行的因素可以在博客下方留言，或者在`Github`上面提`pr`，笔者已经将本文在`Github`上面创建了一个仓库，让我们一起不断完善吧 -。-\n\n`Github`仓库地址：https://github.com/tony-yin/Why-Cronjob-Not-Work\n\n\n[1]: http://www.pantz.org/software/cron/croninfo.html\n[2]: https://unix.stackexchange.com/questions/29578/how-can-i-execute-date-inside-of-a-cron-tab-job\n[3]: https://askubuntu.com/questions/23009/why-crontab-scripts-are-not-working?page=1&tab=votes#tab-top\n[4]: https://stackoverflow.com/questions/14080306/remove-cr-line-terminators\n[5]: http://ow0mgad6r.bkt.clouddn.com/Why-Crontab-Not_Work.png\n","tags":["Crontab"],"categories":["tech"]},{"title":"Daily Article Vol7(2018/6/1 ~ 2018/6/30)","url":"/2018/07/01/Daily-Article-Vol7/","content":"\n<center>![6][51]</center>\n\n这是`Daily Article`系列文章的第七篇，主要记录了`2018/6/1 ~ 2018/6/30`之间的阅读记录。\n\n<!--more-->\n\n六月份主要做了几件事，一个是基于`python`实现一个守护进程，从`unix`实现守护进程的底层原理撸了一遍，加深了对`linux`底层进程一些概念的了解，正好也接触了“骑着企鹅采树莓”系列文章，讲的很不错，通俗易懂；然后把之前基于`pci path`寻找逻辑磁盘的物理位置整理了下，研究了一波块设备持久化命名；然后学习了下`kvm`虚拟化技术，并在此之上研究了下虚拟机迁移技术，这其中包括虚拟机到虚拟机的迁移，`kvm`虚拟机到`vmware`虚拟机的迁移，虚拟机到物理机的迁移，物理机到物理机的迁移等等；最后研究了下多路径设备，概念有点混乱，不断地啃资料终于有点明白了。不得不说，这些底层的知识真是又多又复杂，只能慢慢啃，加油咯。\n\n1. [understanding /dev/disk/by- folders][1](6/1) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n2. [PERSISTENT NAMING][2](6/2) <i class='fa fa-star'></i>\n3. [Understanding Device Names Managed by the udev mechanism][3](6/3) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n4. [Persistent block device naming][4](6/4) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n5. [udev (简体中文)][5](6/9) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n6. [How to find WWN for /dev/sdc][6](6/9)\n7. [SSO单点登录的实现原理是怎样的][7](6/10)\n8. [可爱的 Python：Curses 编程][8](6/10)\n9. [Django cookie和session][9](6/10)\n10. [这也许是你看到过的最通俗易懂的ElasticSearch文章了（理论篇）][10](6/10) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n11. [饿了么在ELasticsearch自动化运维平台和监控平台的应用实践][11](6/10)\n12. [ACCIDENTALLY ADDING A GIT SUBMODULE][14](6/11)\n12. [聊聊MySQL、HBase、ES的特点和区别][12](6/12) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n13. [行存储和列存储][13](6/13)\n14. [【极客时间：左耳听风】洞悉PaaS平台的本质][15](6/13) <i class='fa fa-star'></i>\n15. [【CoolShell】关于高可用的系统][16](6/13) <i class='fa fa-star'></i>\n16. [【极客时间：左耳听风】编程范式游记（1）- 起源][17](6/14)<i class='fa fa-star'></i>\n17. [python fork][19](6/15)\n18. [Python实例浅谈之五Python守护进程和脚本单例运行][20](6/16) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n17. [Python实现守护进程][18](6/18) <i class='fa fa-star'></i>\n18. [在Unix系统上面启动守护进程][21](6/19) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n19. [《APUE》读书笔记—第十三章守护进程][22](6/19) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n20. [【骑着企鹅采树莓】Linux进程基础][23](6/20) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n21. [【骑着企鹅采树莓】Linux进程关系][24](6/20) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n22. [【骑着企鹅采树莓】树莓派：一个关于教育的故事][25](6/21)\n23. [【骑着企鹅采树莓】树莓派心脏：ARM的逆袭][26](6/21)  <i class='fa fa-star'></i>\n23. [【骑着企鹅采树莓】树莓派的大脑：Linux][27](6/22)\n24. [Linux策略路由][28](6/22)\n26. [【迁移Linux系统1】如何迁移备份和裸机恢复Linux系统][30](6/24)\n27. [【迁移Linux系统2】利用 KIWI Imaging System 定制裸机恢复光盘）][40](6/26)\n27. [【虚拟机迁移技术漫谈1】如何在虚拟机和物理机以及虚拟机和虚拟机之间的迁移系统][31](6/24)\n25. [【虚拟机迁移技术漫谈2】KVM虚拟机在物理主机之间迁移的实现][29](6/24) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n26. [【虚拟机迁移技术漫谈3】迁移 VMware 虚拟机到 KVM][32](6/25) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n27. [【虚拟机迁移技术漫谈4】KVM 虚拟机在 IBM System x 上应用][33](6/25) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n28. [【虚拟机迁移技术漫谈5】在IBM集成虚拟化管理器上进行动态分区迁移][34](6/26)\n29. [利用Qemu Guest Agent (Qemu-ga) 实现 Openstack 监控平台][36](6/26)\n30. [利用qemu-guest-agent冻结文件系统][37](6/26)\n31. [kvm 快照][38](6/26)\n32. [How to enable QEMU guest agent in KVM][39](6/26)\n33. [DDOS 攻击的防范教程][45](6/27)\n34. [Libvirt 虚拟化库剖析][35](6/27) <i class='fa fa-star'></i>\n35. [IBM 动态分区迁移(Live Partition Mobility)技术-概述][46](6/27)\n36. [【骑着企鹅采树莓】Linux信号基础][47](6/27) <i class='fa fa-star'></i>\n37. [DM-MULTIPATH 概述][48](6/28)\n37. [Linux 内核中的 Device Mapper 机制][43](6/28) <i class='fa fa-star'></i>\n38. [Multipath 多路径配置实践心得][41](6/28)\n39. [multipath多路径实验01-构建iSCSI模拟环境][42](6/30) <i class='fa fa-star'></i>\n40. [multipath多路径实验02-配置多路径软件][49](6/30)\n41. [multipath 路径切换][50](6/30) <i class='fa fa-star'></i>\n\n[1]: https://unix.stackexchange.com/questions/86764/understanding-dev-disk-by-folders\n[2]: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/online_storage_reconfiguration_guide/persistent_naming\n[3]: http://fibrevillage.com/storage/578-understanding-device-names-managed-by-the-udev-mechanism\n[4]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)\n[5]: https://wiki.archlinux.org/index.php/Udev_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)#.E8.AE.BE.E7.BD.AE.E9.9D.99.E6.80.81.E8.AE.BE.E5.A4.87.E5.90.8D\n[6]: https://www.linuxquestions.org/questions/linux-server-73/how-to-find-wwn-for-dev-sdc-917269/\n[7]: http://www.cnblogs.com/songyaqi/p/4553069.html9.html/l-anaconda/index.html\n[8]: https://www.ibm.com/developerworks/cn/linux/sdk/python/python-6/index.html\n[9]: http://blog.51cto.com/830909/1751106\n[10]: https://www.jianshu.com/p/40ec55c6e614?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[11]: https://elasticsearch.cn/slides/109#page=21\n[12]: https://www.jianshu.com/p/4e412f48e820?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[13]: http://www.cnblogs.com/qiaoyihang/p/6262806.html\n[14]: https://humansky.com/2011/12/accidentally-adding-a-git-submodule/\n[15]: https://time.geekbang.org/column/article/1610\n[16]: https://coolshell.cn/articles/17459.html\n[17]: https://time.geekbang.org/column/article/301\n[18]: https://www.jianshu.com/p/e3f3d49093ca\n[19]: https://blog.csdn.net/asware/article/details/4159366\n[20]: https://blog.csdn.net/taiyang1987912/article/details/44850999\n[21]: http://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p14_launching_daemon_process_on_unix.html#id1\n[22]: https://yq.aliyun.com/articles/41477?spm=5176.100240.searchblog.39.HyGU9I\n[23]: http://www.cnblogs.com/vamei/archive/2012/09/20/2694466.html\n[24]: http://www.cnblogs.com/vamei/archive/2012/10/07/2713023.html\n[25]: http://www.cnblogs.com/vamei/p/6415434.html\n[26]: http://www.cnblogs.com/vamei/p/6424666.html\n[27]: http://www.cnblogs.com/vamei/p/6718942.html\n[28]: https://my.oschina.net/guol/blog/156607\n[29]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm2/index.html\n[30]: https://www.ibm.com/developerworks/cn/linux/l-cn-linux-br1/index.html\n[31]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm1/index.html\n[32]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm3/index.html\n[33]: https://www.ibm.com/developerworks/cn/linux/l-cn-kvm/\n[34]: https://www.ibm.com/developerworks/cn/aix/library/0808_shenlf_ivm/\n[35]: https://www.ibm.com/developerworks/cn/linux/l-libvirt/index.html\n[36]: http://www.cnblogs.com/biangbiang/p/3222458.html\n[37]: http://www.zoues.com/2015/10/13/qemu-guest-agent/\n[38]: http://blog.51cto.com/aishangwei/2124944\n[39]: https://access.redhat.com/solutions/732773\n[40]: https://www.ibm.com/developerworks/cn/linux/l-cn-linux-br2/index.html\n[41]: https://wsgzao.github.io/post/multipath/\n[42]: http://www.cnblogs.com/jyzhao/p/7200585.html\n[43]: https://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\n[44]: https://www.jianshu.com/p/3461c568b82f?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[45]: http://www.ruanyifeng.com/blog/2018/06/ddos.html\n[46]: https://www.ibm.com/developerworks/cn/aix/redbooks/live-mobility/\n[47]: http://www.cnblogs.com/vamei/archive/2012/10/04/2711818.html\n[48]: https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/dm_multipath/mpio_description\n[49]: http://www.cnblogs.com/jyzhao/p/7208620.html\n[50]: https://blog.csdn.net/jiaping0424/article/details/51591257\n[51]: http://ow0mgad6r.bkt.clouddn.com/6.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"块设备持久化命名和多路径","url":"/2018/06/20/Block-device-persistent-name-and-multipath/","content":"\n<center>![persistent name][12]</center>\n\n众所周知，我们看到的磁盘通常是`/dev/sda`，`/dev/sdb`这样的名称，我们可以通过这些磁盘名称对磁盘进行各种操作，例如挂载，分区和格式化等等。但是，如果你的机器上有不止一个`SATA`，`SCSI`或`IDE`磁盘控制器，那么它们所对应的设备节点将会依随机次序添加。这样就可能导致每次引导时设备的名字如`/dev/sda`与`/dev/sdb`互换了，再比如硬盘拔插导致磁盘乱序等等，最终导致系统不可引导、`kernel panic`、或者设备不可见。持久化命名可以解决这些问题。\n\n<!--more-->\n\n## 概述\n\n持久化命名，顾名思义即一次性或者是短暂的命名，它是一种长久的并且稳定靠谱的命名方案。与之形成鲜明对比的就是`/dev/sda`这种非持久化命名，这两种命名方案各有各的用处，本文着重对持久化命名进行介绍。持久化命名方案有四种：`by-label`、`by-uuid`、`by-id`和`by-path`。对于那些使用`GUID`分区表（`GPT`）的磁盘，还有额外的两种方案：`by-partlabel`和`by-partuuid`。你也可以使用`Udev`静态设备名方案，这个我们就不作详细解释。下面我将对每种持久化命名方案进行详细的介绍和讲解，不难发现上面所提到的命名方式在`/dev`目录下都存在一个与之名字对应的文件夹：\n\n## by-label\n\n`label`表示标签的意思，几乎每一个文件系统都有一个标签。所有有标签的分区都在`/dev/disk/by-label`目录中列出。这个目录随着分区标签的变动而被动态地创建和销毁。\n\n```sh\n$ ls -l /dev/disk/by-label\n \ntotal 0\nlrwxrwxrwx 1 root root 10 May 27 23:31 DATA -> ../../sda3\nlrwxrwxrwx 1 root root 10 May 27 23:31 SYSTEM -> ../../sda2\n```\n\n标签必须是唯一的，标签是文件系统的一个属性，所以无法持久地表示单一磁盘阵列设备。\n\n**注意：**`label`是通过从设备中的内容（即数据）获取，所以如果将该内容拷贝至另一个设备中，我们也可以通过`blkid`来获取磁盘的`label`。\n\n## by-uuid\n\n`UUID`是给每个文件系统唯一标识的一种机制，这个标识是在分区格式化时通过文件系统工具生成，比如`mkfs`，这个唯一标识可以起到解决冲突的作用。所有`GNU/Linux`文件系统（包括`swap`和原始加密设备的`LUKS`头）都支持`UUID`。`FAT`和`NTFS`文件系统并不支持`UUID`，但是在`/dev/disk/by-uuid`目录下还是存在着一个更为简单的`UID`（唯一标识）。\n\n```\n$ ls -l /dev/disk/by-uuid/\ntotal 0\nlrwxrwxrwx 1 root root 10 May 27 23:31 0a3407de-014b-458b-b5c1-848e92a327a3 -> ../../sda2\nlrwxrwxrwx 1 root root 10 May 27 23:31 b411dc99-f0a0-4c87-9e05-184977be8539 -> ../../sda3\nlrwxrwxrwx 1 root root 10 May 27 23:31 CBB6-24F2 -> ../../sda1\nlrwxrwxrwx 1 root root 10 May 27 23:31 f9fe0b69-a280-415d-a03a-a32752370dee -> ../../sda4\n```\n\n使用`UUID`方法的优点是，名称冲突发生的可能性大大低于使用`Label`的方式。更深层次地讲，它是在创建文件系统时自动生成的。例如，即使设备插入到另一个系统(可能有一个标签相同的设备)，它仍然是唯一的。\n\n缺点是`uuid`使得许多配置文件(例如`fstab`或`crypttab`)中的长代码行难以读取和破坏格式。此外，每当一个分区被调整大小或重新格式化时，都会生成一个新的UUID，并且必须(手动)调整配置。\n\n## by-path\n\n该目录中的条目提供一个符号名称，该符号名称通过用于访问设备的**硬件路径**引用存储设备，首先引用`PCI hierachy`中的存储控制器，并包括`SCSI host`、`channel`、`target`和`LUN`号，以及可选的分区号。虽然这些名字比使用`major`和`minor`号或`sd`名字更容易，但必须使用谨慎以确保`target`号不改变在光纤通道`SAN`环境中(例如，通过使用持久绑定)，如果一个主机适配器切换到到一个不同的`PCI`插槽的话这个路径也会随之改变。此外，如果`HBA`无法探测，或者如果驱动程序以不同的顺序加载，或者系统上安装了新的`HBA`，那么`SCSI`主机号都有可能会发生变化。附带路径清单的一个例子是:\n\n```\n/dev/disk/by-path/pci-0000:03:00.0-scsi-0:1:0:0\n```\n\n也许还会带着分区号：\n\n```\n/dev/disk/by-path/pci-0000:03:00.0-scsi-0:1:0:0-part1\n```\n\n**注意：** 上面说了很多种情况都会导致`by-path`的值可能发生变化，但是在同一时间来说，`by-path`的值是和物理设备是唯一对应的，也就是说不管怎么说`by-path`是对应物理机器上面的某个位置的，根据`by-path`可以获取对应物理位置的设备。（此前`megaraid`通过逻辑磁盘获取物理磁盘位置就是根据这个原理）\n\n对于`iSCSI`设备，路径/名称映射从目标名称和门户信息映射到`sd`名称。\n应用程序通常不适合使用这些基于路径的名称。这是因为这些路径引用可能会更改存储设备，从而可能导致将不正确的数据写入设备。基于路径的名称也不适用于多路径设备，因为基于路径的名称可能被误认为是单独的存储设备，导致不协调的访问和数据的意外修改。\n\n此外，基于路径的名称是特定于系统的。当设备被多个系统访问时，例如在集群中，这会导致意外的数据更改。\n\n## by-id\n\n此目录中的条目提供一个符号名称，该符号名称通过唯一标识符(与所有其他存储设备不同)引用存储设备。标识符是设备的属性，但不存储在设备的内容(即数据)中。例如:\n\n```\n/dev/disk/by-id/scsi-3600508e000000000ce506dc50ab0ad05\n/dev/disk/by-id/wwn-0x600508e000000000ce506dc50ab0ad05\n```\n\n该`id`从设备的全局`ID`（`WWID`）或设备序列号中获取。`/dev/disk/by-id`条目也可能包含一个分区号。例如:\n\n```\n/dev/disk/by-id/scsi-3600508e000000000ce506dc50ab0ad05-part1\n/dev/disk/by-id/wwn-0x600508e000000000ce506dc50ab0ad05-part1\n```\n`World Wide Identifier`（`WWID`）可用于可靠的识别设备。`SCSI`标准要求所有`SCSI`设备提供一个持久的、系统无关的`ID`。`WWID`标识符保证对每个存储设备都是唯一的，并且独立于用于访问设备的路径。\n\n这个标识符可以通过发出`SCSI`查询来获取设备标识重要厂商数据(第`0x83`页)或单位序列号(第`0x80`页)。从这些`wwid`到当前`/dev/sd`名称的映射可以在`/dev/disk/by-id/`目录中维护的符号链接中看到。\n例如，具有页`0x83`标识符的设备将具有:\n\n```\nscsi-3600508b400105e210000900000490000 -> ../../sda\n```\n\n或者，具有页`0x80`标识符的设备将具有:\n\n```\nscsi-SSEAGATE_ST373453LW_3HW1RHM6 -> ../../sda\n```\n\n`Red Hat Enterprise Linux 5`自动维护从基于`wwid`的设备名称到系统上当前`/dev/sd`名称的正确映射。应用程序可以使用`/dev/ disk/by-id/`的链接引用磁盘上的数据，即使设备的路径改变，甚至当从不同系统访问该设备时都是如此。\n\n但是当设备被插入到硬件控制器的端口时，而这个端口又受另一个子系统控制（即多路径），`by-id`的值也会改变。多路径设备会在下面详细讲解。\n\n## by-partlabel && by-partuuid\n\n这两个和上面提到的`by-label`和`by-uuid`类似，只不过是在`GPT`磁盘上。\n\n## 多路径设备\n\n所谓多路径设备指的是从一个系统到一个设备存在多个路径，这种现象主要出现在光纤网络的`SAN`下，主要是做数据链路冗余以达到高可用的效果，**即对应底层一个物理设备，可能存在多个路径表示它**，具体参考`refer`下面有关文章。\n\n如果从一个系统到一个设备有多个路径，那么` device-mapper-multipath`使用`WWID`来检测它。然后在`/dev/mapper/wwid`中显示一个“伪设备”，例如`/dev/ mapper/3600508b400105df70000000ac0000`。\n\n`Device-mapper-multipath`显示映射到非持久标识符：`Host:Channel:Target:LUN`， `/dev/sd`名称，以及`major:minor`号。\n\n\n```\n3600508b400105df70000e00000ac0000 dm-2 vendor,product \n[size=20G][features=1 queue_if_no_path][hwhandler=0][rw] \n\\_ round-robin 0 [prio=0][active] \n \\_ 5:0:1:1 sdc 8:32  [active][undef] \n \\_ 6:0:1:1 sdg 8:96  [active][undef]\n\\_ round-robin 0 [prio=0][enabled] \n \\_ 5:0:0:1 sdb 8:16  [active][undef] \n \\_ 6:0:0:1 sdf 8:80  [active][undef]\n```\n \n`Device-mapper-multipath`在系统上自动维护每个基于`wwid`的设备名称和其对应的`/dev/sd`名称的正确映射。这些名称即使是在路径发生改变时也是持久的，并且当从不同的系统访问设备时它们仍然是一致的。\n\n## 总结\n\n为了方便管理和使用设备，`linux`操作系统给我们提供了上面这么多持久化命名方式。它们各自有各自的优势和使用场景。`by-label`和`by-uuid`都和文件系统相关，`by-label`是通过读取设备中的内容获取，`by-uuid`则是随着每次文件系统的创建而创建，所以`by-uuid`的持久化程度更高一些；持久化程度最高的要属`by-path`和`by-id`了，因为它们都是根据物理设备的位置或者信息而和链接做对应的，`by-path`会因为路径的变化而变化；而`by-id`则不会因为路径或者系统的改变而改变，它只会在多路径的情况下发生改变。这两个在通过虚拟设备名称寻找物理设备的场景下都十分有用。\n\n多路径设备则帮助我们在`SAN`等场景下提高了数据传输的可用性，目前由于网络带宽的发展，它在`iscsi`场景下也频繁亮相。\n\n## Refer\n\n* [Persistent block device naming][1]\n* [Understanding Device Names Managed by the udev mechanism][2]\n* [Udev][3]\n* [PERSISTENT NAMING][4]\n* [How to find WWN for /dev/sdc][5]\n* [understanding /dev/disk/by- folders][6]\n* [multipath 路径切换][7]\n* [multipath多路径实验01-构建iSCSI模拟环境][8]\n* [multipath多路径实验02-配置多路径软件][9]\n* [Linux 内核中的 Device Mapper 机制][10]\n* [DM-MULTIPATH 概述][11]\n\n[1]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_%28简体中文%29\n[2]: http://fibrevillage.com/storage/578-understanding-device-names-managed-by-the-udev-mechanism\n[3]: https://wiki.archlinux.org/index.php/Udev_%28简体中文%29#.E8.AE.BE.E7.BD.AE.E9.9D.99.E6.80.81.E8.AE.BE.E5.A4.87.E5.90.8D\n[4]: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/online_storage_reconfiguration_guide/persistent_naming\n[5]: https://www.linuxquestions.org/questions/linux-server-73/how-to-find-wwn-for-dev-sdc-917269/\n[6]: https://unix.stackexchange.com/questions/86764/understanding-dev-disk-by-folders\n[7]: https://blog.csdn.net/jiaping0424/article/details/51591257\n[8]: http://www.cnblogs.com/jyzhao/p/7200585.html\n[9]: http://www.cnblogs.com/jyzhao/p/7208620.html\n[10]: https://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\n[11]: https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/dm_multipath/mpio_description\n[12]: http://ow0mgad6r.bkt.clouddn.com/fork-in-the-road.png\n","tags":["Disk"],"categories":["tech"]},{"title":"Python Snack 最佳实践","url":"/2018/06/03/Python-Snack-Best-Practice/","content":"\n<center>![python snack][3]</center>\n\n  了解`linux`的人应该听说过`Newt`，`Newt`是一个为`RedHat`安装程序而设计的基于文本的窗口开发工具，它是由`c`语言编写并不依赖`X`包，`linux`下的`dialog`和`whiptail`都是基于它。而我们今天讨论的`snack`则是`Newt`提供的`python`接口，`redhat`的系统都自带这个模块，本文就如何使用`snack`制作伪终端页面展开讲解，并配合代码展示实现效果。\n\n<!--more-->\n  \n  \n## 用餐介绍\n  \n为啥说是最佳实践呢？因为我使用`snack`的过程中，上网查阅相关资料，发现有关信息甚少。偶尔几篇文章都是处于`API`或者`Demo`的级别，并且讲的都不全，更别说高级扩展功能了。我正好工作需要给我们的一个系统做一个终端部署控制台`UI`，所以我就使用了`python snack`来实现，期间不断新需求，不断迭代，从基本页面到增删改查，再到校验、再到配置导入、再到进度条等等，不断的迭代开发让我对`snack`不断地加深认知，它支持的或不支持的我都想办法一一解决，所以在这把我这段时间的收货进行总结并分享给需要的人。\n\n## 上菜单\n\n本文实践的需求是做一个部署控制台工具，该工具主要分为三个阶段：基础配置、高级配置和部署进度。基础配置页面需要我们创建一些主机，填写一些主机的信息，比如`IP`、`Hostname`和`Password`，然后高级配置我们也需要创建一些主机，不过我们可以复用基础设置的主机，所以我们的工具要支持在高级配置中导入基础配置的功能，在高级配置中我们还有一个全局配置，也就是不限于单个主机的配置（其中具体部署原理和是非，我就不多展开赘述，这不是本文的重点）。最后就是进度条页面，我们可以展示部署的过程阶段和相关时间信息。\n\n<center>![deploy console][1]</center>\n\n> 项目地址：https://github.com/tony-yin/Best-practice-of-python-snack\n\n## 开胃凉菜\n\n### 远古时代\n\n先上几道凉菜，给大家开开胃。所谓的凉菜就是介绍一下`python snack`的基础组件，基础组件很多，类似`html`，主要有：\n\n* Textbox\n* TextboxReflowed\n* Button\n* Compactbutton\n* Checkbox\n* Listbox\n* SingleRadioButton\n* Scale\n* Entry\n\n然后就是一些组合组件，也就是基于上述基础组件封装而得到，主要有：\n\n* RadioBar\n* ButtonBar\n* CheckboxTree\n\n上面这些组件就是所有的基础组件（组合组件也算基础组件），这些组件最终呈现还需要`grid`和`form`这两个组件，`grid`表示“网格”的意思，跟`html`中的`table`类似，由行和列组成，我们的基础组件需要放在网格中来实现页面布局；而`form`也类似“表单”，我们需要把`grid`填充到`form`中，运行后，就可以看到图形化页面了。\n\n### 工业革命\n\n经过上面基础组件的介绍，想必你对`snack`的组件有了充分的了解，这时候你可以参考文末的`refer`做几个小`demo`，做了之后你会发现页面是出来了，emmm... 可是感觉好繁琐哦，很多重复性的代码，而且页面布局也怪怪的，如果要把布局搞好，又需要加很多代码。\n\n我们把用基础组件的阶段称之为“远古时代”，每做一个`window`，都得一瓦一砖地慢慢堆砌，这样效率太低了，所以我们急需一波“工业革命”来提高生产力。\n\n`python snack`似乎考虑到了这个问题，它在上述基础组件之外还提供了`dialog`相关组件，`dialog`组件即集大成者，一个`dialog`组件就是一个`window`，也就是我们上面所说的`form`，并且该`form`中填充了必需的各种基础组件，`dialog`组件主要有：\n\n* ListboxChoiceWindow\n* ButtonChoiceWindow\n* EntryWindow\n\n### 返璞归真\n\n当今社会，大家吃惯了大鱼大肉，反而更是想念农村的野味。同理，我们用惯了“工业革命”的产物，发现虽然可用，但是仅仅停留在基础可用级别上，想换换样式，加加自己的定制化需求，都是有限的，完全达不到新需求的技术实现要求。所以，我们不能只知道用别人实现的现成的产物，我们可以尝试着“返璞归真”一下，回归最初的“远古时代”，自己实现一把“工业革命”。所谓的“dialog”组件无非也就是基础组件的封装而已，我们也可以自己实现一套自己的组件库，这个在前端是非常流行的，例如`font-awesome`、`iview`、`ant-design`等等。这里我们自己实现了以下`dialog`：\n\n* ExtButtonChoiceWindow\n* ExtAlert\n* ExtCheckboxWindow\n* ExtListboxChoiceWindow\n* ExtEntryWindow\n* ExtPwdEntryWindow\n* ExtProgressWindow\n* ExtTextWindow\n\n扩展的功能主要有：\n\n* 热键支持扩展\n* 按钮样式扩展\n* 布局大小自动化扩展\n* 暗文输入框扩展\n* 弹出窗口扩展\n* 进度条窗口信息展示扩展\n* 动态展示扩展\n\n> 扩展组件库地址：[widget extend library][2]\n\n## 管饱正菜\n\n凉菜不够，正菜来凑。上面就是把`python snack`的`API`罗列了一下，做个小`Demo`还行，但是距离产品化还很远，接下来我结合我做部署控制台工具的实践经历分享一下几个“正菜”，必须够硬，不接受反驳，不接受批评， O(∩_∩)O ~\n\n### 热键\n\n`python snack`提供了两种帮助用户使用的途径，一种是窗口下方的操作提示栏，另一个就是热键了。热键就是快捷键，比如我们可以敲击键盘上面的`ESC`键实现页面的返回。我们可以通过调用`grid`的`runOnce`接口获取热键的输入，例如`hotkey = g.runOnce()`，然后我们根据`hotkey`的值进行判断并执行对应的操作。\n\n### 页面切换\n\n当我们存在多个页面的时候，我们需要页面切换的功能，翻阅文档，并没有发现提供类似的功能。在我们的工具中，页面切换主要有两种方式，一种是点击`button`，一种是热键，既然没有原生的页面切换接口，我们就根据触发方式手动切换页面。比如我们想实现页面`1`点击`next`按钮想跳转页面`2`，那我们只需要获取`button`的返回值，判断是否为`next`，如果是`next`，直接调用页面`2`的方法即可，热键同理，即判断热键内容是否为对应热键。\n\n```\nret, button, lb = ExtListboxChoiceWindow(\n    screen, \n    'Distribute Storage Config',\n    'Distribute Storage Config',\n    ips,\n    buttons=(\"prev\", \"next\", \"exit\"),\n    width=50,\n    height=5,\n)                                                                                                                                        \nif button == \"exit\" or ret == \"ESC\":\n    screen.finish()\nelif button == \"prev\":\n    Welcome_Deploy_Window()\nelif button == \"next\":\n    Additional_Config_Window()\nelif lb is not None:\n    Basic_Host_Window(lb)\n```\n\n### 增删改查\n\n增删改查永远是一个软件系统绕不开的基础功能。\n\n**“查”：**\n\n首先是整体查看，我们可以通过一个列表展示所有信息，这时候我们可以用`ExtListboxChoiceWindow`组件来实现；然后就是单个查看了，我们可能有多条信息，我们想查看单个信息的详细内容时，我们可以通过点击具体的`item`进入详细信息的`dialog`，如何实现呢？`listbox`中有一个`current`的概念，也就是`listbox`中每个`li`的唯一标识，我们可以用列表的`index`来填充，因为往往列表页面的信息也无非是数组或者是列表的方式，我们获取到当前的`current`，即获取到数组的索引，然后就是根据索引查值了，我们再调用新增页面，将查到的值赋值到`Textbox`即可，`Textbox`有一个`setText`就是做这个事情的。当然我们的`ExtEntryWindow`组件也可以做到赋值填充。请参考上述代码中的`lb`，其实就是`listbox`的`li.current()`接口。\n\n**“增”：**\n\n我们可以通过一个新增按钮或者`listbox`中的一个`li`作为新增按钮来触发新增操作，点击后出现一个`dialog`，`dialog`中有一些`Textbox`、`Radio`、`Checkbox`等。\n\n```\ndef Basic_Host_Window(current, data=None):\n    buttons = [ 'save', 'cancel', 'exit']\n    if not data:\n        data = ['IP Address:', 'Hostname:', 'Password:']\n        if current != 'add':\n            data = get_format_data(Basic_Config[current], BASIC_TYPE)\n            buttons.insert(1, 'Delete')\n\n    host = ExtEntryWindow(\n        screen,\n        '{} host'.format('Add' if current == 'add' else 'Edit'),\n        'Please fill storage host info.',\n        data,\n        width = 40, \n        entryWidth = 40, \n        buttons = buttons\n    )                            \n```\n\n**“改”：**\n\n修改操作的方法是在`list`页面选中需要修改的项，然后进入详情页面，可以查看之前创建时填写的信息，也就是我们在“查”中查看单个信息提到的方式，我们所要做的就是在用户点击`save`按钮的时候，获取用户编辑后的数据，再进行一次修改即可，在我们工具中，此操作就是根据索引修改数组中对应索引的数据而已。\n\n**“删”：**\n\n有增就有删，这边我暂时还没实现批量删除的功能，一方面`python snack`的支持功能有限，一方面时间有限，所以我只实现了单个删除的功能，在新增和编辑的页面添加一个`delete`按钮即可，为了提醒用户错删，我们还要加上一个确认提示框。\n\n```\nif host[1] == \"delete\":\n    button = ExtButtonChoiceWindow(\n        screen,\n        'Delete host',\n        'Are you sure to delete current host?'\n    )\n    if button == \"ok\":\n        del(Basic_Config[current])\n    else:\n        Basic_Host_Window(current)\n```\n\n### 组件扩展\n\n构建自己的组件库真的很有必要，对于默认的`button`样式，我真是吐槽到不想再吐槽，它居然还认为自己的`border`很`nice`？！所以最终构建自己的组件库的初衷就是想把各个`dialog`中的`button`改为`compactbutton`，没办法，默认的`dialog`组件不给改呀，所以我们得自己返璞归真一下。\n\n当然我们做扩展组件库，也不是仅仅因为一个`button`样式，还有很多新需求都要依赖自己扩展的组件。比如热键，原生`dialog`无法支持热键；还有进度条的进度时间和任务信息展示；还有`Gridform`的动态布局等等。具体就不一一介绍了，想深入了解的直接看代码，做个小`Demo`，一目了然。\n\n## 爽口甜菜\n\n充实的正菜吃饱了，是时候来一波甜菜漱漱口，解解渴了。\n\n在做进度条页面的时候，想除了显示进度任务完成信息之外，还想显示一下开始时间和花费时间。发现`python`的`time`模块比较坑爹，对于时间差的转换支持不行，查阅资料只发现`datetime`可以将时间差转换为微秒、秒和小时三个单位，但是我想实现时间差的自动转换，也就是`60s`自动转换为`1min`，`60min`转为`1h`，`24h`转为`1d`，超越天为单位的我就不进行转换了，逻辑不难，只是拿出来分享给有需要的人，不必重复造轮子罢了。\n\n```\ndef get_time_interval(start_time):\n    start_time = datetime.fromtimestamp(start_time)\n    now_time = datetime.fromtimestamp(time.time())\n    interval = (now_time - start_time).seconds\n    format_interval = get_format_interval(interval)                                                                                          \n    return format_interval\n\n\ndef get_format_interval(interval):\n    if interval < 60:\n        format_interval = \"{}s\".format(str(interval))\n    elif 60 <= interval < 60*60:\n        format_interval = \"{}min {}s\".format(\n            str(interval/60), str(interval%60))\n    elif 60*60 <= interval < 60*60*24:\n        format_interval = \"{}h {}min {}s\".format(\n            str(interval/(60*60)),\n            str(interval%(60*60)/60),\n            str(interval%(60*60)%60)\n        )\n    elif 60*60*24 <= interval:\n        format_interval = \"{}d {}h {}min {}s\".format( \n\t\t\tstr(interval/(60*60*24)),\n            str(interval%(60*60*24)/60*60),\n            str(interval%(60*60)/60),\n            str(interval%(60*60)%60)\n        )\n        \n    return format_interval\n```\n\n## 用餐总结\n\n原本只是想做一个终端图形化的进度条页面，但是后续需求越来越多，导致做成了一个部署控制台工具，整个工程开发和优化花了大约两个星期的时间，项目中遇到的很多难点和问题很多都与`python snack`无关，所以没有做详细解释，就比如上述的甜菜，大家有兴趣的自行翻阅代码即可。\n\n`python snack`还有很多未知的我没有使用，比如`checkbox tree`等，但我相信万变不离其宗，有了这次实践，其他组件的使用和扩展应该不会花很多时间，其实做这个东西，我最深的感触就是前端发展的迅速，`python snack`是`2000`年初的产物了，很多页面逻辑跟`jQuery`比起来要弱的多，更别说现在的`angular`，`vue`等等了，但是领域不同，毕竟是伪终端页面，能做成这样已经不错了。如果是真正的桌面图形化界面（`GUI`），有`pyqt`这种神器，功能貌似很强大。\n\n我在之前的一个项目中，就使用过`python snack`做的控制台，当然当时不知道是用这个技术做的，当时觉得蛮牛的，尝试过修改终端文字成汉子，后来没有成功，便不了了之。这次机缘巧合，工作需要做这么一个控制台，在工作中学习和使用自己感兴趣的技术的感觉真是爽呀。工作中运用技术和自己业余时间学习新技术并做个小`Demo`完全是不一样的，工作中运用会不断有新需求，不断精益求精，不断深入。所以以工作作为平台，实现自己的技术价值，会有很大的成就感，与大家共勉咯。(#^.^#)\n\n> 完整项目代码地址：https://github.com/tony-yin/Best-practice-of-python-snack\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/deploy_console.gif\n[2]: https://github.com/tony-yin/Best-practice-of-python-snack/blob/master/widget_extlib.py\n[3]: http://ow0mgad6r.bkt.clouddn.com/python_snack.jpg\n","tags":["Console"],"categories":["tech"]},{"title":"Daily Article Vol6(2018/5/1 ~ 2018/5/31)","url":"/2018/06/01/Daily-Article-Vol6/","content":"\n<center>![May][26]</center>\n\n五月份主要基于`python sna`实现了一个终端伪图形化界面的部署控制台，提供了填写配置信息、校验信息、导入信息和进度监控等功能，详情请见[【Python Snack 最佳实践】][27]。这是我之前一直感兴趣想做的事情，这次正好借助工作的契机对其有了深入的了解和学习，收获颇多。五月份假期多，团建，还有工作进度赶，导致自己的阅读时间受到了影响，大多数阅读都是围绕着工作内容，极客时间这些`APP`、技术博客和微信公众号的很多技术资源，自己都没有规律地学习。这些都不是借口啦，主要还是陷入吃鸡，不能自拔，哈哈哈。（已经卸了，我这个人容易上瘾然而又有点嫌麻烦，这对我来说是最简单有效的方式）\n\n<!--more-->\n\n加入公司两个多月了，工作内容不断增多，难度也不断加大，由起初的合作工作到现在的独立负责某个`task`，也渐渐地忙了起来，总体来说还是游刃有余，能把自己的工作量`cover`住，希望自己继续加油。不过，这段时间大多数工作内容跟`ceph`关系不大，比如`ctdb rados`、`megaraid location`、`deploy console`等，毕竟是做产品，工作内容还是依赖产品需求，不过我还是不希望以后工作内容长期游离于`ceph`之外，因为今天的期望就是深入`ceph`，深入存储，如果工作上满足不了我的话，自己课余时间要挤一点时间做做研究了。不过好在现在项目上除了`ceph`也有很多东西是目前的我所不懂的，有学习的机会也不错，只要每天都能学习到不熟悉的东西便是极好的。除了存储，我们组也在往大数据和检索的方向靠，这两个我也都蛮感兴趣的，最近也稍微了解了下`elaticsearch`，目前好像很火的样子，六月底南京有一个`ES`线下沙龙，已经报名参加了，吼吼。\n\n加油咯 ~\n\n1. [Django实现CAS+OAuth2][2](5/5) <i class='fa fa-star'></i>\n2. [NVMe over Fabric][3](5/6) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n3. [解析Ceph: Librbd–块存储库][1](5/7)\n4. [Docker 核心技术与实现原理][4](4)(5/11 ~ 5/13) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n5. [【Django】@login_required用法简介][6](5/14)\n6. [request.META里包含了哪些数据？][9](5/15)\n7. [CTDB代码流程简要梳理][11](5/16) <i class='fa fa-star'></i>\n7. [ctdb原理介绍][10](5/17~5/18) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n6. [MegaCli:如何使用命令行监控RAID卡状态][8](5/18)\n5. [Python实现控制台中的进度条][5](5/21)\n6. [用PYTHON写NCURSES UI][7](5/22)\n7. [NEWT 程序设计指南][11](5/23)\n8. [Quick Guide to Python's Snack Module][13](5/23)\n9. [python在linux制作图形界面（snack）][15](5/23)\n9. [python在linux(anaconda)的图形界面（snack）][14](5/24)\n10. [An introduction to npyscreen][16](5/24)\n11. [Creating UI Controls under Linux Shell Console using whiptail Utility][17](5/25)\n12. [Dialog: An Introductory Tutorial][18](5/25)\n13. [Create a progress bar in bash][19](5/25)\n14. [How to Monitor Progress of (Copy/Backup/Compress) Data using ‘pv’ Command][20](5/25)\n15. [Python中如何写控制台进度条的整理][21](5/26) <i class='fa fa-star'></i>\n16. [可爱的 Python：Curses 编程][22](5/26)\n17. [PYTHON读取修改INI配置文件[CONFIGPARSER]][23](5/31)\n18. [Python 解析配置模块之ConfigParser详解][24](5/31) <i class='fa fa-star'></i>\n19. [Python(2.7.6) ConfigParser - 读写配置文件][25](5/31)\n\n[1]: http://www.wzxue.com/ceph-librbd-block-library/\n[2]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[3]: https://weibo.com/p/1001603934517592239583?mod=zwenzhang#_loginLayer_1525517479194\n[4]: https://draveness.me/docker\n[5]: http://www.cnblogs.com/lustralisk/p/pythonProgressBar.html\n[6]: https://blog.csdn.net/feng88724/article/details/7262514\n[7]: https://coolshell.cn/articles/677.html\n[8]: https://opstrip.com/2017/06/14/howto-monitor-raidCard-status-with-commandline-MegaCli/\n[9]: http://www.nowamagic.net/academy/detail/1318909\n[10]: https://blog.csdn.net/liuying_1001/article/details/77994642\n[11]: https://blog.csdn.net/naipeng/article/details/77162764\n[12]: https://www.ibm.com/developerworks/cn/linux/guitoolkit/newt/index.html\n[13]: http://www.wanware.com/tsgdocs/snack.html  \n[14]: https://blog.csdn.net/taiyang1987912/article/details/49273601\n[15]: http://www.cnblogs.com/Xjng/p/3869869.html\n[16]: https://npyscreen.readthedocs.io/introduction.html#example-code\n[17]: https://helloacm.com/creating-ui-controls-under-linux-shell-console-using-whiptail-utility/\n[18]: https://www.linuxjournal.com/article/2807\n[19]: https://askubuntu.com/questions/747143/create-a-progress-bar-in-bash\n[20]: https://www.tecmint.com/monitor-copy-backup-tar-progress-in-linux-using-pv-command/\n[21]: http://blog.ihipop.info/2010/10/1736.html\n[22]: https://www.ibm.com/developerworks/cn/linux/sdk/python/python-6/index.html\n[23]: http://wklken.me/posts/2012/02/19/python-ini-configparser.html\n[24]: https://www.jianshu.com/p/4202a2051668\n[25]: http://www.cnblogs.com/huey/p/4334152.html\n[26]: http://ow0mgad6r.bkt.clouddn.com/5-2.png\n[27]: http://www.tony-yin.top/2018/06/03/Python-Snack-Best-Practice/\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Ctdb Rados（二）：多场景断网高可用","url":"/2018/05/20/Ctdb-Rados-2/","content":"\n<center>![ping][2]</center>\n\n之前写过一篇文章【[Ctdb Rados方式导致All Banned的问题][1]】，谈到了当`ctdb`将`recovery lock`设置成`rados`的方式后，断网`master`节点会造成所有`ctdb`节点`All Banned`，主要原因是`master`意外断网没有释放锁，其他节点无法获取到锁，当时的解决方案是每`5`分钟检查一次`ctdb`状态，如果连续两次发生了`All Banned`的情况，则手动删除`lock`，这种做法在最近的测试中遇到了一些问题，本文对这些问题进行剖析并对相应的解决方案进行分享。\n\n<!--more-->\n\n> 完整代码地址：https://github.com/tony-yin/Ctdb-Rados-Monitor\n\n## 场景一\n\n如果基于原来的做法，`ctdb`发生`All Banned`的情况，需要十分钟的监控时间加上两分钟左右的`recovery`时间，也就是说大概需要十二分钟才能恢复`ctdb`服务，这样看来高可用有点名实其副了，这个也会明显地影响存储业务的正常运行。后来，我们讨论出新的方案：每`5s`检查一次`ctdb`的状态，`All Banned`的次数累计到`5`次才确定为该故障场景，然后手动删除`lock`，最终要保证`ctdb`能够在`2min`内完成恢复。\n\n### 问题1\n\n`cron tab`最短周期只支持分钟级别，所以如何`5s`检查一次便是一个问题。\n\n代码是死的，人是活的，虽然`cron tab`只支持分钟级别，但是我们可以每分钟调用一个脚本，然后在这个脚本中遍历`12`次，每次调用`ctdb monitor`脚本，然后`sleep 5s`，这样就可以达到每`5s`检查一次`ctdb`的效果了。\n\n```shell\n# ctdb_monitor\n* * * * * root /etc/ctdb/cron-seconds\n\n# cron-seconds\n#!/bin/bash\nfor((i=1;i<=12;i++));do\n    ../monitor_ctdb\nsleep 5\ndone\n```\n\n这样检查到`ctdb`发生`All Banned`情况，只需要花费`25s`，剩下的就是`recovery`的时间了。\n\n## 问题2\n\n当`ctdb master`节点的`network`服务断掉，其他两个节点（我的开发环境是三节点的虚拟机环境）便会选举一个为`master`节点，然后去获取`lock`，因为原`master`没有释放锁，导致所有节点`All Banned`，即使我们手动删除了锁，但是这时候其他两个节点仍然处于`Banned`的情况，需要等到`Ban Timeout`才会再次尝试获取锁并开始恢复过程，这个`timeout`的时间是`300s`，即`5min`，这显然是我们不能接受的，所以我们要在删除`lock`后，重启所有节点的`ctdb`服务。\n\n不过该如何触发该重启操作呢？\n\n我们在删除`lock`后将`ctdb`所有节点的`ip`作为对象存进`rados`中，然后在每`5s`监控的脚本中，查看`rados`中是否存在本节点的`ip`对象，如果有，则尝试重启`ctdb`操作，重启后便删除该对象。\n\n```shell\nfunction save_nodes_ip() {\n    nodes=$(ctdb listnodes)\n    for node in $nodes; do\n        echo \"$node\" > $node\n        rados -p rbd put $node $node\n        rm -f $node\n    done\n}\n\nfunction get_current_node_ips() {\n    ips=$(/usr/sbin/ip addr | grep \"inet \" | awk '{print $2}')\n    echo $ips\n}\n\nfunction monitor_nodes_ip_in_rados() {\n    ips=$(get_current_node_ips)\n    for ipinfo in $ips; do\n        ip=${ipinfo%/*}\n        if $(timeout 10 rados -p rbd ls | grep \"$ip\" -qw); then\n            systemctl restart ctdb\n            rados -p rbd rm $ip\n        fi\n    done\n}\n```\n\n至于为什么三个节点的`ip`都要存入`rados`，这个是因为原`master`节点恢复网络后，`ctdb`服务的状态为`failed`，同样需要重启`ctdb`服务才能正常恢复原`master`节点。 \n\n**注意：**\n\n这边有两个问题，当时浪费了我不少时间，问题不是多么深奥，但是不易发现。。。\n\n第一个问题便是`ips=$(/usr/sbin/ip addr | grep \"inet \" | awk '{print $2}')`这行代码，原来的写法是`ips=$(ip addr | grep \"inet \" | awk '{print $2}')`，当时发现`ip`总是获取不到，然后无论是命令行还是脚本运行都可以正常获取到，后来还是同事提醒才发现在`crontab`脚本中，`shell`命令默认是`/usr/bin/`下的，而`ip`命令则是`/usr/sbin/`下，所以这里的命令我们需要全路径。（这个需要格外注意！！！被坑的不要不要的。。。）\n\n第二个问题便是`rados -p rbd ls | grep \"$ip\" -qw`这行代码，当时没注意写成了`rados -p rbd ls | grep \"$ip\" -w`，发现`if`判断时常有问题，一开始还以为不能`grep`数字什么的，后来才发现没有加`q`，`q`表示安静模式，不打印任何标准输出，如果有匹配的内容则立即返回状态值0。\n\n## 场景二\n\n“断网”这个词不够具体，在实际生产环境中，一个集群中，一般都会有多个网络，就拿本人的`ceph`集群环境来说（物理机环境，并非前文提及的虚拟机开发环境），`ceph`有个`public network`和`cluster network`，而`ctdb`也有它的`node network`和`public network`，`ceph`的`public`和`ctdb`的`public`是同一网段，`ceph`的`cluster`是单独网段，`ctdb`的`node`是单独的网段。所以`ctdb master`断网可以分为三种情况：\n\n* 拔掉`ctdb master node`网段网线\n* 拔掉`ctdb master public`网段网线\n* 断掉`ctdb master network`服务\n\n当拔掉`ctdb master public`网段网线，这没有什么好说的，`ctdb master`节点服务还存在，只是`master`节点上的`public address`不可用了，会漂移到其他节点上。\n\n### 问题1\n\n当拔掉`ctdb master node`网段网线后，`master`节点仍然有`public`网卡，（**这里注意**）它仍然可以获取其他`ctdb`节点的状态，而其他节点却不可以获取它的状态，因为`master`的`node`节点`ip`不存在。所以造成的结果就是原`master`节点还默认自己是`master`节点，而其他的节点却又选举出了新的`master`，我们的脚本因为`All Banned`手动删除了`lock`，这时候其他节点可以正常恢复`ctdb`服务，但是当`ctdb master`节点断网再恢复后，它还以为自己是`master`，会不断去获取锁，而原来的锁已经被我们手动删除，这时候新的锁被新的`master`掌握，所以此时产生脑裂，我们要牺牲原`master`节点，也就是断网节点，所以需要重启它。这个重启触发机制我们是通过在每次删除`lock`之后在`rados`中存入`ctdb`所有节点的`ip`作为`object`（这就是为什么要存入所有节点的`ip`），然后只要发现有这个`object`便执行`ctdb`重启操作，然后便删除这个对象。至于为什么要存所有对象是因为除了原`master`需要重启之外，另外两个正常节点发生`All Banned`的情况，默认`timeout`时间是`300s`（这个上面也提到过），我们为了减少恢复时间，直接在删除`lock`后重启`ctdb`；\n\n## 问题2\n\n由于现在`ctdb`的锁是放在`rados`中，而不是以前的`cephfs`的方式了。所以当`master`断网再恢复时，它会不断地去`rados`获取他原来的锁，这是获取锁的进程越来越多，会阻塞住`rados`服务，我们可以通过`ps -ef | grep rados_helper`看到进程不断变多，那么`rados`服务不能正常读写就影响到我们上一条的机制，不能读`rados`中是否含有本节点`ip`的对象，就没办法进行重启操作，那么这样它就会不断地继续获取`lock`，所以我们在这里又加了一个机制，如果`ps -ef | grep rados_helper`的数目超过`6`个，就默认启动重启`ctdb`服务。\n\n```shell\nfunction monitor_get_lock_timeout() {\n    count=$(ps -ef | grep rados_helper | wc -l)\n    if [ $count -ge $RADOS_HELPER_PROCESS_MAX ]; then\n        systemctl restart ctdb\n        update_last_ctdb_restart_time\n    fi\n}\n```\n\n## 问题3\n\n`ctdb`目前重启的机制有点多，有自身自带的故障重启，也有我们监控脚本的异常情况，很容易发生重复重启，还有可能`rados_helper`堆积的进程很多，比如`20`个，我们的脚本是`5s`一次，也许`20`个的时候重启了，过`5s`，进程释放也需要时间，可能此时还有`10`个，那么大于我们规定的`6`个，就会继续重启，这种重复重启没有必要，所以我们要加上`ctdb`重启的周期限定`2min`。\n\n```shell\nfunction get_ctdb_restart_interval() {\n    last_time=$(get_ctdb_restart_last_time)\n    if [ -z \"$last_time\" ]; then\n        interval=$(expr $RESTART_CTDB_INTERVAL_MAX + 1)\n    else\n        current_time=$(date +%s)\n        interval=$(expr $current_time - $last_time)\n    fi\n    echo $interval\n}\n```\n\n考虑并解决以上提到的问题，基本上可以覆盖以上三种断网的场景了，在监控和管理`ctdb`的过程中，一定要小心，不能影响到业务正常运行。\n\n## 总结\n\n生产环境网络结构错综复杂，往往在虚拟机上开发的功能当时好好的，到了物理机上面测试会发生各种问题，此时，我们首先要搞清楚网络拓扑结构，熟悉硬件配置，各网段的作用和相互之间的关联，这样遇到问题我们可以顺藤摸瓜，同样`ctdb`的原理也需要掌握才能了解它各种行为的触发机制，才能更好的定制化监控和管理。之后我会花点时间好好地研究一下`ctdb`，然后再单独做分享。\n\n> 完整代码地址：https://github.com/tony-yin/Ctdb-Rados-Monitor\n\n[1]: http://www.tony-yin.top/2018/04/20/Ctdb-Rados-All-Banned/\n[2]: http://ow0mgad6r.bkt.clouddn.com/ping-600x450.png\n","tags":["Ctdb"],"categories":["tech"]},{"title":"Megaraid 磁盘定位","url":"/2018/05/12/Megaraid_Location/","content":"\n<center>![mega drive][2]</center>\n\n早前写过一篇【[利用Raid卡工具获取逻辑盘是否为SSD][1]】的文章，大概讲述了如何通过`raid`卡工具判断一个逻辑磁盘对应物理磁盘是否为`SSD`，当时主要提到了`megacli`和`sas3ircu`这两种工具，核心是如何通过`raid`卡工具定位到逻辑磁盘对应的物理磁盘的位置，当时的方式现在看来在有些场景会存在缺陷。\n\n<!--more-->\n\n当时的方案主要是先通过`lspci`获取`raid`卡型号，然后找到对应的`raid`卡型号，紧接着通过`lsscsi`命令获取逻辑磁盘的`targetid`，再通过`raid`卡工具根据`targetid`定位到对应的物理盘。当时的方案在多`controller`的场景下存在问题，可能会出现重复`target id`的情况，所以这时候只能再借助`controller id`来定位唯一的磁盘了。总而言之，想真正定位逻辑磁盘对应的物理磁盘，就必须要获取到磁盘的`controller id`，`enclosure id`和`slot number`，有了这三个参数，便可以获取该磁盘的信息，或者对该物理磁盘进行点灯、响音和做`raid`等操作。\n\n那么，具体如何定位逻辑磁盘的物理位置呢？且看下文分析\n\n## 获取 raid 卡信息\n\n通过`lspci`命令可以获取到操作系统上所有`raid`卡信息，我们可以看到每个`raid`卡最前面都有一串数字，比如第一行是`02:00.0`，第二行是`03:00.0`，这里的`02`和`03`表示的是`raid`卡的`busid`，即`raid`卡控制器在`pci`总线上的`id`。\n\n```shell\n[root@tony ~]# lspci | grep \"LSI Logic\"\n02:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3008 [Fury] (rev 02)\n03:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3108 [Invader] (rev 02)\n```\n\n## 获取磁盘 pcipath\n\n在`linux`中，一切皆文件，每个文件都有自己的唯一标识，对于磁盘而言，`pcipath`就是它的唯一标识，`pci`总线上面有很多控制器，比如`scsi`控制器，而磁盘又存在于`scsi`控制器上，所以我们可以在`lsscsi`命令获取到的`scsi`设备列表中查看到操作系统上的磁盘信息。\n\n以`sda`为例，我们可以在`/dev/disk/by-path`目录下查看到磁盘的`pcipath`\n\n```shell\n[root@tony ~]# ll /dev/disk/by-path/\ntotal 4\nlrwxrwxrwx 1 root root   9 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0 -> ../../sda\nlrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part1 -> ../../sda1\nlrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part2 -> ../../sda2\nlrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:1:0 -> ../../sdb\nlrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:10:0 -> ../../sdk\n```\n\n由于在`linux`中，`udev`是用户态的设备管理，所以我们也可以通过`udev`获取。\n\n```shell\n[root@tony ~]# udevadm info --query=symlink --name=sda\ndisk/by-id/scsi-36509a4c0ac86790022337b9105005435 disk/by-id/wwn-0x6509a4c0ac86790022337b9105005435 disk/by-path/pci-0000:02:00.0-scsi-0:2:0:0\n```\n\n这边我们可以得到磁盘`sda`的`pcipath`为`pci-0000:02:00.0-scsi-0:2:0:0`，`02`就是磁盘的`raid`卡的`bus id`，后面的`00`表示`channel id`，再后面的`0:2:0:0`就和`lsscsi`获取的一样了，其中`2`就表示`target id`。\n\n所以通过`bud id`，我们可以获取到磁盘对应的`raid`卡型号，根据对应的`raid`卡工具操作磁盘。这边我们只讨论`megaraid`，所以工具也就是`megacli`了。\n\n## 获取 controller id\n\n上面我们获取到了磁盘的`target id`和对应`raid`卡的`bus id`，而对于`megacli`工具而言，每个`raid`卡都有一个与之对应的`controller`。\n\n```shell\n[root@tony ~]# /opt/MegaRAID/MegaCli/MegaCli64 -AdpGetPciInfo -aall -NoLog\n\nPCI information for Controller 0\n--------------------------------\nBus Number      : 2\nDevice Number   : 0\nFunction Number : 0\n\nPCI information for Controller 1\n--------------------------------\nBus Number      : 3\nDevice Number   : 0\nFunction Number : 0\n\n\nExit Code: 0x00\n```\n\n这边我们可以看到`megacli`获取到了两个`controller`，也就对应上面`lspci`获取到的两张`raid`卡。细心的朋友可以发现这边有一个`Bus Number`，分别为`2`和`3`，而我们上面获取到了`raid`卡的`bus id`分别为`02`和`03`，没错，这边的`Bus Number`和`bus id`是对应的，只是`Bus number`没有自动填补成两位数，所以我们可以通过`bus id`得到`sda`所对应的`controller`为`0`。\n\n> **注意：**\n> 原本系统中版本`8.07.07`的`megacli`工具获取`raid`卡信息的时候会存在问题，每次`Bus Number`都会变化，我们只要升级`megacli`即可，我这边是把`megacli`升级到了`8.07.14`版本。\n> \n> 安装包地址：https://github.com/tony-yin/Megaraid_location/blob/master/MegaCli-8.07.14-1.noarch.rpm\n\n## 获取磁盘组\n\n此时，我们拥有了`controller id`，可以获取该`controller`下所有的磁盘组信息。\n\n```shell\n[root@tony ~]# /opt/MegaRAID_new/MegaCli/MegaCli64  -LdPdInfo -a0 -NoLog\nAdapter #0\n\nNumber of Virtual Disks: 13\nVirtual Drive: 0 (Target Id: 0)\nName                :\nRAID Level          : Primary-1, Secondary-0, RAID Level Qualifier-0\nSize                : 558.375 GB\nSector Size         : 512\nIs VD emulated      : No\nMirror Data         : 558.375 GB\nState               : Optimal\nStrip Size          : 64 KB\nNumber Of Drives    : 2\nSpan Depth          : 1\nDefault Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU\nCurrent Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU\nDefault Access Policy: Read/Write\nCurrent Access Policy: Read/Write\nDisk Cache Policy   : Disk's Default\nEncryption Type     : None\nDefault Power Savings Policy: Controller Defined\nCurrent Power Savings Policy: None\nCan spin up in 1 minute: Yes\nLD has drives that support T10 power conditions: Yes\nLD's IO profile supports MAX power savings with cached writes: No\nBad Blocks Exist: No\nIs VD Cached: No\nNumber of Spans: 1\nSpan: 0 - Number of PDs: 2\n\nPD: 0 Information\nEnclosure Device ID: 32\nSlot Number: 12\nDrive's position: DiskGroup: 0, Span: 0, Arm: 0\nEnclosure position: 1\nDevice Id: 12\nWWN: 50000398181A974C\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n...\n...\n```\n\n然后我们可以根据`target id`获取对应的磁盘组信息，`target id`与上面的`Target Id`所对应，这样我们可以过滤得到唯一的磁盘组信息。这边我们可以看到`sda`对应`Target Id`为`0`的磁盘组，该`raid`类型为`raid1`，虚拟磁盘组中有两块物理盘，然后我们可以获取这两块物理盘的`enclosure id`和`slot number`，这样再加上前文的`controller id`，我们就可以完完全全地定位到具体一块磁盘的物理位置。\n\n## 一键定位\n\n针对这种需求，本人根据以上逻辑写了一个简单的脚本可以一键获取磁盘的定位。\n\n```shell\n[root@tony ~]# ./get_disk_location.py sda\n['0:32:12', '0:32:13']\n```\n\n这边`0:32:12`分别表示磁盘的`controller id`，`enclosure id`和`slot number`。\n\n> 完整代码地址：https://github.com/tony-yin/Megaraid_location/\n\n## 总结\n\n之前的做法大部分场景可行，但是在一些场合偶尔会发现问题，总感觉还是不够靠谱，身边的人还有通过`sda`，`sdb`这种排列顺序来查找和`megacli`中显示磁盘的对应关系的，就更不靠谱了。`linux`操作系统是可以识别到具体硬件设备的，所以是肯定存在方法识别硬件对应的逻辑设备的，本文通过`pcipath`获取到设备的唯一标识，然后根据`pcipath`中的`bus id`和`megacli`中的`cobtroller`建立连接，最后通过`target id`锁定唯一磁盘组中的磁盘信息。\n\n通过这种方式，我们不需要肉眼判断，也不需要顾虑部分场景方案不适用，这完全就是操作系统使用的方式，使用这种最基础，最底层的方式实现，真是让人豁然开朗。这跟看源码类似，了解一个功能的背后具体实现，你才知道最正确的姿势，不用去碰，去凑，这种感觉真好。\n\n给大家推荐一本书《`Linux`设备驱动程序》，这本书详细讲解了`linux`中各种设备与驱动的细节，很底层也很枯燥，不过看完后应该会很有收获。希望大家在使用各种已有工具和框架的基础上，多去了解背后的实现机制，这样可以帮助我们更好地实现更深层次的需求。\n\n## Refer\n\n1. [Persistent block device naming][3]\n2. [hwraid][4]\n3. [showtools][5]\n4. [Linux SCSI 子系统剖析][6]\n\n[1]: http://www.tony-yin.top/2018/01/05/RaidCardToolUtils/\n[2]: http://ow0mgad6r.bkt.clouddn.com/mega_drive-600x450.png\n[3]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)\n[4]: https://github.com/eLvErDe/hwraid\n[5]: https://github.com/louwrentius/showtools\n[6]: https://www.ibm.com/developerworks/cn/linux/l-scsi-subsystem/\n","tags":["Megaraid"],"categories":["tech"]},{"title":"Django CAS Token 解决方案","url":"/2018/05/02/Django_CAS_Token_Solution/","content":"\n<center>![cas][6]</center>\n\n`CAS`单点登录主要是为了解决主系统和子系统的统一登录问题，能够做到任意一个子系统登录成功后，再登录其他子系统后不再需要认证，让用户不用重复地进行登录认证。`CAS`单点登录的方案很多，并且大多数都是采用`session`的方式，而本文结合个人实践，着重讨论`django cas token`的解决方案。\n\n<!--more-->\n\n本方案中，`cas`客户端和服务端都采用了开源项目，服务端是[django-mama-cas][1]，而客户端是[django-cas-ng][2]。\n\n## CAS Server\n\n服务端相比于客户端要简单地多，根据`github`步骤一步步下载和配置就好。\n\n### 下载\n\n```\npip install django-mama-cas\n```\n\n### 配置\n\n```python\n# settings.py\nINSTALLED_APPS = (\n    'mama_cas',\n)\n\n# 重要！，service是client的IP，是个数组，可以在后面添加SERVICE的HOST:PORT。\nMAMA_CAS_SERVICES = [\n    {\n        'SERVICE': 'http://127.0.1.1:8000',\n        'CALLBACKS': [\n            'mama_cas.callbacks.user_model_attributes',     # 返回除了password的所有Field\n            # 'mama_cas.callbacks.user_name_attributes', # 只返回 username\n        ],\n        'LOGOUT_ALLOW': True,\n        'LOGOUT_URL': 'http://127.0.1.1:8000/accounts/callback',\n    },\n]\n\n# urls.py\nurl(r'', include('mama_cas.urls')),\n```\n\n## Client\n\n首先是一些基本的客户端配置，比如`server ip`等，但是`django-cas-ng`默认是通过`session`的方式认证的，而我们需要通过`token`的方式认证，所以如果想继续用`django-cas-ng`来解决问题，那要么查看它是否有原生支持的接口，要么改源码。改源码可能不大友好，所以我优先研究了一下`django-cas-ng`的原生支持，无意中发现[view-wrappers-example][3]可以继承它原生的登录接口做一些封装，而我们完全通过继承原生的登录方法，然后加入我们的`token`相关代码。所以`urls.py`里面登录的方法我们写的是我们写在`view.py`中封装的登录方法，而并非默认的。\n\n```python\n# settings.py\nINSTALLED_APPS = (\n    # ... other installed apps\n    'django_cas_ng',\n)\n\nAUTHENTICATION_BACKENDS = (\n    'django_cas_ng.backends.CASBackend',\n)\n\n# 注意：这是cas server的地址\nCAS_SERVER_URL = 'http://127.0.0.1:8000'\n\n# 存入所有CAS 服务端返回的user数据。\nCAS_APPLY_ATTRIBUTES_TO_USER = True\n\n# urls.py\nimport view import *\nurl(r'^accounts/login$', cas_login, name='cas_login'),\n\n#view.py\nfrom django_cas_ng import views as baseviews\nfrom django.views.decorators.csrf import csrf_exempt\n\n@csrf_exempt\ndef cas_login(request, **kwargs):\n    r = baseviews.login(request, **kwargs)\n    if not request.user.is_anonymous():\n        token = get_token(request)\n        if token:\n            r.set_cookie('token', token)\n        else:\n            print 'Get token error'\n    else:\n        print('User is anonymous')\n    return r\n\ndef get_token(request, *args, **kwargs):\n    user = request.user\n    try:\n        request_hash = AuthToken.get_request_hash(request)\n        try:\n            token = generate_token()    # function used to geneate token, this place won't show more detail codes\n            token.refresh()\n        except IndexError:\n            pass\n    except Exception as e:\n        print e\n        return False\n    return token.key\n```\n\n生成`token`的方法我就不详细描述了，这边主要提供了一个思路，我们将`django-cas-ng`原生的登录方法进行了继承，然后生成`token`并放到了`session`当中。\n\n## 总结\n\n本文主要为`CAS Token`方案提供一个思路，如果集成进已有项目中，肯定会遇到很多细节问题，不过万变不离其宗，我们首先要熟悉手中运用的工具，然后要善于在此基础之上根据自己的定制需求进行开发，多看看文档和源码，每一次可能都会有新的发现。\n\n## Refer\n\n1. [使用django-mama-cas快速搭建CAS服务][4]\n2. [Django实现CAS+OAuth2][5]\n\n\n[1]: https://github.com/jbittel/django-mama-cas\n[2]: https://github.com/mingchen/django-cas-ng\n[3]: https://github.com/mingchen/django-cas-ng#view-wrappers-example\n[4]: https://www.jianshu.com/p/d97a3d367037\n[5]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[6]: http://ow0mgad6r.bkt.clouddn.com/cas-600x450.jpg\n","tags":["CAS"],"categories":["tech"]},{"title":"Daily Article Vol 5 - (2018/4/1 ~ 2018/4/30)","url":"/2018/05/01/Daily-Article-Vol5/","content":"\n<center>![April][45]</center>\n\n这是`Daily Article`系列的第五篇，罗列了`2018`年`4`月的阅读清单。\n\n<!--more-->\n\n本月是在新公司工作的第一个月，主要做了利用`python`搭建`NAS`服务稳定性测试框架、`CTDB`使用`rados object`作为`lock file`、利用`django`做`CAS`单点登录。首先`python`搭建`NAS`服务稳定性测试框架的工作就是通过`python`对`NAS`服务进行连接，并进行读写操作，该框架不需要任何挂载操作便可以对`NAS`服务进行连接和读写操作，目前支持`NFS`、`CIFS`和`FTP`，具体请参考[基于Python的NAS稳定性测试框架][44]；然后就是`CTDB`使用`rados object`作为`lock\nfile`这种方案的实践，期间也遇到了`Nodes All Banned`这种问题并解决了；最后就是采用`django-mama-cas`作为服务端，`django-cas-ng`作为客户端，搭建了一套`CAS`环境，并对目前的项目进行集成。\n\n个人课余时间的话，一方面关注的是`python`和`django`有关的知识，因为工作中要用到，其次就是围绕着`ceph`和存储相关知识的了解和学习了。上周在扫盲区的过程中，看到了很久之前了解的`nvme over fabric`这个概念，然后又去深入地了解了一下。这一了解又引申出了无数的盲区，比如`RDMA`、`nvme`、`nvme over\nip`、`FC`等等，而这些技术也很新，文档不多，并且大多数都是很枯燥的，只能逼着自己慢慢地啃下去。感慨技术变革真是日新月异的同时，也感慨扫盲区的速度远远赶不上盲区生成的速度，每天都会有好多自己不知道或者不是特别了解原理的概念，等待着自己去学习和深入。不过呢，也正如之前一位前辈讲过，很多人都觉得这个技术牛逼，那个技术牛逼，不是那么容易学会的，也正是因为这种心态，很多人都不去学习，而很多学习的人即使没有到达最顶峰，比不学的人多了解一点就强一些。所以人生苦短，没事多看看总没坏处。\n\n碎片时间也会看极客时间上面的专栏，我已经买了两个专栏。从目前的观看效果来说，首先我觉得还是不错的，但是很多内容都是概念上面的东西，仿佛是为了写文章而写文章，缺少落地的东西，经常围绕着一个概念反复强调，我觉得还不如多讲讲具体的场景更容易让别人接受一些。对于我而言，分布式，服务治理，服务分发这些概念和场景我都有经历过，所以我还蛮能接受的，我想很多没有经历过这些开发的童鞋直接看也许会感觉到云里雾里的。但是还是那句话，多看一点总是有好处的，因为很多东西你看了，你没到一定层次是不会了解的，你提前看了，也许之后某一天就会有一种豁然开朗的感觉。但是我们还是要讲究方法的，时间是有限的，我们要在有限的时间学习更多的知识。我认为看这些文章，目的不是为了一步登天，获得什么实质效应，事实上他也不会给你这种回报，我们要做的就是从上面获取一些技术概念、原理和本质，由此作为一个引子，自己找项目，找场景去实践，去深入，光说不练假把式，只有实践了才能深入，然后最好的就是能够在产品中不断运用和学习相关技术了，这样才是最好的学习和掌握。因为很多时间如果固步自封，很多概念你都不知道，更别谈去学习了。最后我会专门整理一篇文章，把我看过的极客时间的文章分享出来，这样可以让一些童鞋免费了解和学习，我这边用的是极客时间提供的分享的功能，也不算违规，每次分享只有十个名额，所以大家先到先得。emmm，看很多同学买课很积极，很多同学却很消极，我觉得前者起码有学习的想法，后者就不谈了，但是针对前者我想说的是，不是花的钱越多就有用的，而是学到手，记到脑子里才属于自己的，希望大家都能不断学习，越变越好，happy everyday！\n\n\n1. [django2.0入门教程第一节][2](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n2. [django2.0入门教程第二节][3](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n3. [django2.0入门教程第三节][4](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n4. [django2.0入门教程第四节][5](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n2. [【极客时间-左耳听风】：分布式系统关键技术：服务调度][1](4/3)\n3. [数据分析入门：初识数据埋点（一）][6](4/7) <i class='fa fa-star'></i>\n4. [docker初体验][7](4/7)\n5. [构建FTP文件传输服务器][8](4/8)\n6. [FTP文件传输协议][9](4/8)\n7. [ftp实现原理以及抓包分析][10](4/8)\n8. [Linux下ftp服务搭建之小试牛刀][11](4/8)\n9. [python 操作samba文件服务器][13](4/10) <i class='fa fa-star'></i>\n10. [佛系程序员的月薪五万指南][14](4/12) <i class='fa fa-star'></i>\n11. [关于python中的setup.py][15](4/13) <i class='fa fa-star'></i>\n9. [【极客时间-左耳听风】：分布式系统关键技术：流量与数据调度][12](4/16) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n10. [分布式高可用CTDB方案][16](4/16)\n11. [CTDB使用rados object作为lock file][17](4/16) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n12. [关于CTDB][18](4/17)\n13. [Cephfs的文件存到哪里了][23](4/21)\n13. [最佳日志实践][19](4/22) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n14. [最佳日志实践（v2.0）][20](4/23) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n15. [使用日志系统graylog获取Ceph集群状态][21](4/23) <i class='fa fa-star'></i>\n16. [Cephfs 操作输出到日志查询系统][22](4/23) <i class='fa fa-star'></i>\n17. [bluestore调研][24](4/23) <i class='fa fa-star'></i>\n18. [Python打包时添加非代码文件的坑][25](4/24)\n19. [cephfs介绍和功能测试][26](4/24)\n20. [块存储的世界][27](4/24) <i class='fa fa-star'></i>\n21. [python virtualenv][28](4/24) <i class='fa fa-star'></i>\n22. [rpmbuild实战][29](4/25) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n23. [使用RPM方式安装Linux软件][30](4/25)\n24. [RPM 打包技术与典型 SPEC 文件分析][31](4/25) <i class='fa fa-star'></i>\n25. [RDMA(远程直接内存访问)技术浅析][32](4/26) <i class='fa fa-star'></i>\n26. [RDMA技术][33](4/26) <i class='fa fa-star'></i>\n27. [Fabric是否代表网络架构的未来？][34] <i class='fa fa-star-half-full'></i>\n28. [2017下一代数据中心网络研究报告][35](4/27)\n29. [统一Fabric和互联云][36](4/27)\n30. [浅谈数据中心网络架构的发展][37](4/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n31. [为了部落：NVMe over Fabric诞生记][38] <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n32. [NVMe over Fabrics：概念、应用和实现][39](4/28)\n33. [NVME概述][40](4/28)\n34. [详谈NVMe over Fabric技术发展简史][41](4/29) <i class='fa fa-star'></i>\n35. [Django实现CAS+OAuth2][42] <i class='fa fa-star'></i>\n36. [NVMe over Fabric][43] <i class='fa fa-star'></i><i class='fa fa-star'></i>\n\n[1]: https://time.geekbang.org/column/article/ed937b37244d4db63f60e5f00be38fce/share\n[2]: https://www.jianshu.com/p/6f67a4b9dad3\n[3]: https://www.jianshu.com/p/b8d73d39f184\n[4]: https://www.jianshu.com/p/2004b8dbebb4\n[5]: https://www.jianshu.com/p/e2a09d2a4a2f\n[6]: http://www.woshipm.com/data-analysis/872543.html\n[7]: https://www.jianshu.com/p/5b33bbd61c48\n[8]: https://www.jianshu.com/p/f43659a58d71\n[9]: https://blog.csdn.net/bear_huangzhen/article/details/41806903?from=singlemessage\n[10]: https://www.jianshu.com/p/05212313d0e2\n[11]: https://www.jianshu.com/p/e99519739b5e\n[12]: https://time.geekbang.org/column/article/b307919cc599a82c542ec39e7aa3ddc7/share\n[13]: https://blog.csdn.net/u014245412/article/details/72286348\n[14]: https://yq.aliyun.com/articles/578927?utm_content=m_45816\n[15]: https://lingxiankong.github.io/2013-12-23-python-setup.html\n[16]: http://blog.sina.com.cn/s/blog_8c243ea30102uxaw.html\n[17]: https://ceph.com/planet/ctdb使用rados-object作为lock-file/\n[18]: https://blog.csdn.net/naipeng/article/details/75045177\n[19]: http://blog.jobbole.com/56574/\n[20]: https://zhuanlan.zhihu.com/p/27363484\n[21]: http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/\n[22]: http://www.zphj1987.com/2017/07/13/CEPHFS-op-to-graylog/\n[23]: http://www.zphj1987.com/2017/04/20/where-is-cephfs-data-store/\n[24]: https://blog.csdn.net/xiaqunfeng123/article/details/56675696?locationNum=1&fps=1\n[25]: https://zhuanlan.zhihu.com/p/24312755\n[26]: https://www.jianshu.com/p/d910a70dfee7\n[27]: http://xiaqunfeng.cc/2017/01/20/%E5%9D%97%E5%AD%98%E5%82%A8%E7%9A%84%E4%B8%96%E7%95%8C/#more\n[28]: https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001432712108300322c61f256c74803b43bfd65c6f8d0d0000#0\n[29]: http://stackeye.com/2014/08/rpmbuild-in-action/\n[30]: https://zhuanlan.zhihu.com/p/28492389\n[31]: https://www.ibm.com/developerworks/cn/linux/l-rpm/index.html\n[32]: http://mp.weixin.qq.com/s?src=11&timestamp=1524706491&ver=839&signature=nEMLgYwV4GI9Pd19glqGgJS6DR7sfrZesONMDXvpIWz34-KhzcYGK8h*WE7DjAFpXU9NYztsheG7Doy29A2zEC-V2bw*zFNgXbyy5VK2vRBR83sVVudSgxEsbAN0R8iW&new=1\n[33]: https://blog.csdn.net/chenhaifeng2016/article/details/78072498?locationNum=4&fps=1\n[34]: https://www.csdn.net/article/1970-01-01/302809\n[35]: http://dy.163.com/v2/article/detail/CDG9CD7G05179LAH.html\n[36]: http://book.51cto.com/art/201105/266135.htm\n[37]: http://net.zol.com.cn/459/4598330.html\n[38]: https://blog.csdn.net/memblaze_2011/article/details/51820631\n[39]: http://www.cnblogs.com/rodenpark/p/6220519.html\n[40]: https://blog.csdn.net/u010616442/article/details/70773956\n[41]: https://mp.weixin.qq.com/s?src=11&timestamp=1524808998&ver=841&signature=56U4bzWNuWUqlDANFQNBFPkSc2dX-R*HeLL9y7vN*ha-Ph0PrOfhkEPUfB8R*Gy-SLlwFeMqCUKDneUkeDPfpgT-igBpw77SSCr6Hhl9Ul29OsEMtzY1B2-JPQbBd1vH&new=1\n[42]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[43]: https://weibo.com/p/1001603934517592239583?mod=zwenzhang#_loginLayer_1525517479194\n[44]: http://www.tony-yin.top/2018/04/08/python_nas_stable_test/\n[45]: http://ow0mgad6r.bkt.clouddn.com/hello_april_600x450.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Ctdb Rados方式导致All Banned的问题","url":"/2018/04/20/Ctdb-Rados-All-Banned/","content":"\n<center>![lock object][2]</center>\n\n`ctdb`最近专门为`ceph`提供了一种`raods object`作为文件锁的方式，`lock file`可以放在对象存储中，而不是`cephfs`，从而大大降低了系统宕机的延时。在此方案的实践中，我们发现`master`节点宕机会导致严重的`All Banned`的问题，本文则围绕该问题展开讨论和提供本人的解决方案。\n\n<!--more-->\n\n很多系统都在用`ctdb`做`HA`，今天我们讨论的是基于`cephfs`的`ctdb HA`方案。`ctdb`的作用是在一个共享文件系统中，当所有节点都访问同一个文件时，`ctdb`会选举出一个`master`节点获得`lock`，我们之前的做法是把这个`lock file`放在`cephfs`的共享目录中，但是当其中某个节点`down`了之后，会导致`cephfs`这个目录卡死，进一步导致`lock file`在其他节点都获取不到，只有等到锁超时了之后才能获取到，而这个超时时间默认是`300s`，再加上`ctdb`的监控检测和恢复的时间，切换的时间少则十几分钟，多则几十分钟，这对于高可用场景来说无疑是灾难级的。\n\n## 具体场景\n\n`ctdb`的编译和安装我就不说了，大家可以参考磨渣的文章：[CTDB使用rados object作为lock file][1]。在`ceph`集群中所有节点安装好`ctdb`后，起服务后通过`systemctl status ctdb`可以发现`reclock`是通过`ctdb_mutex_ceph_rados_helper`的方式，就说明`ctdb rados`的方式配置成功了。\n\n然后我们可以通过`rados -p rbd ls`也可以看到自己配置的锁存在于`rbd pool`中。这时我们断电一个`slave`节点，一分钟左右后可以实现节点切换。但是我们的测试发现当断网`master`节点的时候，就会造成长时间的卡住，且节点并不会切换。详细查看可以发现断网后，`master`节点没有释放`lock`，然后其他的集群节点选举出了`master`节点后，试图获取锁，但是由于之前的`master`节点一直没有释放，所以一直获取不到，然后就不停的去获取，`ctdb`的机制是如果有不断的这种行为，就会让所有节点`All Banned`。因为`slave`节点并不拥有锁，所以不存在之前的问题。\n\n这个问题是比较严重的，因为不存在超时机制，拥有锁的节点断网或者断电，所以不会因为超时就释放锁。所以就会一直就卡着，并且一直实现不了切换节点。这就意味着一旦这种情况发生，客户的业务就会发生中断，这是无法接受的。并且我们也发现了如果使用原来将`lock file`放在`cephfs`目录的方式，断网或者断电主节点并不会发生这种情况，后来大概看了下源码大概是因为`cephfs`自己的机制会强制释放共享目录中文件的锁。\n\n具体报错如下：\n\n```shell\n[root@tony ~]# ctdb status\nWarning: All nodes are banned.\n```\n\n## 解决方案\n\n我们的解决方案没有尝试着修改`ctdb`的源码，而是通过定时监控`ctdb`的状态。如果是主节点上面的`ctdb`，并且如果是`rados`方式的话，每`3`分钟查看一下`ctdb status`的状态，如果有连续两次的状态都是`All Banned`的话，我们就认为目前主节点发生了不释放锁的问题，我们就主动地删除`lock object`。部分代码如下：\n\n```shell\n#! /bin/bash\n\nfunction check_if_master() {\n    MASTER_PNN=$(ctdb recmaster)\n    CURRENT_PNN=$(ctdb pnn)\n    if [ $MASTER_PNN -eq $CURRENT_PNN ]; then\n        echo true\n    else\n        echo false\n    fi  \n}\n\nfunction get_lock_name() {\n    LOCK_INFO=$(grep rados $CTDB_CONFIG_FILE | awk '{print $5}')\n    LOCK_NAME=${LOCK_INFO:0:-1}\n    echo $LOCK_NAME\n}\n\nfunction monitor_lock() {\n    STATUS_FILE=/etc/ctdb/status.txt\n    CTDB_STATUS=$(ctdb status 2>&1)\n    ALL_BANNED=\"Warning: All nodes are banned.\"\n\n    if [ ! -f \"$STATUS_FILE\" ]; then\n        echo \"$CTDB_STATUS\" > $STATUS_FILE\n    else\n        if [ \"$CTDB_STATUS\" = \"$ALL_BANNED\" ]; then\n            LAST_CTDB_STATUS=$(cat $STATUS_FILE)\n            if [ \"$LAST_CTDB_STATUS\" = \"$ALL_BANNED\" ]; then\n                LOCKNAME=$(get_lock_name)\n                echo $(date)\" Ctdb all nodes banned: Second time\" >> /var/log/monitor_ctdb.log\n                echo $(date)\" Remove ctdb rados lock: \"$LOCKNAME >> /var/log/monitor_ctdb.log\n                rados -p rbd rm $LOCKNAME \n                echo -n \"\" > $STATUS_FILE\n            else\n                echo $(date)\" Ctdb all nodes banned: First time\" >> /var/log/monitor_ctdb.log\n                echo \"$ALL_BANNED\" > $STATUS_FILE\n            fi\n        else\n            echo -n \"\" > $STATUS_FILE\n        fi\n    fi\n}\n\nCTDB_CONFIG_FILE=/etc/sysconfig/ctdb\nif $(grep rados $CTDB_CONFIG_FILE -q); then\n    if $(check_if_master); then\n        monitor_lock\n    fi\nfi\n```\n\n> 完整代码地址：`https://github.com/tony-yin/Ctdb-Rados-Monitor`\n\n## 总结\n\n也许我的这种做法不是最优方案，希望遇到同样问题的同学可以一起讨论，拥有更好解决方案的可以一起分享。\n\n[1]: http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/\n[2]: http://ow0mgad6r.bkt.clouddn.com/lock.jpg\n","tags":["HA"],"categories":["tech"]},{"title":"基于Python的NAS稳定性测试框架","url":"/2018/04/08/python_nas_stable_test/","content":"\n<center>![NAS Stable Test][6]</center>\n\n最近公司有个集群一直在跑着，领导想要测测它上面`NAS`服务的稳定性，也就是看看正常持续的读写会不会导致`NAS`服务异常，这个其实通过`fio`或者`cosbench`这类的工具测试起来很容易，但是这样一是没有挑战性，二是比较机械，可扩展性低。比如并行测试、进程保护和异常通知等等这些是机械地运用工具测试所做不到的，所以我们尝试做了一套基于`NAS`稳定性测试的框架。\n\n<!--more-->\n\n## 概述\n\n整个测试框架打包和发布都是通过RPM的方式，方便测试人员一键部署。部署之后测试工作由`supervisor`管理，实时监控后台进程的运行状态，发生异常时可以进行重启等自动化操作。所有读写操作都是通过`python` 连接NAS服务，无需做任何挂载工作。主要测试工作是通过`celery`实现任务调度，支持并行多个NAS服务的读写测试，`broker`和`backend store`都采用了`rabbitmq`。后端注册了`register`、`nfs`、`cifs`和`ftp`四个`job`，定时每`10`分钟执行一次，设置最大开启`worker`数为`5`个。`Job`注册进消息队列中后，`celery worker`会自动去消费，针对服务器中不同的`NAS`服务进行读写操作，每个任务的执行结果最后都会记录在日志中，出了异常通过邮件通知管理员。\n\n## 系统架构\n\n整个项目的框架图如下：\n\n<center>![Nas_Stable_Test架构图][1]</center>\n\n## NAS服务读写流程\n\n由于每个`NAS`服务的测试方式是一致的，所以下面就以单个`NAS`服务的流程来介绍。首先`client`端向`server`的`NAS`服务端口发起连接，`server`端接收到`client`端的请求后建立连接。`Client`在`/tmp`目录下生成固定大小`1G`的文件，并且记录该文件的`MD5`值，然后将该文件上传至远端NAS服务目录（即对`NAS`服务进行写操作），上传完成后将该文件从本地删除。接着对之前上传至`NAS`服务目录的文件进行下载（即对`NAS`服务进行读操作），下载完成后再次记录文件`MD5`值，并删除掉远端`NAS`服务目录对应的文件。最后对两次记录的`MD5`值进行比较，判断上传和下载的文件是否一致，并将比较结果记录在日志中，再次删除本地下载的文件。`NAS`服务读写流程图如下：\n\n<center>![Nas_Stable_Test流程图][2]</center>\n\n## 准备工作\n\n### 安装libnfs\n\n通过`pip`安装：\n\n```\npip install libnfs\n```\n\n一般会报这个错：\n\n```\nlibnfs/libnfs_wrap.c:2969:25: fatal error: nfsc/libnfs.h: No such file or directory\n```\n\n这个错看起来是缺少这个头文件的包，但是通过`yum search libnfs`是找不到相关的包的，所以我们只能去官网下载`rpm`包然后在安装：\n\n```\n## 下载rpm\nwget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-1.9.8-1.el7.nux.x86_64.rpm\nwget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm\n## 安装rpm\nyum localinstall libnfs-1.9.8-1.el7.nux.x86_64.rpm\nyum localinstall libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm\n```\n\n### 安装pysmb\n\n这个比较简单，直接`pip`安装就可以了，也没遇到什么问题。\n\n```\npip install pysmb\n```\n\n## NAS服务相关代码实现\n\n这里只贴出部分`python`连接或者操作具体`Nas`服务的代码实现，如果想要了解或者贡献整个项目，请关注：[Github python_nas项目][3]\n\n### NFS\n\n#### Connect\n\n```\ndef open(self):\n    self.nfs = libnfs.NFS('nfs://{}'.format(self.mount_point))        \n    log.info('nfs connect successfully!')\n```\n\n#### Read\n\n这里有个关键点就是分段读写文件，避免内存溢出。\n\t\n```\ndef read(self):\n\tlog.info('nfs read start...')\n\ta = self.nfs.open('/{}'.format(self.filename), mode='r')          \n\twith open(self.download_path, 'a') as f:                          \n    \twhile True: \n        \tcontent = a.read(1024*1024)                               \n        \tif content == '':\n            \tbreak\n        \tf.write(content)                                          \n\ta.close()\n    log.info('nfs read end...')\n```\n\n#### Write\n\n```\ndef write(self, content):                                             \n\tlog.info('nfs write start...')                                    \n\ta = self.nfs.open('/{}'.format(self.filename), mode='w+')         \n\ta.seek(self.file_size)\n\ta.write(content)\n\ta.close()\n\tlog.info('nfs write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n    log.info('nfs file delete start...')\n\tself.nfs.unlink('/{}'.format(self.filename))\n\tlog.info('nfs file delete end...')\n```\n\n### CIFS\n\n#### Connect\n\n```\ndef open(self):\n    self.smb = SMBConnection(\n        self.username,\n        self.password,\n        self.my_name.encode('utf-8'),\n        self.remote_name.encode('utf-8'),\n        use_ntlm_v2=True\n    )\n    self.smb.connect(self.host, self.port)\n    log.info('cifs connect successfully!')\n```\n\n#### Read\n\n```\ndef read(self):\n    log.info('cifs read start...')\n    file_obj = open(self.download_path, 'wb')\n    self.smb.retrieveFile(\n        self.directory,\n        self.filename,\n        file_obj\n    )\n    file_obj.close()\n    log.info('cifs read end...')\n```\n\n#### Write\n\n```\ndef write(self):\n\tlog.info('cifs write start...')\n\tfile_obj = open(self.client_path, 'rb')\n\tself.smb.storeFile(\n    \tself.directory,\n    \tself.filename,\n    \tfile_obj\n \t)\n\tfile_obj.close()\n\tlog.info('cifs write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n\tlog.info('cifs delete start...')\n\tself.smb.deleteFiles('path3', self.filename)\n\tlog.info('cifs delete end...')\n```\n\n#### Close\n\n```\ndef close(self):\n    self.smb.close()\n```\n\n### FTP\n\n#### Connect\n\n```\ndef open(self):\n    self.ftp = FTP()\n    self.ftp.connect(\n        host=self.host.encode('utf-8'),\n        port=self.port.encode('utf-8')\n    )\n    self.ftp.login(self.username, self.password)\n    log.info('ftp connect successfully!')\n```\n\n#### Read\n\n```\ndef read(self):\n\tlog.info('ftp read start...')                                     \n\tbuff_size = 1024\n\tfp = open(self.download_path, \"wb\")                               \n\tself.ftp.retrbinary(\n    \t\"RETR {}\".format(self.filename),\n    \tfp.write,\n    \tbuff_size                                                     \n\t)   \n\tfp.close()\n\tlog.info('ftp read end...')\n```\n\n#### Write\n\n```\ndef write(self):\n\tlog.info('ftp write start...')                                    \n\tbuff_size = 1024\n \tfp = open(self.client_path, \"rb\")\n\tself.ftp.storbinary(\n    \t\"STOR {}\".format(self.filename),\n    \tfp,\n   \t\tbuff_size\n\t)\n\tfp.close()\n\tlog.info('ftp write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n\tlog.info('ftp delete start...')\n\tself.ftp.delete(self.filename)\n\tlog.info('ftp delete end...')\n```\n\n#### Close\n\n```\ndef close(self):\n    self.ftp.quit(self.filename)\n```\n\n## 总结\n\n还有一些其他的`API`接口就不一一赘述了，具体实现细节大家可以查看`github`上面的项目代码，具体地址我会在文末贴出。整个项目的核心是通过`celery`实现任务的调度，还有全程通过`python`连接和操作`nas`服务，后续还会不断完善~~~\n\n> **项目地址：**https://github.com/tony-yin/python_nas\n\n<br/>\n\n> 参考列表：\n> [python 操作samba文件服务器][4]\n> [python libnfs][5]\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/NAS_Stable_Test%E6%A1%86%E6%9E%B6%E5%9B%BE.png\n[2]: http://ow0mgad6r.bkt.clouddn.com/NAS_Stable_Test%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%9B%BE.png\n[3]: https://github.com/tony-yin/python_nas\n[4]: https://pypi.org/project/libnfs/\n[5]: https://blog.csdn.net/u014245412/article/details/72286348\n[6]: http://ow0mgad6r.bkt.clouddn.com/nas.jpg\n","tags":["NAS"],"categories":["tech"]},{"title":"Daily Article Vol 4 - (2018/3/1 ~ 2018/3/31)","url":"/2018/04/01/Daily_Article_Vol4/","content":"\n<center>![Daily Article 3][29]</center>\n\n这个月主要因为离职和入职，花费了大量时间和相关的人沟通，还有办理了很多手续，包括找房子和搬家等等。真是忙的焦头烂额，奢侈地给自己放了一个星期的假期。\n\n<!--more-->\n\n离开了工作了两年的公司，真是感慨万千。还没毕业就到这家公司实习，然后提前试用、转正。师父耐心的指导，无论是技术上还是生活上都受益匪浅，帮我扎实地掌握了一名软件开发工程师应该具备的技能；钱总作为`CTO`仿佛身上一直贴着技术控的标签，殊不知竟各项全能，机缘巧合带我做起了`Ceph`，这一年中不仅从钱总身上学到了很多高新技术，也从每次吃饭饭桌上、下班路上、地铁上倾听他年轻的故事中受益很多，常常听起来热血沸腾。还有很多帮助过我和关心过我的人就不一一点名了。总之，大家都很厉害，也正是因为如此，在我工作的两年中，我能够不断感受到压力，不断进步，并且以后会一直以你们为我的榜样，以后还要多多指教，多交流多切磋。\n\n入职了新的公司，规模和规范都比原来上了一个档次，正式了许多，上下班都要打卡，一开始不习惯，下班总是忘记。。还有经常开会，要写很多设计和方案，感觉不像以后有师父和钱总这样带我走了，哈哈，但也多了很多参与感，大事小事都可以提出自己的想法，也可以从同事的想法中学习很多。也是巧合，之前网上因博客结缘的一位小伙伴现在和我同组，负责带我熟悉各方面，顿时轻松了很多，他人也很`nice`，看的出来很热爱技术，身上有很多值得我学习的点，算是一个挺优秀的同龄人了。总体来说，对新公司的印象还是不错的，好好加油吧！\n\n这个月我买了极客时间上面陈皓老师（左耳朵耗子）的专栏，没买但想看的朋友可以点击我下面分享的链接观看，每个链接都有十个免费的观看圈，先到先到，如果觉得收获很大的话，建议买了看看，受益绝不止这`199`哦~~~\n\n新公司任务调度用到了`celery`框架，感觉还蛮好用的，还在熟悉中...\n\n\n1. [Ceph: manually repair object][1](3/2) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n1. [【分析】Ceph数据一致性检查 - Scrub的介绍][2](3/5)\n1. [【分析】Ceph数据一致性检查 - 端到端的数据校验][3](3/5)\n1. [洞悉技术的本质，享受科技的乐趣][4](3/17)\n1. [程序员如何用技术变现（上）][5](3/17) <i class='fa fa-star'></i>\n1. [程序员如何用技术变现（下）][6](3/17)\n1. [Equifax信息泄露始末][7](3/18)\n1. [从Equifax信息泄露看数据安全][8](3/18)\n1. [何为技术领导力][9](3/18)\n1. [如何拥有技术领导力][10](3/18)\n2. [每个程序员都该知道的事][13](3/19)\n3. [Go语言，Docker和新技术][14](3/19)\n4. [答疑解惑：渴望、热情和选择][15](3/20)\n5. [如何成为一个大家愿意追随的Leader？][16](3/20)\n1. [NFS服务详细分析][11](3/21) <i class='fa fa-star'></i>\n2. [理解Linux系统/etc/init.d目录和/etc/rc.local脚本][12](3/21)\n3. [分布式系统架构的冰与火][17](3/21) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n4. [从亚马逊的实践，谈分布式系统的难点][18](3/22) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n5. [访问网络文件共享服务-CIFS][19](3/23) <i class='fa fa-star'></i>\n6. [任务调度利器：Celery][20](3/26)\n7. [异步任务神器 Celery 简明笔记][21](3/26)\n8. [Python 并行分布式框架：Celery][22](3/26) <i class='fa fa-star'></i>\n9. [Redis实现简单消息队列][23](3/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n10. [cifs协议与samba服务][24](3/28~3/29) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n11. [SELinux 入门][25](3/29)\n12. [Ring Buffer 有什么特别?][26](3/29) <i class='fa fa-star'></i>\n13. [分布式系统的技术栈][27](3/30) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n14. [分布式系统关键技术：全栈监控][28](3/31) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n\n[1]: http://www.sebastien-han.fr/blog/2015/04/27/ceph-manually-repair-object/\n[2]: http://blog.csdn.net/younger_china/article/details/75150261\n[3]: http://blog.csdn.net/younger_china/article/details/75149045\n[4]: https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx5d7aad8e7ec33bfd&redirect_uri=https%3A%2F%2Faccount.geekbang.org%2Faccount%2Foauth%2Fcallback%3Ftype%3Dwechatopen%26ident%3D60f4b1%26redirect%3Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare%253Ffailedurl%253Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare&response_type=code&scope=snsapi_userinfo&state=d2b1c68fbd5a953da5f04515e46f9b48&connect_redirect=1#wechat_redirect\n[5]: https://time.geekbang.org/column/article/e3b6ea4c823d2c319bcbce9a1dc23501/share\n[6]: https://time.geekbang.org/column/article/c260269c38db5276c17f1be49718f74a/share\n[7]: https://time.geekbang.org/column/article/70f255c35f7e58cb008e4410659cb39e/share\n[8]: https://time.geekbang.org/column/article/3aa6350a995faf76c32e61ba6f7db3e9/share\n[9]: https://time.geekbang.org/column/article/66c547af27e04afd4b7c2f8444d0971f/share\n[10]: https://time.geekbang.org/column/article/0294f13512d5b6608115a601eb373287/share \n[11]: http://blog.51cto.com/hongtengfei/1684809\n[12]: http://blog.csdn.net/acs713/article/details/7322082\n[13]: https://time.geekbang.org/column/article/5df06f61f6d635f5fc71b27ccc39902d/share\n[14]: https://time.geekbang.org/column/article/af44ced83be43d287e728d2eaee10afc/share\n[15]: https://time.geekbang.org/column/article/946c705caf3299894fcc991bc59992de/share\n[16]: https://time.geekbang.org/column/article/ef3daa17d473e583835df8248082f6fc/share\n[17]: https://time.geekbang.org/column/article/73253891c16c684d3c9dcfd02dfcb54f/share\n[18]: https://time.geekbang.org/column/article/a3b4de20403567fbdf305bdd4e403026/share\n[19]: http://blog.csdn.net/a18829898663/article/details/71065999\n[20]: https://www.liaoxuefeng.com/article/00137760323922531a8582c08814fb09e9930cede45e3cc000\n[21]: https://www.jianshu.com/p/1840035cb510\n[22]: http://www.open-open.com/lib/view/open1426298834326.html\n[23]: https://www.jianshu.com/p/9c04890615ba\n[24]: https://www.cnblogs.com/davidshen/p/8145984.html\n[25]: https://linuxtoy.org/archives/selinux-introduction.html\n[26]: http://www.cnblogs.com/shanyou/archive/2013/02/04/2891300.html\n[27]: https://time.geekbang.org/column/article/f9126577b469b13317889a99ea70d2f0/share\n[28]: https://time.geekbang.org/column/article/8efeb52c4015735a7ea424e0cff861c1/share\n[29]: http://ow0mgad6r.bkt.clouddn.com/2018_march.jpg\n","tags":["Daily-Article"],"categories":["read"]},{"title":"硬件环境测试环境模拟","url":"/2018/03/09/Hardware-Test-Tool/","content":"\n<center>![hardware][1]</center>\n\n最近在做一个`feature`，测试的时候需要硬件环境的支撑。一般我们开发环境都是虚拟机，所以针对这种开发工作的自测无法进行，比如虚拟机上没有物理磁盘，没有`raid`卡等，为了一个小功能的测试，需要出`build`，需要硬件环境的部署和安装，这个工作量着实不小。\n\n往往针对这种情况，作为开发人员可以针对硬件环境的具体需求，尽可能在自己的环境上做模拟，也就是我们俗称的“打桩”。\n\n<!--more-->\n\n就拿我这次做的需求来说吧，我想获取磁盘的相关信息，获取方式是通过`raid`卡工具，可能是`megacli`，也可能是`sas3ircu`等等，这个取决于`lspci`查看`raid`卡的型号，然后还要通过`lsblk`和`lsscsi`工具获取相关信息。\n\n我的做法是针对这些工具，自己写一个简易的小工具，就比如`lsblk`我也写一个`lsblk`的脚本，里面的代码也很简单，先找一个硬件环境，将`lsblk`读取的内容重定向到文件中，这时候我们自己写的脚本直接去读这个文件就可以了。务必要保证各个软件工具的一致性。\n\n以`lsblk`为例，我可能需求两种情况`lsblk`和`lsblk -l`：\n\n`lsblk`：\n\n```shell\n#! /bin/bash                                                        \n\nlsblk_path=$(dirname $0)\nif [ $# -eq 0 ]; then\n    echo \"$(cat $lsblk_path\"/lsblk.txt\")\"\nfi\n\nwhile getopts \":l\" opt; do\n    case $opt in  \n        l)  \n            echo \"$(cat $lsblk_path\"/lsblk_list.txt\")\"\n            ;;  \n        \\?) \n            echo \"Invalid option: -$OPTARG\"\n            ;;  \n    esac\ndone\n```\n\n`lsblk.txt`：\n\n```shell\nNAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT \nsda        8:0    0   3.7T  0 disk \n├─sda1     8:1    0  30.5M  0 part ar\n├─sda2     8:2    0 488.3M  0 part ar\n├─sda3     8:3    0  93.1G  0 part /rt\n├─sda4     8:4    0   256G  0 part [SWAP]WA\n└─sda5     8:5    0   3.3T  0 part /data/osd.0sd\nsdb        8:16   0 372.1G  0 disk \n└─sdb1     8:17   0 372.1G  0 part ar\nsdc        8:32   0   2.7T  0 disk \n└─sdc1     8:33   0   2.7T  0 part /data/osd.1sd\nsdd        8:48   0   2.7T  0 disk \n└─sdd1     8:49   0   2.7T  0 part /data/osd.2sd\nsde        8:64   0   2.7T  0 disk \n└─sde1     8:65   0   2.7T  0 part /data/osd.3sd\nsdf        8:80   0   2.7T  0 disk \n└─sdf1     8:81   0   2.7T  0 part /data/osd.11d.\nsdg        8:96   0   2.7T  0 disk \n└─sdg1     8:97   0   2.7T  0 part /data/osd.12d.\nsdh        8:112  0   2.7T  0 disk \n└─sdh1     8:113  0   2.7T  0 part ar\nsdi        8:128  0   2.7T  0 disk \n└─sdi1     8:129  0   2.7T  0 part /data/osd.8sd\nsdj        8:144  0   2.7T  0 disk \n└─sdj1     8:145  0   2.7T  0 part /data/osd.9sd\nsdk        8:160  0   2.7T  0 disk \n└─sdk1     8:161  0   2.7T  0 part /data/osd.10d.\nsdl        8:176  0   2.7T  0 disk \n└─sdl1     8:177  0   2.7T  0 part ar\nsdm        8:192  0   2.7T  0 disk \n└─sdm1     8:193  0   2.7T  0 part ar\nsdn        8:208  0   2.7T  0 disk \n└─sdn1     8:209  0   2.7T  0 part ar\nsdo        8:224  0   2.7T  0 disk \n└─sdo1     8:225  0   2.7T  0 part\nsdp        8:240  0   2.7T  0 disk\n└─sdp1     8:241  0   2.7T  0 part\nsdq       65:0    0   2.7T  0 disk\n└─sdq1    65:1    0   2.7T  0 part\nsdr       65:16   0   2.7T  0 disk\n└─sdr1    65:17   0   2.7T  0 part\nsds       65:32   0   2.7T  0 disk\n└─sds1    65:33   0   2.7T  0 part\nsdt       65:48   0   2.7T  0 disk\n└─sdt1    65:49   0   2.7T  0 part\nsdu       65:64   0   2.7T  0 disk\n└─sdu1    65:65   0   2.7T  0 part\nsdv       65:80   0 744.7G  0 disk\n├─sdv1    65:81   0    50G  0 part\n├─sdv2    65:82   0    50G  0 part\n├─sdv3    65:83   0    50G  0 part\n├─sdv4    65:84   0    50G  0 part\n├─sdv5    65:85   0    50G  0 part\n├─sdv6    65:86   0    50G  0 part\n├─sdv7    65:87   0    50G  0 part\n├─sdv8    65:88   0    50G  0 part\n├─sdv9    65:89   0    50G  0 part\n├─sdv10   65:90   0    50G  0 part\n├─sdv11   65:91   0    50G  0 part\n└─sdv12   65:92   0    50G  0 part\n```\n\n`lsblk_list.txt`：\n\n```shell\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0   3.7T  0 disk \nsda1     8:1    0  30.5M  0 part \nsda2     8:2    0 488.3M  0 part \nsda3     8:3    0  93.1G  0 part /\nsda4     8:4    0   256G  0 part [SWAP]\nsda5     8:5    0   3.3T  0 part /data/osd.0\nsdb      8:16   0 372.1G  0 disk \nsdb1     8:17   0 372.1G  0 part \nsdc      8:32   0   2.7T  0 disk \nsdc1     8:33   0   2.7T  0 part /data/osd.1\nsdd      8:48   0   2.7T  0 disk \nsdd1     8:49   0   2.7T  0 part /data/osd.2\nsde      8:64   0   2.7T  0 disk \nsde1     8:65   0   2.7T  0 part /data/osd.3\nsdf      8:80   0   2.7T  0 disk \nsdf1     8:81   0   2.7T  0 part /data/osd.11\nsdg      8:96   0   2.7T  0 disk \nsdg1     8:97   0   2.7T  0 part /data/osd.12\nsdh      8:112  0   2.7T  0 disk \nsdh1     8:113  0   2.7T  0 part \nsdi      8:128  0   2.7T  0 disk \nsdi1     8:129  0   2.7T  0 part /data/osd.8\nsdj      8:144  0   2.7T  0 disk \nsdj1     8:145  0   2.7T  0 part /data/osd.9\nsdk      8:160  0   2.7T  0 disk \nsdk1     8:161  0   2.7T  0 part /data/osd.10\nsdl      8:176  0   2.7T  0 disk \nsdl1     8:177  0   2.7T  0 part \nsdm      8:192  0   2.7T  0 disk \nsdm1     8:193  0   2.7T  0 part \nsdn      8:208  0   2.7T  0 disk \nsdn1     8:209  0   2.7T  0 part \nsdo      8:224  0   2.7T  0 disk \nsdo1     8:225  0   2.7T  0 part\nsdp      8:240  0   2.7T  0 disk\nsdp1     8:241  0   2.7T  0 part\nsdq     65:0    0   2.7T  0 disk\nsdq1    65:1    0   2.7T  0 part\nsdr     65:16   0   2.7T  0 disk\nsdr1    65:17   0   2.7T  0 part\nsds     65:32   0   2.7T  0 disk\nsds1    65:33   0   2.7T  0 part\nsdt     65:48   0   2.7T  0 disk\nsdt1    65:49   0   2.7T  0 part\nsdu     65:64   0   2.7T  0 disk\nsdu1    65:65   0   2.7T  0 part\nsdv     65:80   0 744.7G  0 disk\nsdv1    65:81   0    50G  0 part\nsdv2    65:82   0    50G  0 part\nsdv3    65:83   0    50G  0 part\nsdv4    65:84   0    50G  0 part\nsdv5    65:85   0    50G  0 part\nsdv6    65:86   0    50G  0 part\nsdv7    65:87   0    50G  0 part\nsdv8    65:88   0    50G  0 part\nsdv9    65:89   0    50G  0 part\nsdv10   65:90   0    50G  0 part\nsdv11   65:91   0    50G  0 part\nsdv12   65:92   0    50G  0 part\n```\n\n还有一些其他工具我就不详细贴代码了，具体请去`github`上查看。\n\n项目地址：https://github.com/tony-yin/Hardware_Test_Tool\n\n操作步骤：\n\n1.下载代码\n\n```shell\ngit clone git@github.com:tony-yin/Hardware_Test_Tool.git\n```\n\n2.安装工具\n\n```shell\n./build install\n```\n\n3.卸载工具\n\n```shell\n./build uninstall\n```\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/hardware-600x450.jpg\n","tags":["Test"],"categories":["tech"]},{"title":"Daily Article Vol 3 - (2018/2/1 ~ 2018/2/28)","url":"/2018/03/01/Daily-Article-Vol3/","content":"\n<center>![Daily Article 3][19]</center>\n\n这段期间，一方面是因为过年，另一方面因为换工作，所以诸事缠身，导致每天阅读学习的量严重受影响，不能保持每天都有固定的时间学习和阅读，还是自己的问题，不应该找借口。之后，不管什么事情，必须要保证每天的学习量和阅读量，养成雷打不动的好习惯。\n\n这是`Daily Article`系列的第三篇，罗列了`2018`年`2`月的阅读清单。\n\n<!--more-->\n\n---\n1. [ceph的数据存储之路(10) -----ceph对象存储的ls命令实现及思考][4](2/1 ~ 2/5) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [八大排序算法的 Python 实现][5](2/6)\n1. [Ceph解析-消息处理模块][3](2/7) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [解析Ceph: 网络层的处理][7](2/7)\n1. [ceph存储 ceph集群消息处理][6](2/8)\n1. [ceph的数据存储之路(1) ---rbd设备介绍][1](2/11) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(2) ----- rbd到osd的数据映射][2](2/11)\n1. [张大胖学递归][10](2/16) <i class=\"fa fa-star\"></i>\n1. [张大胖的socket][9](2/17) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(4) ----- rbd client 端的数据请求处理][8](2/18~2/20) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [学习面向对象的令狐冲][11](2/21) <i class=\"fa fa-star\"></i>\n1. [张大胖学数据库][13](2/22)\n1. [数据库村的旺财和小强][14](2/23) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(5) -----osd数据处理][12](2/24 ~ 2/26) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [ Ceph开发每周谈 Vol 70 | RGW 同步到 AWS S3 | Elastic Search API 整合][15](2/26)\n1. [Ceph OSD从filestore 转换到 bluestore的方法][16](2/27)\n1. [ceph的数据存储之路(6) -----pg的创建][17](2/28)\n1. [XSKY在OpenStack场景下的优势][18](2/28)\n\n\n\n[1]: https://my.oschina.net/u/2460844/blog/531646\n[2]: https://my.oschina.net/u/2460844/blog/531686\n[3]: https://georgezhuo.github.io/georgezhuo.github.io/2015/12/22/ceph-message/\n[4]: https://my.oschina.net/u/2460844/blog/669769\n[5]: http://python.jobbole.com/82270/\n[6]: http://blog.csdn.net/skdkjzz/article/details/41980631\n[7]: http://www.wzxue.com/ceph-network/\n[8]: https://my.oschina.net/u/2460844/blog/532755?p=2&temp=1519178944837#blog-comments-list\n[9]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513387&idx=1&sn=99665948d0b968cf15c5e7a01ffe166c&chksm=80d679e8b7a1f0febad077b57e8ad73bfb4b08de74814c45e1b1bd61ab4017b5041942403afb&scene=21#wechat_redirect\n[10]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513393&idx=1&sn=c1d6caca8ef9972f1105df982f15bb58&chksm=80d679f2b7a1f0e410fd53691d68a9ada158aac2b8814fe329d6dfed86ee54b68a53eb84cdb0&scene=21#wechat_redirect\n[11]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513353&idx=1&sn=a5dc69542fae6aabf0fef9b5f5881a9d&chksm=80d679cab7a1f0dc530bd1745c2c9552b739afc701ecb2f8e1eba8624d1fefc2c3cc64cd1d30&scene=21#wechat_redirect\n[12]: https://my.oschina.net/u/2460844/blog/534390?p=1&temp=1519628036891#blog-comments-list\n[13]: http://mp.weixin.qq.com/s/wyt-0y3lEhewa2cpeILYSQ\n[14]: https://mp.weixin.qq.com/s/tSF_w9xUOj3Q2hmOxJkwLg\n[15]: https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&mid=2651256389&idx=1&sn=e11edcce5722853f442b9a7b8211787e&chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&mpshare=1&scene=23&srcid=0502SazrSPsWnszP3xfdEId4#rd\n[16]: http://ceph.com/planet/ceph-osd%E4%BB%8Efilestore-%E8%BD%AC%E6%8D%A2%E5%88%B0-bluestore%E7%9A%84%E6%96%B9%E6%B3%95/\n[17]: https://my.oschina.net/u/2460844/blog/535007\n[18]: https://www.sogou.com/link?url=6IqLFeTuIyjnBL6rnEdhCp_rFXz42FvmEhOk1arvVrbkYalcctcthKCfjFS1STqz\n[19]: http://ow0mgad6r.bkt.clouddn.com/february-600x450.jpg\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Daily Article Vol 2 - (2018/1/1 ~ 2018/1/31)","url":"/2018/02/01/Daily-Article-Vol2/","content":"\n<center>![Daily Article Vol2][44]</center>\n\n这是`Daily Article`系列的第二篇，罗列了`2018`年`1`月的阅读清单。\n\n<!--more-->\n\n1. [OLTP vs. OLAP][1](1/2)\n1. [OLTP和OLAP的区别][2](1/2)\n1. [Ceph开发每周谈 Vol 31｜ZetaScale 开源 | DMClock][3](1/2)\n1. [Ceph开发每周谈 Vol 32｜Ceph-osd on 4.x 内核异常][4](1/3)\n1. [Ceph开发每周谈 Vol 33｜Encode 改进方案][5](1/3)\n1. [Ceph开发每周谈Vol 34 | Ceph Days 亚太路演 | Ceph OSD CLASS 支持 LUA 编程][6](1/3)\n1. [Ceph开发每周谈 Vol 35 | Ceph Developer Month][7](1/3)\n1. [Ceph开发每周谈 Vol 36|Ebay 的 CephFS 使用深度报告][8](1/4)\n1. [Ceph开发每周谈 Vol 37｜NVME Over Fabric｜FAST][9](1/4)\n1. [【码农翻身】 浏览器：一个家族的奋斗][10](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>\n1. [【码农翻身】 浏览器家族的安全反击战][11](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [【码农翻身】 黑客三兄弟][12](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [【码农翻身】 GitHub/Stackoverflow 找工作时有什么用？][13](1/6)\n1. [【码农翻身】 我是一个线程(修订版)][14](1/7) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [Linux下安装编译IPVS内核模块][15](1/8) <i class=\"fa fa-star\"></i>\n1. [How to Repack Deb Files on Debian and Ubuntu][16] <i class=\"fa fa-star\"></i>(1/8)\n1. [什么是DevOps][18] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i></i><i class=\"fa fa-star-half-full\"></i>(1/8)\n1. [我是一个线程][19] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/8)\n1. [Javascript: 一个屌丝的逆袭][20] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/9)\n1. [TCP/IP 之 大明王朝的邮差][21] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/9)\n1. [TCP/IP 之 大明内阁][22] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/9)\n1. [TCP/IP 之 蓟辽督师][23] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/10)\n1. [CPU 阿甘][24] <i class=\"fa fa-star\"></i>(1/10)\n1. [CPU 阿甘之烦恼][25] <i class=\"fa fa-star\"></i>(1/10)\n1. [开源万岁！2018 年开源技术的 10 大发展趋势][17](1/10)\n1. [面试过阿里等互联网大公司，我知道了这些套路][26](1/11)\n1. [访谈 | 鸟哥惠新宸：程序员应该不断提升自身的不可替代性][27](1/15)\n1. [黑客三兄弟（续）][28] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/19)\n1. [我是一个网卡][30] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/20)\n1. [我是一个路由器][29] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/21)\n1. [我是一个进程][31] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/22)\n1. [我是一块硬盘（上）][32] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/22)\n1. [我是一块硬盘（下）][33] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/23)\n1. [如何维护一个好的技术博客？][34] <i class=\"fa fa-star\"></i>(1/23)\n1. [算法分析神器—时间复杂度][35] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/23)\n1. [我是一个键盘][36] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i> (1/24)\n1. [PHP 面试题 - 如果没有 mb 系列函数，如何切割多字节字符串][37](1/28)\n1. [ceph读写流程分析][38](1/28)\n1. [Token 认证的来龙去脉][39](1/29) <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>\n1. [ceph 读流程(1)][42](1/29) <i class=\"fa fa-star\"></i>\n1. [一个项目做完以后，不能就让它这么“完”了][40](1/30)\n1. [科学与星球大战：当科幻遇到现实][41](1/30) <i class=\"fa fa-star\"></i>\n1. [ceph 读流程(2)][43](1/31) <i class=\"fa fa-star\"></i>\n\n\n\n\n[1]: http://datawarehouse4u.info/OLTP-vs-OLAP.html\n[2]: http://blog.csdn.net/zhanghaocore/article/details/9820215\n[3]: https://www.xsky.com/tec/ceph-weekly-vol-31/\n[4]: https://www.xsky.com/tec/ceph-weekly-vol-32/\n[5]: https://www.xsky.com/tec/ceph-weekly-vol-33/\n[6]: https://www.xsky.com/tec/ceph-weekly-vol-34/\n[7]: https://www.xsky.com/tec/ceph-weekly-vol-35/\n[8]: https://www.xsky.com/tec/ceph-weekly-vol-36/\n[9]: https://www.xsky.com/tec/ceph-weekly-vol-37/\n[10]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514131&idx=1&sn=a11640045e1458c0e3ba866d23541526&chksm=80d67cd0b7a1f5c6c4dee2cb64b6f173524d443dae8be471bb901eebeec4db5d1773818080b7&scene=21#wechat_redirect\n[11]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514143&idx=1&sn=28ea209c00309e6b93d8d1f76032d7a4&chksm=80d67cdcb7a1f5ca81d8d454a98af56d58b22f6058f100e21ff30e70867ea6e3e922a4f000bf&scene=21%23wechat_redirect\n[12]: https://mp.weixin.qq.com/s/YvYvL0siJT1UhO0tXnYVNA\n[13]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513220&idx=1&sn=bb9c4df63cf6994d6aab9d77a10fe628&scene=21#wechat_redirect\n[14]: https://mp.weixin.qq.com/s/-BMCUuIWYE3O_oC3ZNNJRg\n[15]: http://www.freeoa.net/osuport/botinstal/instal-compile-linux-ipvs-mod_3170.html\n[16]: https://www.tuicool.com/articles/RfmI3u\n[17]: https://mp.weixin.qq.com/s?__biz=MjM5NTU2MTQwNA==&mid=2650655754&idx=1&sn=0b03b942ecdb25ef45ea3a9cbff686c3&chksm=beffc6d989884fcf49c79c662162ed7852b17e192b7f40fcda040308eb10b26a9b187f675940&scene=0&key=78733623947167cee9eb37fa8148459e73e455140b258998bf251e0d57524ceaaa6187e820f481781748d004b780efc58804a25dfdf285670ca35369d0e62d2d8e7cd8ca84306d36da7105f9c8042683&ascene=0&uin=MjEzMTAwMzgyNQ%3D%3D&devicetype=iMac+MacBookAir7%2C1+OSX+OSX+10.12.6+build(16G29)&version=12020810&nettype=WIFI&lang=zh_CN&fontScale=100\n[18]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514224&idx=1&sn=5b802dfab658a626a197635cf56c9bac&chksm=80d67cb3b7a1f5a585f778b28ceb88855b140ff090c0d0b00f10e9e9b889c81c20190ba80a06&scene=38#wechat_redirect\n[19]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=416915373&idx=1&sn=f80a13b099237534a3ef777d511d831a&scene=21#wechat_redirect\n[20]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513059&idx=1&sn=a2eaf97d9e3000d15a33681d1b720463&scene=21#wechat_redirect\n[21]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513094&idx=1&sn=a2accfc41107ac08d74ec3317995955e&scene=21#wechat_redirect\n[22]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513375&idx=1&sn=e11745d5cb28fa1f89465f8d0e5fae1a&chksm=80d679dcb7a1f0cadec0a1db45b3fa3f6eabde4a9e56a77acf551fbc1511e6b8f055ee2c97e3&scene=21#wechat_redirect\n[23]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513384&idx=1&sn=82c45e4430618270a744e212d2f57990&chksm=80d679ebb7a1f0fd27fe1814765a0e60b1c293ca4e09403271b4bb3f38f317439956fdca1bc1&scene=21#wechat_redirect\n[24]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513017&idx=1&sn=5550ee714abd36d0b580713f673e670b&scene=21#wechat_redirect\n[25]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513254&idx=1&sn=a4d1912b6259c3e65c0e172fb5a10dbb&scene=21#wechat_redirect\n[26]: https://mp.weixin.qq.com/s?__biz=MzIxMTE0ODU5NQ%3D%3D&mid=2650236979&idx=1&sn=71f07d1741a57f8fd429d76d37fd8a07&chksm=8f5a026fb82d8b7931f95e747224b1049d1b0cf72f72babbf6629e183fa15c72570be4fc7253\n[27]: https://www.oschina.net/question/2928191_2272289\n[28]: https://mp.weixin.qq.com/s/_-7C_ZfFfvNKhBQzSB6j4Q\n[29]: https://mp.weixin.qq.com/s/VyGQ4-Dn4UX2Z0CrCHgUqw\n[30]: https://mp.weixin.qq.com/s/vyHlB9pem4rv4htJS9ca6Q\n[31]: https://mp.weixin.qq.com/s/KtHxMoc1_3sQd4d_MuKKJA\n[32]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513289&idx=1&sn=ab19dcad7b1dc217463f155fe106091a&scene=21#wechat_redirect\n[33]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513292&idx=1&sn=5b1c87dc72a20c92883924080174b16b&scene=21#wechat_redirect\n[34]: https://mp.weixin.qq.com/s/gliuPj9tfZkr9oXQn9re2w\n[35]: https://mp.weixin.qq.com/s/643URs9k_EQIMWGmb35QkQ\n[36]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513299&idx=1&sn=264f4d0891e1b96fb5e356dc7b6c91dc&scene=21#wechat_redirect\n[37]: https://mengkang.net/1129.html\n[38]: http://www.quts.me/ceph-readwrite/\n[39]: https://segmentfault.com/a/1190000013010835\n[40]: https://mp.weixin.qq.com/s/gI1TjeAYjqgNo4RKqEqF-Q\n[41]: https://mp.weixin.qq.com/s/hDKQ1ITzTvi20kTA11xa1g\n[42]: https://bean-li.github.io/ceph-read-flow/\n[43]: https://bean-li.github.io/ceph-read-2/\n[44]: http://ow0mgad6r.bkt.clouddn.com/2018-01-600x450.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"实现RBD导出NFS高可用（二）：提供多虚拟IP访问","url":"/2018/01/28/RBD-HA-2/","content":"\n<center>![RBD-HA-2][2]</center>\n\n之前分享过一篇[【通过 Keepalived 实现 Ceph RBD 的高可用】][1]，主要讲解了将`RBD`导出为`NFS`，然后通过`keepalived`实现高可用，保证当提供虚拟`IP`节点发生故障时，可以自动切换节点，使得业务不发生中断。\n\n这样可以基本使用`RBD`代替`CephFS`对外提供`Ceph`服务，至于为什么不用`CephFS`就不多说了，不清楚的可以去看上一篇。虽然说这样可以保证无单点故障，但是有一点还是不如`CephFS`，那就是`CephFS`可以实现多节点同时提供服务，而`RBD`说白了其实同时只有一个节点能提供服务，当客户端流量高的时候，`RBD`方式的带宽并不能满足需求。就比如都是三个节点，`CephFS`可以将客户端流量分流到三个节点，而`RBD`只能用一个节点，而带宽上限又取决与网卡、磁盘和`IO`等等原因，所以同样的硬件设施`RBD`的带宽性能是跟不上的，本文就多虚拟`IP`暴露访问方式进行分享。\n\n<!--more-->\n\n## CephFS & RBD\n\n此前的文章我们`Ceph`集群只有一个`RBD image`，并且只通过一个`vip`暴露这个`image`让客户端通过`NFS`访问。这与`CephFS`的差距就在没有充分利用每个节点的资源，所以我们可以大胆设想一下是否可以通过`RBD`对外提供多个`vip`，每个节点都能被`NFS`访问呢？理想很美好，现实很残酷。如果一个`RBD`对多个节点同时提供读写的话，会导致不一致的后果，现在`RBD`并不能做到`CephFS`那样多个节点同时提供服务且保证读写一致。那怎么办呢？\n\n虽然一个`RBD image`不能同时被多客户端访问，但是我们是否可以创建多个`RBD image`，然后利用多个`vip`对外提供访问呢？这样听起来貌似可行，但是还是存在诸多问题，比如如何暴露多虚拟`IP`，如何将`IP`绑定到具体的`RBD image`，如何保证多`RBD image`的高可用等等，下文将就这些技术细节进行详细地分析。\n\n## 需求背景\n\n客户端有多种应用场景，对流量要求较高的情况下，我们可以为每一种应用场景都提供一个`vip`用于`NFS`方式访问`Ceph`存储集群。然后每个`vip`各自对应集群中的一个`RBD image`，`RBD image`尽量均匀的分布到各个节点上，这样才能把性能提升到最高，比如集群有三个节点的话，如果暴露三个`vip`，那么必须要分布到三个不同的节点上，如果要提供四个`vip`的话，那么前三个`vip`均匀地分布到三个节点上，第四个`vip`就在第一个节点上暴露，以此类推，这边说的第一个节点只是我们自己将三个节点进行逻辑上的排序，我们需要通过一些算法确保`vip`分布均匀，具体看下文分析。\n\n\n## 整体设计\n\n一般在完成一个`feature`之前，我们往往需要做一个`design`，对要做的事情和流程进行设计和评估，这样不但可以梳理流程，使得之后动手的时候思路清晰，更重要的是可以预见一些问题和难点，尽早与 团队成员进行交流，选择最佳方案，防止真正做的时候走弯路。这边涉及的技术点主要有：\n\n### Keepalived暴露多个VIP\n\n`keepalived`暴露单个`vip`很常见，具体格式网上都有，而暴露多个`vip`就要注意一些细节，比如`router_id`，`ins_name`，`priority`等等，对于一个节点而言，它上面`keepalived`暴露`vip`的情况完全是由配置文件`keepalived.conf`所决定的，而对于`keepalived.conf`而言，一个`vip`其实就是`ins`，而`ins_name`和`router_id`要求同一个`keepalived`组内成员相同，我们这边就默认`router_id`就是`vip`隔着小数点的四位整数相加的和，而`ins_name`则是将`vip`的小数点换成下划线。\n\n\n### VIP动态均匀分布\n\n`vip`均匀分布要保证尽可能的均匀，比如三个节点，如果要提供两个`vip`的话，那就随意挑选两个节点作为`vip`绑定，如果四个`vip`的话，则是三个节点各自绑定一个`vip`后再任意选择一个节点作为第四个`vip`绑定。我们这边的做法是先将所有节点进行排序，将两个节点作为一个`keepalived`组，下两个节点为另外一组，假设有三个节点，我们设为`1, 2, 3`，那么如果要暴露三个`vip`，我们就需要三个`keepalived`组，这边三个组分别是`1, 2`，`3, 1`和`2, 3`，然后组内其中第一个节点为`master`，第二个节点为`backup`。这样可以基本保证所有`vip`的均匀分布，具体算法实现参见下文。\n\n### 多RBD高可用\n\n上一篇文章中只有一个`RBD`，所以高可用就围绕它一个，发生故障后随意切换节点即可，因为我们每个节点都是一个`keepalived`组的成员。但是如果有多个`RBD`的话，我们如果随意切换的话，那么`RBD`分布就会变得不均匀。上文提及的算法可以保证`vip`的均匀分布，两两节点作为一个`keepalived`组，这样我们即使一个节点掉了，切换也只会在当前组内切换，而`vip`一开始绑定节点的时候就根据相应算法保证了每个`RBD`的均匀分布，所以这边组内切换不会影响分布的均匀性。\n\n上一篇文章中提过`keepalived`的机制，当主节点`down`了，主节点会触发我们自己写的`ChangetoBackup.sh`，而副节点则会触发`ChangetoMaster.sh`。之前由于只有一个`RBD`，所以当时做的比较无脑，`ChangetoMaster.sh`直接遍历当前节点上面的所有`RBD`，然后通过之前记录的`RBD`和`UI`上创建的`目录`的映射关系进行挂载，而`ChangetoBackup.sh`也是一样的`umount`所有`RBD`的挂载点。针对目前的多`RBD`的情况，这样的做法肯定是不行的，因为现在我们一个节点可能是一个或多个`vip`的`master`，也可能是另外一个或多个`vip`的`backup`，如果我们还是像之前那样一股脑的全部卸载或者挂载，那么造成的后果显而易见，就是业务中断，暴露服务节点紊乱。所以最合理的应该对号入座，一个`vip`对应一个`RBD image`，哪个`vip`出现了问题，作为该`vip`的`master`节点，应该只`umount`该`vip`绑定`RBD`所对应的目录，而`backup`节点应该只`mount`对应的目录。其他不相关`RBD`和其对应的目录，我们都不应该有所操作。那么我们只有在触发`ChangetoMaster.sh`和`ChangetoBackup.sh`这两个脚本的时候加上“目录”这个参数，具体实现详见下文分析。\n\n### 大容量RBD image的创建和删除\n\n我们系统的实现是`UI`上创建目录，后端`daemon`轮询根据目录信息做对应的事情，比如前端`UI`创建了目录，后端就是在创建`RBD image`，而生产环境上面的容量的要求都是很高的，往往都是几十`T`，甚至上百`T`,但是熟悉`RBD`的朋友都知道创建如此大的`RBD image`是需要很长的时间的，那这样就不但会影响当前目录能够提供服务的时间，也会阻塞住代码，影响之后目录的创建。之前我们的做法是一开始我们可以创建一个比较小的`image`，然后我们后台选择在业务不繁忙的时候进行定时扩容，这也可以算是暂时止血了。但是后来测试发现删除`image`才是真的慢，这边就不像创建那样有曲线救国的方式了，所以这边无论是创建还是删除`RBD image`我们都不能做成同步的方式了，我们采取了另起一个线程单独做这个事情，不影响后端业务的正常处理。\n\n### 快照保证扩容的安全性\n\n在我们的测试过程中，发现对`RBD image`扩容会偶尔发生文件系统出错的情况，这种情况是很危险的，一旦文件系统发生问题，并且用`e2fsck`等工具修复不了的话，那么数据恢复是很困难的，我们必须要保证客户数据的安全性。所以我们用了`RBD`的`snapshot`的功能，在每次扩容之前为`RBD image`做快照，这样即使发生了问题，我们起码可以做到最小程度的损失。\n\n## 具体代码实现\n\n### Keepalived暴露多个VIP\n\n当`UI`创建一个`vip`的时候，我们就要加一个`ins`，以下就是我们添加一个`ins`的`API`，本文所有代码都是`python`写的，大家凑合看吧。（部分代码和接口不是很全，文章尾部将会贴出详细代码的地址）\n\n```python\ndef add_keepalived_ins(self, vip, folder, state):\n    vrrp_ins = \"\"\"\nvrrp_instance VI_{ins_name} {{\n    state {state}\n    interface {pubif}\n    priority {priority}\n    virtual_router_id {router_id}\n    advert_int 1\n    authentication {{\n        auth_type PASS\n        auth_pass 1111\n    }}\n    track_script {{\n        chk_nfs\n    }}\n    notify_master \"/etc/keepalived/ChangeToMaster.sh {folder}\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh {folder}\"\n    virtual_ipaddress {{\n        {vip}\n    }}\n}}\n\"\"\".format(ins_name = vip.replace('.', '_').replace('/', '_'),\n           state = state,\n           priority =  200 if state == \"MASTER\" else 100,\n           router_id = self.get_router_id(vip),\n           pubif = get_public_interface(),\n           folder = folder,\n           vip = vip)\n        return vrrp_ins\n```\n\n这边我们可以看到`ins_name`和`router_id`都是根据`vip`转换成特定格式，标识`ins`的唯一性。而`priority`则是根据`state`来决定，`state`为`master`时，`priority`为`200`，而`backup`的`priority`为`100`。至于如何获取`state`，这个涉及到`vip`均匀算法，后续会讲。\n\n### VIP动态均匀分布\n\n假设三个节点，为`1, 2, 3`，三个`vip`，为`a, b, c`，那么最后`a`对应的节点为`1, 2`，`b`对应的节点为`3, 1`，`c`对应的节点为`2, 3`，具体实现算法是先将所有`vip`进行排序，获取要操作`vip`的`index`，然后获取集群内所有节点，然后将上面获取的`index`乘以`2`，再对所有节点的个数做余数，然后可以获得一个整数，这个整数就是`vip`对应`master`节点在所有节点数组中的`index`，这种算法大家应该很容易从规律中推算出来。\n\n```python\ndef get_my_state(self, vip_idx):\n    nodes = get_all_nodes()\n    nodes.sort()\n    idx = vip_idx * 2 % len(nodes)\n    my_ip = get_public_ip()\n    if my_ip == nodes[idx]:\n        return 'MASTER'\n    elif my_ip == nodes[(idx + 1) % len(nodes)]:\n        return 'BACKUP'\n    else:\n        return None\n```\n\n### 多RBD高可用\n\n我们在创建目录的时候，需要获取当前节点是否为`master`，之前那个只有一个`vip`，所以当前节点要么是`master`，要么是`backup`，但是这边的话，一个节点可能是一个`vip`的`master`的同时也可能是另一个`vip`的`backup`，所以是否为`master`是要根据目录而定的。在这边我们在创建目录、删除目录、创建`vip`和删除`vip`时，更新一个`vip`和目录之间的映射关系。这个`map`我是存在`ceph`的`leveldb`中，至于为什么不存在节点本地，是因为这份数据必须要保证所有节点强一致，放在本地节点，可能会因为一些故障原因导致之后内容不一致的情况。\n\n这边我们要求在创建目录前，必须要存在空闲`vip`可以提供目录绑定。所以当创建一个`vip`时，此时应该没有目录需要绑定，我们建立一个`key`和`value`都是`vip`的字典；当创建一个目录的时候，随机找到一个空闲`vip`进行绑定，建立一个`key`为`vip`，`value`为目录名的字典；当删除`vip`时，肯定是存在其他空闲`vip`的，所以在删除原来对应`map`后，我们要找到其他一个空闲`vip`与之前删除`vip`对应的目录进行绑定；当删除目录时，只要将对应关系中的`value`换成`key`，也就是对应的`vip`了。\n\n有了这个`map`，我们就可以实时获取目录和`vip`的信息和之间的对应关系。\n\n`vip.py`\n\n负责当`vip`发生变化时，更新`ip_folder_map`，以及`ip_folder_map`的读写`API`\n\n```python\ndef get_ip_folder_map():\n    result = {}\n    ip_folder_map = LevelDB(\"ip_folder_map\")\n    result = json.loads(ip_folder_map)[\"ip_folder_map\"]\n    return result\n    \ndef set_ip_folder_map(ip_folder_map):\n    ip_folder_map = LevelDB(\"ip_folder_map\")\n    ip_folder_map.set(json.dumps({\"ip_folder_map\": ip_folder_map}))\n    ip_folder_map.save()\n    \ndef update_ip_folder_map_by_ip(ips):\n    ip_folder_map = get_ip_folder_map()\n    old_ips = ip_folder_map.keys()\n    if len(ips) > len(old_ips):\n        new_ip = list(set(ips) - set(old_ips))[0]\n        ip_folder_map[new_ip] = new_ip\n    else:\n        del_ip = list(set(old_ips) - set(ips))[0]\n        folder = ip_folder_map[del_ip]\n        del ip_folder_map[del_ip]\n        if folder != del_ip:\n            for k, v in ip_folder_map.iteritems():\n                if k == v:\n                    ip_folder_map[k] = folder\n                    break\n    set_ip_folder_map(ip_folder_map)\n```\n\n`folder.py`\n\n负责当`folder`发生变化时，更新`ip_folder_map`\n\n```python\nimport vip\n\ndef update_ip_folder_map_by_folder(folder, type):\n    ip_folder_map = vip.get_ip_folder_map()\n    folder = get_folder_path(folder)\n    if type == \"add\":\n        for k, v in ip_folder_map.iteritems():\n            if k == v:\n                ip_folder_map[k] = folder\n                break\n    elif type == \"delete\":\n        for k,v in ip_folder_map.iteritems():\n            if v == folder:\n                ip_folder_map[k] = k\n                break\n    vip.set_ip_folder_map(ip_folder_map)\n```\n\n上面说了在切换节点的时候，需要传递目录参数，保证只操作对应目录。而脚本是静态的，目录确是动态的，所以我们需要在目录或者`vip`发生变化的时候对原来的`keepalived.conf`进行更新，添加目录参数。也就是说当`vip`发生变化时，我们根据当前`vip`选择添加或者减少`ins`，并且更新每个`ins`调用脚本后面追加的参数；而`folder`发生变化时，`vip`调用脚本后面追加的参数也需要更新，要么是`vip`，要么是`folder`。这边也需要用到上面的`ip_folder_map`，因为每个`ins`就是一个`vip`，而每个`vip`对应一个`folder`。所以我们这边当目录或者`vip`发生变化时，会根据`ip_folder_map`更新`keepalived.conf`，具体实现代码如下：\n\n```python\n    def update_keepalived_conf(self):\n        kconf = \"\"\"global_defs {\n    notification_email {\n    }\n    \n    router_id NFS_HA_112\n}\n\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"\n    interval 2\n}\n\"\"\"\n        vips = self.ip_folder_map.keys()\n        vips.sort()\n        for vip, folder in self.ip_folder_map.items():\n            vip_idx = vips.index(vip)\n            state = self.get_my_state(vip_idx)\n            if state is not None:\n                kconf += self.add_keepalived_ins(vip, folder, state)\n        with open(KEEPALIVED_CONF_PATH, 'w') as f:\n            f.writelines(kconf)\n        do_shell('service keepalived reload')\n```\n\n下面是添加一个`ins`的模板，上面也贴过代码，至于这边再次贴一遍的目的是想侧重展示一下脚本后面参数的动态变化的实现方式。\n\n```python\n    def add_keepalived_ins(self, vip, folder, state):\n        vrrp_ins = \"\"\"\nvrrp_instance VI_{ins_name} {{\n    state {state}\n    interface {pubif}\n    priority {priority}\n    virtual_router_id {router_id}\n    advert_int 1\n    authentication {{\n        auth_type PASS\n        auth_pass 1111\n    }}\n    track_script {{\n        chk_nfs\n    }}\n    notify_master \"/etc/keepalived/ChangeToMaster.sh {folder}\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh {folder}\"\n    virtual_ipaddress {{\n        {vip}\n    }}\n}}\n\"\"\".format(ins_name = vip.replace('.', '_').replace('/', '_'),\n           state = state,\n           priority =  200 if state == \"MASTER\" else 100,\n           router_id = self.get_router_id(vip),\n           pubif = get_public_interface(),\n           folder = folder,\n           vip = vip)\n        return vrrp_ins\n```\n\n触发脚本：\n\n`ChangetoMaster.sh`\n\n```bash\n#!/bin/bash\n\nfolder=\"$(dirname $1)/$(basename $1)\"\nfname=$(basename $folder)\n\nif [ -d $folder ]; then\n    if $(mount | grep -q \"$folder \"); then\n        umount -f $folder > /dev/null\n    fi\n    device=$(rbd showmapped | awk '/image_'$fname' / {print $5}')\n    if [ -b \"$device\" ]; then\n        mount $device $folder\n    fi\nfi\nservice nfs-kernel-server restart\n```\n\n`ChangetoBackup.sh`\n\n```bash\n#!/bin/bash\n\nfolder=$1\nservice nfs-kernel-server stop\nif [ -d $folder ]; then\n    if $(mount | grep -q \"$folder \"); then\n        umount -f $folder > /dev/null\n    fi\nfi\nservice nfs-kernel-server start\n```\n\n### 大容量RBD image的创建和删除\n\n在另外一个端口另起一个线程，通过异步的方式实现，主要利用`python`的`rpyc`模块实现，忧郁项目保密性等原因，只贴上部分关键代码，给大家提供一些思路。\n\n以删除`RBD image`为例，调用`remove_image`方法，进入装饰器，从而在新现成做删除操作，不再阻塞之前进程的流程。\n\n```python\ndef rbd_background():\n    conn = connect('localhost', RBD_PORT)\n    module = conn.modules['rbd_utils']\n    async_func = rpyc.async(getattr(module, func_name))\n    \n    return async_func\n    \n@rbd_utils.rbd_background\ndef remove_image(pool, image):\n    while True:\n        try:\n            logger.info('rbd {} delete start'.format(image))\n            do_shell('rbd rm {}/{} >> /var/log/rbd_rm.log'.format(pool, image))\n            logger.info('rbd {} delete finish'.format(image))\n            break\n        except Exception:\n            logger.error('rbd {} delete error'.format(image))\n            time.sleep(30)\n```\n\n### 快照保证扩容的安全性\n\n首先介绍一下定时扩容的脚本：\n\n`monitor_rbd.sh`：当`RBD image`可利用空间小于`50%`或者小于`50T`时，扩容`50T`\n\n```bash\n#!/bin/bash\n\nfunction convert_to_MB()\n{\n    size=$1\n    unit=${size:(-1):1}\n    nr=${size/$unit/}\n    case $unit in\n        (k|K|\\)) echo \"$nr / 1024\" | bc;;\n        (m|M|\\)) echo \"$nr\";;\n        (g|G|\\)) echo \"$nr * 1024\" | bc;;\n        (t|T|\\)) echo \"$nr * 1024 * 1024\" | bc;;\n        (p|P|\\)) echo \"$nr * 1024 * 1024 * 1024\" | bc;;\n        *) echo \"Error: cannot convert to MB: $size\";;\n    esac\n}\n\nfunction get_available_size()\n{\n    disk=$1\n    unit_size=$(convert_to_MB '50T')\n    \n    disk_size=$(df -h | grep $disk | awk '{print $2}')\n    disk_size=$(convert_to_MB $disk_size)\n    pool=$(rbd showmapped | grep $disk | awk '{print $2}')\n    available_pool_size=$(ceph df | grep $pool | awk '{print $5}')\n    available_pool_size=$(convert_to_MB $available_pool_size)\n    if [ $(echo \"$available_pool_size < $unit_size\" | bc) -eq 1 ]; then\n        new_size=$(echo \"$disk_size + $available_pool_size\" | bc)\n    else\n        new_size=$(echo \"$disk_size + $unit_size\" | bc)\n    fi\n    echo ${new_size%.*}\n}\n\nfunction check_and_enlarge_disk()\n{\n    disk=\"$1\"\n    if [ \"$disk\" = \"\" ]; then\n        echo \"Error: You must specify the disk name\"\n        return 1\n    fi\n    echo \"Checking the disk [/dev/$disk] ...\"\n    if ! rbd showmapped | grep -q $disk; then\n        echo \"Error: Cannot find the disk [$disk]\"\n        return 2\n    fi\n    disk_usage=$(df | grep $disk | awk '{print $5}')\n    available_disk_size=$(df | grep $disk | awk '{print $4}')\n    available_disk_size=$(convert_to_MB \"${available_disk_size}k\")\n    echo \"  The disk use% is ${disk_usage}\"\n    disk_usage=${disk_usage/\\%/}\n    if [ $disk_usage -lt 50 -a $available_disk_size -gt 1024 * 1024 * 50 ]; then\n        echo 'Less then 50% use and more then 50TB available space left, just quit'\n        return 0\n    fi\n    echo 'Enlarging the disk ...'\n    new_size=$(get_available_size $disk)\n    echo \"  the new size is ${new_size}MB\"\n    pool=$(rbd showmapped | grep $disk | awk '{print $2}')\n    image=$(rbd showmapped | grep $disk | awk '{print $3}')\n    rbd resize --size $new_size -p $pool $image\n    sleep 3\n    resize2fs /dev/${disk} \"${new_size}M\"\n    echo \"Done\"\n}\n\ndisks=$(lsblk | grep rbd | awk '{print $1}')\nfor disk in $disks\ndo\necho \"==============================================\"\ncheck_and_enlarge_disk \"$disk\"\necho \"==============================================\"\ndone\n```\n\n这边我们采用`ceph`提供的原生`python`的接口，完成`RBD`的定时快照的创建和删除\n\n```python\n#!/usr/bin/python\n\nimport os\nimport time\nimport rados\nimport rbd\nfrom folder import get_all_folder_info\nfrom vip import get_ip_folder_map\n\nCEPH_CONF = '/etc/ceph/ceph.conf'\nMAX_SNAP_COUNT = 5\n\ndef create_snap(pool, rbd_image):\n    now = time.localtime()\n    snap = time.strftime(\"%Y_%m_%d_%H_%M_%S\", now)\n    with rados.Rados(conffile=CEPH_CONF) as cluster:\n        with cluster.open_ioctx(str(pool)) as ioctx:\n            with rbd.Image(ioctx, rbd_image) as image:\n                image.create_snap(snap)\n                \ndef get_images():\n    pubif = get_public_interface()\n    pub_ips = do_cmd(\"ip addr show {} | awk '/inet/ {{print $2}}'\".format(pubif)).split()\n    vip_folders = get_ip_folder_map(gwgroup)\n    my_folders = []\n    for pip in pub_ips:\n        if pip in vip_folders and pip != vip_folders[pip]:\n            my_folders.append(os.path.basename(vip_folders[pip]))\n    folders = get_all_folder_info()\n    images = []\n    for folder in folders:\n        if folder in my_folders:\n            images.append({\n                'image': 'image_{}'.format(folder),\n                'pool': folders[folder]['pool']\n            })\n    return images\n    \ndef remove_old_snap(pool, rbd_image):\n    with rados.Rados(conffile=CEPH_CONF) as cluster:\n        with cluster.open_ioctx(str(pool)) as ioctx:\n            with rbd.Image(ioctx, rbd_image) as image:\n                snaps = sorted(image.list_snaps(), key=lambda snap: snap['name'])\n                if len(snaps) > MAX_SNAP_COUNT:\n                    for snap in snaps[0:len(snaps)-MAX_SNAP_COUNT]:\n                        image.remove_snap(snap['name'])\n                        \ndef main():\n    images = get_images()\n    for image in images:\n        create_snap(image['pool'], image['image'])\n        remove_old_snap(image['pool'], image['image'])\n        device = do_shell(\"rbd showmapped | awk '/{}[ \\t]*{}/ {{print $5}}'\".format(image['pool'], image['image']))\n        do_shell('/usr/local/bin/monitor_rbd.sh {}'.format(os.path.basename(device)))\n        \nif __name__ == \"__main__\":\n    main()\n```\n\n## 总结\n\n内容和代码都比较多，其实每一个技术点都可以单独拿出来写一篇，但是我觉得这是一个完整`feature`，想让大家能够代入，了解到完成这样一个`feature`周边需要支持的各种技术点和注意点，一个`feature`往往是经过不断迭代和维护，很多实现方法也随着时间和应用场景不断发生变化。\n\n完成这样一个`feature`，我也是反复修改，就比如异步实现`RBD image`的创建和删除，很多场景在生产环境和测试环境中的 情况是完全不一样的，比如我开发的时候创建的`image`都是`1G`的，当然很快，也不能存在什么阻塞的问题，也遇到很多问题和想不通的地方，感谢我的同事和前辈提供的帮助和启发。\n\n最后，衷心希望`ceph`能够早日将`CephFS`完善，保证其在生产环境中的稳定性和性能。这样我们也就不用绞尽脑汁这般曲线救国了，哈哈。\n\n最后的最后，贴上部分代码地址，由于项目保密性等原因，我只能贴出比较关键的代码，大家请见谅，我觉得这些代码应该足够了，足够给大家提供一个思路了，其实往往思路比代码更重要，相信很多人的实现方式要比我更加优秀呢！\n\n> 代码地址：https://github.com/tony-yin/Multi_RBD_HA\n\n如果大家觉得有帮助的话，欢迎`Star`哦 ~\\(≧▽≦)/~\n\n[1]: http://www.tony-yin.top/2017/12/07/RBD-HA/\n[2]: http://ow0mgad6r.bkt.clouddn.com/rbd2-600x450.jpg\n\n\n","tags":["Keepalived"],"categories":["tech"]},{"title":"scp 免交互式和 ssh 免交互式脚本","url":"/2018/01/10/scp-and-ssh/","content":"\n<center>![Scp-And-Ssh][1]</center>\n\n简单实现了`scp`的免交互式脚本和`ssh`免交互式脚本。\n\n<!--more-->\n\n## scp\n\n场景：\n\n需要将以下源主机上的三个文件拷贝到以下目的主机的对应目录下\n\n> `src host`: 192.168.1.1\n> `dist host`: 192.168.1.2\n> `files`: `/root/1/1.txt`, `/root/2/2.txt`, `/root/3/3.txt`\n\n这时候如果手动做的话，将会很繁琐，所以这时候需要一个脚本能够实现文件的自动复制，并且脚本需要自动把密码验证的步骤也覆盖。\n\n这里的关键就是如何实现`shell`交互式命令行的自动化，这边可以用分界符`EOF`，`EOF`范围中的字符将会被作为命令输入到交互式命令行中，具体脚本如下：\n\n```shell\nSOURCEHOST=192.168.1.1\nDISTHOST=$1\nFILE1=/root/1/1.txt\nFILE2=/root/2/2.txt\nFILE3=/root/3/3.txt\nFOLDER1=/root/1/\nFOLDER2=/root/2/\nFOLDER3=/root/3/\nPASSWORD=123456\nscp FILE1 ${DISTHOST}${Folder1} << EOF\n$PASSWORD\nEOF\nscp FILE2 ${DISTHOST}${Folder2} << EOF\n$PASSWORD\nEOF\nscp FILE3 ${DISTHOST}${Folder3} << EOF\n$PASSWORD\nEOF\n```\n\n调用方式：`sh scp.sh 192.168.1.2`\n\n## ssh\n\n场景：\n\n上面的场景是建立在登陆`192.168.1`主机的基础上，现在我想我在任意主机上都可以实现上面将`192.168.1.1`的上述文件拷贝到`192.168.1.2`的对应目录下\n\n这里的关键是实现`ssh`的免密登陆，这时我们需要用到`sshpass`，具体介绍可以自行搜索下，用法如下：\n\n```shell\n#! /bin/bash\n\nSOURCEHOST=192.168.1.1\nDISTHOST=$1\nFILE1=/root/1/1.txt\nFILE2=/root/2/2.txt\nFILE3=/root/3/3.txt\nFOLDER1=/root/1/\nFOLDER2=/root/2/\nFOLDER3=/root/3/\nPASSWORD=123456\n\nsshpass -p $PASSWORD ssh $SOURCEHOST \\\n    sshpass -p $PASSWORD scp ${FILE1} ${DISTHOST}${FOLDER1} && \\\n    sshpass -p $PASSWORD scp ${FILE2} ${DISTHOST}${FOLDER2} && \\\n    sshpass -p $PASSWORD scp ${FILE3} ${DISTHOST}${FOLDER3}\n```\n\n调用方式：`sh scp.sh 192.168.1.2`\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/open_ssh-600x450.png\n","tags":["Linux"],"categories":["tech"]},{"title":"利用Raid卡工具获取逻辑盘是否为SSD","url":"/2018/01/05/RaidCardToolUtils/","content":"\n<center>![RaidCardToolUtils][2]</center>\n\n网上很多获取一块盘是否为`SSD`的方式都是不靠谱的，不能覆盖到所有情况。一般我们在操作系统上的硬盘都是虚拟出来的逻辑盘，比如`/dev/sda`这种，它可能对应一块单独的物理硬盘，也有可能对应的是几块盘组成的`raid`。我们有时候想获取一块盘的具体信息，比如磁盘类型、插槽号、序列号等等，这时候我们就得借助对应的`raid`卡工具了，最常见的如`Megacli`，通过逻辑盘找到对应的物理盘，然后读取信息。\n\n<!--more-->\n\n## Raid卡简介 \n\n所谓`raid`卡，就是为了更好的统一管理物理硬盘而存在的，在出现单独的`raid`卡之前，对硬盘做`raid`操作，需要`cpu`完成其中的计算操作，这个会很影响其他依赖`cpu`的应用或进程的性能，后来就将`raid`卡单独提取出来，并且在其之上存在一个小型`cpu`供来完成`raid`相关操作的计算，这其中最常见的`raid`工具应该非`Megacli`莫属了。\n\n为什么说最常见的呢？因为`raid`卡工具对应不同型号的`raid`卡是不一样，`LSI`只是一个半导体厂商，负责提供`raid`芯片，最后还需要集成到服务器厂商的机器上，所以最后的工具还是由厂商决定和提供，也可以理解为特定型号的`raid`对应各自的工具。\n\n## HBA卡简介\n\n近来，又出现了一种`HBA`卡，只从`HBA`的英文解释`HOST BUS ADAPTER`（主机总线适配器）就能看出来，他肯定是给主机用的，一般`HBA`就是给主机插上后，给主机扩展出更多的接口，来连接外部的设备。大多数讲到`HBA`卡都是指光纤的`HBA`卡，给主机提供光纤接口的。也有`ISCSI`的`HBA`卡，链接`ISCSI`设备的，从这种功能上说，我们也可以把独立网卡称为`HBA`卡，通过独立网卡扩展出网口来连接外部网络设备或主机。不过习惯上大部分`HBA`只是称光纤卡或者`iscsi`卡。\n\n简而言之，这种`HBA`卡本身是为了扩展外部连接设备而存在的，但是它具有部分`raid`功能，与`raid`卡相比它的优势在于它价格便宜，性价比高；劣势在于虽然具有`raid`功能，但是都是基础的功能，没有`raid`卡那么完善。\n\n> 这篇文章讲`raid`卡和`HBA`卡讲的挺好的：[HBA卡 和 RAID卡][1]\n\n## 需求和背景\n\n据我所知，这类工具往往是运维人员用的居多，但是往往开发中也会需要用到。本文通过获取逻辑盘对应盘的类型展开描述，并借此讲解获取逻辑盘的一类信息或通过逻辑盘操作对应物理盘。因为这其中的关键就是找到逻辑盘和物理盘之间的对应关系。无论是`raid`卡工具还是`HBA`卡工具都是罗列所有硬盘的信息，所以你要从中找到你选择的逻辑盘所对应的便是重中之重。\n\n逻辑盘对应的物理盘可能为单独的硬盘，也可能是`raid`，单独的可以直接读取硬盘类型，`raid`的话我们认为只会将同样类型的盘做`raid`，混合的情况不考虑。\n\n`raid`卡工具的话，我只对`Megacli`和`Sas3ircu`进行讲解，所以阅读本文前最好有使用以上两个工具的相关经验。首先我会根据目前存在的`raid`卡类型建立一个`map`关系，然后通过`raid`卡类型自动获取对应`raid`卡工具，每个`raid`卡都是一个类，然后里面的方法都是为该工具定制化的操作。\n\n## 获取raid卡工具\n\n目前就考虑两种型号的`raid`卡，以后有新的再往`map`里面填充就好了。`NotSupport`指的是其他不支持型号的`raid`卡和虚拟机。\n\n> `do_shell`是本人封装的一个在`python`中执行`shell`命令的方法，大家可以根据自己的情况对该方法进行转换\n\n通过获取的`card mode`，根据`map`找到对应的`tool`，然后实例化对应的工具类\n\n```python\nclass RaidCardToolFactory():\n    RaidCardMap = {\n        'SAS2208': MegaraidTool,\n        'SAS3008': HBATool,\n        'NotSupport': NotSupport\n    }\n    \n    def getTool(self):\n        card_model = self.get_raidcard_model()\n        tool = self.RaidCardMap[card_model]()\n        return tool\n        \n    def get_raidcard_model(self):\n        card_model = 'NotSupport'\n        card_info = do_shell(\"lspci | grep 'LSI Logic'\")\n        if card_info == '':\n            return card_model\n        card = card_info.strip().splitlines()[0].split()\n        if 'RAID bus controller' in card_info:\n            card_model = card[10] + card[11]\n        elif 'Attached SCSI controller' in card_info:\n            card_model = card[10]\n        return card_model\n```\n\n## Megaraid工具类\n\n1. 先通过`lsscsi`命令获取逻辑盘是否为`raid`；\n2. 如果是`raid`，那么直接根据`lsscsi`获取当前逻辑盘的`target id`，也就是第三个号，然后通过`megacli cfgdsply -aALL`获取所有`raid`信息，根据逻辑盘的`target id`对应物理盘中的`Target Id`找到对应`raid`，然后只要获取`raid`中第一块物理盘的硬盘类型即可，也就是`Media Type`，具体参见下方`API`: `get_ld_type`\n3. 如果不是`raid`，那么直接根据`lsscsi`获取当前逻辑盘的`target id`，也就是第三个号，这边的`target id`直接对应`megacli`中每一块单盘中的`Device Id`字段，所以根据`target id`匹配`megacli pdlist aAll`获取磁盘列表的每一项的`Device Id`便可以找到对应的物理盘，具体参见下方`API`: `get_pd_type`。\n\n```python\nclass MegaraidTool():\n    def get_disk_type(self, disk_name):\n        scsi_info = do_shell(\"lsscsi | grep {} -w\".format(disk_name))\n        target_id = scsi_info.split()[0].split(\":\")[2]\n        serial_nu = scsi_info.split()[3].strip()[2:]\n        if \"LSI\" in scsi_info:\n            disk_type = self.get_ld_type(target_id, serial_nu)\n        else:\n            disk_type = self.get_pd_type(target_id)\n        return disk_type\n        \n    def get_ld_type(self, target_id, serial_nu):\n        disk_type = ''\n        cmd = MEGACLI + ' cfgdsply -aALL -NoLog|grep -E \"Product Name|Target Id|Media Type\"'\n        output = do_shell(cmd)\n        adapters = output.split('Product Name')\n        for adapter in adapters:\n            if serial_nu not in adapter:\n                continue\n            lines = adapter.split('\\n')\n            for line in lines:\n                if \"Target Id: {}\".format(target_id) in line:\n                    index = lines.index(line)\n                    if 'Solid State Device' in lines[index + 1]:\n                        disk_type = \"SSD\"\n                    else:\n                        disk_type = \"HDD\"\n                    break\n            if disk_type != '':\n                break\n        return disk_type\n        \n    def get_pd_type(self, target_id):\n        disk_type = ''\n        cmd = MEGACLI + ' pdlist aAll | grep -E \"Device Id|Media Type\"'\n        output = do_shell(cmd)\n        lines = output.split('\\n')\n        if 'Device Id: {}'.format(target_id) not in lines:\n            return ''\n        index = lines.index('Device Id: {}'.format(target_id))\n        if 'Solid State Device' in lines[index + 1]:\n            disk_type = \"SSD\"\n        else :\n            disk_type = \"HDD\"\n        return disk_type\n```\n\n## HBA工具类\n\n1. `HBA`类用的工具是`sas3ircu`，首先我们需要根据命令`sas3ircu list`获取所有的`controller`，然后每次获取信息都需要遍历所有`controller`；\n2. 第一步依旧是判断逻辑盘是否为`raid`；\n3. 如果是`raid`，获取逻辑盘的`target id`，与之匹配的是`sas3ircu`中的`Initiator at ID`字段，找到对应的`raid`，然后通过获取其下第一个物理盘的类型，这边类型字段变成了`Drive Type`，具体参考下方`API`: `get_ld_type`；\n4. 如果非`raid`，我匹配的是`sas3ircu`中的`Sas Address`字段，那么逻辑盘的`Sas Address`如何获取呢？这边我用的方式是通过`udev`获取逻辑盘的`symlink`，这里面有很多`address`，而我们需要的是`by-path`，我这边就简单做了，看`sas3ircu`每个盘的`Sas Address`是否被`udev`获取的`symlink`包含，如果包含了，那么也就匹配到了，然后直接获取`Drive Type`字段就可以得到磁盘类型类；具体参考下方`API`: `get_pd_type`\n\n```python\nclass HBATool():\n    def get_disk_type(self, disk_name):\n        scsi_info = do_shell(\"lsscsi | grep {} -w\".format(disk_name))\n        if \"LSI\" in scsi_info:\n            target_id = scsi_info.split()[0].split(\":\")[2]\n            disk_type = self.get_ld_type(target_id)\n        else:\n            sas_address = do_cmd('udevadm info --query=symlink --name={}'.format(disk_name))\n            disk_type = self.get_pd_type(sas_address)\n        return disk_type\n        \n    def get_ld_type(self, target_id):\n        disk_type = ''\n        controllers = self.get_controllers()\n        for controller in controllers:\n            cmd = 'sas3ircu {} display|grep -E \"Initiator at ID|Drive Type\"'.format(controller)\n            output = do_shell(cmd)\n            if 'Initiator at ID #{}'.format(target_id) in output:\n                lines = output.splitlines()\n                index = lines.index('Initiator at ID #{}'.format(target_id))\n                if 'HDD' in lines[index + 1]:\n                    disk_type = 'HDD'\n                else:\n                    disk_type = 'SSD'\n                break\n        return disk_type\n        \n    def get_pd_type(self, sas_address):\n        disk_type = ''\n        controllers = self.get_controllers()\n        for controller in controllers:\n            cmd = 'sas3ircu {} display|grep -E \"SAS Address|Drive Type\"'.format(controller)\n            output = do_shell(cmd)\n            lines = output.splitlines()\n            for i in xrange(0, len(lines), 2):\n                address = lines[i].split()[-1].replace('-', '')\n                if address in sas_address:\n                    if 'HDD' in lines[i + 1]:\n                        disk_type = 'HDD'\n                    else:\n                        disk_type = 'SSD'\n                    break\n            if disk_type != '':\n                break\n        return disk_type\n        \n    def get_controllers(self):\n        cmd = 'sas3ircu list | awk \\'{print $1}\\''\n        list = do_shell(cmd).splitlines()\n        index = list.index('Index') + 2\n        controllers = []\n        for i in range(index, len(list) - 1):\n            controllers.append(list[i])\n        return controllers\n\n```\n\n## 调用方式\n\n```python\nfrom mcs3.raidcardutils import RaidCardToolFactory\n\ntool = RaidCardToolFactory().getTool()\ndisk_type = tool.get_disk_type(disk_name)\n```\n\n## 总结\n\n其实这其中的关键就是先找到每一块物理盘的唯一标识，然后我们根据工具获取列表中的唯一标识字段，获取逻辑盘对应的信息，就比如上面的`Device Id`，对应的是逻辑盘的`target id`。\n\n> 完整代码地址：https://github.com/tony-yin/RaidCardTool/\n> 如果有所帮助的话，帮忙`star`一下哦 ^_^\n\n\n[1]: http://www.cnblogs.com/weikunzz/p/6707395.html\n[2]: http://ow0mgad6r.bkt.clouddn.com/raid-600x450.png\n\n","tags":["Raidcard"],"categories":["tech"]},{"title":"Daily Article Vol 1 - (2017/11/1 ~ 2017/12/31)","url":"/2018/01/01/Daily-Article-2017/","content":"\n<center>![Daily Article 2][62]</center>\n\n书籍可以系统的学习一些知识，并且需要比较长的时间集中注意力学习。而现在网络越来越发达，各种社区的流行，还有开源分享精神的传播，导致现在互联网上很多优秀文章、博客、微信公众号等出现，这些文章贴近热点，往往都很新，并且篇幅有长有短，我们可以利用一些碎片时间来吸收这些知识。优秀文章很多，所以每天读个一两篇文章可以作为一个习惯养成，这样日积月累相信会获益良多。\n\n`Daily Article`系列就是为了记录我每天的阅读历程，以月为单位，每个月出一篇大的总结，一是为了约束自己每天按时按量阅读，也给自己打气，二是给自己每个月的阅读内容做一个总结，用于回头阅读，三是将其中有质量的内容分享给有需要的人。由于我是从`2017`九月才可以记录，所以趁着年底索性把十一和十二月的记录一次性发出来。\n\n目前我采取星星的方式给文章评级，最高三颗星，最低没有星星，一般我放上来的都不会是太水的文章，所以如果是一般的工具类或者没有很大特色的文章我不会进行标记。一颗星表示`good`，即这篇文章有特色，对自己有帮助；两颗星表示`very\ngood`，说明这篇文章内容很好，有深度有广度，是一篇很有质量的文章；三颗星表示`excellent`，说明这篇文章不仅内容技术讲的很到位，文章文笔也很出色，实践结合理论，让人很容易理解，看完后收获很大或者是顿悟，总而言之是一片很优秀的文章。当然我还会在这三个等级中结合半个星星进行调节，反正就是对文章的一个个人看法而已，仁者见仁，智者见智吧。\n\n<!--more-->\n\n## 十一月\n\n1. [大话 Ceph -- CephX 那点事儿][1](11/15-11/17) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n2. [互联网架构为什么要做服务化？][2](11/18) <i class='fa fa-star'></i>\n3. [基于开源软件构建高性能集群NAS系统][3](11/20)\n4. [虚拟IP原理][4](11/21)\n5. [究竟为什么要引入数据库中间件][5](11/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n6. [程序员如何用技术变现（上）][6](11/28)\n7. [Go语言，Docker和新技术][8](11/28)\n8. [秒杀系统架构优化思路][7](11/29) <i class='fa fa-star'></i>\n\n## 十二月\n\n1. [Ceph开发每周谈首发][8](12/1)\n1. [Ceph开发每周谈Vol2][9](12/4)\n1. [Ceph开发每周谈Vol3][9](12/5)\n1. [Ceph开发每周谈Vol4][9](12/5)\n1. [Ext4 Filesystem][13](12/11)\n1. [Formatted 'Ext4' External Hard Disk is Busy][14](12/12)\n1. [Ext4lazyinit detail][15](12/12)\n1. [Ceph开发每周谈Vol5][16](12/13)\n1. [Ceph开发每周谈Vol6][17](12/13)\n1. [Ceph开发每周谈Vol7][18](12/13)\n1. [Ceph开发每周谈Vol8—社区加快开发节奏, CDS 变更为 CDM, Firefly 结束版本支持][19](12/13)\n1. [Ceph开发每周谈Vol9—Ceph开发每周谈 Vol 9—加密、压缩、EC 全场景支持][20](12/13)\n1. [Ceph开发每周谈 Vol 10—NFS 已经被 RadosGW 支持|用户态][21](12/13)\n1. [Ceph开发每周谈 Vol 11—RadosGW 支持 KeyStone V3, AWS v4, 多站点多活][22](12/13)\n1. [Ceph开发每周谈 Vol 12—Scrub 增强，Jewel 小结][23](12/13)\n1. [An introduction to Linux filesystems][24](12/15 ~ 12/17) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [linux lsof命令详解][25](12/18)\n1. [linux中的ldd命令简介][26](12/18)\n1. [linux-->ldd命令的介绍][27](12/18)\n1. [【Linux笔记】ldconfig、ldd][28](12/18)\n1. [Linux 的 EXT4 文件系统的历史、特性以及最佳实践][29](12/18) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n1. [理解inode][30](12/18)\n1. [Ceph开发每周谈 Vol 13 — Cache on SPDK / bufferlist][31](12/19)\n1. [Ceph开发每周谈 Vol 14 — LDAP/ BlueStore SMR][32](12/19)\n1. [Ceph开发每周谈 Vol 15—Unix Socket / BlueStore 压缩和 Checksum][33](12/19)\n1. [Everything is a file][34](12/21) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [Overview of the Directory Tree][35](12/21)\n1. [没错，它就是存储界的“大胃王”][36](12/22)\n1. [Ceph开发每周谈 Vol 16—Jewel RC Release!][37](12/22)\n1. [Ceph开发每周谈 Vol 17 — ARM Status | RBD 一致性组合 | 内核模块 转至元数据结尾][38](12/22)\n1. [Ceph开发每周谈 Vol 18 — EXT4 废弃论战? | 去重支持][39](12/22)\n1. [Ceph开发每周谈Vol19 | Ceph Next 2016 闭门会议资讯独家大放送][40](12/26)\n1. [Ceph开发每周谈Vol 20 | NVMe Over Fabric/Kernel Multi Queue][41](12/26)\n1. [Ceph开发每周谈Vol 104 | NFS Ganesha VS Kernel Client][42](12/27)\n1. [Ceph开发每周谈Vol 21 | ZetaScale | CMP/WriteSame][43](12/27)\n1. [Ceph开发每周谈Vol 22 | 全球最大Ceph集群到底有多大？][44](12/28)\n1. [Ceph开发每周谈vol23｜BlueStore新动向][45](12/28)\n1. [Ceph开发每周谈 Vol 24｜Jewel 10.2.1 第一个 Bug 修复版本释出][46](12/28)\n1. [Ceph开发每周谈 Vol 25 | Ceph & DPDK 网络插件开源][47](12/29)\n1. [Ceph开发每周谈Vol 27｜主线分支默认启用 AsyncMessenger][48](12/29)\n1. [Ceph开发每周谈 Vol 28 | OSD 心跳 | Jewel RBD 测试][49](12/30)\n1. [Ceph开发每周谈 Vol 29 — RBD Cache 警告: 数据不一致风险][50](12/30)\n1. [Ceph开发每周谈 Vol 30 — ISA-L 和 BlueStore 性能有哪些进展？][51](12/30)\n1. [用GDB调试程序（一）][52](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（二）][53](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（三）][54](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（四）][55](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（五）][56](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（六）][57](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（七）][58](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [rados put striper功能的调试][59](12/31) <i class='fa fa-star'></i>\n1. [准备Ceph开发环境][60](12/31) <i class='fa fa-star'></i>\n1. [ceph编译源码、单机搭建调试环境][61](12/31) <i class='fa fa-star'></i>\n\n[1]: http://www.xuxiaopang.com/08/23/easy-ceph-CephX/\n[2]: http://mp.weixin.qq.com/s/UiDd-1zwrqIsk3-KEcAQaA\n[3]: http://blog.csdn.net/liuaigui/article/details/7163482\n[4]: http://blog.csdn.net/whycold/article/details/11898249\n[5]: http://mp.weixin.qq.com/s/8aI9jS0SXJl5NdcM3TPYuQ\n[6]: https://time.geekbang.org/column/article/183\n[7]: https://h5.ele.me/hongbao/#hardware_id=&is_lucky_group=True&lucky_number=10&track_id=&platform=0&sn=29dad164ef30a0c9&theme_id=1745&device_id=\n[8]: https://time.geekbang.org/column/article/294\n[9]: https://www.xsky.com/tec/ceph-weekly-vol-1/\n[10]: https://www.xsky.com/tec/ceph-weekly-vol-2/\n[11]: https://www.xsky.com/tec/ceph-weekly-vol-3/\n[12]: https://www.xsky.com/tec/ceph-weekly-vol-4/\n[13]: https://www.thomas-krenn.com/en/wiki/Ext4_Filesystem\n[14]: https://www.hecticgeek.com/2015/01/ext4-external-hard-disk-busy-at-idle-fix/\n[15]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=bfff68738f1cb5c93dab1114634cea02aae9e7ba\n[16]: https://www.xsky.com/tec/ceph-weekly-vol-5/\n[17]: https://www.xsky.com/tec/ceph-weekly-vol-6/\n[18]: https://www.xsky.com/tec/ceph-weekly-vol-7/\n[19]: https://www.xsky.com/tec/ceph-weekly-vol-8/\n[20]: https://www.xsky.com/tec/ceph-weekly-vol-9/\n[21]: https://www.xsky.com/tec/ceph-weekly-vol-10/\n[22]: https://www.xsky.com/tec/ceph-weekly-vol-11/\n[23]: https://www.xsky.com/tec/ceph-weekly-vol-12/\n[24]: http://link.zhihu.com/?target=https%3A//opensource.com/life/16/10/introduction-linux-filesystems\n[25]: http://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316599.html\n[26]: http://blog.csdn.net/stpeace/article/details/47069215\n[27]: http://blog.csdn.net/qq_26819733/article/details/50610129\n[28]: http://www.cnblogs.com/lyongde/p/4190588.html\n[29]: https://zhuanlan.zhihu.com/p/27875337\n[30]: http://www.ruanyifeng.com/blog/2011/12/inode.html\n[31]: https://www.xsky.com/tec/ceph-weekly-vol-13/\n[32]: https://www.xsky.com/tec/ceph-weekly-vol-14/\n[33]: https://www.xsky.com/tec/ceph-weekly-vol-15/\n[34]: https://opensource.com/life/15/9/everything-is-a-file\n[35]: http://www.tldp.org/LDP/sag/html/dev-fs.html\n[36]: https://www.xsky.com/news/20171218/\n[37]: https://www.xsky.com/tec/ceph-weekly-vol-16/\n[38]: https://www.xsky.com/tec/ceph-weekly-vol-17/\n[39]: https://www.xsky.com/tec/ceph-weekly-vol-18/\n[40]: https://www.xsky.com/tec/ceph-weekly-vol-19/\n[41]: https://www.xsky.com/tec/ceph-weekly-vol-20/\n[42]: http://mp.weixin.qq.com/s/h9mz5gWN8tKRz8by2-Sn6w\n[43]: https://www.xsky.com/tec/ceph-weekly-vol-21/\n[44]: https://www.xsky.com/tec/ceph-weekly-vol-22/\n[45]: https://www.xsky.com/tec/ceph-weekly-vol-23/\n[46]: https://www.xsky.com/tec/ceph-weekly-vol-24/\n[47]: https://www.xsky.com/tec/ceph-weekly-vol-24-2/\n[48]: https://www.xsky.com/tec/ceph-weekly-vol-27/\n[49]: https://www.xsky.com/tec/ceph-weekly-vol-28/\n[50]: https://www.xsky.com/tec/ceph-weekly-vol-29/\n[51]: https://www.xsky.com/tec/ceph-weekly-vol-30/\n[52]: http://blog.csdn.net/haoel/article/details/2879\n[53]: http://blog.csdn.net/haoel/article/details/2880\n[54]: http://blog.csdn.net/haoel/article/details/2881\n[55]: http://blog.csdn.net/haoel/article/details/2882\n[56]: http://blog.csdn.net/haoel/article/details/2883\n[57]: http://blog.csdn.net/haoel/article/details/2884\n[58]: http://blog.csdn.net/haoel/article/details/2885\n[59]: http://www.zphj1987.com/2017/04/26/rados-put-strip-debug/\n[60]: https://ivanjobs.github.io/2016/05/11/prepare-ceph-dev-env.html\n[61]: https://my.oschina.net/u/2460844/blog/515353\n[62]: http://ow0mgad6r.bkt.clouddn.com/2018-1b.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"译：一切皆文件","url":"/2017/12/21/Everything-is-a-file/","content":"\n<center>![Everything is a file][7]</center>\n\n这里先提一个技巧性的问题:以下哪一个是文件?\n\n* 目录\n* `Shell`脚本\n* `Office`文档\n* 串行端口（`Serial ports`）\n* 内核数据结构\n* 内核调优参数\n* 硬盘驱动器\n* 分区\n* 逻辑卷（`LVM`）\n* 打印机\n* 套接字（`Sockets`）\n\n也许你不会相信，但是对于`Unix`和`Linux`，它们都是文件。这是最令人惊奇的概念之一——这样做使得许多管理任务可以被一些非常简单但功能强大的方法执行，否则这些任务实现起来可能非常困难甚至不可能。\n\n<!--more-->\n\n## 备份主引导记录\n\n举个简单任务的例子，考虑一下为你的硬盘驱动器地主引导记录（`MBR`）做一个备份工作。有时候我需要恢复或重新创建我的`MBR`，尤其是分区表。从头开始重新创建它是非常困难的。但是从保存好的文件中恢复出来这是非常容易的。`Linux`有一个很强大的`GNU`工具 --- `dd`，它可以实现这个和其他很多功能。\n\n`dd`表示`disk dump`的缩写，意为“磁盘转储”，但是我们很多资深管理员一直认为它是`disk destroyer`的缩写，因为如果你不是很小心的话，这个工具会准确无误地执行你告诉它要做的事情，包括将硬盘上或者分区上所有的数据都破坏掉。\n\n以下命令将会备份你的`MBR`，它必须要是`root`用户执行，因为非`root`用户没有访问`/dev`目录下硬盘驱动器[设备文件][1]的权限。`BS`是`Block Size`缩写，表示块大小，`count`表示从源文件读取的块的个数。这个命令将在`/tmp`目录创建一个`myMBR.bak`的文件。这个文件的大小将为`512`字节，包含了`MBR`的内容，包括引导代码和分区表等。\n\n```\ndd if=/dev/sda of=/tmp/myMBR.bak bs=512 count=1\n```\n\n如果`MBR`被损坏了，就需要引导到一个修复盘并执行下面的命令，这个命令本质上就是上面的反向操作。值得注意的是这条命令没有必要指定块大小和块个数这两个参数，因为`dd`命令将会把备份文件简单地拷贝到硬盘的第一个扇区，并且当它执行到源文件末尾后停止。\n\n```\ndd if=/tmp/myMBR.bak of=/dev/sda\n```\n\n## 都是文件系统的一部分\n\n`Linux`计算机上的所有内容都可以作为文件系统空间的文件被访问。这是非常重要的，这使得我们 可以[使用通用的工具访问不同的东西][2]。\n\n`dd`命令可用于将硬盘的整个分区拷贝到一个文件或者如下所示的其他硬盘。在这里`dd`命令再次将数据拷贝到输入设备的末尾并停止。请确保输出设备的容量要大于输入设备。\n\n```\ndd if=/dev/sdf2 of=/dev/sdg3\n\ndd if=/dev/sda of=/dev/sdg\n```\n\n此外文件系统还有其他工具可以达到此作用。比如，`cat`命令可以用来将任意文件的内容发送到标准输出，这包括分区和整个硬盘。然后，输出还可以被重定向到一个文件。\n\n```\ncat /dev/sda1 > partition1.backup\n```\n\n但是，`cat`命令没有`dd`命令的控制功能。例如，不能指定从源设备或者源文件读取的数据量。\n\n\n下面是一个有趣的实验，它将正面一切皆文件的事实。大多数`Linux`发行版都有多个虚拟控制台，其中`1`到`7`可以用来登录到一个带有`shell`接口的本地控制台会话。可以通过一些组合键访问它们，比如`Ctrl-Alt-F1`是控制台`1`，`Ctrl-Alt-F2`是控制台2，以此类推。\n\n按`Ctrl-Alt-F2`切换到控制台2。在一些发行版中，登录信息包括与此控制台相关的`tty`（`Teletype`）设备，但是也有很多发行版不包括。页面应该显示`tty2`的信息，因为你当前在控制台`2`。\n\n用一个非`root`登录，你可以通过`who am i`这个命令来确定哪一个`tty`设备连接到当前控制台。\n\n在我们实际执行这个实验之前，请看一下`/dev`目录下的`tty2`和`tty3`设备的列表清单。\n\n```\nls -l /dev tty[23]\n```\n\n有大量的定义过的`tty`设备，但是它们其中的大多数我们并不关心，我们只关系`tty2`和`tty3`设备。作为设备文件，它们没有什么特殊之处；它们只是简单的字符类型的设备。我们将用这些设备做这个实验。`tty2`设备连接到虚拟控制台`2`，`tty3`设备连接到虚拟控制台`3`。\n\n按`Ctrl-Alt-F3`组合键切换到控制台`3`，再次以同样的非`root`用户登录。\n\n现在在控制台`3`输入以下命令：\n\n```\necho \"Hello world\" > /dev/tty2\n```\n\n按`Ctrl-Alt-F2`组合键返回控制台`2`。字符串“Hello world”（没有引号）将显示在控制台`2`上。\n\n这个实验也可以在`GUI`桌面的终端模拟器上进行。桌面上的终端会话在`/dev`树中使用伪终端设备，比如`/dev/pts/1`。通过`Konsole`或者`Xterm`开启两个终端会话，确定它们连接到哪个伪终端后，使用其中一个发送消息给另一个。\n\n现在继续试验，使用`cat`命令在不同的终端显示`/etc/fstab`文件。\n\n\n另一个有趣的实验是使用`cat`命令直接将文件打印到打印机上。假设你的打印机设备是`/dev/usb/lp0`，并且你的打印机可以直接打印`PDF`文件，下面的命令将会在你的打印机上打印一个`PDF`文件。\n\n```\ncat test.pdf > /dev/usb/lp0\n```\n\n`dd`命令也可以用来打印一个准备打印的文件。不过，我认为`cat`命令实际上更适合这个任务。\n\n## “一切皆文件”的含义\n\n“一切都是文件”的含义是深远的，远远超过了像这篇文章所列举的那样。你们已经在前面的实验中看到过一些例子，但这里有一个包含这些和更多的简短列表。\n\n1. 克隆硬盘。\n2. 备份分区。\n3. 备份主引导记录(`MBR`)。\n4. 在`u`盘上安装`ISO`镜像。\n5. 与其他终端用户沟通。\n6. 将文件打印到打印机。\n7. 更改`/proc pseudo`文件系统中的某些文件的内容，以修改运行内核的配置参数。\n8. 用随机数据或零覆盖文件、分区或整个硬盘驱动器。\n9. 将不需要的输出重定向到`/dev/null`设备，它将永远不会显示。\n10. 等等，等等，等等。。。\n\n这里有太多的例子，任何一个列表都只是表面的一部分。我相信，你肯定会想出或指出许多比我这里提到更有创造性的方式，来使用`Linux`的这个特性。我很乐意看到你对如何使用“一切都是文件”的评论。\n    \n## 附加信息\n\n有关`/dev/`目录和你可能在那里找到的设备的更多信息，请参阅`Linux Journal`上的[这篇文章][3]。有关单个设备的更详细信息，[Linux文档项目][6]中的[这篇文章][4]和[这篇文章][5]会有所帮助。\n\n> 原文地址：https://opensource.com/life/15/9/everything-is-a-file\n\n[1]: https://en.wikipedia.org/wiki/Device_file\n[2]: http://yarchive.net/comp/linux/everything_is_file.html\n[3]: http://www.linuxjournal.com/article/2597\n[4]: http://www.tldp.org/LDP/sag/html/dev-fs.html\n[5]: http://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/dev.html\n[6]: http://www.tldp.org/\n[7]: http://ow0mgad6r.bkt.clouddn.com/file-600x450.png\n","tags":["Filesystem"],"categories":["read"]},{"title":"译：Linux 文件系统介绍","url":"/2017/12/17/Linux-Filesystem/","content":"\n<center>![Linux Filesystem][1]</center>\n\n本文旨在对`Linux`文件系统概念进行深层次的讨论。本文既不准备对某个特定类型的文件系统（比如`ext4`）进行基础性的描述，也不打算作为一个讲解文件系统命令的教程。\n\n<!--more-->\n\n每台通用的计算机都需要把各种类型的数据存储在硬盘驱动器(`HDD`)或者一些同样功能的设备上，比如`USB`。存储在这些设备上有几个原因，首先,`RAM`会在计算机电源关闭时丢失内容，虽然也有非易失性类型的内存，可以在电源关闭后维持数据存储不丢失(如`flash`内存也就是闪存使用的`USB`和固态硬盘)，但`flash`内存要比一些标准的、挥发性的内存比如`DDR3`和其他类似的类型昂贵的多。\n\n数据需要存储在硬盘驱动器上的第二个原因是，即使是标准的`RAM`也要比磁盘空间更昂贵。`RAM`和磁盘成本都在迅速下降，但如果按每字节的成本来算的话还是`RAM`更高。我们就基于`16GB RAM`和`2TB`硬盘的成本，快速计算其每个字节的成本，结果显示  `RAM`比硬盘驱动器的价格高约`71`倍。目前，`RAM`的典型成本大约每字节`0.0000000043743750`。\n\n更加直截了当说的话，在计算机的早期，一种内存是基于`CRT`屏幕上的点的，每一比特大约`1`美元，这是非常非常昂贵的!\n\n\n## 定义\n\n你也许会听到人们经常以不同的或者混淆的方式谈论文件系统这个词。这个词本身可能有多重含义，你可能需要从讨论或文档的语境中辨别真正的意思。\n\n我将尝试根据我在不同情况下使用它的方式来定义“文件系统”的各种含义。注意，在试图遵循标准的“official”含义时，我的意图是根据它的各种用法定义术语。在本文后面的小节中，将更详细地讨论这些含义。\n\n1. 整个`Linux`目录结构从顶部(/)根目录开始。\n2. 各种特定类型的数据存储格式，如`EXT3`、`EXT4`、`BTRFS`、`XFS`等。`Linux`支持近`100`种类型的文件系统，包括一些非常古老的文件系统，以及一些最新的文件系统。每个文件系统类型都使用自己的元数据结构来定义如何存储和访问数据。\n3. 一个分区或被格式化为特定类型文件系统的逻辑卷，可以被挂载到`Linux`文件系统上的指定挂载点上。\n\n## 基本的文件系统功能\n\n磁盘存储是必需的，它带来了一些有趣且不可避免的细节。显然，文件系统的设计目的是为数据的非易失性存储提供空间，这是它的最根本的功能。但是它还有许多其他重要的功能满足不同的需求。\n\n所有文件系统都需要提供一个命名空间（`namespace`）——即一个命名和组织的方法。它定义了如何命名文件，具体来说是文件名的长度和可用于文件名的字符的子集，这些字符可以从全部字符集中获取。它还定义了磁盘上数据的逻辑结构，例如使用目录来组织文件，而不是将它们集中在一个单一的、巨大的文件集中。\n\n一旦定义了名称空间，就需要一个元数据结构来为该名称空间提供逻辑基础。其中元数据包括支持分层目录结构所需的数据结构；用于确定磁盘上哪些块空间已经被使用和哪些可用的结构；允许维护文件和目录名称的结构；文件相关的信息，比如它们的大小和时间，比如创建时间、修改时间或最后访问时间等等；以及属于文件的数据在磁盘上面的位置。还有一些其他元数据用于存储关于磁盘划分的高级信息，如逻辑卷和分区。这个更高级别的元数据和它所代表的结构包含了描述存储在驱动器或分区上的文件系统的信息，这些元数据独立于上面提到的一般文件系统元数据。\n\n文件系统还需要`API`接口为系统函数调用提供访问，这些系统函数调用操作文件和目录等文件系统对象。`APIs`提供诸如创建、移动和删除文件之类的接口。它还提供了一些算法来确定文件放置在文件系统上的位置。这些算法还有确定速度或最小化磁盘碎片等作用。\n\n现代文件系统还提供了一个安全模式，它是一个为文件和目录定义访问权限的方案。`Linux`文件系统安全模式有助于确保用户只能访问他们自己的文件，而不是其他人或操作系统本身的文件。\n\n最后的构建块是实现所有这些功能所需的软件。为了改提高系统和程序员效率，`Linux`使用了一种`two-part`的软件实现方式。\n\n<center>![filesystem diagram][2]</center>\n<center>图1：Linux two-part 文件系统软件实现方式</center>\n\n这两部分实现的第一部分是`Linux`虚拟文件系统。这个虚拟文件系统为内核和开发人员提供了访问**所有类型**文件系统的一组命令。虚拟文件系统软件调用特定的设备驱动程序来连接到各种类型的文件系统。文件系统特定的设备驱动程序是实现的第二部分。设备驱动程序将文件系统命令的标准集根据特定分区或逻辑卷上的文件系统类型做转换和解释。\n\n## 目录结构\n\n作为一个通常很有条理的处女座，我喜欢把东西放在那些小而有组织的地方，而不是一个大的桶。使用目录可以帮助我存储和定位我想找的文件。目录也被称为文件夹，因为它们可以被看作实际生活中办公桌上保存文件的文件夹。\n\n在`Linux`和许多其他操作系统中，目录可以以树状的层次结构来构造。`Linux`目录结构在[Linux文件系统层次标准(FHS)][3]中得到了很好的定义和记录。在访问它们时引用这些目录是通过使用由前斜杠(`/`)连接的顺序较深的目录名称(`/`)来实现的，例如`/var/log`和`/var/spool/mail`。这些被称为路径。\n\n下表提供了一个非常简短的标准、众所周知的和定义在顶层上的`Linux`目录及其用途的列表。\n<center>![top level directory][8]</center>\n<center>表1：Linux文件系统层次结构的顶层</center>\n\n表`1`中显示的目录及其子目录及其子目录的子目录，其中背景色为蓝色的目录被认为是根文件系统中不能缺少的组成部分。也就是说，它们不能作为单独的文件系统创建，并且在启动时安装。这是因为它们(特别是它们的内容)必须在引导时出现，以便系统正确引导。\n\n`/media`和`/mnt`目录是根文件系统的一部分，但它们不应该包含任何数据。相反，它们只是临时的挂载点。\n其余的目录，在表`1`中没有背景颜色的目录不需要在引导序列中出现，但是会在以后安装，在启动序列中准备主机来执行有用的工作。\n\n可以通过参考官方的[Linux文件系统层次标准][3]`(FHS)web`页面，以了解这些目录及其许多子目录的详细信息。维基百科对`FHS`的描述也很好。应尽可能密切地遵循这一标准，以确保业务和职能的一致性。不管主机上使用的文件系统类型是什么，这个分层目录结构都是相同的。\n\n## Linux 统一目录结构\n\n在一些非`linux PC`操作系统中，如果有多个物理硬盘或多个分区，每个磁盘或分区都被分配一个驱动器号。想定位到文件或程序所在的硬盘的位置，驱动器号是必需的，比如`C:`或`D:`。然后，以命令的形式发出驱动器字母`D:`例如，要更改到`D:`驱动器，然后使用`cd`命令更改到正确的目录来定位所需的文件。每个硬盘都有自己独立的和完整的目录树。\n\n`Linux`文件系统将所有物理硬盘和分区统一到一个目录结构中。这一切都是从顶部(`/`)目录开始的。所有其他目录及其子目录都位于`Linux`根目录下。这意味着只有一个单独的目录树来搜索文件和程序。\n\n它们之所以能工作,都是因为文件系统,如`/home`、`/tmp`、`/var`、`/opt`、`/usr`可以被创建在单独的物理硬盘上一个不同的分区,或一个不同的逻辑卷`/`(根)文件系统,然后安装在一个挂载点(目录)作为根文件系统树的一部分。即使是可移动的驱动器，如闪存盘或外部`USB`或`ESATA`硬盘驱动器也将安装到根文件系统中，并成为该目录树的一个不可分割的部分。\n\n在从一个版本的`Linux`发行版升级到另一个版本，或者从一个发行版切换成到另一个发行版时，文件系统有一个很好的理由可以做到这一点。一般来说，除了`Fedora`的`dnf`升级之类的升级工具之外，偶尔重新格式化包含操作系统的硬盘驱动器是明智的，因为在升级过程中，硬盘驱动器会清除任何随时间积累的东西。如果`/home`是根文件系统的一部分，它将被重新格式化，然后必须从备份中恢复。通过将`/home`格式化为一个单独的文件系统，那么在根文件系统格式化时它将识别成一个单独的文件系统，并且可以跳过当前步骤。这也适用于数据库、电子邮件收件箱、网站和其他可变用户和系统数据存储的目录`/var`。\n\n维护`Linux`目录树的某些部分作为单独的文件系统还有其他原因。例如，很久以前，当我还没有意识到围绕着所有需要的`Linux`目录都作为`/(root)`文件系统的一部分的潜在问题时，我曾用大量非常大的文件填充了我的主目录。由于`/home`目录和`/tmp`目录都不是独立的文件系统，而只是根文件系统的子目录，所以整个根文件系统都被填满了。操作系统没有空间创建临时文件或扩展现有的数据文件。起初，应用程序开始抱怨没有空间保存文件，然后操作系统本身开始变得非常奇怪。引导到单用户模式，并清除我的主目录中的问题文件，这让我可以重新开始。然后，我使用一个相当标准的多文件系统设置重新安装了`Linux`，并能够防止完全的系统崩溃再次发生。\n\n我曾经还遇到过一个情况，`Linux`主机继续运行，但是阻止用户使用`GUI`桌面登录。我能够使用一个[虚拟控制台][4]本地使用命令行接口(`CLI`)，并远程使用`SSH`。问题是，`/tmp`文件系统已经填满了，而`GUI`桌面所需的一些临时文件在登录时无法创建。由于`CLI`登录不需要在`/tmp`中创建文件，因此缺少空间并没有阻止我使用`CLI`进行登录。在这种情况下，`/tmp`目录是一个单独的文件系统，在卷组中有大量可用的空间，`/tmp`逻辑卷是其中的一部分。我只是将`/tmp`[逻辑卷扩展][5]到一个够大的容量（其实就是`LVM`扩容），以适应我对该主机所需要的临时文件空间数量的新需求，并解决了问题。请注意，此解决方案不需要重新启动，并且当`/tmp`文件系统被放大后，用户可以登录到桌面。\n\n> 逻辑卷扩展也可以参考我之前总结的一篇文章，简洁明了：[LVM动态扩展][6]\n\n另一种情况发生在我在一家大型科技公司做实验室管理员的时候。我们的一个开发人员在错误的位置（`/var`）安装了应用程序（我个人认为不能说是装在错误的位置，只能说装的位置的可用空间不合适）。应用程序崩溃是因为`/var`文件系统已经满了，而存储在`/var/log/`上的日志文件由于缺少空间，不能添加新的消息。但是，由于关键的`/(root)`和`/tmp`文件系统没有填充，系统仍然保持运行。删除违规应用程序并将其重新安装到`/opt`文件系统中解决了这个问题。（其实通过`LVM`动态扩容也是可以解决这个问题，要么扩展空间大小，要么换大空间的文件系统）\n\n## 文件系统类型\n\n`Linux`支持读取大约`100`个分区类型，它只可以在其中的几个而不是所有的分区上创建或写文件。但是，在同一个根文件系统上的不同类型的挂载文件系统是可以做到的，也是非常常见的。在此上下文中，我们讨论的是在硬盘或逻辑卷的分区上存储和管理用户数据所需的结构和元数据。这里提供了`Linux fdisk`命令识别的文件系统分区类型的完整列表，这样您就可以了解`Linux`与许多类型的系统之间的高度兼容性。\n\n```\n0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 <32M 40 Venix 80286 85 Linux extended c7 Syrinx 5 Extended 41 PPC PReP Boot 86 NTFS volume set da Non-FS data 6 FAT16 42 SFS 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux plaintext de Dell Utility 8 AIX 4e QNX4.x 2nd part 8e Linux LVM df BootIt 9 AIX bootable 4f QNX4.x 3rd part 93 Amoeba e1 DOS access a OS/2 Boot Manag 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi ea Rufus alignment e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD eb BeOS fs f W95 Ext'd (LBA) 54 OnTrackDM6 a6 OpenBSD ee GPT 10 OPUS 55 EZ-Drive a7 NeXTSTEP ef EFI (FAT-12/16/ 11 Hidden FAT12 56 Golden Bow a8 Darwin UFS f0 Linux/PA-RISC b 12 Compaq diagnost 5c Priam Edisk a9 NetBSD f1 SpeedStor 14 Hidden FAT16 <3 61 SpeedStor ab Darwin boot f4 SpeedStor 16 Hidden FAT16 63 GNU HURD or Sys af HFS / HFS+ f2 DOS secondary 17 Hidden HPFS/NTF 64 Novell Netware b7 BSDI fs fb VMware VMFS 18 AST SmartSleep 65 Novell Netware b8 BSDI swap fc VMware VMKCORE 1b Hidden W95 FAT3 70 DiskSecure Mult bb Boot Wizard hid fd Linux raid auto 1c Hidden W95 FAT3 75 PC/IX bc Acronis FAT32 L fe LANstep 1e Hidden W95 FAT1 80 Old Minix be Solaris boot ff BBT\n```\n\n拥有支持读取这么多分区类型的能力的主要目的是允许兼容性和与其他计算机系统的文件系统的某些互操作性。使用`Fedora`创建新文件系统时可用的选项如下所示。\n\n* btrfs\n* **cramfs**\n* **ext2**\n* **ext3**\n* **ext4**\n* fat\n* gfs2\n* hfsplus\n* minix\n* **msdos**\n* ntfs\n* reiserfs\n* **vfat**\n* xfs\n\n其他发行版支持创建不同的文件系统类型。例如，`CentOS 6`只支持创建上面列表中粗体显示的文件系统。\n\n## 挂载\n\n在`Linux`中，`to mount`一词指的是早期的计算机中，当一个磁带或可移动的磁盘包需要在适当的驱动器上进行物理安装时。在物理上放置磁盘之后，磁盘包上的文件系统将由操作系统逻辑上挂载，以使操作系统、应用程序和用户能够访问这些内容。\n\n挂载点仅仅是一个目录，就像任何其他的目录一样，它是作为根文件系统的一部分创建的。例如，`home`文件系统安装在目录`/home`上。文件系统可以安装在其他非根文件系统上的挂载点上，但这并不常见。\n`Linux`根文件系统安装在根目录上(`/`)非常早的引导序列。其他文件系统会被安装在后面，由`Linux`启动程序，无论是在`SystemV`下的`rc`，还是在新的`Linux`版本中的`systemd`，在启动过程中挂载文件系统是由`/ etc/fstab`配置文件管理的。一个容易记住的方法是`fstab`表示“文件系统表”，它是要挂载的文件系统的列表，还有它们指定的挂载点，以及特定文件系统可能需要的任何选项。\n\n文件系统安装在现有的目录（挂载点）上，使用`mount`命令。一般来说，任何被用作挂载点的目录都应该是空的，并且没有包含其中的任何其他文件。`Linux`不会阻止用户将一个文件系统安装到一个已经存在的文件系统上，或者在一个包含文件的目录上。如果你在现有的目录或文件系统上安装了一个文件系统，那么原始的内容将会被隐藏，只有新挂载的文件系统的内容才会可见。\n\n## 结论\n\n我希望本文能够把围绕“文件系统”这个术语的一些可能的混淆给理清楚。我花了很长时间才真正理解并理解`Linux`文件系统的复杂性、优雅和功能。\n\n如果你有问题，请把它们加到下面的评论中，我会尽量回答。\n\n## 下个月\n\n另一个重要的概念是，对于`Linux`，一切都是文件。这个概念为用户和系统管理员提供了一些有趣且重要的实际应用程序。我之所以提到这一点，是因为您可能想在我下个月的`/dev`目录下的文章前阅读我的[一切皆文件][7]的文章。\n\n> 原文地址：https://opensource.com/life/16/10/introduction-linux-filesystems\n\n“一切皆文件”这篇文章我也进行了翻译\n\n> 翻译地址：http://www.tony-yin.top/2017/12/21/Everything-is-a-file\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/filesystem-600x450.png \n[2]: http://ow0mgad6r.bkt.clouddn.com/filesystem_diagram.png\n[3]: http://www.pathname.com/fhs/\n[4]: https://en.wikipedia.org/wiki/Virtual_console\n[5]: https://opensource.com/business/16/9/linux-users-guide-lvm\n[6]: http://www.tony-yin.top/2017/11/14/LVM-Space-Expansion/\n[7]: https://opensource.com/life/15/9/everything-is-a-file\n[8]: http://ow0mgad6r.bkt.clouddn.com/linux-filesystaem.png\n","tags":["Filesystem"],"categories":["read"]},{"title":"阅读感悟：《白夜行》","url":"/2017/12/10/Into-Withe-Night/","content":"\n<center>![Into White Night][1]</center>\n\n阅读周期：2017/12/08 ~ 2017/12/10\n阅读评分：4.9\n阅读人群：还是成年的人看着好一点，不大适合未成年人\n\n> 我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借着这份光，我便能把黑夜当成白天。我从来就没有太阳，所以不怕失去。\n\n<!--more-->\n\n之前一直在看技术书，这周末想稍微放松一下自己，翻了翻买了许久却没阅读的《白夜行》，之前听身边很多人称赞过这本书，果然看了就没停的下来，花了两天直接拉满读完。。。\n\n坊间传言这是东野圭吾写的最好的一本书，我只看过《嫌疑人x的献身》，对比那个的话我觉得这一本更加细腻和深远。\n\n看完了这本书，我去看了很多的书评，各有各的见解，并且都很有道理，我想这就是东野圭吾的厉害支持了，真正厉害的推理小说，不是作者在推理，而是最后是读者在推理。\n\n对我个人而言，这本书最大的亮眼之处，就是作者重来不主动把某件事情说明，而是留下种种线索，相关关联，埋下伏笔，这样就导致永远没有一个确定性的答案，作者在意的也许不是具体的场景和手段，他着重刻画的是人性的本质、作恶的动机和灵魂的交融。\n\n回过头一看，全书亮司与雪穗的生活宛如两条平行线，男女主角之间竟然没有任何对白，也没有交待任何他们见面的情节。书最后亮司突然死了，故事也很突然就这样结束了。让人反应不过来，因为她的太阳失去了。亮司承担了所有的罪恶，为了想让她生活在阳光下。\n\n很多人都在讨论桐源和雪穗之间的关系，大多数人都认为他们是深爱着的。我个人觉得他们或许有爱，但是更多的是相互依靠和相互依附，也就是双方都是彼此的灵魂，活下去的羁绊，我觉得这是和单单的爱情是不一样的，否则他们大可以在一起，他们有的是钱，完全可以隐性瞒名的私奔。\n\n我觉得桐源应该更爱雪穗一些，比如一直在他身边守护着她，最后关头为了保住她选择自杀，还有就是只有和她ML的时候才不会迟泄（这个在夕子那个宾馆的章节可以看出），但是桐源尊重雪穗，他只是一直在她身边守护着，永远不会过分打扰她的生活，即使她和别的男人恋爱，即使结婚，即使离婚再结婚，在他心里，雪穗永远是最重要的，满足她想要的和想做的是他一生的追求。桐源一直是作恶的执行者，雪穗背后的指使者，很多读者都为桐源不值，觉得他为了雪穗付出了那么多，最后得到确是冷漠的不回头，其实他并不后悔，看到《嫌疑人x的献身》的同学都知道最后时刻男主多希望女主也能像这边的女主雪穗一样头也不回地离开，这样他的付出才是值得的。桐源对典子应该也是有感情的，从他愿意带她去大阪，跟她讲童年时光，不过这种感情是建立在先利用之后感情培养出来的，但是在桐源心中雪穗永远是最重要的，即使典子获取桐源所有的爱情，当要做出选择的时候，我相信桐源还是选择雪穗吧，因为我觉得在他心中爱情远没有他和雪穗之间的羁绊重要，他们之间的关系关乎到灵魂和生死，他们是对方相互的太阳。\n\n> 曾经拥有的东西被夺走，并不代表就会回到原来没有那种东西的时候。                    ---典子\n\n而雪穗难道对桐源没感情吗？我想肯定有，但是他们俩成为互相羁绊的时间太小了，才十一二岁，在太早的时间双方互相守护秘密，更像是相互保护，少了爱情的起源，所以她们也不知道双方之间是不是爱，雪穗说过她不知道如何去爱一个人，这个人说的可能是高工城，可能是一成，也有可能是桐源，桐源和雪穗经常就是在实行某个行动需要对方配合帮忙的时候才会聚头，比如夕子的那件事，雪穗肯定是通过手或者嘴帮助桐源完成了SJ，但是雪穗从来没有为高工城做过这样的事情，桐源在雪穗的心目中的位置可见一斑。但是也许她自己都不确定这是不是爱情，就像高工城内心的疑问一样，总感觉这份感情里面掺杂了很多其他东西。而雪穗对一诚或许在感情上面和别的人有一些不同，但是这也仅仅是有一点点不同，除了感情我觉得还有一个原因，雪穗不喜欢被别人无视，当大学社团一诚喜欢江利子忽略雪穗时，这让她感觉到嫉妒了，她一直认为自己是完美的，是所有人的焦点所在。\n\n很多人都在愤怒雪穗最后的冷漠，上面我也说了，除了桐源希望用自己的生命保全雪穗之外，雪穗的行为是头也不回的离去，要知道雪穗在此之前无论是生母、养母、前夫等等，都表现出自己的伤心，唯独这次雪穗没有表现正常，也算是她的失误了，我想她不仅因为是失去了她的太阳觉得无助，最重要的是他也不敢面对这个现实，她面对谁都可以表演的很好，但是除了桐源，她做不到，也许她过不了多久也会选择离开这个世界，要么就永远在白夜中度过。。。\n\n店已打烊女主确上楼而不是离开，是否会意味着要与亮一同结束在梦魇开始的地方？\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/white_night.jpg\n","tags":["Read"],"categories":["read"]},{"title":"故障修复：文件系统导致OSD启动失败","url":"/2017/12/08/Filesystem-Bug-Fix/","content":"\n<center>![Bug fix][1]</center>\n\n记一次因为文件系统导致`OSD`无法启动的故障修复。\n\n<!--more-->\n\n## 集群状况：\n\n`Cluster`：\n\n```\nroot@gigabyte:~# ceph -s\n    cluster de3627bb-c748-4623-8cb1-b88c646ff5d5\n     health HEALTH_WARN 42 pgs degraded; 1 pgs down; 1 pgs peering; 17 pgs recovering; 25 pgs recovery_wait; 359 pgs stale; 42 pgs stuck degraded; 1 pgs stuck inactive; 359 pgs stuck stale; 244 pgs stuck unclean; recovery 6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%); 2/23 in osds are down\n     monmap e1: 1 mons at {erxdl=10.16.180.28:6789/0}, election epoch 2, quorum 0 erxdl\n     mdsmap e654: 1/1/1 up {0=irlhy=up:active}\n     osdmap e218: 23 osds: 21 up, 23 in\n      pgmap v8943: 7936 pgs, 16 pools, 639 GB data, 159 kobjects\n            105 GB used, 76827 GB / 76933 GB avail\n            6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%)\n                 359 stale+active+clean\n                 201 active+remapped\n                7333 active+clean\n                  25 active+recovery_wait+degraded\n                  17 active+recovering+degraded\n                   1 down+peering\n```\n\n`OSD`：\n\n```\nroot@test:/data# ceph osd tree\n# id\tweight\ttype name\tup/down\treweight\n-3\t82.29\tpool 4T\n-4\t82.29\t\thost test1\n0\t0\t\t\tosd.0\tup\t    1\t\n8\t3.578\t\t\tosd.8\tup\t    1\t\n17\t3.578\t\t\tosd.17\tup\t    1\t\n5\t3.578\t\t\tosd.5\tup\t    1\t    \n...\n...\n16\t3.578\t\t\tosd.16\tup\t    1\t\n10\t3.578\t\t\tosd.10\tup\t    1\t\n15\t3.578\t\t\tosd.15\tup\t    1\t\n13\t0\t\t\tosd.13\tdown\t1\t\n```\n\n由上可知：`osd.0`和`osd.13`已经被集群剔除，并且权重变为了`0`\n\n## 报错日志：\n\n### 报错1：\n\n```\n2017-12-06 10:18:05.180802 7f4809ae07c0 -1 osd.13 234 set_disk_tp_priority(22) Invalid argument: osd_disk_thread_ioprio_class is  but only the following values are allowed: idle, be or rt\n\n2017-12-06 10:10:44.974634 7f4cbcfff700 -1 os/FileStore.cc: In function 'virtual int FileStore::read(coll_t, const ghobject_t&, uint64_t, size_t, ceph::bufferlist&, bool)' thread 7f4cbcfff700\n\n2017-12-06 10:10:44.972299 os/FileStore.cc: 2851: FAILED assert(allow_eio || !m_filestore_fail_eio || got != -5)\n```\n\n### 报错2:\n\n```\n2017-12-06 10:32:56.602065 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is supported and appears to work\n\n2017-12-06 10:32:56.602095 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is disabled via 'filestore fiemap' config option\n \n2017-12-06 10:32:56.620337 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: syncfs(2) syscall fully supported (by glibc and kernel)\n\n2017-12-06 10:32:56.739219 7f576071b7c0  0 filestore(/ceph/osd.13) limited size xattrs\n```\n\n### 报错3：\n\n```\n2017-12-06 10:16:32.442815 7f06cddf2700 -1 journal aio to 1398235136~434176 got (5) Input/output error\n\n2017-12-06 10:16:32.443787 7f06cddf2700 -1 os/FileJournal.cc: In function 'void FileJournal::write_finish_thread_entry()' thread 7f06cddf2700\n\n2017-12-06 10:16:32.442867 os/FileJournal.cc: 1383: FAILED assert(0 == \"unexpected aio error\")\n```\n\n### 报错4：\n\n```\n2017-12-06 00:52:33.814785 7fb24be877c0 -1 filestore(/ceph/osd.13) FileStore::mount: unable to access basedir '/ceph/osd.0': (30) Read-only file system\n\n2017-12-06 00:52:33.814801 7fb24be877c0 -1 osd.13 0 OSD:init: unable to mount object store\n\n2017-12-06 00:52:33.814806 7fb24be877c0 -1 ^[[0;31m ** ERROR: osd init failed: (30) Read-only file system\n```\n\n## 解决方案\n\n首先我们肯定是要尝试把`OSD`起来嘛，所以要做的就是先给`osd`加权重，接着加入集群，最后再启动。\n\n```\nceph osd crush add osd.0 3.578 host=test1\nceph osd in osd.0\nservice ceph start osd.0\n\nceph osd crush add osd.13 3.578 host=test1\nceph osd in osd.13\nservice ceph start osd.13\n```\n\n这时候发现`OSD`还是起不来，我们就去看`osd.0`和`osd.13`的`log`，也就会发现以上一系列的报错日志，错误较多；\n\n`filesystem`出现较多`limited size xattrs`这一行引起了我的注意，由于此环境`osd`用的文件系统是`ext4`，而`ext4`对存储`xattr`的大小有限制，使得OSD信息不能安全的保存。\n\n所以在`ceph`中如果`osd`采用`ext4`文件系统时，需要在配置项里面加入相关配置实现用`omap`来存储`xattr`，而`xfs`文件系统由于对`xattr`的存储是足够的，所以不存在这个问题。\n\n所以解决这个问题有三个方案：\n\n### 方案1\n\n更改文件系统，将`ext4`改成`xfs`\n\n### 方案2 \n\n文件系统还是采用`ext4`，配置让`Ceph filestore`中的`omap`存储`xattr`，在`/etc/ceph/ceph.conf`中`global section`或`osd section`中插入一行以下配置：\n\n```\nfilestore xattr use omap = true\n```\n\n### 方案3 \n\n限制对象的长度大小，同样是修改`ceph.conf`，在`global section`或者`osd section`中加入以下配置：\n\n```\nosd max object name len = 256 \nosd max object namespace len = 64 \n```\n\n然后再次重启以下对应的`osd`服务就OK了！\n\n\n## 总结\n\n在`Ceph`中如果用`ext4`文件系统的话，一定要注意配置将`xattr`存在`omap`中。\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/bugfix.png\n","tags":["Bugfix"],"categories":["tech"]},{"title":"通过 Keepalived 实现 Ceph RBD 的高可用","url":"/2017/12/07/RBD-HA/","content":"\n<center>![HA][5]</center>\n\n由于`Cephfs`很不稳定，并且性能差，很难达到用户在性能上的需求，所以`Cephfs`也难以应用于生产环境之上。而`RBD`可以说是一直非常稳定的存储接口方案，用户可以将`image`挂载到客户端进行访问读写，然而很多用户不愿意在本地安装`Ceph`客户端，所以我们常常需要自己封装一层，给客户端暴露出一个通用的接口进行访问，现在一般都是默认用`NFS`，所以本文就`Ceph RBD`如何实现高可用暴露`NFS`给客户端访问进行分享。\n\n<!--more-->\n\n## 环境：\n\n> `Linux Distribution` : `ubuntu`\n> `Ceph` : `Giant`\n> `Keepalived` : `v1.2.2`\n> 集群信息 ：三节点，`IP`分别为`192.168.1.111`，`192.168.1.112`，`192.168.1.113`\n\n## Keepalived 简介\n\n建议先简单了解一些`keepalived`的机制再看下面的内容~\n\n> `Keepalived`的作用是检测集群中服务器的状态，如果有一台服务器死机，或工作出现故障，`Keepalived`将检测到，并将有故障的服务器从集群中剔除，当服务器工作正常后`Keepalived`自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。\n\n下面从网上找了几张图片，方便大家理解一下其原理和机制：\n\n### Keepalived 内部结构：\n\n<center>![keepalive layer][4]</center>\n\n### 双机热备：\n\n<center>![keepalive two host master and backup][3]</center>\n\n### 负载均衡、应用分层：\n\n<center>![keepalive layer][2]</center>\n\n### 配置文件解析\n\n已下摘录自：http://blog.csdn.net/love_is_simple/article/details/47903527\n\n```\nglobal_defs {\n   notification_email {\n     sai@localhost # 报警邮件接收人的地址\n   }\n   notification_email_from root@localhost # 发送报警邮件发件人地址\n   smtp_server 127.0.0.1# 发送邮件的服务器地址\n   smtp_connect_timeout 30# 邮件超时时间(可以根据自己的需求进行设定)\n   router_id LVS_DEVEL# 一个实例的标识地址(可以有多个实例但不能相同)\n}\nvrrp_script monitor_nginx {\n  script “/root/scripts/monitor_nginx.sh”#根据自己的实际路径放置脚本文件\n  interval 1# 脚本执行间隔\nWeight -5#脚本结果导致的优先级变更:5表示优先级加5；-5表示优先级减5\n}\nvrrp_instance VI_1 {# 虚拟路由器自己的名字\n    state MASTER# 设置服务器模式，当前为主节点,master端\n    interface eth0# 实例网卡,也就是提供服务的网卡，来发送vrrp通告\n    virtual_router_id 51# 设置vrid,这里非常重要,相同的vrid为一个组,他决定,它将决定多播的MAC地址.（建议不要使用默认地址,以免发生冲突）\n    priority 100#  设置本节点的优先级,优先级高的为master\n    advert_int 1# 检查间隔,默认为1秒\n    authentication {\n        auth_type PASS# 认证方式,可以是pass或者AH两种认证方式\n        auth_pass 1111# 认证密码\n    }\n    virtual_ipaddress {# 设置vip,虚拟ip地址(实现高可用,转移的vip地址)\n        10.0.1.230# 此地址并不存在,当成为主节点时,此ip地址将会自动生成\n}\nscript_track {\nmonitor_nginx  #跟踪这个monitor_nginx脚本;就是不断去检查这个脚本\n}\n}\n```\n## 原理分析\n\n### RBD 导出 NFS\n\n首先我们要实现`RBD`导出`NFS`功能，毕竟只有先能让客户端通过`NFS`访问后端存储，然后才有必要谈后端存储集群的高可用方案。\n\n我们需要在`Ceph Server`集群创建`RBD image`；然后在三个节点上都建立`RBD Map`关系，最终只会有一个块设备被`mount`，其余两个既用于占位（防止多`image`的情况下造成节点间块设备名称不一致），又是为了作为备机在主机发生故障时转换角色，挂载当前节点的块设备。\n\n接着在三个节点上分别在指定目录下创建目录，本文是在`/vol/`目录下创建目录，比如创建目录`/vol/ec`，这个目录就是块设备对应的挂载目录。\n\n如果有童鞋对`rbd`导出`nfs`过程有兴趣的话，具体请参考：[使用NFS挂载RBD][1]。\n\n### Keepalived 实现 HA\n\n我们后端存储集群最终只会暴露出一个接口或者说是一个`IP`，`keepalived`中有`VIP`这种配置可以支持，所以我们需要在三个节点上配置`keepalived.conf`文件，然后启动`keepalived`所有节点会选举一个`master`节点并暴露虚拟`IP`。\n\n然后我们在`master`节点上将块设备挂载到之前创建的目录`/vol/ec`，同步信息至`/ect/exports`，可以通过`showmount -e vip`可以发现`/vol/ec`已经暴露到了`vip`上，客户端便可以通过`NFS`将上一步创建的目录`/vol/ec`挂载到本地目录，比如`/client_ec`；\n\n这时候客户端已经可以通过虚拟`IP`对`RBD image`进行读写了，但是如果这时候`master`节点`down`了咋办呢？\n\n为了防止集群中主节点不能给`client`提供访问，我们需要实现高可用，也就是当主节点`down`了后，集群自动切换主机，并且针对`RBD`做自动相应挂载操作，让用户无感知访问存储后端。\n\n我们需要配置`keepalived.conf`，当节点角色转为`backup`时，触发停止`NFS`并卸载暴露目录等操作；当节点角色转为`master`时，触发挂载`RBD image`并启动`NFS`等操作；定时检查当前`NFS`，一旦`NFS`服务停止了，尝试重启，如果重启失败，停止`keepalived`服务触发节点角色切换等等。\n\n这些操作对用户来说是无感知的，我们还可以针对`keepalived`做相关邮件配置提醒服务器发生故障等等。\n\n## 具体步骤\n\n### 准备环境\n\n#### 搭建 Ceph 集群\n\n这个就不多说了，基本操作。\n\n#### 安装 Keepalived\n\n本位基于`ubuntu`，`redhat`派可以转换成对应的命令再操作\n\n```\napt-get install libpopt-dev     // 安装依赖\napt-get install keepalived\n```\n\n### RBD 导出\n\n#### 创建 image\n\n这里默认在`test1 pool`中创建`1G`的`image`，请根据自己的场景转换大小，生产环境一般都要几十`T`，甚至上百`T`\n\n```\nrbd create --size 1024 -p test1 image_ec\n```\n\n#### 建立 map 关系\n\n```\nrbd map -p test1 image_ec\n# 输出块设备名称： /dev/rbd0\n```\n\n#### 为块设备格式化文件系统\n\n```\nmkfs.ext4 -i 400000 /dev/rbd0\n```\n\n#### 将块设备挂载到本地目录\n\n```\nmount -o noatime,user_xattr /dev/rbd0 /vol/ec\n```\n\n### 配置 Keepalived\n\n#### 配置 VIP\n\n`VIP`必须是当前集群不存在的`ip`，通过将配置个节点上`keepalived.conf`，为`virtual_ipaddress`选项添加`IP`，我这边用的是`192.168.1.13/24`\n\n```\nvirtual_ipaddress {\n    192.168.1.13/24\n}\n```\n\n#### 配置优先级和网卡\n\n三个节点的角色都配置为`BACKUP`，并且配置`nopreempt`，这样就可以实现不抢占模式，当主节点`down`恢复后不会抢占成为主节点，对我而言哪个是主节点并不重要，频繁切换反而会造成客户端延时。我这边的对外网卡是`eth0`，`priority`是真正决定一开始初始化选举`master`的因素，最大值的节点是`master`节点，一旦切换角色，这个值并不会改变。\n\n```\nvrrp_instance VI_1 {\n    state BACKUP\n    interface eth0\n    priority 100\n    nopreempt\n}\n```\n\n#### 配置角色切换后触发相关操作\n\n当主机`down`之后，如果没有关机，角色转换为`backup`后需要做卸载相关操作；而之前的备机如今成为了主机，也要做挂载等相关操作，这些需求我们可以通过配置`keepalived`，当角色转换时触发相关脚本，这里的配置就表示当节点角色切换为了`master`时则需要执行`/etc/keepalived/ChangeToMaster.sh`，角色切换为`backup`则会执行`/etc/keepalived/ChangeToBackup.sh`：\n\n```\nvrrp_instance VI_1 {\n    notify_master \"/etc/keepalived/ChangeToMaster.sh\" \n    notify_backup \"/etc/keepalived/ChangeToBackup.sh\"\n}\n```\n\n#### 配置定时检测 NFS 状态\n\n如果一旦`NFS`服务断了，我们不及时处理的话，客户端就可以明显地感知到无法读写了。所以我们需要定时不断检测`NFS`的状态，这个也可以通过配置`track_script`选项执行某个脚本并指定间隔时间：\n\n```\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"       # 调用脚本\n    interval 2      # 设置间隔时间为 2s\n}\nvrrp_instance VI_1 {\n    track_script {\n        chk_nfs     # 调用上面的chk_nfs函数\n    }\n}\n```\n\n#### 整个配置文件内容\n\n暂时还是比较精简的，邮件什么的都没配置，`keepalived`还是可以做很多事情的，有兴趣的童鞋可以深入研究\n\n```\nglobal_defs {\n    notification_email {\n    }\n\n    router_id NFS_HA_222\n}\n\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"\n    interval 2\n}\n\nvrrp_instance VI_1 {\n    #state MASTER\n    state BACKUP\n    interface eth0\n    priority 100\n    virtual_router_id 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    track_script {\n        chk_nfs\n    }\n    nopreempt\n    notify_master \"/etc/keepalived/ChangeToMaster.sh\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh\"\n    virtual_ipaddress {\n        192.168.1.13/24\n    }\n}\n```\n\n#### 相关脚本\n\n> 大家可以手动从下面复制，也可以去我的`github`上面获取，欢迎点赞！\n\n>地址： https://github.com/tony-yin/ceph_scripts#keepalived\n\n这些脚本都是针对我当前环境的，需要针对自己的环境和需求进行相应更改\n\n`ChangeToBackup.sh`：\n\n```\n#!/bin/bash\n\nservice nfs-kernel-server stop\nfor folder in $(ls /vol)\ndo\n    if $(mount | grep $folder -q); then\n        umount -f /vol/$folder\n    fi\ndone\n```\n\n`ChangeToMaster.sh`：\n\n```sh\n#!/bin/bash\nfor folder in $(ls /vol)\ndo\n    if $(mount | grep $folder -q); then\n        umount /vol/$folder > /dev/null\n    fi\n    device=$(grep $folder /etc/block_map -w | awk '{print $1}')\n    mount $device /vol/$folder\ndone\nservice nfs-kernel-server start\n```\n\n`check_nfs.sh`：\n\n```sh\n#!/bin/sh\nvip=$(grep -A 1 virtual_ipaddress /etc/keepalived/keepalived.conf | grep -v virtual_ipaddress | tr -d [:blank:] | cut -d '/' -f 1)\nif ! /sbin/ip addr | grep -q $vip; then\n    exit\nfi\n\n# check nfs service\n/sbin/service nfs-kernel-server status >/dev/null\nif [ $? -ne 0 ]; then\n    # abnormal, try to restart the nfs service\n    /sbin/service nfs-kernel-server restart\n    /sbin/service nfs-kernel-server status >/dev/null\n    if [ $? -ne 0 ]; then\n        # still abnormal\n        for folder in $(ls /vol)\n        do\n            if $(mount | grep $folder -q); then\n                umount -f /vol/$folder\n            fi\n        done\n        # stop keepalived service\n        /sbin/service keepalived stop\n    fi\nfi\n```\n\n配置完后，分别在三个节点上执行`service keepalived restart`重启服务，然后分别在三个节点上执行`ip addr`查看`IP`情况，可以发现`VIP`暴露在了`node2`上，说明我这里`node2`在`keepalived.conf`里面配置`priority`的值是最大的\n\n`node1`：\n\n```\nroot@node1:/etc/keepalived# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n`node2`：\n\n```\nroot@node2:/etc# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n```\n\n`node3`：\n\n```\nroot@node3:~# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n### 客户端通过 NFS 访问 RBD\n\n客户端检查`VIP`对外暴露接口\n\n```\n[root@tony /]# showmount -e 192.168.1.13\nExport list for 192.168.1.13:\n/vol/ec1 *\n```\n\n将`server`端挂载块设备的目录`/vol/ec1`再次挂载到客户端上的`ec1`目录\n\n```\n[root@tony /]# mkdir ec1\n[root@tony /]# mount -o rw,hard,intr -t nfs 192.168.1.13:/vol/ec1 /ec1\n[root@tony /]# cd ec1\n[root@tony ec1]# ls\nlost+found      # 此时是没有数据的\n```\n\n我们可以测试一下读写，先看下读，比如我们在`node2`的`/vol/ec1`目录下写一个文件：\n\n```\nroot@node2:/vol/ec1# ls\nlost+found\nroot@node2:/vol/ec1# echo 'hello' > hello.txt\n```\n\n然后客户端查看`/ec1`目录：\n\n```\n[root@tony ec1]# ls\nhello.txt  lost+found\n[root@tony ec1]# cat hello.txt \nhello\n```\n\n接下来测写，我们可以在客户端写一个文件，然后到服务端查看\n\n```\n[root@tony ec1]# echo 'i am client' > client.txt\n[root@tony ec1]# ls\nclient.txt  hello.txt  lost+found\n```\n\n服务端查看：\n\n```\nroot@node2:/vol/ec1# ls\nclient.txt  hello.txt  lost+found\nroot@node2:/vol/ec1# cat client.txt \ni am client\n```\n\nok，读写正常，目前为止客户端访问后端存储集群一切顺利！\n\n### 测试高可用\n\n分三个测试：\n\n- 手动停止主机`NFS`\n- 手动停止主机`Keepalived`\n- 手动关机主机\n\n#### 手动停止主机 NFS\n\n这个主要是测试`check_nfs.sh`这个脚本是否在实时监控`NFS`状态，可以看到刚`stop`再次查看状态已经是`running`了，本测试通过~\n\n```\nroot@node2:/vol/ec1# service nfs-kernel-server stop\n * Stopping NFS kernel daemon                                                                                                                                                         [ OK ] \n * Unexporting directories for NFS kernel daemon...                                                                                                                                   [ OK ] \nroot@node2:/vol/ec1# service nfs-kernel-server status\nnfsd running\n```\n\n#### 手动停止主机 Keepalived\n\n手动停止主机`node2`的`keepalived`服务，发现`VIP`已经在`node2`上面消失不见\n\n`node2`：\n\n```\nroot@node2:/vol/ec1# service keepalived stop\n * Stopping keepalived keepalived                                                                                                                                                     [ OK ] \nroot@node2:/vol/ec1# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n我们可以在`node1`发现了上面消失不见得`VIP`，可知如今角色发生了改变，`node1`已经成为了新的`master`节点\n\n`node1`：\n\n```\n# 出现了 VIP：192.168.1.13/24\nroot@node1:/etc/keepalived# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n# 查看node1的/vol/ec1目录     \nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found\n# 查看mount信息\nroot@node1:/etc/keepalived# mount\n/dev/sda3 on / type ext4 (rw,errors=remount-ro)\n/dev/sdb2 on /data/osd.0 type ext4 (rw,noatime,user_xattr)\nnfsd on /proc/fs/nfsd type nfsd (rw)\n...\n/dev/rbd0 on /vol/ec1 type ext4 (rw)\n```\n\n此时我们可以再次测试一下读写：\n\n```\n# 新的主机读\nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found\n\n# 新的主机写\nroot@node1:/etc/keepalived# echo 'new service 111' > /vol/ec1/new_server.txt\nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found  new_server.txt\n\n# 客户端读\n[root@tony ec1]# ls\nclient.txt  hello.txt  lost+found  new_server.txt\n[root@tony ec1]# cat new_server.txt\nnew service 111\n\n# 客户端写\n[root@tony ec1]# echo 'hello new server' > hello_new_server.txt\n# 可以看到刚刚客户端写的文件\nroot@node1:/vol/ec1# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt\n```\n\nok，本测试通过~\n\n#### 手动关机主机\n\n关闭主机`node1`，稍等片刻，确定完全关闭再测试\n\n```\nroot@node1:/vol# reboot\n\nBroadcast message from root@node1\n\t(/dev/pts/3) at 9:16 ...\n\nThe system is going down for reboot NOW!\n```\n\n等待完全关闭，我们在`node3`上看到了`VIP`：\n\n```\nroot@node3:~# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n```\n\n客户端读写测试：\n\n```\n# 客户端读\n[root@tony ec1]# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt\n# 客户端写\n[root@tony ec1]# echo 'reboot' >  reboot.txt\n[root@tony ec1]# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt\n```\n\n`node3`:\n\n```\nroot@node3:~# ls /vol/ec1\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt\nroot@node3:~# cat /vol/ec1/reboot.txt \nreboot\n```\n\nok，本测试通过~\n\n### 测试小结\n\n通过上面三个测试，我们已经基本确保了`keepalived`会保证集群中主机发生异常时还是可以很好地对外提供服务，并且真正地做到了高可用，低延时，高可靠。\n\n\n## QA 环节\n \n因为我实现这个`RBD`高可用是在我们项目中做的，我们项目中`UI`上可以创建共享目录，但是之前都是用的`cephfs`实现的，而我做的就是将`cephfs`方式使用`RBD`替代，大家应该都清楚作为`POSIX`文件接口的`cephfs`内部已经做好了很多事情，它可以将所有节点挂载的目录做到真正的共享，也就是共享目录三个目录都有，改一个其他两个都会随之而改变，而不是像我们`RBD`同时只会针对某一个主机访问。\n\n而`RBD`替换的话必然存在很多困难和问题，在这里我就以`QA`问答的方式分享一下我实现过程中遇到的种种问题和别人提出的需求。\n\n### 问题1\n\n问：如何通过代码实现三个节点都做相关操作的，比如创建`image`和目录等等？\n\n答：我们项目是这样实现的，将前端`UI`的增删改查（比如创建或删除目录等）和后端具体实现共享目录业务分离，也就是说前端只负责做这些信息的增删改查，对应的后端也只是负责这些信息的增删改查，所以用户会即时收到反馈。而真正做事的是在共享业务后端，这个和`UI`对应的后端做事有所区别，这边共享业务后端是做成了一个`Daemon`每隔几秒就会去扫描`UI`后端存储数据是否变化，发生了变化就会做相关操作，比如多了一个文件夹就要创建`image`等，删除一个文件夹也要做一系列的事情。\n\n### 问题2\n\n问：如何判断哪个节点是`master`？\n\n答：这个很简单，就通过`ip addr`命令查找`VIP`就好了，不能通过配置文件中的`priority`来判断，因为即使角色切换，那个值也不会变化的，也就是说即使`priority`是最大值也有可能当前节点不是主节点，这里要注意的是不能仅仅是包含`VIP`，而是要精确匹配才行，比如`VIP`是`192.168.1.12`，如果此时还有个`192.168.1.123`，如果只是字符串包含的话，那这个也会被匹配，所以要精确匹配。\n\n### 问题3\n\n问：创建文件夹后端实现的逻辑是什么样的？\n\n答：后端`Daemon`当扫描存储的目录信息相比于上一次扫描时新增的话，那么后端就会做事情了。首先我们要判断是否为主节点，如果是主节点，那么就创建`image`，然后做`Map`，接着就要`format`文件系统，创建目录，然后再做挂载。这时候要注意其他两个备节点也要做`rbd map`操作，这样做的原因，一是为了占位，比如当创建的项目多了之后，`backup`节点再`map`的时候顺序会乱掉，二是为了当主机`down`，备机转换为`master`后要找同样的块设备挂载，比如都是`/dev/rbd0`\n\n### 问题4\n\n问：删除文件夹后端实现的逻辑是什么样的？\n\n答：扫描当前目录少于上一次扫描的目录，那么就针对这些目录，主节点要先`umount`，再`umap`，然后`rm image`，最后删除目录，对于备节点的话就`umap`，然后再删除目录就好\n\n### 问题5\n\n问：三个节点的`Daemon`可能执行的顺序不一样，不一定是主节点先执行，那么这个时候备节点将无法`map`，同理很有可能在删除`image`的时候，别的节点的`image`都还没`unmap`，这样的话`image`是会删除失败的，这里怎么处理节点间的冲突呢？\n\n答：\n\n首先是创建目录，这时候主节点我们已经做得比较好了，主要担心备节点`map`的时候`image`还没有创建，那么我们这边就要判断一下，如果`image`还不在指定`pool`中，那么就要设置当前目录情况还为上一次的目录信息，这样下一次扫描代码就会又以为有新目录了，那么该段代码就会又执行一次，此时应该成功了，反正只要成功的时候才会把当前目录信息更改为最新的目录信息。\n\n再来说删除目录，这个和创建`map`不同的地方在于，我要删除`image`的时候，我无法知道这个`image`还有没有和其他节点有`map`关系，所以我们只有尝试去删除，这边加一个异常捕获，因为`rm image`报错我们不处理的话会造成代码出错，所以外面包一层异常，这样就可以和上面类似的操作了。这边要注意的是一旦发生异常，我们还必须要在`map`回去，否则我们无法获取`pool`等信息了，因为我们是通过`rbd showmapped`来获取相关信息的。\n\n### 问题6\n\n问：请问`keepalived`如何做自动化的？\n\n答：由于`keepalived`也比较简单，三个配置文件相关配置信息都配一样的，我们要做的就是网卡和`VIP`，网卡的话我们就从项目中获取`public ip`，而`VIP`就是`UI`上面配置的，然后读写文件就好了。\n\n### 问题7\n\n问：一旦节点关机的话，下次开机后块设备就会没了，我们该如何做呢？\n\n答：这个问题的确存在，所以我们要提前将对应关系存在文件中，下次开机的时候根据文件然后对应做`map`工作\n\n### 问题8\n\n问：多个块设备的时候，`keepalived`触发的脚本如何做？\n\n答：其实这个的做法我已经暴露在上面我分享的三个脚本里面了，要做的就是遍历`/vol`目录下的所有目录或者所有`rbd*`，这边要注意的就是`/vol/`或者指定目录下存在的必须只有创建的共享目录。\n\n\n## 总结\n\n本文通过`keepalived`初步实现了`RBD`的高可用，简单地替代了`cephfs`导出`NFS`，针对`cephfs`性能不行的问题，应该有很多小伙伴有这种需求，希望这篇文章能给大家带来一些思路和帮助\n\n之后，我会尝试研究`CTDB`做高可用，因为`keepalived`由于比较简单，所以功能也就比较局限了。然后本文只有`NFS`，后续应该还会有`smb`，`iscsi`等等\n\n通过本文，我对`RBD`和高可用的认识又深入了一些，其实本文涉及到的技术还是比较简单的，还有很多更复杂、更牛逼的高可用方案，这里不禁要说一句，后端还是有意思呀！（相比于前端而言），起码对我来说是这样的，以后会不断精进这些技术，加油！\n\n\n\n[1]: http://www.tony-yin.top/2017/10/31/RBD-Mount-NFS/\n[2]: http://ow0mgad6r.bkt.clouddn.com/keepalived_layer.png\n[3]: http://ow0mgad6r.bkt.clouddn.com/keepalived_two_host.png\n[4]: http://ow0mgad6r.bkt.clouddn.com/keepalived_space.png\n[5]: http://ow0mgad6r.bkt.clouddn.com/HA.jpg\n","tags":["Keepalived"],"categories":["tech"]},{"title":"Cephx 实战演练","url":"/2017/11/30/Cephx-practice/","content":"\n<center>![cephx][2]</center>\n\n本文就阅读完[徐小胖的大话Cephx][1]后，针对一些猜测和疑惑进行了实战演练，对原文的一些说法和结论进行了验证，并进行了一系列的扩展的思考猜想和总结。最后收获满满，不仅对原文的一些结论进行了验证，也发现了其中的一些问题，更多的是自己动手后一些奇妙的场景和发现。\n\n<!--more-->\n\n本文实战任务和完成情况如下：\n\n- [x] 删除`client.admin.keyring`\n- [x] 修改`cephx`配置\n- [x] 修改`Monitor keyring`\n- [x] 修改`OSD keyring`\n- [x] 修改`client.admin.keyring`，通过`Mon`找回正确的`keyring`\n- [x] `Mon Cap`\n- [x] `OSD Cap`\n- [x] 删除所有`keyring`文件再恢复\n- [x] 删除`ceph.conf`再恢复\n- [ ] 关闭`CephX`后不重启`OSD`\n- [x] 通过`osd.keyring`访问集群\n- [ ] 配置只能访问一个`RBD`的用户权限\n\n## 删除 client.admin.keyring\n\n主节点开始存在`keyring`，可以正常访问集群\n\n```\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n \n  services:\n    mon:     3 daemons, quorum node1,node2,node3\n    mgr:     no daemons active\n    osd:     6 osds: 5 up, 5 in\n    rgw:     1 daemon active\n    rgw-nfs: 1 daemon active\n \n  data:\n    pools:   10 pools, 444 pgs\n    objects: 257 objects, 36140 kB\n    usage:   6256 MB used, 40645 MB / 46901 MB avail\n    pgs:     63.288% pgs not active\n             311/771 objects degraded (40.337%)\n             158 undersized+degraded+peered\n             158 active+undersized+degraded\n             65  down\n             58  incomplete\n             5   active+clean+remapped\n```\n\n将`keyring`文件移动到其他地方，相当于删除了`keyring`，这时访问集群报错\n\n```\n[root@node1 ceph]# mv ceph.client.admin.keyring /tmp/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-mgr.keyring  ceph.bootstrap-osd.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring  rbdmap\n[root@node1 ceph]# ceph -s\n2017-11-23 18:07:48.685028 7f63f6935700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 18:07:48.685094 7f63f6935700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 18:07:48.685098 7f63f6935700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n再拷贝回来又可以访问集群了\n\n```\n[root@node1 ceph]# mv /tmp/ceph.client.admin.keyring ./\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n```\n \n`node3`由于`/etc/ceph/`目录下没有`keyring`文件，所以也无法连接集群\n\n```\n[root@node3 ceph]# ls\nceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# ceph -s\n2017-11-23 17:59:16.659034 7fbe34678700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 17:59:16.659085 7fbe34678700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 17:59:16.659089 7fbe34678700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n**结论：**\n\n> 当`ceph.conf`中的`auth`配置为`cephx`的时候，访问集群是需要秘钥文件的\n\n## 修改 cephx 配置\n\n在`node3`节点上的`/etc/ceph/`目录下操作，首先将`ceph.client.admin.keyring`文件删除，然后将`auth`配置从`cephx`改为`none`，然后先重启`monitor`，再重启`osd`，这时候依然不可以访问集群，因为`cephx`是面向整个集群的，而不是某个节点，接下来需要在其他节点做一样的操作，更改`cephx`为`none`，然后重启`monitor`和`osd`，这时候便可以在没有`keyring`文件的情况下访问集群了。\n\n```\n# 删除keyring文件\n[root@node3 ~]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# mv ceph.client.admin.keyring /tmp/\n# 更改cephx配置\n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# vim ceph.conf \n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = none\nauth_service_required = none\nauth_client_required = none\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon.target\n[root@node3 ceph]# systemctl restart ceph-osd.target\n# 更改单个节点配置后依然不可以访问集群\n[root@node3 ceph]# ceph -s\n2017-11-27 23:05:23.022571 7f5200c2f700  0 librados: client.admin authentication error (95) Operation not supported\n[errno 95] error connecting to the cluster\n# 相应的更改其他几个节点并重启，便又可以正常访问集群了\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n**结论：**\n\n> 当`auth`配置为`cephx`的时候访问集群必须要借助秘钥文件，而当`auth`配置为`none`的时候，不再需要秘钥文件就可以访问集群了。（**更改配置需要集群所有节点都做才可以生效，而不是单一节点**）\n\n## 删除monitor秘钥\n\n`/etc/ceph`和`/var/lib//ceph/mon/ceph-node1`各有一个`mon keyring`\n```\n[root@node1 ceph-node1]# cd /etc/ceph/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  keyring  kv_backend  store.db  systemd\n```\n\n先删除`/etc/ceph/ceph-mon.keyring`，还是可以访问集群\n\n```\n[root@node1 ceph]# rm ceph.mon.keyring \nrm: remove regular file ‘ceph.mon.keyring’? y\n[root@node1 ceph]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2\n...\n...\n```\n\n再删除`/var/lib/ceph/mon/ceph-node1/keyring`\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph-node1]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph-node1]# ceph -s\n\n```\n\n访问集群一直`timeount`，查看`log`文件发现`Mon`初始化失败\n\n```\n2017-11-24 00:33:55.812955 7fa16f995e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:33:55.812991 7fa16f995e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:33:55.812999 7fa16f995e40 -1 failed to initialize\n```\n\nok，那我们再试试将`/var/lib/ceph/mon/ceph-node1/keyring`删除，将`etc/ceph/ceph.mon.keyring`拷贝回来，这时候意外发生了，居然`mon`初始化失败\n\n**结论：**\n\n> `Monitor`启动是需要`keyring`文件进行秘钥认证的，并且这个文件必须是`/var/lib/ceph/mon/ceph-node1/`目录下的，`/etc/ceph/`目录下的`ceph.mon.keyring`并不起作用\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring  \n[root@node1 ceph]# ceph -s\n// timeout\n...\n```\n\n`mon.log`中的现象：\n\n```\n2017-11-24 00:44:26.534865 7ffaf5117e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:44:26.534901 7ffaf5117e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:44:26.534916 7ffaf5117e40 -1 failed to initialize\n```\n\n至此，我们可以得出结论`monitor`初始化的时候依赖的文件是`/var/lib/ceph/mon/ceph-node1/keyring`而不是`/etc/ceph/ceph.mon.keyring`\n\n## 修改 Mon keyring\n\n\n### 原始的 keyring\n\n```\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 将中间的五个A替换成了五个C\n\n```\n[root@node1 ceph-node1]# vim keyring \n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 重启查看 Mon keyring\n\n理想结果：\n\n```\n[root@node1 ceph-node1]# systemctl restart ceph-mon.target\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n令人疑惑的现实：\n\n```\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n可以看到一会是修改之前的`keyring`，一会是修改之后的`keyring`，那遇到这种问题，我们就通过`log`观察如何获取`keyring`的\n\n`node1`的`mon.log`中日志：\n\n```\n2017-11-24 09:30:08.697047 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:08.697106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1169357136' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:10.020571 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:10.020641 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2455152702' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:11.393391 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:11.393452 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1704778092' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:12.669987 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:12.670049 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/275069695' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:14.113077 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:14.113147 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/3800873459' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:15.742038 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:15.742106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1908944728' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:17.629681 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:17.629729 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2193002591' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n`node2`的`mon.log`中日志：\n\n```\n2017-11-24 09:29:23.799402 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4284881078' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:26.030516 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:26.030588 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4157525590' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:38.637677 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:38.637748 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4028820259' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n**结论：**\n\n* `Monitor`的秘钥哪怕被修改过了，也不会影响`Monitor`的启动，也就是说`Monitor`启动时只要存在秘钥文件就好，内容忽略并不重要\n* `Monitor`启动的时候读取秘钥文件是随机的，并不一定是当前节点的，具体选择机制需要后期去看源代码了\n\n\n## 修改OSD keyring和修复\n\n`OSD`启动的时候需要秘钥才可以登录集群，这个秘钥会存在`Monitor`的数据库中，所以登录的时候就会拿本地的`keyring`和存在`Monitor`中的`keyring`相匹配，正确的话才可以启动成功。\n\n下面我们将本地的`OSD keyring`故意改错，然后重启`OSD`查看效果\n\n```\n# 更改秘钥文件\n[root@node3 ceph]# cd /var/lib/ceph/osd/ceph-2\n[root@node3 ceph-2]# ls\nactivate.monmap  active  block  bluefs  ceph_fsid  fsid  keyring  kv_backend  magic  mkfs_done  ready  systemd  type  whoami\n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = BBBp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service\n# 重启后发现OSD的状态时down\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2    down  1.00000 1.00000 \n```\n\n查看日志，发现`init`失败，原因是`auth`认证出错\n\n```\n2017-11-27 23:52:18.069207 7fae1e8d2d00 -1 auth: error parsing file /var/lib/ceph/osd/ceph-2/keyring\n2017-11-27 23:52:18.069285 7fae1e8d2d00 -1 auth: failed to load /var/lib/ceph/osd/ceph-2/keyring: (5) Input/output error\n...\n2017-11-27 23:52:41.232803 7f58d15ded00 -1  ** ERROR: osd init failed: (5) Input/output error\n```\n\n我们可以通过查询`Monitor`数据库获取正确的`keyring`，将错误的`keyring`修正过来再重启`OSD`\n\n```\n# 查询Monitor数据库中的osd keyring\n[root@node3 ceph-2]# ceph auth get osd.2\nexported keyring for osd.2\n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n\tcaps mgr = \"allow profile osd\"\n\tcaps mon = \"allow profile osd\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service \n# 重启OSD后可以发现osd.2状态已经变为up\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2      up  1.00000 1.00000 \n```\n\n**结论：**\n\n> `OSD`启动需要正确的`keyring`，错误的话则无法启动成功，正确的`keyring`会被存在`Monitor`的数据库中\n\n\n## 修改Client keyring和修复\n\n之前我们通过删除`client keyring`验证了当`auth=cephx`的时候，客户端需要`keyring`才可以访问集群，那么它是像`Monitor`一样内容不被`care`还是和`OSD`一样需要精确匹配`keyring`呢？\n\n```\n# 修改ceph.client.admin.keyring\n[root@node3 ceph-2]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = BBBB7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群出错\n[root@node3 ceph]# ceph -s\n2017-11-28 00:06:05.771604 7f3a69ccf700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:06:05.771622 7f3a69ccf700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:06:05.771634 7f3a69ccf700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n```\n\n可以看出访问集群需要正确的`keyring`，这时候如何修复呢？大家应该能够猜到，它和`OSD`的原理是一样的，正确的`keyring`也存在与`Monitor`的数据库\n\n```\n# 直接获取client.admin出错\n[root@node3 ceph]# ceph auth get client.admin\n2017-11-28 00:08:19.159073 7fcabb297700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:08:19.159079 7fcabb297700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:08:19.159090 7fcabb297700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n# 需要加上monitor的keyring文件才可以获取client.admin.keyring\n[root@node3 ceph]# ceph auth get client.admin --name mon. --keyring /var/lib/ceph/mon/ceph-node3/keyring\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph]# vim ceph\nceph.client.admin.keyring  ceph.conf                  ceph-deploy-ceph.log       \n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群成功\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n出现了令人惊奇的一幕，就是上面通过`ceph auth`获取`OSD`的`keyring`可以正常获取，而获取`client.admin.keyring`却要加上`monitor.keyring`，原因可以从报错信息看出，`ceph auth`需要以客户端连接集群为前提。\n\n结论：\n\n> `Client`访问集群和`OSD`一样，需要正确的`keyring`与存在`Monitor`数据库中对应的`keyring`相匹配，并且当`client.admin.keyring`\n不正确时，通过`ceph auth`读取`keyring`的时候需要加上`monitor keyring`的选项\n\n## Mon Caps\n\n\n### r 权限\n\n`Monior`的`r`权限就是拥有读权限，对应的读权限都有哪些操作？在这里的读权限其实就是拥有读取`Monitor`数据库中信息的权限，`MON`作为集群的状态维护者，其数据库(`/var/lib/ceph/mon/ceph-$hostname/store.db`)内保存着集群这一系列状态图(`Cluster Map`)，这些`Map`包含但不限于：\n\n* `CRUSH Map`\n* `OSD Map`\n* `MON Map`\n* `MDS Map`\n* `PG Map`\n\n所以接下来我们可以创建一个新的只拥有读权限的用户，进行相关操作验证读权限具体拥有哪些权限\n\n```\nceph auth get-or-create client.mon_r mon 'allow r' >> /root/key\n[root@node3 ceph]# ceph auth get client.mon_r\nexported keyring for client.mon_r\n[client.mon_r]\n\tkey = AQABvRxaBS6BBhAAz9uwjYCT4xKavJhobIK3ig==\n\tcaps mon = \"allow r\"\n\t\nceph --name client.mon_r --keyring /root/key -s      // ok\n\nceph --name client.mon_r --keyring /root/key osd crush dump     // ok\nceph --name client.mon_r --keyring /root/key osd getcrushmap -o crushmap.map        // ok\n\nceph --name client.mon_r --keyring /root/key osd dump       // ok\nceph --name client.mon_r --keyring /root/key osd tree       // ok\nceph --name client.mon_r --keyring /root/key osd stat       // ok\n\nceph --name client.mon_r --keyring /root/key pg dump        // ok\nceph --name client.mon_r --keyring /root/key pg stat        // ok\n```\n\n尝试了下两个写操作，都显示报错权限拒绝\n\n```\n[root@node3 ceph]# rados --name client.mon_r --keyring /root/key -p testpool put crush crushmap.map\nerror putting testpool/crush: (1) Operation not permitted\n\n[root@node3 ceph]# ceph --name client.mon_r --keyring /root/key osd out osd.0\nError EACCES: access denied\n```\n\n**注意：**\n\n虽然上面有`osd`和`pg`等信息，但是这些都隶属于`crush map`的范畴中，所以这些状态数据都是从`Monitor`获取的\n\n**结论：**\n\n> `Monitor`的读权限对应的是从`Monitor`数据库获取一系列的`Map`信息，具体的上面也都讲的很详细了，并且该权限只能读取状态信息，不能获取具体数据信息，且不能进行`OSD`等守护进程写操作\n\n### w 权限\n\n`w`权限必须配合`r`权限才会有效果，否则，单独`w`权限执行指令时，是会一直`access denied`的。所以我们在测试`w`权限时，需要附加上`r`权限才行：\n\n```\nceph auth get-or-create client.mon_rw mon 'allow rw' >> /root/key\n```\n\n而`w`权限就可以做一些对组件的非读操作了，比如：\n\n```\n# 踢出OSD\nceph osd out\n# 删除OSD\nceph osd rm \n# 修复PG\nceph pg repair\n# 替换CRUSH\nceph osd setcrushmap\n# 删除MON\nceph mon rm\n...\n# 还有很多操作，就不一一赘述\n```\n\n**结论：**\n\n> `Mon`的`r`权限可以读取集群各个组件的状态，但是不能修改状态，而`w`权限是可以做到的\n\n**注意：**\n\n> 这里的`w`权限能做到的写权限也只是修改组件的状态，但是并不包括对集群对象的读写权限，因为这些组件状态信息是存在`Mon`，而对象信息是存在`OSD`里面的，而这里的`w`权限也只是`Mon`的写权限，所以也很好理解了。\n\n### x 权限\n\n`MON`的`x`权限很局限，因为这个权限仅仅和`auth`相关，比如`ceph auth list`，`ceph auth get` 之类的指令，和`w`权限类似，`x`权限也需要`r`权限组合在一起才能有效力：\n\n```\n# 用上面创建拥有rw权限的用户访问auth list后auth报错\n[root@node3 ~]# ceph --name client.mon_rw --keyring /root/key auth list\n2017-11-28 21:28:10.620537 7f0d15967700  0 librados: client.mon_rw authentication error (22) Invalid argument\nInvalidArgumentError does not take keyword arguments\n# 创建rw权限的用户访问auth list成功\n[root@node3 ~]# ceph --name client.mon_rx --keyring /root/key auth list\ninstalled auth entries:\n\nosd.0\n\tkey: AQDaTgBav2MgDBAALE1GEEfbQN73xh8V7ISvFA==\n\tcaps: [mgr] allow profile osd\n\tcaps: [mon] allow profile osd\n\tcaps: [osd] allow *\n...\n...\n```\n\n这边需要注意的是徐小胖的原文应该是笔误，他是用的`client.mon.rw`访问的，所以说实践可以发现很多光看发现不了的东西\n\n**结论：**\n\n> `x`权限也需要和`r`权限搭配才有效果，该权限只能处理与`auth`相关的操作\n\n### * 权限\n\n这没什么好说的，猜也能猜到了，就是拥有`rwx`所有权限\n\n## OSD Caps\n\n这一章需要研究一波再发出来\n\n## 丢失所有秘钥的再恢复\n\n如果所有秘钥全部删除，是否真的能恢复？所有秘钥包括\n\n* `MON` ： `/var/lib/ceph/mon/ceph-$hostname/keyring`\n* `OSD` ： `/var/lib/ceph/osd/ceph-$hostname/keyring`\n* `Client` ：`/etc/ceph/ceph.client.admin.keyring`\n\n```\n# 删除 mon keyring\n[root@node1 ceph-node1]# mv keyring /root/\n# 删除 ceph.conf\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.conf /root/\n# 删除 client.admin.keyring\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.client.admin.keyring /root\n# 尝试访问集群报错\n[root@node1 ceph-node1]# ceph -s\n2017-11-29 23:57:14.195467 7f25dc4cc700 -1 Errors while parsing config file!\n2017-11-29 23:57:14.195571 7f25dc4cc700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195579 7f25dc4cc700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195580 7f25dc4cc700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n# 尝试获取auth list报错\n[root@node1 ceph-node1]# ceph auth list\n2017-11-29 23:57:27.037435 7f162c5a7700 -1 Errors while parsing config file!\n2017-11-29 23:57:27.037450 7f162c5a7700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037452 7f162c5a7700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037453 7f162c5a7700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n```\n\nok，下面开始修复：\n\n### 伪造 Mon keyring\n\n在`ceph`中除了`mon.`用户以外的的账户密码都保存在`Mon`的数据库`leveldb`中，但是`mon.` 用户的信息并没有保存在数据库里，而是在`MON`启动时读取`Mon`目录下的`keyring` 文件得到的，这也是我们之前验证后得到的结论。所以，我们可以随便伪造一个`keyring`，放到`Mon` 目录下去。然后同步到各个`Mon`节点，然后重启三个`Mon`。\n\n```\n[root@node1 ceph-node1]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  kv_backend  store.db  systemd\n[root@node1 ceph-node1]# vim keyring\n# 伪造 keyring，可以看到里面还有tony的字样，可以看出明显是伪造的\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCtonyZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n# 重启 mon\n[root@node1 ceph-node1]# service ceph-mon@node1 restart\nRedirecting to /bin/systemctl restart  ceph-mon@node1.service\n```\n\n可以看到效果：\n\n```\n# monitor log显示mon.node1@0初始化成功，并被选举成了monitor leader\n2017-11-30 00:15:04.042157 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.042299 7f8c4e28a700  1 mon.node1@0(electing).elector(934) init, last seen epoch 934\n2017-11-30 00:15:04.048498 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.048605 7f8c4e28a700  1 mon.node1@0(electing).elector(937) init, last seen epoch 937, mid-election, bumping\n2017-11-30 00:15:04.078454 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1@0 won leader election with quorum 0,1,2\n```\n\n**注意（很重要）：**\n\n> 虽然说`mon`在启动的时候读取对应的`keyring`，不在乎内容的正确性，但是不代表这个`keyring`可以胡乱修改。也就是说这个`keyring`是要**符合某种规范和格式的**，在实践过程我发现`keyring`前三位必须为大写的`AQC`，当然还有其他的格式要求，比如结尾是否必须要是`==`？长度是否是固定的？这个格式要求可能很多，我没有时间一个一个手动无脑验证，这个可以日后查看源码了解实现思路，有兴趣的童鞋可以试试，说不定可以发现很有趣的现象。当然说了这么多是否意味着很难伪造呢？这个我们也不必担心，最好的做法是从别的集群的`Mon keyring`拷贝一份过来就可以了，自己胡乱伪造启动会报错如下：\n\n```\n2017-11-29 23:49:50.134137 7fcab3e23700 -1 cephx: cephx_build_service_ticket_blob failed with error invalid key\n2017-11-29 23:49:50.134140 7fcab3e23700  0 mon.node1@0(probing) e1 ms_get_authorizer failed to build service ticket\n2017-11-29 23:49:50.134393 7fcab3e23700  0 -- 192.168.1.58:6789/0 >> 192.168.1.61:6789/0 conn(0x7fcacd15d800 :-1 s=STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH pgs=0 cs=0 l=0).handle_connect_reply connect got BADAUTHORIZER\n```\n\n\n###　还原 ceph.conf\n\n没有`/etc/ceph/ceph.conf`这个文件，我们是没法执行`ceph`相关指令的，所以我们需要尽可能的还原它。首先`fsid`可以通过去任意`osd`目录（`/var/lib/ceph/osd/ceph-$num/`）读取`ceph-fsid`文件获得，然后`mon_initial_members`和`mon_host`代表着集群每个节点的`hostname`和`ip`，这些都是我们知道的。\n\n```\n# 还原 ceph.conf\n[root@node1 ceph-node1]# cat /var/lib/ceph/osd/ceph-0/ceph_fsid \n99480db2-f92f-481f-b958-c03c261918c6\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.conf\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.conf\n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\n\n# 通过 mon keyring 访问集群状态成功\n[root@node1 ceph-node1]# ceph -s --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n### 恢复 ceph.client.keyring\n\n有了`Mon keyring`，并且可以执行`ceph`指令，那么我们就可以通过`ceph auth get`去`Monitor leveldb`获取任意`keyring`\n\n```\n# 通过 Mon 获取 client.admin.keyring\n[root@node1 ceph-node1]# ceph --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring auth get client.admin\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 创建 /etc/ceph/ceph.client.admin.keyring，并将上面内容更新到该文件\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.client.admin.keyring\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.client.admin.keyring\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n\n# 用默认 ceph -s 测试一下，发现可以正常访问了\n\n[root@node1 ceph-node1]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n## 总结\n\n首先感谢徐小胖给我提供了`cephx`方面的思路，希望日后多出好文，我也在不断地拜读这些优质文章。这篇文章花了我很长时间，大家从日志的时间可以看出来，跨度已经有好几天了，很多实践真的不是一蹴而就的，需要反复的尝试和思考才能得到最后的成功。`Ceph`还是要多动手，看别人文章是好事，但是记得要加以实践，否则再好的文章也只是想当然，作者说什么你就跟着他的思路走，你永远不知道别人一句简短的话语和结论的背后花了多少时间去推敲和实践，你看起来一条命令执行成功或者在某一步执行某个命令那也许是别人失败了无数次总结出来的。所以我们要自己实践去验证，除了可以验证原文的观点正确与否，往往可以发现一些其他有用的知识。\n\n经历这次总结，收获满满，我对`cephx`的理解又上了一个层次。本文就`cephx`在不同组件中的角色扮演和依赖关系进行梳理，然后再对各组件的`cap`进行了研究，最后针对各个`keyring`的恢复给出了详细的指南和步骤。然后还剩两项任务没有完成，等有空进行完善！\n\n[1]: http://www.xuxiaopang.com/2017/08/23/easy-ceph-CephX/\n[2]: http://ow0mgad6r.bkt.clouddn.com/cephx-600x450.jpg\n","tags":["Cephx"],"categories":["tech"]},{"title":"阅读感悟：《Ceph Cookbook》","url":"/2017/11/26/read-ceph-cookbook/","content":"\n<center>![ceph cookbook][1]</center>\n\n阅读周期：2017/10/15 ～ 2017/11/26\n阅读人群：拥有`ceph`基础知识想进阶的同学\n阅读评分：4.8\n\n这本书和之前一本《ceph分布式存储学习指南》是同一个作者：卡兰.辛格。是`ceph`界最先出版的学习书籍了，对他表示崇高的敬意和感谢。这本书绝不是上一本书的复制品，我觉得我的运气很好，先阅读了上一本然后才看的这一本，没错，在我看来这本书是上一本的进阶版，内容更加饱满和细致。很多方面还是讲解的比较全面的，而不是笼统的说那些道理，基本上每个章节都可以让读者进行实战代码演练。这本书让我对`cookbook`系列的书籍有着很好的印象。\n\n<!--more-->\n\n## 第1章 Ceph介绍和其他\n\n第1章没什么好说的，就是传统的介绍和与其他存储做比较。然后第一章还把`ceph`的部署也插入了进来，估计是不想浪费过多的章节讲解这么基础的 东西吧，嗯，很棒。\n\n## 第2章 使用Ceph块存储\n\n哇，我只想说讲的太全面了，第一本讲解的方式永远是三个存储方式放在一起介绍一下，而这本书是每一个存储方式分了一章。从讲解块设备的创建、映射、调整、快照、克隆到和`openstack`的集成，面面俱到，为了阐述`rbd`是`openstack`的最佳匹配，分别就`openstack`的`glance`、`cinder`和`nova`讲解了详细的对接挂载方式，过程是可以自己实操的。\n\n## 第3章 使用Ceph对象存储\n\n这一章除了讲解老一套的`rados`网关配置与`S3`和`Swift`对接之外，还讲了`rados`网关和`openstack keystone`的集成。最大的特色是讲解了`radosgw`多区域网关配置和测试，无限逼近线上大规模生产环境的场景。最后就`RGW`创建文件同步和共享服务进行了介绍，干货多多。\n\n## 第4章 使用Ceph文件系统\n\n借着`cephfs`介绍了`mds`一番，然后讲解了多种访问`cephfs`的方式，然后就是熟悉的将`cephfs`导出为`nfs`，这算是很常见的场景了，但是据我所知目前`ceph`版本的`cephfs`的性能堪忧，以至于用的比较多的还是`rbd`，希望`ceph`可以在之后的版本中将`cephfs`做的越开越好。最后介绍了`ceph-dokan`的`cephfs`的`windows`客户端，很牛逼，还在持续迭代中。还介绍了如何使得`cephfs`替换`hdfs`，这个我之前在`ceph`线下沙龙南京站听那位中兴的工程师讲的`cephfs`和`hadoop`的恋爱史印象非常深刻，讲的蛮好的。\n\n## 第5章 用Calamari监控Ceph集群\n\n先科普了一下各大监控命令和手段，然后介绍了`Calamari`这个工具的编译和搭建，这个工具主要就是一个`dashboard`，并不是那种能在`UI`上操作和管理集群的工具，并且已经很久没再更新了，用于生产环境的请注意，自己玩玩倒是还可以。\n\n## 第6章 操作和管理Ceph集群\n\n先是介绍了两种方式管理各种守护进程和服务。然后讲解了横向扩展和纵向扩展。最后介绍了集群的升级。\n\n## 第7章 深入Ceph\n\n这一章蛮重要的，首先讲解了`crush`的机制和算法，然后介绍了`ceph`的身份验证，也就是`cephx`，最后结合`crush`和`pg`创建了定制化的`osd`和`pool`。\n\n## 第8章 Ceph生产计划和性能优化\n\n这本书在讲解这一部分明显要比上本书要全面的多，分别针对操作系统、`osd`、`filestore`、`cilent`等性能优化给出参数配置建议。然后介绍了纠删码的各种插件，配了结构图很好理解，最后还是缓存分层，做了测试，更能清楚了了解分层的原理。\n\n## 第9章 Ceph虚拟存储管理器（VSM）\n\n这个工具就要比上面提到的`calamari`之类的工具要强得多了，不仅有`dashboard`可以监控集群，还可以操作和管理集群，还有牛逼的是升级`ceph`方面，你也只需要修改配置要升级版本的网址，便可以实现自动升级，这是`intel`开发并提供开源的一款产品，值得去尝试一番。\n\n## 第10章\n\n同样的基准测试，不一样的味道。这一章提供了`n`多种的方式。有`dd`，有`rados bench`，有`rados load-gen`，`rdb bench-write`和`fio`等等。从测试网络，池，块设备和模拟负载应有尽有，关键这些都是`ceph`自带的除了`fio`。然后介绍了`ceph daemon`和`ceph tell`两个高校操作手段，最后介绍了使用`Ansible`部署`Ceph`，它可以通过配置文件配置集群信息而搭建集群，也就是说不需要手动的一个一个创建了，这样就显得很高效了。\n\n## 总结\n\n`ceph`已经看完两本书了，按理来说不应该再是那种一无所知的小白了，我觉得我缺少的还是大量的实践，所以之后我会对着这两本书照着例子再把代码敲一遍加深印象，然后对`ceph`应该会有另一番理解吧。接下来的进阶路线应该是中兴出版的《ceph原理和源码分析》了把，这本今年才出版的书我已经入手了，看了第一章，总体觉得还是蛮深入的，大量的源码，需要一定的知识铺垫，正好之后我也有看`ceph`源码和提供贡献的想法，所以正好那这本书过渡一下，加油！\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/cookbook-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Ceph分布式存储学习指南》","url":"/2017/11/26/read-ceph-guide/","content":"\n<center>![ceph guide][1]</center>\n\n阅读周期：2017/9/5 ～ 2017/10/7\n阅读人群：入门ceph的研发或者运维\n阅读评分：4.7\n\n\n这本书不到两百页，但是也讲解了`ceph`的方方面面。对于入门`ceph`来说，看官网文档是必要的，但是很多时候官方文档太基础，太零散了，而这样一本书籍就可以弥补这样的缺陷，它把所有组件进行总结和概括，并且进行了实战例子的演示。\n\n<!--more-->\n\n## 目录结构\n\n* 第1章 Ceph存储介绍\n* 第2章 Ceph实战部署\n* 第3章 Ceph架构和组件\n* 第4章 Ceph内部构件\n* 第5章 Ceph部署\n* 第6章 Ceph存储配置\n* 第7章 Ceph操作及管理\n* 第8章 监控Ceph集群\n* 第9章 Ceph与Openstack的集成\n* 第10章 Ceph性能调优和基准测试\n\n## 具体分析\n\n### 第1章\n\n第1章主要介绍了一下`ceph`的特点和历史发展，通过与`raid`，其他存储相比较得出自身的优势和特色，然后着眼于未来的一些设计和解决方案。最后针对`ceph`三大存储接口：块存储、文件系统存储和对象存储进行了介绍。\n\n### 第2章\n\n这一章其实就是利用`ceph-deploy`手动搭建集群的一个演示，后续章节还会有相应的补充\n\n### 第3章\n\n第3章重点讲解了`ceph`的几大组件，`rados`存储网关，`osd` 讲的比较多，也是应该的，毕竟是存储的核心，首先对比了目前主流三个文件系统的优劣，然后对日志进行了科普，顺带讲解了一下不推荐在`ceph`集群中采用`raid`的原因。之后就是一些`osd`，`monitor`和`mds`的基础命令了。\n\n### 第4章\n\n如果说上一章主要讲的是`osd`，`monitor`，`mds`的话，那么这一章很符合标题内部构件，对应讲解的是`pg`，`crush`，对象和`pool`。`pg`是对象的载体，`pg`的状态往往决定着集群的状态，徐小胖有一篇文章叫大话`pg`讲的挺简单明了的，其实写一个数据然后查看一下存储目录结构就知道七七八八了。`ceph`池是存储对象的逻辑分区，用户可以根据自己的需求定制池的类型和规则。比如副本池还是`EC`池，也可以为`pool`定制`crush rule`。\n\n### 第5章\n\n这一章也是部署，跟第2章不同的是这一章不用`ceph-deploy`工具而采用了手工部署的方式。这样做可能要做的事情多一点，繁琐一些，但是会更灵活一些，因为`ceph-deploy`这种自动化的工具往往存在局限性，比较适用于测试环境，并不适用于生产环境。往往生产环境需要做很多的定制化的改变。最后还讲解了一些如何进行`ceph`升级。\n\n### 第6章\n\n这一章干货满满，分别就块设备，文件系统和对象存储进行了讲解，还是挺全面的，建议一边看书一边实践，这些命令还是要多用，否则很快也会忘记。目前`rbd`还是`ceph`最稳定的存储方式，相比`cephfs`而言他的性能、稳定和快照复制等功能都做的很全面了。`cephfs`虽然越来越被更多的应用内嵌，但是还是不能应用与生产环境之上。最后介绍了一下`rados`网关配置分别和`s3`，`swift`对接的流程。\n\n### 第7章\n\n这一章主要介绍了如何运行和管理集群里面的守护进程和相关服务。然后就集群扩容和所容等扩展操作进行了讲解。最后重点讲解了如何管理`crushmap`，通过配置确定集群存储的结构，定制`pool`的规则等等。\n\n### 第8章\n\n这一章对应标题讲解了一系列的监控命令，有`osd`，有`pg`，还有`mon`等等。最后引入了一些开源的监控软件，比如`kraken`，`ceph-dash`和`Calamari`，讲的比较简洁，想要具体了解还需要自己动手去搭建，不过貌似有一些已经长时间不维护了，建议动手前看一下。\n\n### 第9章\n\n这一章主要讲`ceph`和`openstack`的完美集成，不过讲的也很简洁，其实就是了解歌大概情况，我觉得这对刚入门的小白来说挺好的，讲得太细也没什么用，反而云里雾里的，其实这本书对我最大的帮助就是了解了存储和`ceph`大概是个什么情况，然后知道了一些命令的用法和应用场景。第一遍熟悉一下就可以了，然后之后可以在深入，否则第一次接触就搞`openstack`这种高级玩意估计吃不消呢。\n\n### 第10章\n\n最后一章先分别对软件和硬件的性能调优提出了建议，这个东西我觉得还是得到生产环境中不断尝试，虚拟机的话没有任何参考建议。然后集群优化分别介绍了`osd`，`filestore`和客户端的相关参数，这个我们可以熟悉一下，自己没事动手改改，看看效果。然后针对`ec`和缓存分层进行了一些介绍，也是局限于基本够用的层面上。最后介绍了`rados bench`这个基准测试工具。\n\n## 总结\n\n这本书专为入门而生，就是在官网文档的基础之上进行规整和总结，如果看不下去官网文档的可以尝试这个，书很薄，很快就可以看完了，看完大概就对`ceph`没那么陌生了，还是一个比较不错的入门书籍。如果已经了解`ceph`一段时间的朋友我就不是很推荐了。\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/book-rocket-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Python编程快速上手---让繁琐工作自动化》","url":"/2017/11/26/read-python-automation/","content":"\n<center>![python automation][1]</center>\n\n> 阅读感悟这个系列是针对每次阅读一本书后，先对全本书内容进行整体的总结和评价，然后针对每一章节进行具体分析，分享一下自己从中的收获，这样不但可以在看完整本书后对全书有个完整的加深回顾，还可以对没有阅读过本书的人提供经验和建议或者跟同样阅读过的伙伴相互交流，交换感悟和经验。\n\n- 阅读周期：2017/7/15 ～ 2017/10/7\n- 阅读人群：无语言基础的小白，有其他语言基础让入门`python`的，熟练`python`想深入学习的请勿下手\n- 阅读评分：4.7\n\n<!--more-->\n\n大家都知道其实一本书可能并不是很贵，一般都是几十元，就算是本烂书对大家的经济损失来说其实并不是很严重，最关键的就是时间了，在这个压力山大和节奏飞快的时代，时间无疑是最宝贵的，所以很多时候我买书之前都会很仔细的看下评论，我个人是比较喜欢买书的，一般会选择在亚马逊上面购买，对比京东而言的话我觉得他的优惠策略更加直接，不需要抢什么券什么的，还有他的物流非常快，最多隔天就到了，包装也很精致，很少出现盗版书这种情况，最后也是最重要的是相比京东而言亚马逊书籍评论更加贴切一些，往往京东一本书下面会有上千条评论，你很少会看到负面的评价，而亚马逊就不一样了，一般评论的都是骨灰级读者了，一旦评论都是很认真并且很全面的，所以也很难得在亚马逊上面看到全五星的书籍。\n\n这本书是我入门`python`的第一本书，在此之前我有`C`，`Java`和`PHP`的语言经验。无论是在阅读这本书的过程中还是看完了之后，我都有一种庆幸感，那就是我当初选择这本书真的没错。在阅读前三章的时候，我一度觉得这本书太浅了，这主要可能因为我有其他语言基础，而这本书面对的读者不仅仅有语言基础的，而他这么做对那种小白来说就非常`nice`了。\n\n这本书给我最深的印象可以概括为四个字：深入浅出。就是在讲的很细致的同时，同时也会配有一些深入的内容。全本书的最大特色莫过于插图和`demo`了。一般的书一般是以代码围绕着讲解，而这本书的话往往是通过一些小项目或者小例子讲解一些概念和用法。第一部分主要讲解了语言的基础概念和`python`重要的数据结构：列表和字典。第二部分主要讲解了自动化知识，就比较深入了，具体比如正则表达式、对文件的操作、调试技巧、获取`web`信息、处理`excel`文件、处理`pdf`和`word`文档、处理`CSV`文件和`JSON`数据、计划任务、邮件和短信、操作图像和操作鼠标和键盘。掌握了列表、字典和字符串这些数据结构就可以进行基础的`python`编程了，然后学习了正则表达式和文件操作后，就可以很轻松了写一些自动化脚本了。从`web`抓取信息这一章是专门为喜爱爬虫的小伙伴准备的，提供了很多有用的工具和手段。之后还有对各种类型文件的操作，操作图像这一章我也很喜欢，准备之后做一个小项目，因为我个人对好看的图片很有兴趣。\n\n## 目录结构\n* 第一部分 Python编程基础\n    * 第1章 python基础\n    * 第2章 控制流\n    * 第3章 函数\n    * 第4章 列表\n    * 第5章 字典和结构化数据\n    * 第6章 字符串操作\n* 第二部分\n    * 第7章 模式匹配与正则表达式\n    * 第8章 读写文件\n    * 第9章 组织文件\n    * 第10章 调试\n    * 第11章 从Web抓取信息\n    * 第12章 处理Excel电子表格\n    * 第13章 处理PDF和Word文档\n    * 第14章 处理CSV文件和JSON数据\n    * 第15章 保持时间、计划任务和启动程序\n    * 第16章 发送电子邮件和短信\n    * 第17章 操作图像\n    * 第18章 用GUI自动化控制键盘和鼠标\n\n\n## 章节分析\n\n### 第1章、第2章和第3章\n\n这3章对我来说没什么好说的，就是一些语言基础，比如字符串拼接啊，控制流啊，函数什么的，对没有语言基础的小伙伴来说非常的贴心和容易理解，老司机的话可以快速阅读或者略过。\n\n### 第4章\n\n列表是`python`比较有特色的数据结构了，操作起来非常的方便。这一章分别阐述了获取列表、操作列表（merge、copy、delete）、列表循环和多重赋值技巧。然后就是一些深入的操作技巧了，比如排序、插入和删除。最后引入了字符串和元组和列表加以比较，讲解了可变类型、转换类型、传递引用和深拷贝等，总的来说还是很实用。\n\n### 第5章、第6章\n\n首先围绕字典讲解了一些基本的`API`，然后通过很好的棋盘建模模拟了字典的数据结构，很形象。字符串的话也差不多一样。\n\n### 第7章\n\n相比其他语言的正则来说，我觉得`python`的正则表达式无论是理解还是使用上面还算是比较简单的。先讲解了单个匹配和全局匹配，然后针对邮件，电话等常见场景给出了例子。\n\n\n### 第8章\n\n先讲解了`os`模块很多有用的`API`，大多数都是跟文件和目录有关，然后讲解了文件的读写操作，其中提到了`shelve`模块，这个在需要确保文件保密的场景非常有用。最后通过一个小例子讲解了`pyperclip`模块如何进行剪切板操作。\n\n### 第9章\n\n讲解了通过`shutil`模块复制文件和文件夹、移动、重命名、永久删除和安全删除。然后讲解了围绕`zipfile`模块相应的操作。\n\n### 第10章\n\n断言给我的印象比较深，还有`python`自带的`IDLE`的调试器的使用，我本人平时基本上是在`linux`下`vim`工作，所以这个工具平时也用不到，以后可以在`windows`下尝试一下。\n\n### 第11章\n\n这一章学习到了很多通过`python`取获取`web`信息的手段，这会在爬虫的时候很受用。比如通过`webbrowser`模块启动浏览器；通过`request`模块下载网页；通过`BeautifulSoup`模块解析`html`，这个模块非常强，简直和`js`操作`html`相媲美。\n\n### 第12章、第13章、第14章\n\n这三章分别针对`excel`、`pdf`、`word`、`csv`和`json`类型的文件或数据格式进行操作。对`excel`的支持还蛮强的，但是`pdf`和`word`的话就或多或少可能有问题，这个就不赘述了，用到的时候再查就好了。\n\n### 第15章、第16章、第18章\n\n这几章没什么好说的\n第15章主要讲解了保持时间、计划任务和启动程序等功能\n第16章主要讲解了消息通知，包括了邮件和短信接口\n第18章主要讲解了如何通过`python`控制键盘和鼠标，其中比较有意思的就是控制键盘原生不存在的一些特殊字符\n\n\n### 第17章\n\n`python`操作图像还是蛮好用的，本章主要讲解了对图片的裁剪，旋转，改变像素等，后续我会做一个小项目，通过`python`对图片进行一些操作，顺便深入了解一下这部分的知识。\n\n\n## 总结\n\n第一篇阅读总结感觉写的并不是很好，篇幅虽然还算长，但是感觉内容并不是很饱满，很多地方略显啰嗦。想了一下，之后可以把每一章的具体分析放在平时，每阅读完一章节就可以进行总结了，然后读完全本书就进行一个整体总结就可以了，这样每一章的总结就更加细致了，就不用读完全本书的时候再去回忆，这样效率很低。最后我会在之后有空的情况下分别做一个`python`的爬虫项目和一个操作图像的项目。\n\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/python-automation-600x450.jpg\n","tags":["Python"],"categories":["read"]},{"title":"博客主题 shadow","url":"/2017/11/19/hexo-theme-shadow/","content":"\n<center>![blog theme shadow][4]</center>\n\n使用了[black-blue][1]主题一段时间后，有些地方不符合我个人的审美和习惯，然后自己偶尔改了一些，久而久之还改了不少东西，为了方便存档和以后持续更新，就打造一个自己的专属主题：[hexo-theme-shadow][2]，大家可以去[我的博客][3]看下实际效果\n\n如果大家有任何批评和建议，随时可以在`github`上面提出`PR`，要是觉得主题还不错的话，欢迎留下你的`star`哦 ^_^\n\n<!--more-->\n\n具体如何配置主题和搭建博客这些基础通用的东西我就不讲了，这边我主要罗列一下我已经更新和以后可能要做的功能点\n\n## 已经更新的功能\n\n* 添加左侧栏`segmentfault`小图标\n* 更新`requirejs`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`jquery`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`baidu`分享源到本地，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`markdown code`的风格，具体参见博客\n    * 更改了全局的代码颜色样式\n    * 原来项目编号中的代码样式是白色的，很丑陋，也进行了相应的更新\n* 将头像从懒加载改为了直接加载，懒加载头像需要很长时间，用户体验很差\n* 网站背景音乐支持\n* 代码块样式修改，每个代码块上面有个蓝色的条条很不好看\n* 三级、四级和五级标题样式修改\n* 链接样式修改\n* 畅言评论支持\n* 打赏支持\n* 书签样式修改\n* 代码块样式修改\n* 项目编号样式修改\n* 表格样式修改\n\n## 使用\n\n具体使用教程请参考我主题的`github`地址，在下方有写\n\n如果大家有什么建议或者问题，可以在`github`上面提`issue`或者`PR`，也可以通过邮箱联系到我\n\n`github`地址： `https://github.com/tony-yin/hexo-theme-shadow`\n邮箱地址： `1241484989@qq.com`\n\n[1]: http://geeksblog.cc/\n[2]: https://github.com/tony-yin/hexo-theme-shadow\n[3]: http://www.tony-yin.top/\n[4]: http://ow0mgad6r.bkt.clouddn.com/do_blog_600x450.png\n","tags":["博客"],"categories":["tech"]},{"title":"LVM动态扩展","url":"/2017/11/14/LVM-Space-Expansion/","content":"\n<center>![LVM Space Expansion][1]</center>\n\n在平时的开发工作中，经常会创建磁盘不足够大的虚拟机，然后往集群里面写一些数据导致磁盘满了。手动编辑虚拟机的磁盘大小是不会文件系统识别的，大多数同学只能无奈的重新装`OS`，这里我介绍一种基于`LVM`实现动态的方式。\n\n<!--more-->\n\n## LVM了解\n\n`LVM`是逻辑盘卷管理（`LogicalVolumeManager`）的简称，它是`Linux`环境下对磁盘分区进行管理的一种机制，`LVM`是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。通过LVM系统管理员可以轻松管理磁盘分区，如：将若干个磁盘分区连接为一个整块的卷组（`volumegroup`），形成一个存储池。管理员可以在卷组上随意创建逻辑卷组（`logicalvolumes`），并进一步在逻辑卷组上创建文件系统。管理员通过`LVM`可以方便的调整存储卷组的大小，并且可以对磁盘存储按照组的方式进行命名、管理和分配。\n\n\n## 查看分区\n\n当前默认只有一个采用`lvm`的分区，一开始`sda`磁盘容量为`16G`，后来发现不够用了，编辑磁盘大小为`50G`，但是可以发现这`50G`并没有起到扩展分区容量的效果。\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       14G  3.4G  9.6G  26% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n└─sda2                           8:2    0 15.5G  0 part \n  ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n  └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n```\n\n## 分区\n\n可以通过新增一块其他磁盘来扩容，我这边采取的是增大当前磁盘的容量实现扩容。\n\n有时候因为系统设备处于繁忙状态，所以分区需要重启后才会生效。\n\n```\n[root@tony-play ~]# fdisk /dev/sda\nWARNING: DOS-compatible mode is deprecated. It's strongly recommended to\n         switch off the mode (command 'c') and change display units to\n         sectors (command 'u').\nCommand (m for help): n\nCommand action\n    e   extended\n    p   primary partition (1-4)\np\nPartition number (1-4): 3\nFirst cylinder (2089-6527, default 2089):   // 直接回车，用默认值就可以了\nUsing default value 2089\nLast cylinder, +cylinders or +size{K,M,G} (2089-6527, default 6527):    // 直接回车，用默认值就可以了\nUsing default value 6527\nCommand (m for help): w\nThe partition table has been altered!\n\n# 可以看到新建的分区sda3已结被创建出来了，采取默认值会将剩余所有空间都分到分区中\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n├─sda2                           8:2    0 15.5G  0 part \n│ ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n│ └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n└─sda3                           8:3    0   34G  0 part \n```\n\n## 查看当前文件系统\n\n当前文件系统为`ext4`\n\n```\n[root@tony-play ~]# mount\n/dev/mapper/vg_tonyplay-lv_root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext4 (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n```\n\n## 为新分区创建文件系统\n\n```\n[root@tony-play ~]# sudo mkfs.ext4 /dev/sda3\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n2228224 inodes, 8912727 blocks\n445636 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=4294967296\n272 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks: \n    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n    4096000, 7962624\n\nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 39 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n\n## 查看卷组信息\n\n```\n[root@tony-play ~]# vgdisplay\n  --- Volume group ---\n  VG Name               vg_tonyplay     // 卷组名在下面扩展中会用到\n  System ID             \n  Format                lvm2\n  Metadata Areas        1\n  Metadata Sequence No  3\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                2\n  Open LV               2\n  Max PV                0\n  Cur PV                1\n  Act PV                1\n  VG Size               15.51 GiB\n  PE Size               4.00 MiB\n  Total PE              3970\n  Alloc PE / Size       3970 / 15.51 GiB\n  Free  PE / Size       0 / 0   \n  VG UUID               Y9usSM-nDU5-ZAUd-Y3Te-u5Pd-uFBr-gcYHf0\n```\n\n## 创建新物理卷\n\n```\n[root@tony-play ~]# pvcreate /dev/sda3\n  Physical volume \"/dev/sda3\" successfully created\n```\n\n## 扩展到卷组\n\n```\nvgextend vg_tonyplay /dev/sda3  // 卷组名在查看卷组信息中\n  Volume group \"vg_tonyplay\" successfully extended\n```\n\n## 查看逻辑分区\n\n`/dev/vg_tonyplay/lv_root`就是根分区，也是我们要扩展的分区。\n\n```\n[root@tony-play ~]# lvdisplay \n  --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_root   // 根分区\n    LV Name                lv_root\n    VG Name                vg_tonyplay\n    LV UUID                IPd7lm-Sx8g-pe7k-llNL-j1wc-mbA2-2cAdsy\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:58:53 -0400\n    LV Status              available\n    # open                 1\n    LV Size                13.91 GiB\n    Current LE             3561\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:0\n\n    --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_swap\n    LV Name                lv_swap\n    VG Name                vg_tonyplay\n    LV UUID                qX637q-iD6i-8blp-hmmS-MvLy-xZ0y-b4D0BF\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:59:07 -0400\n    LV Status              available\n    # open                 1\n    LV Size                1.60 GiB\n    Current LE             409\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:1\n```\n\n## 扩展容量到逻辑分区\n\n```\n[root@tony-play ~]# lvextend /dev/vg_tonyplay/lv_root /dev/sda3\n  Size of logical volume vg_tonyplay/lv_root changed from 13.91 GiB (3561 extents) to 47.91 GiB (12264 extents).\n  Logical volume lv_root successfully resized\n```\n\n## 刷新逻辑分区容量使扩展生效\n\n`ext4`用`resize2fs`，`xfs`用`xfs_growfs`\n\n```\n[root@tony-play ~]# resize2fs /dev/vg_tonyplay/lv_root\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vg_tonyplay/lv_root is mounted on /; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 3\nPerforming an on-line resize of /dev/vg_tonyplay/lv_root to 12558336 (4k) blocks.\nThe filesystem on /dev/vg_tonyplay/lv_root is now 12558336 blocks long.\n```\n\n## 查看逻辑分区容量\n\n可以发现`/dev/mapper/vg_tonyplay-lv_root`已经从开始的`14G`扩展到了`48G`。ok，这就说明大功告成了，再也不用通过重装系统这种蹩脚的方式扩容了\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       48G  3.4G   42G   8% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n```\n\n## 小结\n\n至此，`lvm`扩容工作的过程应该是比较清楚了，之后有机会的话我会再补充一下`LVM`的压缩、删除等操作过程。\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/work-for-love-600x450.png\n","tags":["LVM"],"categories":["tech"]},{"title":"Ceph 编译（Giant版本）","url":"/2017/11/14/Ceph-Compile/","content":"\n<center>![Ceph Compile][1]</center>\n\n如今入门`Ceph`的时候，大家一般用`ceph-deploy`工具比较多，这个工具的确很强大，很方便，对应的也就是很无脑。如果之后想深入`Ceph`或者想在生产环境中部署`Ceph`的话，就得熟悉`Ceph`源码编译了。这对我们熟悉`Ceph`的`feature`的变化，`component`的相互关系以及围绕`Ceph`各种定制化扩展都大有裨益。本文就笔者`Ceph`源码编译过程和遇到的问题作出分享，希望有人能够得益于此。\n\n<!--more-->\n\n## 环境声明\n\n* OS： Centos 6.8\n* Ceph： Giant\n\n## 源码下载\n\n### 指定 giant 分支\n\n```shell\n git clone -b giant git://github.com/ceph/ceph.git\n ```\n\n### 下载子模块\n\n```shell\ngit submodule update --init --recursive\n```\n\n这一步我始终更新不了，尝试了一些办法未果，所以就去直接手动下载了源码，这个问题以后有时间看下\n\n## 预检\n\n```shell\ncd ceph \n./autogen.sh \n./configure\n```\n\n### Autogen\n\n这一步会频繁的报错一些`m4`文件没有，这需要我们手动创建这些`m4`文件夹即可，\n\n一开始我在`ceph`根目录创建了`m4`文件夹，并生成了包括`acx_pthread.m4`在内的文件，但是还是报错：`acx_pthread.m4 not exist`，后来发现这时候已经**切换目录**了，不止一个地方需要`m4`文件夹，一共有这几个地方需要手动创建目录：`mkdir m4`\n\n```shell\n./src/rocksdb/m4\n./src/gtest/m4\n./src/erasure-code/jerasure/jerasure/m4\n./src/erasure-code/jerasure/gf-complete/m4\n./m4\n```\n\n如果还是报错一些文件不存在，并且通过上述方法不能自行初始化生成的话，可以从网上或者已经编译过的`ceph`环境拷贝过来\n\n```shell\n报错：umdefined macro\n```\n\n下载地址：https://www.gnu.org/software/autoconf-archive/ax_check_classpath.html#ax_check_classpath\n\n### Configure\n\n这一步是编译过程中可能出错的次数最多的，因为可能会因为你的环境缺少相应的包不断报错。不过数量虽多，解决起来还是比较容易的，就根据报错的缺包对应下载安装就好了，下面我先给出一个所有包的安装步骤，然后再针对每个报错环节给出具体的解决方案\n\n#### 总体解决方案\n\n```shell\n# 1. 通过yum安装所有可以安装的包\nyum install -y yasm libuuid-devel libblkid-devel libudev-devel cryptopp-devel fuse-devel libunwind-devel libedit-devel libatomic_ops-devel snappy-devel leveldb-devel libaio-devel xfsprogs-devel boost*\n\n# 2. 部分yum不能安装的可以通过rpm安装\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n#### 具体解决方案\n\n1.yasm\n\n报错：\n\n```shell\nyasm command not found\n```\n\n解决：\n\n```shell\nyum install yasm -y\n```\n\n2.libuuid\n\n报错：\n\n```shell\nconfigure: error: libuuid not found\n```\n\n解决：\n\n```shell\nyum install libuuid-devel -y\n```\n\n3.libblkid\n\n报错：\n\n```shell\nconfigure: error: blkid/blkid.h not found (libblkid-dev, libblkid-devel)\n```\n\n解决：\n\n```shell\nyum install libblkid-devel -y\n```\n\n4.libudev\n\n报错\n\n```shell\nconfigure: error: libudev.h not found (libudev-dev, libudev-devel)\n```\n\n解决：\n\n```shell\nyum install libudev-devel -y\n```\n\n5.crypto\n\n报错：\n\n```shell\n configure: error: no suitable crypto library found\n ```\n\n 解决：\n\n ```shell\n yum install cryptopp-devel -y\n ```\n\n 6.fuse\n\n 报错：\n\n ```shell\n configure: error: no FUSE found (use --without-fuse to disable)\n ```\n\n 解决：\n\n ```shell\n yum install fuse-devel -y\n ```\n\n 7.tcmalloc\n\n 报错：\n\n ```shell\n configure: error: no tcmalloc found (use --without-tcmalloc to disable)\n ```\n\n 解决：\n\n ```shell\n# 1. 需要先安装libunwind-devel，被gperftools依赖\n yum install libunwind-devel -y\n\n# 2. 安装tcmalloc（yum无法安装，需要通过rpm的方式）\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n8.libedit\n\n报错：\n\n```shell\nconfigure: error: No usable version of libedit found.\n```\n\n解决：\n\n```shell\nyum install  libedit-devel -y\n```\n\n9.libatomic-ops\n\n报错：\n\n```shell\nconfigure: error: no libatomic-ops found (use --without-libatomic-ops to disable)\n```\n\n解决：\n\n```shell\nyum install libatomic_ops-devel  -y\n```\n\n10.libsnappy\n\n报错：\n\n```shell\nconfigure: error: libsnappy not found\n```\n\n解决：\n\n```shell\nyum install snappy-devel -y\n\nor \n\nwget ftp://195.220.108.108/linux/centos/6.9/os/x86_64/Packages/snappy-devel-1.1.0-1.el6.x86_64.rpm\nrpm -ivh snappy-devel-1.1.0-1.el6.x86_64.rpm\n```\n\n11.libleveldb\n\n报错：\n\n```shell\nconfigure: error: libleveldb not found\n```\n\n解决：\n\n```shell\nyum install leveldb-devel\n```\n\n12.libaio\n\n报错：\n\n```shell\nconfigure: error: libaio not found\n```\n\n解决：\n\n```shell\nyum install libaio-devel -y\n```\n\n13.libxfs\n\n报错：\n\n```shell\nconfigure: error: xfs/xfs.h not found (--without-libxfs to disable)\n```\n\n解决：\n\n```shell\nyum -y install xfsprogs-devel\n```\n\n14.boost\n\n报错：\n\n```shell\nCan't find boost spirit headers\n```\n\n解决：\n\n```shell\nyum install boost* -y\n```\n\n## 编译安装\n\n机器配置不好的话，编译需要时间比较长。可以使用`make -j`增加并发度，`4`表示同时执行的`make`方法数。\n\n```shell\nmake -j4\nmake install（可选）\n```\n\n## 小结\n\n`ceph`手动源码编译遇到的问题还是蛮多的，如果不自己动手经历一下的话很多东西都不知道，当然这都是经验的积累，不断地锻炼自己解决问题的能力，要学会见招拆招，通过问题发现原理和本质。\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/ceph-600x450.png\n","tags":["编译"],"categories":["tech"]},{"title":"通过ganesha-nfs导出Ceph为NFS（Luminous）","url":"/2017/11/08/Ceph-NFS-Ganesha/","content":"\n<center>![ganesha][8]</center>\n\n自从`Jewel`版本，`nfs-ganesha`开始支持`ceph`，并且把对接点选择了`rados`。`Ganesha`支持两种方式将`Ceph`导出为`NFS`，一种通过`RGW`，一种通过`CephFS`，通过`FSAL`模块 连接到`RGW`或者`CephFS`， 其中，`FSAL_RGW`调用`librgw2`将`NFS`协议转义为`S3`协议再通过`RGW`存入到`Ceph`中，`FSAL_CEPH` 调用`libcephfs1`将`NFS`转义为`Cephfs`协议再存入到`Ceph` 中。所以需要额外安装这两个包。\n\n本文就`Luminous`版本的`ceph`基于`ganesha`导出`nfs`部署，并且测试一下`rgw`和`cephfs`的性能。[@徐小胖][1]已经就`jewel`版本的过程进行了大致的讲解，我这边主要分享一下我遇到他文章没提到的和`Luminous`场景导致的问题。\n\n<!--more-->\n\n## 参考链接：\n\n* [通过ganesha-nfs将 Ceph 导出为 NFS][1]\n* [RGW+Ganesha环境部署][2]\n\n## 环境声明\n\n* os： centos7\n* ceph： luminous\n* nfs-gnesha： **v2.5 stable**（important）\n\n## 安装依赖\n\n一些编译需要的公共库\n\n```\nyum install gcc git cmake autoconf libtool bison flex doxygen openssl-devel gcc-c++ krb5-libs krb5-devel libuuid-devel nfs-utils -y\n```\n\n`ubuntu`的我也试了一下，主要有以下几个包不同：\n\n* `gcc-c++`   ->   `g++`\n* `libuuid-devel` ->  `uuid-dev`\n* `nfs-utils`   ->  `nfs-kernel-server` \n\n如果要生成`FSAL_RGW`模块，需要安装`librgw2-devel`（我装的`librgw-devel`也可以`work`，看了`so`文件发现就是`2`，只是建立了映射关系，这个看了源码知道了是因为`K`版本对`librgw`编译这块做了升级，加了`2`标识一下）\n\n```\nyum install librgw2-devel -y \n```\n\n如果要生成`FSAL_CEPH`模块，需要安装`libcephfs1-devel`\n\n```\nyum install libcephfs1-devel -y\n```\n\n## 源码下载\n\n上面两篇文章一个下载的是`v2.3 stable`，一个是`v2.4 stable`，两个我都试过，都会在`make`到`80%`左右的时候报错，应该是源码版本和库的版本有冲突导致的，这个问题耽误我挺长时间的，后来猜想可能是版本问题，尝试了一下`v2.5 stable`的源码就可以了\n\n```\ngit clone -b V2.5-stable https://github.com/nfs-ganesha/nfs-ganesha.git --recursive\n```\n\n**注意：（重要）**\n\n最近我同事根据这篇文档部署`nfs-ganesha`的时候，发现之后`cmake`的操作后`USE_FSAL_RGW`始终是`OFF`的状态，一开始检查了一下环境发现`ganesha`是`v2.5`，然后`ceph`也是`L`版本，`centos`也是`7`。\n\n报错如下：\n\n```\nlocal RGW version is 1.1.4 not suitable match, but required latest RGW version is 1.1.6\n```\n\n很好理解，就是我们安装的`RGW`达不到`nfs-ganesha`的要求，首先我们尝试去安装新的版本，但是`yum`的源已经配置了`luminous`，并且网上搜索了一圈并没有找到更新的。\n\nok，那就第二步，那就找原因同样的环境为啥我可以成功？而这位同事却不行？莫非因为我帅？不能这么逆天吧^_^ （明明可以靠脸吃饭，偏偏靠才华，哎……）\n\n言归正传，通过报错信息查看源代码？定位到了报错文件`src/CMakeLists.txt`，然后代码中对`RGW`的版本要求也的确是`1.1.6`，回过头再看我之前下载下来的源代码，怪异的现象发生了，我代码中对`RGW`的版本要求只有`1.1.3`，此时我的第一反应就是应该这个文件被修改过导致两份代码不一样，出于本能直接上了`github`，果不其然，发现了`mattbenjamin`同学在`10`月`17`号在`v2.5`的分支上提交了一个`commit`针对`RGW`版本做出了修改！具体参看[commit详情][3]，而这位贡献者几乎同时也在`ceph`提交了相关代码。这样想想就说得通了，我在搭建`nfs-ganesha`的时候恰好是在他提交之前的，所以我本地`RGW`本地版本是`1.1.4`是比要求版本`1.1.3`要高的，虽然不是完全`match`，但是也基本符合了要求，而我这位同事脸就比较黑了。。。\n\n那该怎么解决这个问题呢？\n\n有两种解决方案：\n\n1. 手动编译`Luminous Ceph`把最新的`RGW`包编译出来，因为现在这个资源还没被公开分享出来；\n2. 克隆这个改动之前的代码\n\n这么看来第二种更为简单一些，我们也是采用的这种方式，`git clone v2.5 stable`后，然后`git checkout`恰当的版本号即可。\n\n**总结：**\n\n其实直接`clone`实时的代码这是不合理的，因为你不知道他什么时候会更新。正确的方式是我们应该找到稳定的`release`版本，`nfs-ganesha`也有提供[一系列的release][4]，而我们通过查看`2.5 tree`上面的`commit`信息，可以知道`RGW`的改动是介于`2.5.3`和`2.5.4`之间的，所以我们下载`2.5.3`较为合适，下载地址在[这里][5]。\n\n这边还需要注意一个问题，这边`release`包下载下来你会发现`libntrirpc`这个文件夹没有内容，出现这个问题是因为`libntrirpc`对应的也是一个`git`仓库，我们需要去[ntrirpc][6]手动克隆，然后通过`nfs-ganesha`的目录发现对`ntrirpc`的版本要求是`fadcbde`（这些都是上`github`可以看到的），我们需要做的就是在下载下来的`ntrirpc`目录下切到`fadcbde`这个`commit`上，并把当前的内容拷贝到`nfs-ganesha`的`libntrirpc`目录下。\n\n实在搞不定的童鞋，我提供一个我克隆的`nfs-ganesha`的代码包，你们可以对比一下，这个我是可以成功部署的\n\n附：我的`nfs-ganesha`[下载地址][7]\n\n## 编译\n\n编译`nfs-ganesha`， **注意打开对应的模块：**\n\n* 如果需要生成`FSAL_RGW`模块，则在编译选项中添加： `-DUSE_FSAL_RGW=ON`\n* 如果需要生成`FSAL_CEPH`模块，则在编译选项中添加： `-DUSE_FSAL_CEPH=ON`\n\n```\ncd src/\nmkdir build\ncd /build/\ncmake -DUSE_FSAL_RGW=ON -DUSE_FSAL_CEPH=ON ../\n```\n\n`cmake`的过程中会有以下输出：\n\n```\n-- Looking for ceph_ll_lookup_root in cephfs - found\n-- Found cephfs libraries: /usr/lib64/libcephfs.so\n-- Found CEPHFS: /usr/include  \n-- Looking for rgw_mount in rgw\n-- Looking for rgw_mount in rgw - found\n-- Found rgw libraries: /usr/lib64/librgw.so\n-- Found RGW: /usr (found suitable version \"1.1\", minimum required is \"1.1\") \n...\n-- USE_FSAL_CEPH = ON\n-- USE_FSAL_CEPH_MKNOD = OFF\n-- USE_FSAL_CEPH_SETLK = OFF\n-- USE_FSAL_CEPH_LL_LOOKUP_ROOT = ON\n-- USE_FSAL_RGW = ON\n```\n\n这一步，很重要，很多时候会因为没有装好的`librgw2-devel`或者`libcephfs1-devel`导致这边的`USE_FSAL_RGW`或者`USE_FSAL_CEPH`状态为`OFF`\n\n**确保，确保，确保：** `-- USE_FSAL_CEPH`为`ON`，以及`-- USE_FSAL_RGW`为`ON`。\n\n如果是`OFF`，请检查下`librgw2-devel`或者`libcephfs1-devel`是否有安装，如果这两个包都已经安装了，还显示为`OFF`， 可以尝试下清空编译目录：`rm -rf build/*`，再进行编译，如果依旧为 `OFF`，可以尝试下删除所有的`Ceph`包，再重新`yum install ceph librgw2-devel libcephfs1-devel -y`。\n\n编译和安装，在`build`目录下\n\n```\nmake\nmake install\n```\n\n> PS:\n> 在`make install`生成的输出中，可以看到:\n>\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/rgw.conf\n> ...\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/ceph.conf\n>\n> 这两个文件就是配置将`RGW`和`CephFS`配置为`ganesha-nfs`的配置模板。\n \n## 编辑配置文件\n\n注意`Path`后面的路径需要加引号，`rgw`替换成创建`S3`用户生成的用户信息\n\n```\ncat /etc/ganesha/ganesha.conf\nEXPORT\n{\n        Export_ID=1;\n        Path = \"/\";\n        Pseudo = /cephfs;\n        Access_Type = RW;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = CEPH;\n        }\n}\nEXPORT\n{\n        Export_ID=2;\n        Path = \"/\";\n        Pseudo = /rgw;\n        Access_Type = RW;\n        Squash = No_root_squash;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = RGW;\n                User_Id = \"admin\";\n                Access_Key_Id =\"1MWH3LWM1BS4ZF4HN5IH\";\n                Secret_Access_Key = \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\";\n        }\n}\nRGW {\n    ceph_conf = \"/etc/ceph/ceph.conf\";\n}\n```\n\n## 启动 Ganesha\n\n```\nganesha.nfsd -f /etc/ganesha/ganesha.conf -L /var/log/nfs-ganesha.log -N NIV_DEBUG\n```\n如果一切顺利，你应该可以看到`ganesha.nfsd` 进程在那，如果进程不在，那么查看`Log`，记得在启动进程前，关闭所有`CephX`配置。\n\n**重要**：`librgw init failed (-5)` 解决方法\n\n报错内容如下\n\n```\nRGW-1 : nfs-ganesha-2232083[main] create_export :FSAL :CRIT :RGW module: librgw init failed (-5)\nRGW-1 : nfs-ganesha-2232083[main] mdcache_fsal_create_export :FSAL :MAJ :Failed to call create_export on underlying FSAL\n\n```\n\n经过多次尝试，包括在`ganesha.conf`内添加`init_args`指定秘钥和`Ceph`的用户，`ganesha-nfs` 均无法启动，报的错如标题，解决方法就是关闭`CephX`，将`/etc/ceph/ceph.conf`内的三个 `cephx`改为`none`，然后重启`ceph-mon`，`ceph-osd`，`ceph-radosgw`，`ceph-mds` 进程，再启动`ganesha-nfs`，即可正常运行。\n\n**需要注意的是：**\n\n当你在当前节点上，关闭`cephx`后，你用`ceph -s`查看集群状态时，这时候会报错说明由于没有`auth`认证导致无法连接集群，所以我当时试了很久，尽管`nfs-ganesha`已经运行了，但是`mount`都没有成功`export`，查看`log`才发现原来在`init`配置的时候就失败了，导致这个问题的原因是我只关闭了一个节点的`cephx`，所以需要做的就是将集群内所有节点的`cephx`全部关闭，然后集群就可以正常访问了，并且`nfs-ganesha`也不会因为`cephx`导致加载配置出错\n\n## Check Export\n\n```\n[root@node1 build]# showmount -e  \nExport list for node1:  \n/ (everyone)  \n/ (everyone)  \n```\n\n## 挂载 NFS\n\n```\n[root@node1 mnt]# mount -t nfs4 192.168.1.1:/  /mnt/ceph/  \nroot@node1 mnt]# ls ceph/*  \nceph/cephfs:  \ntest  \n   \nceph/rgw:  \nmy-new-bucket \n```\n\n说明`CephFS`和`RGW`都已经正常对接。\n如果，你所使用的`admin`用户名下有很多的桶，那么这些桶都会以`/mnt/rgw/xxbucket`的结构显示出来，如果你在`/mnt/rgw/`下建立的一个目录，那么就相当于通过`RGW`建立了一个桶，所以，你执行`touch /mnt/rgw/123`是会报错的，因为不符合`S3`的对象必须位于桶内的规定，简单点说，就是把`/mnt/rgw/`和`S3`的根目录一一对应即可。\n\n同样，`CephFS`内的内容都会显示在`/mnt/cephfs/`目录下。可以开始愉快的玩耍了！！\n \n[1]: http://www.xuxiaopang.com/2017/03/27/ganesha-nfs-deploy/#more\n[2]: http://blog.csdn.net/younger_china/article/details/73432726\n[3]: https://github.com/nfs-ganesha/nfs-ganesha/commit/8d039cd139ea1dcfbe316b7e93e96efe755669ef\n[4]: https://github.com/nfs-ganesha/nfs-ganesha/releases\n[5]: https://github.com/nfs-ganesha/nfs-ganesha/releases/tag/V2.5.3\n[6]: https://github.com/nfs-ganesha/nfs-ganesha/tree/next/src\n[7]: http://ovv1r40we.bkt.clouddn.com/nfs-ganesha.tar.gz\n[8]: http://ow0mgad6r.bkt.clouddn.com/ganesha-600x450.png\n","tags":["Luminous"],"categories":["tech"]},{"title":"RGW 安装和创建","url":"/2017/11/08/Ceph-RGW/","content":"\n<center>![Ceph RGW][2]</center>\n\n本文通过`ceph-deploy`安装和创建`RGW`，然后分别创建`S3`和`Swift`接口并提供了相应的方案。\n\n`Ceph RGW`基于`librados`，是为应用提供`RESTful`类型的对象存储接口。`RGW`提供两种类型的接口：\n\n* S3：兼容`Amazon S3 RESTful API`\n* Swift：兼容`OpenStack Swift API`\n\t\n`S3`和`Swift API`共享同一个命名空间，所以可以使用两种`API`访问相同的数据。\n\n<!--more-->\n\n参考链接: [Ceph：创建RGW][1]\n## 部署 RGW\n\n### 进入 ceph 目录\n\n```\ncd /etc/ceph\n```\n\n### 安装 ceph object gateway\n\n我这边是`node1`，根据`hostname`对号入座就行\n\n```\nceph-deploy install --rgw node1 \n```\n\n### Gather keys\n\n```\nceph-deploy gatherkeys node1\n```\n\n### 创建 rgw 实例\n\n```\nceph-deploy rgw create node1\n```\n\n### Ceph CLI\n\n`Ceph CLI`工具需要在管理员模式下运行，因此需要执行以下命令\n\n```\nceph-deploy admin node1\n```\n\n### 测试是否安装成功\n\n一旦`RGW`开始运行，就可以通过端口`7480`（如果没有修改的话）来访问。如： \n`http://node1:7480`，如果`RGW`运行正常，它应该返回类似的信息： \n\n```\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\n<ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<Owner>\n<ID>anonymous</ID>\n<DisplayName/>\n</Owner>\n<Buckets/>\n</ListAllMyBucketsResult>\n```\n\n## 创建 S3 用户\n\n想正常的访问`RGW`，需要创建相应的`RGW`用户，并赋予相应的权限，`radosgw-admin`命令实现了这些功能。\n\n其中`keys`中的`user`，`access_key`和`secret_key`用于之后的`S3`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin user create --uid=\"admin\" --display-name=\"First user\"\n2017-11-08 16:51:39.883217 7fb6868fac40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 创建 Swift 用户\n\n`Swift`用户是作为子用户`subuser`被创建的，执行以下命令：\n\n其中`swift_keys`中的`user`和`secret_key`用于之后的`swift`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin subuser create --uid=admin --subuser=admin:swift --access=full\n2017-11-08 16:55:05.371174 7fb4cbfc2c40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [\n        {\n            \"id\": \"admin:swift\",\n            \"permissions\": \"full-control\"\n        }\n    ],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [\n        {\n            \"user\": \"admin:swift\",\n            \"secret_key\": \"PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P\"\n        }\n    ],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 测试 S3 接口\n\n需要创建一个`python`测试脚本来测试`S3`访问。该脚本会连接`RGW`，创建一个`bucket`并打印输出所有的`bucket`。其中，变量`access_key`和`secret_access`的值，来自于创建`S3`用户命令时，`radosgw-admin`命令返回的`keys->access_key`和`keys->secret_key`。\n\n### 安装 python-boto库\n\n执行以下步骤，首先安装`python-boto`库，该库用于连接`S3`：\n\n```\nyum install -y python-boto \n```\n\n### 创建脚本\n\n```\n#!/usr/bin/python  \n# -*- coding:utf-8 -*-  \n\nimport boto.s3.connection  \n   \naccess_key = '1MWH3LWM1BS4ZF4HN5IH'     \nsecret_key ='cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O'  \nconn = boto.connect_s3(  \n        aws_access_key_id=access_key,  \n        aws_secret_access_key=secret_key,  \n        host='{hostname}',port={port},  \n        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),  \n        )  \n   \nbucket = conn.create_bucket('my-new-bucket')  \nfor bucket in conn.get_all_buckets():  \n    print\"{name} {created}\".format(  \n            name=bucket.name,  \n            created=bucket.creation_date,  \n            )  \n   \n```\n\n需要将上面的`{hostname}`替换成对应的`hostname`或者`IP`,例如`192.168.1.1`；将`{port}`替换成`RGW`运行的端口，默认为`7480`；前者为字符串，后者为数字\n\n### 执行脚本测试\n\n```\n[root@node1 ceph]# python s3test.py \nmy-new-bucket 2017-11-01T10:58:53.670Z\n```\n\n## 测试 Swift 接口\n\n### 安装相关软件包\n\n```\npip installpython-setuptools    \npip installpython-swiftclient  \n```\n\n### 命令行访问\n\n替换`{ip}`，`{port}`和`{swift_secret_key}`，其中`{swift_secret_key}`为创建`Swift`用户时，`radosgw-admin`命令返回的`swift_keys->secret_key`的值\n\n```\nswift -A http://{ip}:{port}/auth/1.0 -Utestuser:swift -K '{swift_secret_key}' list \n```\n\n正常输出应该如下：\n\n```\n[root@node1 ceph]# swift -Ahttp://192.168.1.1:7480/auth/1.0 -U admin:swift -K 'PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P' list  \nmy-new-bucket \n```\n\n## 小结\n\n`ceph-deploy`命令，大大地简化了对`Ceph`集群和`RGW的`安装和配置，可以很快速的搭建测试环境，达到测试效果，但如果要在生产环境中应用的话，可能需要手动做一些配置就，不能完全依赖`ceph-deploy`了。\n\n[1]: http://blog.csdn.net/younger_china/article/details/73410918\n[2]: http://ow0mgad6r.bkt.clouddn.com/gateway-600x450.png\n","tags":["RGW"],"categories":["tech"]},{"title":"使用NFS挂载RBD","url":"/2017/10/31/RBD-Mount-NFS/","content":"\n<center>![RBD][3]</center>\n\n**具体场景**\n\n- 在`Server`端创建`RBD`块设备并挂载到某个目录，然后在`Client`端通过`NFS`将`Server`端之前说的目录再挂载到本地，就可以实现在客户端写文件通过`NFS`到`Server`端的块设备上\n- 这波操作的目的是因为测试`EC`性能时，`Ceph`版本还是`Hammer`，`CephFS`不是很稳定，性能不行，所以想用`RBD`的方式取代，但是`Hammer`版本中`EC`是只支持`RGW`的，并不支持`CephFS`和`RBD`，至于这个场景具体流程之后会在其他文章分享\n- 这波操作在之前`Ceph`的`Hammer`版本做过，这次是要在新版本`Luminous`上实践，由于遇到了很多问题，大部分都是新版本缘故引入的，故与大家分享\n\n> PS：`Luminous`新版本提供了新的`NFS`的处理方式，不再需要用这种老式的`RBD`的手段，之后也会在其他文章分享出来\n\n<!--more-->\n\n## Hammer版本\n\n操作步骤\n\n```\n1. 使用RBD方式导出NFS，步骤如下：\n\n\t1.1 create RBD block device image (size unit: MB)\n\t\t# rbd create --size 1024 ec-pool/test_image\n\t\t\n\t1.2 map RBD device\n\t\t# rbd map ec-pool/test_image\n\t\t\n\t\t[Optional] check mapped device\n\t\t# rbd showmapped\n\t\t\n\t1.3 create filesystem on the device\n\t\t# mkfs.ext4 /dev/rbd0\n\t\t\n\t1.4 mount it,\n\t\t# mkdir -p /vol/test_image\n\t\t# mount /dev/rbd0 /vol/test_image/\n\t\t\n\t1.5 export it, modify the /etc/exports\n\t\t# cat /etc/exports\n\t\t/vol/test_image *(rw,async,no_subtree_check,no_root_squash)\n\t\t# service nfs-kernel-server restart\n\t\t# exportfs -r\n\t\t\n\t1.6 check mount info for NFS server\n\t\t# showmount -e 192.168.1.167\n\t\n2. 在客户端以nfs方式mount上述导出的文件夹\n\t# mount -t nfs 192.168.1.167:/vol/test_image /mnt\n```\n\n## Luminous版本\n\n### Create RBD image\n\n之前`EC Pool`只能为`RGW`服务 ，直到`Luminous`版本，`EC Pool`也可以应用在`RBD`和`CephFS`，并且增加了`data-pool`这样的新特性，具体请参考官网：http://docs.ceph.com/docs/luminous/rados/operations/erasure-code/#erasure-coding-with-overwrites\n\n所以创建`image`的方式也有所改变\n\n#### Enable ec overwrites\n\n```\nceph osd pool set ec_pool allow_ec_overwrites true\n```\n\n#### Create pool and image\n\n由于`EC`不支持`omap`，所以在`CephFS`或者`RBD`场景使用`EC`时，需要将`data`存在`EC Pool`中，将`metadata`存在`Replicated Pool`中\n\n```\n// 创建pool\nceph osd pool create ec-pool 12 12 erasure  // ec pool, store data\nceph osd pool create md-pool 12 12 replicated   // replicated pool, store metadata\nceph osd pool create ssd-pool 12 12 replicated  // replicated pool, used as cache pool\n// 创建image\nrbd create --size 1024 --data-pool ec-pool md-pool/test_image\n```\n\n### Map RBD\n\n这一步之前的操作是`rbd map ec-pool/test_image`，所以当前需要执行\n\n```\n rbd map md-pool/test_image\n```\n\n但是一直`timeout`，通过查看`dmesg`和网上资料发现是因为`linux kernel`版本太低，所以升级了一下内核到`4.3`版本之后就可以`work`了\n\n升级内核版本请参考：\n> * [内核版本过低导致RBD Feature不支持][1]\n> * [RBD Feature][2]\n\n### Check map\n\n```\n rbd showmapped\n```\n\n### Create filesystem on RBD\n\n```\nmkfs.ext4 /dev/rbd0\n```\n\n### Mount\n\n```\n mkdir -p /vol/test_image\n mount /dev/rbd0 /vol/test_image/\n```\n\n### Modify export for nfs on server\n\n```\nvim /etc/exports\n/vol/test_image *(rw,async,no_subtree_check,no_root_squash) // Modify it to /etc/exports\n```\n\n#### Server\n\n##### Install NFS on server and client\n\n`Server`端执行一下操作\n\n##### Install\n\n```\nyum -y install nfs-utils rpcbind\n```\n\n##### Start service\n\n`nfs`依赖`rpcbind`，所以必须先启动`rpcbind`，这很重要\n\n```\nservice rpcbind start\nservice nfs start\nexportfs -r\n```\n\n#### Client\n\n客户端也执行以上操作，假设客户端`ip`为`192.168.1.1`，服务端为`192.168.1.2`\n\n##### Check mount\n\n查看是否可以挂载\n\n```\nshowmount -e 192.168.1.2\n```\n\n##### Mount server to client\n\n```\nmount -t nfs 192.168.1.2:/vol/test_image /mnt\n```\n\n## 总结\n\n遇到的主要问题\n\n* `EC Pool`创建`image`因为新版本改动\n* `rbd map`因为`linux kernel`版本低\n* `nfs`因为`nfs`在`rpcbind`前启动\n\n希望帮助大家尽量少些踩坑 ~~~\n\n作者： Tony\n日期： 2017-10-31 22:33\n\n[1]: http://www.jianshu.com/p/66a724a1f3af\n[2]: http://xiaqunfeng.cc/2017/06/06/ceph-rbd-map-failed/\n[3]: http://ovv1r40we.bkt.clouddn.com/rbd-london-christmas-600x450.jpg\n","tags":["RBD"],"categories":["tech"]},{"title":"Python Profiler","url":"/2017/10/10/Python-Profiler/","content":"\n<center>![Python Profiler][10]</center>\n\n`OSD`启用过程耗时较长，需要进行性能优化。期间通过`python profilers`对代码进行性能分析和数据统计，有坑，有收获，总而言之，这是一个不错的工具\n\n<!--more-->\n\n## Profilers简介\n\n`python profilers`内置的主要有三种`cprofile`, `profile`和`hotshot`,`cprofile`是基于`profile`之上做的扩展，性能要比后者好很多，所以我用的就是`cprofile`\n\n更详细的介绍可以查看官网，`python profilers`的好处在于不用看教程，只要看着官网简短的概述，就能掌握其使用方法\n\n* [Python Profile][1]\n* [关于Python Profilers性能分析器][2]\n\n\n## Cprofile快速使用\n\n### 官网例子\n\n#### 代码\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")')\n```\n\n#### 分析结果\n\n```\n197 function calls (192 primitive calls) in 0.002 seconds\nOrdered by: standard name\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1    0.000    0.000    0.001    0.001 <string>:1(<module>)\n     1    0.000    0.000    0.001    0.001 re.py:212(compile)\n     1    0.000    0.000    0.001    0.001 re.py:268(_compile)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)\n     4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)\n   3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)\n```\n\n#### 图解\n\n<center>![cprofile][3]</center>\n\n## Cprofile深入\n\n上面的基本用法可以在脚本中测试某个语句或者函数，然后打印到控制台。`cprofile`也可以将结果输出到文件中，这是比较常见的做法，因为打印到控制台，第一不能保存结果，第二如果数据量多没法全部浏览，并且影响阅读效果。而放在文件中还可以对结果进行按需排序、筛选等操作\n\n### 输出文件\n\n1. 参数方式\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")', 'restats')\n```\n\n2. CLI方式\n\n```shell\npython -m cProfile [-o output_file] [-s sort_order] myscript.py\n```\n\n## Cprofile优雅使用\n\n上述方式可以使得`cprofile`在一些测试环境中受用，但是在一些复杂的环境中不能很好的`work`。由于`cprofile`是根据`python`在每个事件中存放的`hook`进行性能分析，所以在`cprofile.run()`的时候，要保证他就是最上层，他是调用的源头。\n\n但是实际场景中，我们经常会对一些`API`中的某个方法进行性能分析，如果在被调用处使用`cprofile`，会出现变量或者模块`undefined`的现象，模块不能识别还可以在`run`方法中引入，然后通过分号分隔，例如`cprofile.run(import re, re.compile(\"foo|bar\"))`，具体可以参考这篇文章：[Python Profile 工具性能分析][5]\n\n变量无法识别更是让人头疼，所以为了达到测试效果，你会不得不修改一些并不是很少量的源代码，并且测一个方法就要搞一次很麻烦。还有一些多进程或者跨机器的场景导致代码异步执行，这样`cprofile`更不能达到用户的需求\n\n还好我们可以通过`python`装饰器的机制来做，这样既不用改动源代码，也可以很方便的切换函数分析\n\n### 装饰器接口\n\n这里要注意设置全局变量\n\n```shell\nexport PROFILING=y\n```\n\n接口定义：\n\n```python\nimport cProfile\nimport pstats\nimport os\n# 性能分析装饰器定义\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"tottime\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n### 分析使用\n\n这时候只需要在调用的函数上面加一个装饰器即可\n\n```python\n@do_cprofile('filename')\ndef run():\n    print 'hello world'\n```\n## pstats分析工具\n\n`pstats`可以根据`cprofile`生成的文件进行排序、筛选等处理，呈现更主要的结果\n\n```python\nimport pstats\n \n# 创建Stats对象\np = pstats.Stats(\"result.out\")\n \n# strip_dirs(): 去掉无关的路径信息\n# sort_stats(): 排序，支持的方式和上述的一致\n# print_stats(): 打印分析结果，可以指定打印前几行\n \n# 和直接运行cProfile.run(\"test()\")的结果是一样的\np.strip_dirs().sort_stats(-1).print_stats()\n \n# 按照函数名排序，只打印前3行函数的信息, 参数还可为小数,表示前百分之几的函数信息 \np.strip_dirs().sort_stats(\"name\").print_stats(3)\n \n# 按照运行时间和函数名进行排序\np.strip_dirs().sort_stats(\"cumulative\", \"name\").print_stats(0.5)\n \n# 如果想知道有哪些函数调用了sum_num\np.print_callers(0.5, \"sum_num\")\n \n# 查看test()函数中调用了哪些函数\np.print_callees(\"test\")\n```\n\n上述代码摘自：[使用cProfile分析Python程序性能][4]，原文还提供了`pstats`命令行交互工具方式\n\n## 图形可视化\n\n上面的命令行界面的确是有点反人类，不易一下子清晰地分析性能瓶颈，有很多图形可视化工具可以帮助我们生成简洁明了的图片\n\n工具有：\n\n* gprof2dot\n* vprof\n* RunSnakeRun\n* KCacheGrind & pyprof2calltree\n\n最终我选择了`gprof2dot`，比较符合我的口味\n\n### 安装\n\n我的机器是`ubuntu`，其他类型机器找对应方式，具体参考：[Github gprof2dot][6]\n\n```\napt-get install python graphviz\npip install gprof2dot\n```\n\n注意：\n\n如果`pip`安装软件包报错：'Cannot fetch index base URL  http://pypi.python.org/simple/'\n\n解决办法\n\n```\n1. windows下创建/%user%/pip/pop.ini，并添加以下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n2. linux创建文件~/.pip/pip.conf，并添加一下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n3. 再次使用pip安装相应的包即可。\n```\n\n### 使用\n\n根据`cpofile`输出的文件生成图片，这边输出的文件名为`osd.out`，生成的图片名为`osd.png`\n\n```shell\ngprof2dot -f pstats osd.out | dot -Tpng -o osd.png\n```\n\n具体参考这篇文章：[Python优化第一步: 性能分析实践][7]，写的很好，也很具体\n\n### 效果图\n\n这是我进行性能分析产生的两张图\n\nPicture 1：\n<center>![enable_osd1][8]</center>\n\nPicture 2：\n<center>![enable_osd2][9]</center>\n\n## 继续深入\n\n* `pstats`深入了解\n* `gprof2dot`深入了解\n* 其他的可视化工具\n\n不过最终的目的都是通过性能分析找到性能瓶颈，然后进行优化，适合自己的就好\n\n[1]: https://docs.python.org/2/library/profile.html\n[2]: http://www.cnblogs.com/btchenguang/archive/2012/02/03/2337112.html\n[3]: http://images.cnblogs.com/cnblogs_com/btchenguang/201202/201202031502512968.png\n[4]: http://xianglong.me/article/analysis-python-application-performance-using-cProfile/\n[5]: http://ju.outofmemory.cn/entry/46805\n[6]: https://github.com/jrfonseca/gprof2dot\n[7]: https://zhuanlan.zhihu.com/p/24495603/\n[8]: http://ovv1r40we.bkt.clouddn.com/enable_osd3.png\n[9]: http://ovv1r40we.bkt.clouddn.com/enable_osd3-2.png\n[10]: http://ovv1r40we.bkt.clouddn.com/publicenemyno1-600x400.png\n","tags":["性能优化"],"categories":["tech"]},{"title":"启用OSD性能剖析脚本","url":"/2017/10/10/Enable-OSD-Profile-Script/","content":"\n\n<center>![Profile][1]</center>\n\n针对`OSD`启用过程缓慢进行性能剖析，用到了一系列的脚本，例如批量创建`OSD`,批量启用`OSD`,`Cprofile`装饰器接口等\n\n<!--more-->\n\n`docprofile.py`：提供`cprofiler`的装饰器接口\n\n```python\nimport cProfile\nimport pstats\nimport os\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            print filename\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"cum\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n`pstat.py`：用于处理生成的`cprofiler`文件，做一些排序，截取和打印等工作,默认按`culativetime`排序，打印前`100`条记录，有其他需求的可以适当修改参数\n\n```python\nimport pstats, sys\nfile = sys.argv[1]\np = pstats.Stats(file)\np.strip_dirs().sort_stats('cum').print_stats(100)\n```\n\n[1]: http://ovv1r40we.bkt.clouddn.com/pozza-600x450.png\n","tags":["Profile"],"categories":["tech"]},{"title":"批量创建和删除磁盘分区脚本","url":"/2017/10/02/Batch-Create-And-Delete-Disk-Partition-Script/","content":"\n<center>![Disk Partition][1]</center>\n\n最近在做批量启用`OSD`的性能优化，需要反复的删除磁盘分区，以保证彻底的删除数据，然后就要批量创建分区，保证足够量的分区数量创建足够多的`OSD`，由于数量较多，每次手动搞都要二十几次，所以针对创建和删除写了两个脚本，支持单一、批量和选择大小等功能\n\n<!--more-->\n\n## 清空分区\n\n### 脚本名称 \n\n`clean_disks.py`\n\n### 使用说明\n\n* `python clean_disks.py [disk_name1, disk_name2 ...]` \n* 支持单磁盘和多磁盘清空\n* 支持脚本参数，如果没有参数，默认清空所有磁盘分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会清空系统磁盘，脚本会做过滤工作\n\n### 使用范例\n\n* 清空所有磁盘分区\n\n```\npython clean_disks.py\n```\n\n* 清空`sdb`的分区\n\n```\npython clean_disks.py sdb\n```\n\n* 清空`sdb`和`sdc`的分区\n\n```\npython clean_disks.py sdb sdc\n```\n\n### 脚本代码\n\n> `do_exec`是我自己写一个模块，可以调用`shell`命令，所以用的时候需要转换一下\n\n```\nimport sys\nimport do_exec\n\ndef clean_disk(disk_name):\n    print 'disk: {} clean start ...'.format(disk_name)\n    do_exec('sgdisk -Zog /dev/{}'.format(disk_name))\n    print 'disk: {} clean done ...'.format(disk_name)\n\nmount_info = do_exec('mount')\nsys_disk_name = mount_info[5:8]\n\nif len(sys.argv) > 1:\n    disks = sys.argv[1:]\n    for disk_name in disks:\n        if disk_name == sys_disk_name:\n            print '{} is system disk, can\\'t be clean!'.format(disk_name)\n        else:\n            clean_disk(disk_name)\nelse:\n    all_disks = do_exec('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                clean_disk(disk_name)\n```\n\n## 创建分区\n\n### 脚本名称\n\n`create_partitions.py`\n\n### 使用说明\n\n* `python create_partitions.py size num [disk_name1, disk_name2 ...]`, `size`为分区大小，`num`为分区数\n* 支持单磁盘和多磁盘创建分区\n* 支持脚本参数，如果没有参数，默认为所有磁盘创建分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会为系统磁盘创建磁盘分区，脚本会做过滤工作\n\n### 使用范例\n\n* 为`sdb`和`sdc`分别都创建两个大小为`100G`的分区\n\n```\npython create_partitions.py 100G 2 sdb sdc \n```\n\n* 为所有磁盘创建三个大小为`10G`的分区\n\n```\npython create_partitions.py 10G 3\n```\n\n* 为`sdc`创建四个大小为`1T`的分区\n\n```\npython create_partitions.py 1024G 4 sdc\n```\n\n### 脚本代码\n\n```\nimport sys\nimport do_exec\n\nsize = sys.argv[1]\nnum = sys.argv[2]\nmount_info = do_exec('mount')\nsys_disk_name = mount_info[5:8]\n\ndef parted_disks(num, size, disk):\n    for i in range(int(num)):\n        do_exec('sgdisk -n {}:0:+{} /dev/{}'.format(i+1, size, disk))\n        print 'disk {} partition {} done ...'.format(disk, i+1)\n\nif len(sys.argv) > 3:\n    disks = sys.argv[3:]\n    for disk in disks:\n        if disk == sys_disk_name:\n            print '{} is system disk, can\\'t be parted!'.format(disk)\n        else:\n            parted_disks(num, size, disk)\nelse:\n    all_disks = do_exec('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                parted_disks(num, size, disk)\n```\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/floppy_disk_600-450.png\n","tags":["Script"],"categories":["tech"]},{"title":"JS判断对象是否为空","url":"/2017/09/28/JS-Object-Null/","content":"\n<center>![JS][3]</center>\n\n今天在改代码的时候中发现一个`Bug`，通过`debug`发现最终的原因便是判断对象是否为空导致的错误，一个小小的错误反映出`Javascript`这门语言的不严谨性，也提醒我们得熟悉它的一些技巧和原理，避免经常在一些问题上踩坑\n\n<!--more-->\n\n场景还原：通过`if(object)`判断对象内容不为空，\n\n```\nobj = {};   // obj其实是通过其他方式获得，但是它的内容为空\nif (obj) {\n        xxx;    // 判断当obj不为空的时候，执行判断内的代码\n}\n```\n\n那么问题就出现了，即使对象内容为空，这个判断代码块里面的代码还是会被执行\n用`chrome`简单的验证了一下：\n\n<center>![object empty][1]</center>\n\n这和一些其他语言不一样，比如`php`,`python`是可以通过`if(object)`来检查对象是否为空的，但是`JS`不可以，这个问题要不是遇到了肯定想不到\n\n那么`JS`如何判断对象内容是否为空呢？网上搜了下，大概有以下方式：\n\n### 通过JSON.stringify将对象转换成字符串进行判断\n\n```\nvar tt = {};\nif (JSON.stringify(tt) == \"{}\") {\n        console.log('tt is not empty!');\n}\n```\n\n### 通过读取对象元素\n\n```\nvar tt = {};\nvar t = {id: 1};\nif (tt.id) {\n        console.log('tt is not empty!');\n} \nif (t.id) {\n        console.log('t is not empty');\n}\n```\n\n### 总结\n\n方法1相对于方法2来说更加严谨，阅读性高，而方法2的效率却比方法1高\n\n### jQuery提供了判断对象是否为空的API\n\n如果用的是`jquery`的话，可以使用其提供的现有方法：`isEmptyObject`,这是`jQuery1.4`版本后加入的功能，具体用法：\n\n```\nvar tt = {}; \nif ($.isEmptyObject(tt)) {  // jQuery.isEmptyObject(tt)这样调用也可以\n        console.log('tt is empty!');\n}\n```\n\n### 判断对象是否存在\n\n借助上面的问题，顺便了解了一下：[如何判断Javascript对象是否存在][2]\n\n这篇文章里面讲的还是比较全面的，针对不同场景给出了不同的方案\n\n\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/object_empty.png\n[2]: http://www.ruanyifeng.com/blog/2011/05/how_to_judge_the_existence_of_a_global_object_in_javascript.html\n[3]: http://ow0mgad6r.bkt.clouddn.com/jsbanana-600-450.png\n","tags":["Javascript"],"categories":["tech"]},{"title":"OSD创建和删除全过程","url":"/2017/09/27/OSD-Create-And-Delete/","content":"\n<center>![create and delete process][1]</center>\n\n`OSD`是`Ceph`中最基本也是最常用的功能，所以经常的创建和删除操作少不了，然后这个两个过程并不是一两个命令那么简单，本文就`OSD`的创建和删除进行内容进行步骤分离和讲解，结尾还有一份一键删除指定`OSD`的脚本\n\n<!--more-->\n\n## 创建过程\n\n### 创建一个OSD\n\n```\nceph osd create [uuid]   #若没有uuid参数，则该命令自动生成一个uuid。该命令产生一个新的osd-number\n```\n\n### 为新的OSD创建默认的工作目录\n\n```\nmkdir -p /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 为新的OSD准备一块磁盘\n\n```\nmkfs.xfs -f /dev/vde \nmount /dev/vde /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 初始化新的OSD工作目录\n\n```\nceph-osd -i {osd-number} --mkfs --mkkey\n```\n\n>  注意：在执行上述命令前要求新的OSD工作目录必须为空\n\n### 注册新的OSD认证密钥\n\n```\nceph auth add osd.{osd-number} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-number}/keyring\n```\n\n### 将新的OSD添加到Crush Map中\n\n```\nceph osd crush add osd.{osd-number} {weight} [{bucketype}={bucket-name}......}\n```\n\n此步骤也可以添加buckets后再添加osd，即：\n\n```\nceph osd crush add-bucket node5 host     #创建一个名字为node5的bucket\nceph osd crush move node5 root=default    #将创建出来的bucket放到root下\nceph osd crush create-or-move osd.{osd-number} 1.0 root=default host=node5     #将新的OSD添加到node5下\n```\n\n### 将新的OSD信息填写到ceph.conf中\n\n```\n[osd.{osd-number}]\nhost = {hostname}\ndevs = /dev/vde\n```\n\n### 启动新的OSD\n\n```\n/etc/init.d/ceph start osd.{osd-number}\n```\n\n> 此时通过集群状态查看命令#ceph -s可以看到OSD数量以及up和in的数量都发生了变化，此时再通过命令#ceph -w可以看到ceph经过peering状态后，最终达到active+clean状态\n\n## 一键创建OSD脚本\n\n```\n#! /bin/bash\n\nstart_time=`date +%s`\n\necho \"start time: `date -d @$start_time \"+%Y-%m-%d %H:%M:%S\"`\"\n\ndisk=/dev/$1\n\nosd_id=`ceph osd create`\n\nosd_dir=/data/osd.$osd_id\n\nhost=10.16.100.99\n\nbucket=default_$host\n\necho \"osd $osd_id is created ...\"\n\nmkdir -p $osd_dir\n\necho \"osd directory: /data/osd.$osd_id is created ...\"\n\nmkfs -t ext4 -m 0 $disk\n\necho \"disk $disk is built with ext4 file system ...\"\n\nmount -o noatime,user_xattr $disk $osd_dir\n\necho \"device: $disk is mounted on directory: $osd_dir ...\"\n\nceph mon getmap -o /tmp/monmap\n\nceph-osd -i $osd_id --monmap /tmp/monmap --mkfs --mkjournal\n\necho \"osd $osd_id is initialized ...\"\n\nosd_uuid=`ceph-osd -i $osd_id --get-osd-fsid`\n\ncat >> /etc/ceph/ceph.conf <<EOF\n\n[osd.$osd_id]\nhost = $host\npublic addr = $host\ncluster addr = $host\nosd uuid = $osd_uuid\nEOF\n\necho 'ceph config file is configured ...'\n\nservice ceph start osd.$osd_id\n\necho \"osd $osd_id start ...\"\n\nceph osd crush add $osd_id 0 pool=default host=$bucket\n\necho \"osd $osd_id is added in crush ...\"\n\necho 'all works done ...'\n\nend_time=`date +%s`\n\necho \"end time: `date -d @$end_time \"+%Y-%m-%d %H:%M:%S\"`\"\n\ntime_consuming=$(($end_time - $start_time))\n\necho \"The total time consuming is $time_consuming s\"\n```\n\n## 删除过程\n\n### 将需要删除的OSD在集群中的状态修改为out状态\n\n假定`osd`的`id`为`1`\n\n```\nceph osd out osd.1\n```\n\n### 停止OSD进程\n\n```\n/etc/init.d/ceph stop osd.1\n```\n\n### 将OSD从crush map中删除\n\n```\nceph osd crush remove osd.1    #删除指定的OSD\nceph osd crush remove node1    #删除OSD所在的bucket（此步骤可以不做）\n```\n\n### 清除到OSD的认证密钥\n\n```\nceph auth del osd.1\n```\n\n### 在OSD Map中清除OSD\n\n```\nceph osd rm 1\n```\n\n### 从ceph.conf中清除OSD\n\n```\n[osd.1]\n    host = {hostname}\n    ...\n```\n\n## 一键删除OSD脚本\n\n```\n#! /bin/bash\n\nosd_id=$1\nceph osd out osd.$osd_id\n/etc/init.d/ceph stop osd.$osd_id\nceph osd crush remove osd.$osd_id\nceph auth del osd.$osd_id\nceph osd rm $osd_id\n# 清空 ceph.conf\n```\n\n参考链接：\n> https://my.oschina.net/linuxhunter/blog/539312\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/5ways-facebook-600-450.png\n","tags":["OSD"],"categories":["tech"]},{"title":"如何打造一个好的产品","url":"/2017/09/21/How-To-Forge-Product/","content":"\n<center>![product][1]</center>\n\n今天`BOSS`带来一个曾经的得力干将。目前是腾讯一家游戏公司的老总，职位为`Producer`，在产品方面分享很多观点，受益匪浅。下面是分享过程中我总结的一些自己觉得正确的观点和心得\n\n> 虽热目前我是一个只负责敲代码的程序猿，产品这种事情好像跟我没多大关系，应该是产品经理的事情，但是其实一个`coder`想要进步，不仅仅是技术水平越来越强，对产品的理解也非常重要。这能理清自己逻辑，能够发散思维，能够帮你做出更好的产品，你开发出一个没人用的产品和一个上线客户量很大的产品对你自己来说无论是成就感还是技术提升都是天壤之别。也许未来我们可能也会创业，那到时候我们的职责可能是老板，可能是产品，可能是研发，更可能是各种职能集一身，对产品拥有良好理解和恰当的打磨技巧对我们来说都是宝贵的财富。\n\n<!--more-->\n\n## 技能心态\n\n### 工作强度 \n\n`BAT`加班不比任何一家中小公司少，腾讯一直把自己当做创业型公司，时刻保持压力和紧张感，这或许是其成功的一个很重要的原因（意料之外，之前没想到`BAT`工作强度这么大）\n\n### 不用过多的Design \n\n因为没有一个产品可以凭借设计文档一步到位的，几乎每个成功的产品都和原来的设计有着天壤之别（仁者见仁，智者见智，把握一个度吧，没必要死扣设计为了最完美化而浪费大量的时间）\n\n### 没有具体的Deadline \n\n每天都是`Deadline`，每天规定的任务做好了再回去\n\n### 谁的声音大听谁的 \n\n不要在内部为了争执一个方案纠结和浪费太多时间，谁的声音大听谁的，因为这绝不是最后一次修改（有性格）\n\n### 产品是每个人的\n\n一个人不应该被他的职位所局限，产品团队中的每个人都应该为这个产品在设计、宣传和包装等方面提意见\n\n### Just Do It \n\n很多事情自己不做一下是不知道的，很多行为看起来很low，不符合你的职位，但是只有真正去做了，才代表走过这条路，你才知道这条路可不可行，还有哪些其他路可以尝试（那位大佬制作某款卡牌游戏时，自己找了同类`50`多种游戏都玩了一遍，而且都玩到了很`6`的地步，用他的话说最后玩到吐。。。）\n\n### 相信自己的潜力 \n\n永远不要觉得自己是做技术的，所以理所当然`UI`不擅长，宣传不擅长，包装不擅长，做一行就要把自己的头埋进去，逼着自己不断尝试，慢慢可能会发现自己做的不错（他们经常会亲自去`QQ`群、贴吧和其他社交平台宣传，我去，托这种事原来真的存在）\n\n### 学会成熟 \n\n对很多事情都要慢慢做到心里有底，比如清楚上线必有`Bug`，设计必会被颠覆（他说上线没问题他会很慌。。。）\n\n### 明确最终目的 \n\n要明确做产品，开发等工作的目的不是按时完成任务，而是要把产品做起来，最终目的是赚钱，这样你做其他类型的事情就可以想明白很多，有时候开发并不一定是最优先最重要的，植物，但是作为一个开发经历了这些事情你的思维会不一样，做事方向也会不一样\n\n### 观察用户习惯\n\n可以尝试花钱请人来使用产品，内部开发人员在背后观察它的使用习惯，不轻易提示\n\n### 用数据说话\n\n当产品上线后，通过数据采集和分析，哪个页面或者操作用户比较感兴趣，哪一步开始用户量变少，出现这种情况，可以尝试做几个版本，然后让用户使用，选择最优的方案（在这个环节不以职位高低说话，完全靠数据评断）\n\n### 要相信自己的技术\n\n相信自己可以实现用户提的所有需求，当你全身心投入一件事的时候，手段很多，解决的方案也很多，比如这位大佬述说他收集一款游戏的同类游戏的经历，收集渠道除了iOS的还有android的，还有韩国市场的，还有模拟器的，还有页游等等（只有想不到，没有做不到）\n\n### 抄袭不丢脸\n\n互联网产品从`0`到`1`很少很少，大家在做一个产品之前都会看看同类产品怎么做的，然后学习每个产品中的精华部分，这个说直接一点就是抄袭，但是不要因为这个觉得丢脸，因为抄袭容易，抄袭得好并不容易（谈到腾讯，大家都会因为**抄袭**而骂声不绝，但是静下想想这的确是个正常套路，一切为了快速出产品）\n\n### 不要害怕被告\n\n等哪天你某些方面抄袭别人的，导致被告，那么说明你已经做大了，产品在某个方面来说已经相当成功了\n\n### 被骂是极好的\n\n很多时候，用户会因为产品一些蹩脚的地方而骂声不断，这时候不要觉得丢脸或者懊恼，相反应该开心，因为很多时候别人愿意骂你说明很大程度上还是很喜欢这个产品的，而真正不喜欢的懒得留下评论\n\n## 产品本身\n\n### 快速出Demo\n\n以上说的不用过多`Design`也好，还是不要过分争执也好，都是为了尽快做出一个产品`Demo`，就是最起码可以一个`work`的东西，这很重要，一旦`work`之后很多想法就会不一样了，他们往往两个星期出一个`Demo`，这对他们`UI`来说很有挑战，因为一般设计就要一个星期，然后原型图又是一个星期，但是两个星期就得弄出来，不管是借鉴别人的也好，还是网上买模板也好\n\n### 不用太完善 \n\n快速做出来不是说实现所有功能和面面俱到，而是可以做到静态页面或者是设计图去找用户，询问他们的观点和建议，这样既可以缩短时间，也可以通过询问客户的意见走向正确的道路，避免不必要的返工\n\n### 尽快让产品上线\n\n产品上线和不上线是天壤之别，因为自己内部永远是在想当然，而客户的反馈才是最重要的，这样才会往正确的方向不断迭代\n\n### 永远不要给用户看到错误信息\n\n能解决的让系统自己解决，不能解决的人工处理，不要太**实诚**，否则用户会感觉到惊恐\n\n### 不要给用户太多选择 \n\n如果一个页面存在两个按钮让用户点击，`80%`的用户会点`A`按钮，`20%`的用户会点`B`按钮，那么页面就显示一个`A`按钮就好了，只要保证`B`按钮能够在页面其他地方能找到就可以了，太多选择会让用户心里没底自己选择的对不对（相信大家脑海里都会浮现某个用户做完一个操作询问对不对的场景吧？）\n\n### 数字必有颜色\n\n这是一个约定俗成的习惯，保证页面的数字有合适的颜色，让用户一眼就能看到最想呈现的元素\n\n### 科技感很重要\n\n要让页面充满科技感，让别人觉得这是一家充满互联网气息的新公司，第一印象好\n\n### 恰当的图标\n\n图标选择要有意义，凸出按钮的作用\n\n### 恰当的名词\n\n用词准备具体，不要让用户去猜，比如下载就是下载，不要说保存\n\n### 易懂高大上的名词\n\n用词不要太专业，这样会让用户不明白，可以高大上一些，比如远程数据复制可以叫云复制\n\n### 抓住用户的期望\n\n产品介绍要突出用户最期望的东西，而不是过分强调技术上面的高档，这不是客户最期望的东西，比如存储你与其说内部实现机制怎么分片，怎么自动控制，不如和用户说我能保证多少数据量的存储，能保证永远不会因为故障丢失数据，能存储多长时间等等\n\n### 调节气氛的话语\n多用网络诙谐的话语进行提示，比如亲，朕等等，这样会舒畅用户的心情\n\n\n作者： Tony\n日期： 2017/09/19\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/product-600-450.jpg\n","tags":["产品"],"categories":["summary"]},{"title":"PHP-Excel 开发解决方案","url":"/2017/09/18/PHP-Excel/","content":"\n<center>![office][1]</center>\n\n 过去工作中使用`PHPExcel`较多，碰到并解决了各种大大小小的问题，总结出这样一篇文章，一方面记录自己踩过的坑，一方面与大家分享，让大家少走弯路，并不断完善之，欢迎大家去`github`上面`star`和提交`pull request`，不断补充和优化，打造最全面的`PHPExcel`开发解决方案\n\nGithub 地址：https://github.com/tony-yin/PHPExcel_Road\n原文地址：https://tony-yin.github.io/2017/09/18/PHP-Excel/\n\n<!--more-->\n\n## 基础：小试牛刀\n\n### 1. 引用文件\n\n```\nyourpath . /phpexcel/PHPExcel.php\n```\n\n### 2. 实例化phpexcel类\n\n```\n$excel = new PHPExcel();\n```\n\n### 3. 获取当前单sheet（多sheet会在下面讲）\n\n```\n$objexcel = $excel->getActiveSheet();\n```\n\n### 4. 合并单元格\n\n```\n$objexcel->mergeCells('A1:M1');\n```\n\n### 5. 获取一个cell的样式\n\n```\n$objexcel->getStyle('A1');\n```\n\n+ 获取一个cell的字体样式\n\n```\n$cellFont = $objexcel->getStyle('A1')->getFont();\n```\n+ 设置字体大小\n\n```\n$fontStyle->setSize(15);\n```\n+ 设置字体是否加粗\n\n```\n$fontStyle->setBold(true);\n```\n+ 设置字体颜色\n\n```\n$fontStyle->getColor()->setARGB(PHPExcel_Style_Color::COLOR_RED);\n```\n+ 获取一行样式\n\n```\n$rowStyle = $objexcel->getStyle(1)->getRowDimension();\n```\n+ 设置行高度\n\n```\n$rowStyle->setRowHeight(2);\n```\n+ 获取一列样式\n\n```\n$columnStyle = $objexcel->getStyle('A')->getColumnDimension();\n```\n+ 设置列宽度\n\n```\n$columnStyle->setWidth(10);\n```\n+ 获取一列对齐样式\n\n```\n$alignStyle = $objexcel->getStyle('A')->getAlignment();\n```\n+ 设置水平居中：同一水平线上居中，即为左右的中间\n\n```\n$alignStyle>setHorizontal(PHPExcel_Style_Alignment::HORIZONTAL_CENTER);\n```\n+ 设置垂直居中：同一垂直线居中，即为上下的中间\n\n```\n$alignStyle->setVertical(PHPExcel_Style_Alignment::VERTICAL_CENTER);\n```\n+ 自动换行\n\n```\n$$alignStyle->setWrapText(true);\n```\n\n### 6. 获取指定版本excel写对象\n如需更早的版本可将`Excel2007`换成`Excel5`\n```\n$write = PHPExcel_IOFactory::createWriter(\"xcel, 'Excel2007');\n```\n\n## 进阶：一些有用的小知识\n\n### 1.行列数字索引方法\n> `phpexcel`一般获取`cell`或者获取列都是通过`ABC`这样的英文字母获取的，它也可以通过`0、1、2、3`这样的数字表示sheet中的列，从`0`开始，`0`对应`A`，`1`对应`B`，基本上大多数方法都是数字行列索引，例如`getStyleByColumnAndRow($col,$row)`,默认列参数在前，行参数在后，更多的可以参加`phpexcel`源码；\n\n### 2. 单行或单列参数格式\n> 有的时候一个方法需要行列两个参数，例如只需要某一行参数可写成`(null, $row)`,例如只需要获得某一列参数可写成`($col, null)`\n\n### 3. 列的数字索引格式和字母索引格式互转\n\n+ 数字转字符串\n\n```php\nPHPExcel_Cell::columnIndexFromString('A');  // Return 1 not 0;\n```\n+ 字符串转数字\n\n```php\nPHPExcel_Cell::stringFromColumnIndex(0);    // Return 'A';\n```\n\n### 4.PHPExcel读取数字类型 \n> `PHPExcel`读取的`cell`数字，类型都是`double`型，可用`gettyle()`方法检测类型，当初我一直使用`is_int()`方法无果，搞得焦头烂额。。。\n\n### 5. 多cell边框线设置\n`PHPExcel`生成的表格如果你不加处理，是不会帮你生成边框线的，生成边框线的方法如下：\n```php\n$borderArray = array(\n    'borders' => array(\n        'allborders' => array(\n            'style' => PHPExcel_Style_Border::BORDER_THIN\n        )\n    )\n);\n$objexcel->getStyle($col1, $row1,$col2, $row2)->applyFromArray($borderArray);\n```\n>注：\n> 1: `getStyle()`可以看需求改为`getStyleByColumnAndRow()`方法通过数字行列索引读取`style`\n> 2: array中`PHPExcel_Style_Border::`后面有三种格式分别是`BORDER_THIN`和`BORDR_MEDIUM`，表示边框线的粗细；\n> 3: `getStyle()`中的索引可以是静态的，也可以是动态的，一般是在导出`excel`的数据`set`完毕后填写左上角的单元格行列索引和右下角的单元格行列索引；\n\n> 参考资料\n>\n> http://phpexcel.codeplex.com/workitem/22160\n> http://phpexcel.codeplex.com/workitem/20150\n\n### 6. 多cell字体加粗处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(  \n        'bold' => true,                                              \n    ),                                                               \n));\n```\n\n### 7. 多cell字体颜色处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(\n        'color' => array(\n            'rgb' => 'ff0000',\n            ),\n        ),\n    )\n);\n```\n\n### 8. 多sheet导入\n\n动态为当前`sheet`设置索引，然后获取当前`sheet`，便可循环读取每一个`sheet`内容\n\n```php\n$objexcel->setActiveSheetIndex($index);   //$index = 0 1 2 3\n$objexcel->getActiveSheet();    //return sheet1 sheet2 sheet 3\n```\n\n### 9. 固定格式excel读取在写入\n\n> 当需求是给定一个一个模板`excel`，需要往里面塞数据，我们不一定要通过代码给它设定样式，如果这个模板变化不大，我们完全可以存放一个格式相同的静态文件，然后通过`PHPExcel`读取，再往里面塞数据，最后进行保存操作，可以达到一样的效果，并且可以节省大量的资源。\n\n### 10. 合并单元格导入问题\n\n> 在特殊的表格中，合并单元格普遍存在，而多个单元格合并成的一个单元格，只能`setValue()`一次，而我们如何判断合并单元格的具体行列呢？\n\n```php\n$range = $start_cell->getMergeRange();  // 通过合并单元格的开始单元格比如‘A1’，获取合并范围‘A1:A4’\n$cell->isInRange($range);    // 遍历之后每一个单元格便可通过isInRange()方法判断当前单元格是否在合并范围内\n```\n## 高级：特殊场景特殊手段\n\n### 1. 单元格文本格式数据处理 \n> 一般excel单元格中数据的格式为数据类型，而`PHPExcel`中的`getValue()`方法读取的也是数据类型，当把数据从数据类型改为文本类型后，在`PHPExcel`中读出来的是`PHPExcel_RichText`类型，`getValue()`读取返回`PHPExcel_RichText`是一个`object`类型（`PHPExcel_RichText`数据保存格式）；那如何读取这一类的数据呢？仔细查看读取出来的对象，不难发现有`getPlainText()`这样的方法可以读取文本类型数据，所以我们只要判断当当前数据为文本数据时用`getPlainText()`读取，一般数据用`getValue()`读取\n\n```php\nif ($cell->getValue() instanceof PHPExcel_RichText) {\n    $value = $cell->getValue();\n} else {\n    $value = $cell->getValue();\n}\n```\n> 参考资料\n>\n> http://www.cnblogs.com/DS-CzY/p/4955655.html\n> http://phpexcel.codeplex.com/discussions/34513\n\n### 2. 单元格数据算法处理\n> `excel`拥有强大的算法功能，一般算法格式为`=A3+A4`这类的，复杂的更多，如果使用`PHPExcel`提供的默认读取方法`getValue()`读取出来的结果则为字符串`=A3+A4`,好在`PHPExcel`也足够强大，提供了相应的接口：`getCalculatedValue()`，这个方法专门读取算法数据，但是我们不能将这个方法作为默认读取方法，因为这样可能会将一些本来要读成字符串的读成算法数据，而且`PHPExcel`没有将它作为默认读取方法的另一个重要原因就是算法方式读取很耗时间和性能，一般数据读取根本没有必要这样浪费资源，所以我们可以采用以下这种方式\n\n```php\nif (strstr($cell->getValue(), '=')) {   \n    // 判断如果cell内容以=号开头便默认为算法数据\n    $value = $cell->getCalculatedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 3. 日期数据处理\n\n> 除了以上所说的文本数据和算法数据外，我还遇到过日期类型数据，比如`2016-12-28`输入到`excel`中，它会默认转换成`2016/12/28`，如果采用一般的`getValue()`方式读取也会读取到错误的数据，`PHPExcel`也提供了相应的接口`getFormattedValue()`,并提供了适配的识别方式`PHPExcel_Shared_Date::isDateTime($cell)`,所以代码就很好实现了\n\n```php\nif (PHPExcel_Shared_Date::isDateTime($cell)) {\n    $value = $cell->getFormattedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 4. 读取方法封装\n> 针对`excel`各种数据类型，我们可以写一个函数，将原有的`getValue()`封装一下，这样以后就不用每次都判别一下数据类型了，目前我只遇到上面三种特殊格式，如果有新的，欢迎大家补充，封装函数如下\n\n```php\nfunction get_value_of_cell($cell) {\n    if (strstr($cell->getValue(), '=')) {   \n        $value = $cell->getCalculatedValue(); \n    } else if ($cell->getValue() instanceof PHPExcel_RichText) {\n        $value = $cell->getValue();\n    } else if (PHPExcel_Shared_Date::isDateTime($cell)) {\n        $value = $cell->getFormattedValue(); \n    } else {\n        $value = $cell->getValue();\n    }\n}\n```\n\n### 5. 导出文件在IE、360等浏览器中文件名中文乱码问题\n\n```php\n$filename = 'xxx导出表';\n// 判断如果是IE内核形式的浏览器采用urlencode处理文件名\nif (!preg_match(\"/Firefox/\", $_SERVER[\"HTTP_USER_AGENT\"])) {\n    $filename = urlencode($filename);\n}\n```\n\n> 未完待续。。。\n> 期待你的补充和优化\n\n[1]: http://ow0mgad6r.bkt.clouddn.com/office-card-game.png\n","tags":["PHPExcel"],"categories":["tech"]},{"title":"磁盘反复拔插记录","url":"/2017/09/10/Disk-pull-and-plug-record/","content":"\n<center>![pull disk][10]</center>\n\n最近在做磁盘拔插后的`OSD`热恢复功能。在反复拔插盘的实践过程中，遇到了一个难题，就是时常会由于拔盘，然后再插盘后盘符发生了改变，`kernel`和`udev`接收不到任何`device`增加或删除的事件消息，\n由于我是基于`udev`监测磁盘的插拔，所以一旦感知不到的话，就无法进行相应的恢复操作，所以在此对拔盘的记录进行一些记录，并在大量操作中得出总结。\n\n<!--more-->\n\n## Operation Record\n\n`normal`: 表示正常情况，没有被拔\n\nenv | udev monitor | UI data | disk alias | device | lsblk | mount info | mount point \n:--:|:----:|:----:|:---:|:---:|:---:|:---:|:---:|\nnormal | - | /dev/sdg1 | osd4-data | /dev/sdg | exist | exist | /data/osd.1\nfirst pull | remove sdg | empty | remove | remove | remove | exist | exist, readerror\nfirst plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\nsecond pull | no remove | empty | osd4-data | /dev/sdd | not change | exist | exist, readerror\nsecond plug | All Not Change | - | - | - | - | - | - \nclear mount | - | - | exist | - | exist | remove | change style\nthird pull | remove sdd | empty | remove | remove | remove | remove | change style\nthird plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\n\n## Before pull\n<center>![before pull][1]</center>\n\n## First pull\n<center>![First pull][2]</center>\n\n## Read error\n<center>![Read error][9]</center>\n\n## First plug\n<center>![First plug][3]</center>\n\n## Second pull\n<center>![Second pull][4]</center>\n\n## Second plug\n<center>![Second plug][5]</center>\n\n## Clear mount info\n<center>![Clear mount info][6]</center>\n\n## Third plug udev monitor\n<center>![Third pull][7]</center>\n\n## Third plug \n<center>![Third plug][8]</center>\n\n## 注意：\n通过上面的表格和图片，如果拔盘前没有卸载`mount`链接，那么可以发现：\n\n* 下次插盘，磁盘名称不再是原来的名称，在当前排序最后的一个磁盘名往后追加，比如当前最后一块盘是`sdf`，那么就改名为`sdg`\n* `lsblk`显示`device`列表不会变化，`alias name`也是不会变化\n* 再次拔盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的删除的事件信息\n* 再次插盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的添加的事件信息\n\n如果清除了`mount`信息，插拔变得正常起来\n\n* `udev`可以正常接收设备添加和删除的信息\n* `lsblk`可以正常显示设备列表，会随着磁盘的拔盘而更新显示，`alias name`也是如此\n* 磁盘名称变回正常，即开机最原始的名称，例如上文的`sdd`\n\n所以针对一块硬盘的拔插只要做好即时清除`mount`就够了，那么这个时候拔另外插槽的一块盘呢？\n结果是：\n\n* 第一次拔盘： `udev`感知到（`remove` 当前磁盘名），`device`列表和`alias name`消失 \n* 第一次插盘： `udev`感知到（`add` **sdg**），`device`增加了`sdg`，`alias name`增加，没有变化\n* 第二次拔盘： 一切都没了反应\n* 清除`mount`信息后，插盘：无反应\n* 删除`alias name`： 无反应\n* 无论怎么样都没反应，也许除了开机或者这快盘插在别的插槽内\n\n## 总结\n\n### 问题1：\n\n目前来看，只要拔盘超过两块，一旦第二块盘的磁盘名称变了跟之前名称不一样，那么之后再怎么清理`mount`信息或者反复拔插都不会被`udev`感知到\n理论上来说，要实现硬盘热替换，那就得保证`udev`每次都能检测到硬盘的插入，拔盘检测倒不是那么重要，\n针对这种情况，目前没发现什么比较好的解决方案，唯一一种也是我当前采用的方案，在检测拔盘事件时会触发一个脚本，做拔的盘的`mount`信息的检查，如果存在`mount`信息，那么就`umount`掉，这是一种比较完美的场景，也就是说要保证每次插盘前，当前盘在系统中都不存在任何`mount`信息\n\n这种方案还是太完美化了，还需要大量的测试，期望能找出更能经得起推敲的方案\n\n### 问题2\n\n测试过程中偶然发现每次硬盘插入如果被`udev`感知到，也会执行大量的`udev rules`，其中包括创建设备，建立连接，建立连接这个事情就包括硬盘分区的`alias`，而我们感知到磁盘设备插入执行脚本在这之前，找了一系列的方案，都没能够使得让脚本在它`alias`之后再执行\n\n这里有一份官方的解释：http://www.reactivated.net/writing_udev_rules.html#external-run\n\n至于为什么脚本依赖这个别名，那是因为系统存的`data path`还是`journal path`都是以别名的形式，如果我们能够在别名之后执行脚本，那么理想情况下系统`db`之前存的别名会自动和插入的盘匹配建立连接，不需要我们再做过多的事情，否则我们得在每次创建`osd`的时候再存原始路径，记录分区信息等等，还有`/etc/fstab`存的都是别名，这样就导致有些操作无法进行，还有一些其他的矛盾暂时还没想到\n[1]: http://ovv1r40we.bkt.clouddn.com/before_pull.png\n[2]: http://ovv1r40we.bkt.clouddn.com/first_pull.png\n[3]: http://ovv1r40we.bkt.clouddn.com/first_plug.png\n[4]: http://ovv1r40we.bkt.clouddn.com/second_pull.png\n[5]: http://ovv1r40we.bkt.clouddn.com/second_pull.png\n[6]: http://ovv1r40we.bkt.clouddn.com/clear_mount.png\n[7]: http://ovv1r40we.bkt.clouddn.com/third_plug.png\n[8]: http://ovv1r40we.bkt.clouddn.com/third_plug_alias.png\n[9]: http://ovv1r40we.bkt.clouddn.com/readerror.png\n[10]: http://ow0mgad6r.bkt.clouddn.com/open-uri20150611-11-s7rw56.png\n","tags":["Linux"],"categories":["ceph"]},{"title":"Linux Udev","url":"/2017/09/10/Linux-Udev/","content":"<center>![udev][21]</center>\n\n在我入门`udev`，看着各种各样的资料觉得这个东西很高深的时候，突然瞄到一篇文章里面说道： 要树立一个信念：`udev`很简单，当时不以为然，现在深表赞同。看起来很高端很遥远的`Title`，比如`Linux Kernel`，比如软硬件交互，比如热插拔等等，其实掌握几个`rules`，然后再动手实践实践，对比着讲解，就能知道七七八八了，想要再深入，就多看看文档实现和内核源码，其实很多东西都是这样，了解了就很简单，没事了解一点就比别人多知道一些，共勉 \n\n<!--more-->\n## Udev 是什么\n`udev`是`linux kernel`的设备管理器，处理硬件的添加和删除，但它的功能远远不仅于此。比如`U`盘插入，电脑是如何即时的做出响应？耳机插入又是如何做到自动关闭外部音量？这些都跟`udev`有关。虽然`udev`很强大，功能很多，本来就针对`SCSI`硬盘设备的插拔做一些经验的分享。至于为什么我会研究到这个东西，是因为最近在做`ceph`相关的项目，需要实现一个类似热插拔的`feature`，懂的人应该很清楚，就是拔插硬盘后，`osd`自动恢复，主要应用于硬盘寿命即将到期，替换新盘的场景，不明白的同学也不要紧，因为本文主要讨论的是`udev`\n\n## Udev Rules\n`udev`管理各种各样的设备，比如`usb`,`scsi`,`cd`等等，为了区别设备类型，精确匹配，`udev`制定了一套特色的`rules`，为什么说特色呢？因为我觉得它用最简洁的语法满足了方方面面的需求\n\n这里推荐一些有用的`link`：\n\n* [UDEV Primer][3]  `udev`入门必读，可以快速将`udev`了解一番\n* [Writing udev rules][4]  经典文章，看完这个基本就能掌握`udev rules`\n* [Writing udev rules 修订版][5]  国内论坛对格式样式做了一些修改，有洁癖的可以看看\n* [Writing udev rules 译][6]  翻译版本，英文不好的可以中英文对比着看\n* [udev 中文手册][7]  这位金步国先生为开源事业做出了杰出的贡献，早前就看过他翻译的`Apache`的手册\n* [Writing udev rules and kernel examples][8]  看够了上面各种规则，来看看具体的例子吧\n* [Udev 详解][11]  文章如名，真的很详细\n* [Udev 使用方法][20]  一些特殊的技巧  \n\n大家看完上面的文章应该知道`rules`文件的位置和规则,下面我贴一些我实践中的代码和例子\n\n### Rules Example\n这是两条分别针对`SCSI`硬盘设备的添加和删除的`rules`，设备事件分别会触发相应的脚本：\n```\nKERNEL==\"sd[a-z]\", ACTION==\"add\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k add\"\nKERNEL==\"sd[a-z]\", ACTION==\"remove\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k remove\"\n```\n\n### Notice\n* 执行外部命令必须加上环境变量路径, 上面的这个也可以不加`/usr/bin/python`，但是必须在`py`脚本头部加上`shebang`：`#! /usr/bin/python`\n* `RUN`可以运行程序外的程序，而`PROGRAM`不是做这个事情的，而是为了生成之后需要的变量\n\n### Link\n* [hashtag and shebang][1]\n* [shebang][2]\n\n## Udevadm\n`udevadm`是一个`udev`的管理工具，可以用来获取设备信息(`info`)，监视和控制udev运行时的行为(`control`)，请求内核事件(`trigger`)，管理事件队列(`settle`)，以及提供简单的调试机制(`test`)。很实用，很好用！！！\n\n### 推荐 Link\n\n* [udevadm 中文手册][9]  很全，很基础 ~~\n* [udevadm command examples][10]  很多`udevadm`的命令可以借鉴，说不定以后可以用到呢？\n\n介绍几个我具体使用过的例子：\n### udevadm info\n可以从`udev`数据库中提取设备信息,此外，还可以从`sysfs`中提取设备的属性， 以帮助创建与此设备匹配的`udev`规则\n\n获取`/dev/sdb`的`path`信息\n```\nudevadm info -q path -n /dev/sdb\n```\n### udevadm test\n\n模拟向`udev`发送设备信息，并不会真的触发，**而是将触发后的过程信息打印出来**，对调试很有帮助\n\n``` \nudevadm test --action='add' `udevadm info -q path -n /dev/sdb`\n```\n\n**注意：** `test`不会真正触发事件，而`trigger`可以\n\n### udevadm trigger\n\n可以真正触发`udev`事件，但是不会真正改变硬件，只是触发`kernel`和`udev`的事件，会触发`udev rules`\n\n```\nudevadm trigger -v --action=add --sysname-match=sda\n```\n\n**注意：** \n\n* 不加`sysname-match`那么会触发所有设备，如果是`action=remove`，那么则会移除很多设备，甚至导致`sshd`服务停掉\n* 后面是`sda`而不是`/dev/sda`因为它是根据`pci`路径来定位`name`的，比如`sda`的路径是：\n```\n/sys/devices/pci0000:00/0000:00:10.0/host2/target2:0:0/2:0:0:0/block/sda\n```\n\n### udevadm control\n\n* 重载`udev rules`，对之后触发的新设备有效，对之前已经触发的无效\n```\nudevadm control --relaod-rules\n```\n* 更改`udev`日志等级为`info`，`udev`默认等级为`err`，可以在`/etc/udev/udev.conf`中查看，日志具体信息可以在`syslog`中查看\n```\nudevadm control --log-priority=info\n```\n\n## More about udev\n`udev`还有很多有意思的地方，如果想要更加深入的学习，那么除了熟练地掌握`rules`等技巧，还要对其实现原理有了解，这就得多看文档，多看源码了，建议大家有条件的情况下多看源码，多看原始的东西，因为经过别人的总结就可能会变味，相互对比，不要一味依赖别人总结的产物\n\n* [Linux 自动挂载usb设备 小例子][12]  如何在`usb`设备插入后自动挂载？\n* [udev使用笔记][13]  这篇文章里面那张图片不错\n* [写个程序检测我们的设备插拔][14]  模仿内核源码简单地实现一个设备插拔的检测\n* [linux下热插拔事件的产生是怎样通知到用户空间][15]  事件传递过程，`udev`源码解析\n* [热插拔的工作原理 hotplug机制和udev][16]  `udev`原理讲解\n* [Uevent 上报event事件给上层的详细讲解][17]  `uevent`源码讲解\n* [linux内核空间和用户空间的是怎样区别的，如何交互，如何从用户空间进入内核空间][18]  内核空间和用户空间的一些剖析\n* [Netlink实现热拔插监控 ][19]  消息传输用的是什么？`Netlink`是如何做到的\n\n[1]: http://www.cnblogs.com/daishuguang/p/4192703.html\n[2]: http://smilejay.com/2012/03/linux_shebang/\n[3]: http://webpages.charter.net/decibelshelp/LinuxHelp_UDEVPrimer.html\n[4]: http://www.reactivated.net/writing_udev_rules.html\n[5]: http://www.lai18.com/content/1612267.html\n[6]: http://blog.csdn.net/nhczp/article/details/4099647\n[7]: http://www.jinbuguo.com/systemd/udev.html#\n[8]: http://wiki.dreamrunner.org/public_html/Embedded-System/kernel/udev-rules.html\n[9]: http://www.jinbuguo.com/systemd/udevadm.html\n[10]: http://fibrevillage.com/sysadmin/93-udevadm-usage-examples\n[11]: http://blog.csdn.net/lqrensn/article/details/8097288\n[12]: https://my.oschina.net/bwhite/blog/808995\n[13]: http://www.jianshu.com/p/dd6cecd7755a\n[14]: http://blog.csdn.net/fjb2080/article/details/5009791\n[15]: http://www.itnose.net/detail/6691523.html\n[16]: http://blog.csdn.net/bingqingsuimeng/article/details/7950639\n[17]: http://blog.csdn.net/sunweizhong1024/article/details/7928530\n[18]: http://blog.csdn.net/bingqingsuimeng/article/details/7924756\n[19]: http://blog.chinaunix.net/uid-24943863-id-3223000.html\n[20]: http://www.cnblogs.com/qigaohua/p/6673930.html\n[21]: http://ow0mgad6r.bkt.clouddn.com/pingu.png\n","tags":["Linux"],"categories":["tech"]},{"title":"Network Down Notification","url":"/2017/09/10/network-down-notification/","content":"\n<center>![network][3]</center>\n\n项目中要做一个`network` `down`了之后给用户发送邮件提醒的功能，这里面涉及到的问题有：\n* 如何监测 network 状态\n* network down之后，如何发送邮件\n* public network 和 storage network的用处和场景\n\n<!--more-->\n\n## get role interfaces\n\n## get interfaces\n\n## get interface info\n\n### 获取 MAC 和 busid \n\n```\nsyspath = os.path.join(\"/sys/class/net/\", interface)  // interface such as 'eth0'\nmac = linecache.getline(os.path.join(syspath, \"address\"), 1)[:-1].upper()\nbuspath = os.path.realpath(syspath)\nbus_id = os.path.basename(os.path.dirname(os.path.dirname(buspath)))\n```\n\n* MAC: 直接在`syspath`目录下读取`address`文件\n* 假设为`eth0`,查看`eth0`目录可见该目录被建立了软连接，并不是真实地址，所以需要取出它的真实地址，这个需要使用`python`中的`os.path.realpath()`处理获得`0000:03:00.0`,作为下一步`lspci`的参数\n\n```\nroot@ceph133:/sys/class/net# ll eth0\nlrwxrwxrwx 1 root root 0  8月  8 01:26 eth0 -> ../../devices/pci0000:00/0000:00:15.0/0000:03:00.0/net/eth0/\n```\n\n### 获取网卡信息\n\n`lspci -s`后面的参数为`pci`设备的地址，也叫`pci寻址`，这个地址由总线编号、插槽编号和功能块编号信息组成\n```\nlspci -m -s xxx // 这边的参数就是上面的bus_id\n```\n\n> 具体`lspci`信息请参照：\n> [lspci命令详解][1]\n> [LSPCI命令详解分析][2]\n\n### 获取网卡状态\n\n通过`ethtool`工具获取网卡状态，就是看`Link detected`是否为`yes`，`yes`表示已激活\n\n```\n/sbin/ethtool eth0 | grep 'Link detected: yes' | wc -l\n```\n\n## bond filter\n\n## Network down notification\n**1.** 目前无论是`storage`还是`public`网卡只要`down`其中一个都无法发送`email`:\n\n* public: `visit foreign network and connect smtp server`\n* storage: `connect ceph rados to get notification info, such as send account and smtp server etc`\n所以只能`monitor`除这两个以外的网卡\n\n**2.** `Python`\n\n* `log`打印的文件取决于当前文件进程的初始化`log level`\n* `logger`: `handler`, `formatter`, `filter`\n* `dictdefault`\n\n\n  [1]: http://coolnull.com/2246.html\n  [2]: http://blog.csdn.net/xiadidi/article/details/18408169\n  [3]: http://ow0mgad6r.bkt.clouddn.com/it_specialist_kit8-net.png\n","tags":["Notification"],"categories":["tech"]},{"title":"Python操作Oracle","url":"/2017/09/10/Python-Oracle/","content":"<center>![Python][3]</center>\n\n近期在做`Intel`的测试，需要监测各种故障情况下的热恢复或者热迁移等功能，比如拔线测试，拔盘测试，断电测试等，来检验分布式的稳定性和灵活性，以`Oracle`为例，需要一直向数据库中写数据，然后在发生故障时，通过观察插入数据的情况来判断是否能够处理异常情况，所以我需要写一个脚本，能够做到通过`python`自动连接`oracle`数据库并自动插入数据。\n\n<!--more-->\n\n> 当前环境：Linux Centos 7\n\n### 1. 下载安装包cx_Oracle\n\n由于我本地`Python`版本是`2.7`,所以选择是`2.7`版本\n\n```\nwget https://pypi.python.org/packages/e1/18/00987c6a9af9568ee87d1fcba877407684a3f1b87515e5eb82d5d5acb9ff/cx_Oracle-6.0rc1-py27-1.x86_64.rpm#md5=ff8f5cce07bd5ec8ed3ce5c79d451474\n```\n\n参考:\n\n> - [python-cx_Oracle][1]\n> - [安装包列表][2]\n\n\n[1]: https://oracle.github.io/python-cx_Oracle/\n[2]: https://pypi.python.org/pypi/cx_Oracle/5.3\n\n### 2. 安装cx_Oracle\n\n- 安装：\n\n```\nrpm -ivh cx_Oracle-6.0rc1-py27-1.x86_64.rpm\n```\n\n- 查看当前`python`引入模块库的路径\n\n```\n[root ~]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys\n>>> print sys.path\n['', '/usr/lib/python2.7/site-packages/AliyunUtil-0.0.1-py2.7.egg', '/usr/lib/python2.7/site-packages/cloud_init-0.7.6-py2.7.egg', '/usr/lib64/python27.zip', '/usr/lib64/python2.7', '/usr/lib64/python2.7/plat-linux2', '/usr/lib64/python2.7/lib-tk', '/usr/lib64/python2.7/lib-old', '/usr/lib64/python2.7/lib-dynload', '/usr/lib64/python2.7/site-packages', '/usr/lib/python2.7/site-packages']\n```\n\n- 检查安装路径，如果`cx_Oracle.so`文件不在以上`python`引入路径内需要将该文件拷贝到以上`python`引入路径,所以我这里不需要进行拷贝操作\n\n```\n[root ~]# rpm -qa | grep cx_Oracle | xargs rpm -ql\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/PKG-INFO\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/SOURCES.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/dependency_links.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/top_level.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle.so\n```\n\n### 3. 下载 Oracle Library\n\n由于`Oracle`官网下载东西存在`Auth`认证，所以简单点就点击选中的`rpm`包保存在`windows`上，然后上传到`linux`上即可\n\n**注意：**\n> - 下载的版本要和连接的`Oracle`基本一致，我需要连接的`oracle`版本是`11.2`，所以我下载的是`oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm`\n> - **操作系统要和之前下载的`cx_Oracle`要一致**，这很重要！！！我在这个上面栽了跟头，花了比较多的时间。。。所以我需要下载`64`位的\n> - 在`Oracle`官网下载东西你得先勾选`Accept License Agreement`，否则你下载不了\n> - 其次你得有`Oracle`的账号\n\n### 4. 安装 Oracle Library\n\n```\nrpm -ivh oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm\n```\n\n### 5. 配置环境变量\n\n- **修改`Oracle`安装路径**\n\n将`client64`改为`client`，至于为什么要改暂时还不清楚，只知道不改的话会报错`Oracle Client library cannot be loaded: libclntsh.so: cannot open shared object file: No such file or directory`\n\n```\ncd /usr/lib/oracle/11.2/\nmv client64 client\n```\n\n- 打开`/etc/profile`,追加环境变量路径\n\n```\nvim /etc/profile\n```\n\n- 配置环境变量\n\n```\n// 查看Oracle安装路径\n[root@iZbp180vortqapb7yf8217Z ~]# rpm -qa | grep oracle | xargs rpm -ql\n/usr/lib/oracle/11.2/client64/bin/adrci\n/usr/lib/oracle/11.2/client64/bin/genezi\n/usr/lib/oracle/11.2/client64/lib/libclntsh.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libnnz11.so\n/usr/lib/oracle/11.2/client64/lib/libocci.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libociei.so\n/usr/lib/oracle/11.2/client64/lib/libocijdbc11.so\n/usr/lib/oracle/11.2/client64/lib/ojdbc5.jar\n/usr/lib/oracle/11.2/client64/lib/ojdbc6.jar\n/usr/lib/oracle/11.2/client64/lib/xstreams.jar\n// 追加ORACLE_HOME环境变量路径\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/lib\n```\n\n- 生效配置\n\n```\nsource ~/.bashrc\n```\n\n### 6. 检查是否可以引入`cx_Oracle`\n\n`import`没有报错，说明之前的辛苦没有白费\n\n```\n[root client64]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import cx_Oracle\n>>>\n```\n\n### 7. 连接 Oracle\n\n一个插入10条数据的Python脚本demo：\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport sys\nimport cx_Oracle\nimport time\n\nrows = int(sys.argv[1])\ncon = cx_Oracle.connect('test/test@sampledb')\ncur = con.cursor()\ncur.execute(\"SELECT MAX(ID) FROM PTTEST\")\nmax_id = cur.fetchone()[0] or 0\n\nstart_id = max_id + 1\nfor i in xrange(rows):\n    cur.execute(\"INSERT INTO PTTEST (ID, NAME, AGE, GENDER, SALARY)VALUES(%d, 'pt', 15, 'male', 50000)\" % int(i + start_id))\n    if i % 100 == 0:\n        time.sleep(3)\n        print 'Insert rows [%d]:%d' % (int(time.time()), i)\n        con.commit()\ncur.close()\ncon.close()\n```\n[3]: http://ow0mgad6r.bkt.clouddn.com/we-love-python-6000-450.png\n","tags":["Oracle"],"categories":["tech"]},{"title":"Oracle Linux And Oracle Database 11g R2 Intsallation","url":"/2017/09/10/oracle_install/","content":"\n<center>![oralce][9]</center>\n\n最近工作中用到了`Oracle`作为`Intel`测试的数据库，在装`Oracle Linux`和`Oracle Database 11g R2`折腾了很久，这篇文章就这两点对大家分享一些我安装过程中遇到的问题和注意点，`rac`暂时还未配置，稍后找时间补上\n\n<!--more-->\n\n> 友情提醒： 本文参照`link`较多，请细心并且耐心地进行每一步操作\n### Resource Link\n> * [Oracle Linux ISO][1]\n> * [Linux Oracle Database 11g R2][2] \n> * [Windows Oracle Database 11g R2][3] \n\n### Oracle Linux Installation\n**参考列表：**\n\n  * [oracle linux 6.5 安装 oracle 11gR2图文详解][4]\n  * [Oracle 11g R2（自己整理的Word，带详细RAC配置）][5]\n\n> 以下操作步骤基本是按上面两条`link`走的，有一些地方不一样或者是没必要的我会在下面详细讲解，我装的时候以`link1`为主线，然后参照`link2`的`word`文档进行，没有特殊标记的一般都是根据`link1`操作，大家也可以这样操作，可以少遇一些坑，节省一些时间~~~\n\n**注意点：**\n  \n  * `Base server`这一步很关键\n    - 勾选`Oracle Linux Server`, `Resilient Storage`和 `Scalable Filesystem Support`（参照`link2`）\n    - 勾选`Customize Now`，这一步依然非常关键，只有勾选了才可以装一些`Oracle Linux`需要的套件\n    - 需要手动添加安装的套件（系统默认勾选的不要删掉，这边的操作只添加勾选）：\n      * `Base System`：`Compatibility libraries`\n      * `Servers`: `System administration tools`\n      * `Desktops`: `Desktop`、`Desktop Platform`、`Fonts`、`General Purpose Desktop`、`Graphical Administration Tools`、`Input Methods`、`X Window System`\n      * `Development`: `Additional Development`、`Development Tools`\n      * `Appilication`: `Internet Browser`\n  * 安装应该是`1390`个包，装好直接`reboot`\n  * 这时候创建普通用户的时候，千万不要创建`oracle`用户，否则之后会有很多麻烦\n\n### Oracle Linux Configuration（important and complicated）\n**下面讲的主要是和两个文档不同的地方（依然以`link1`为主线）：**\n\n**1.** `selinux`需要`dsiable`\n\n**2.** `/etc/sysctl.conf`参照`link2`，`kernel.shmmax = 536870912`（这行参照`link2`）\n\n**3.** `/etc/security/limits.conf`\n```\n  oracle soft nproc 16384\n  oracle hard nproc 16384\n  oracle soft nofile 4096\n  oracle hard nofile 65536\n  oracle soft stack 10240\n```\n\n**4.** `oracle linux 6.5`光盘下载包（我是通过`yum`安装,没有安装的大部分都是系统已经安装了，少部分是没必要安装）\n* 需要安装：\n  * `ksh`\n  * `libaio-devel.i686`\n  * `libaio-devel.x86_64`\n  * `libaio.i686`\n  * `libstdc++.i686`\n  * `libstdc++-4.4.7-18.el6.i686`\n  * `compat-libstdc++-33.i686`\n* 不需要安装：\n  * `numactl-devel-2*x86_64*`\n\n> 由于我本地的是`libstdc++-4.4.7-4.el6.x86_64`，而安装的是`libstdc++-4.4.7-18.el6.i686`，版本不匹配会造成`yum`报错，而这个库作为众多库的依赖还不能删除，所以只能强行升级：`yum install libstdc++-4.4.7-18.el6.i686`，然后再安装`i686`文件\n\n**5.** 路径、权限与环境变量配置\n这一步参照: [Automating Database Startup and Shutdown on Linux][6]\n\n**附上脚本和`~/.bash_profile`:**\n\n`start_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbstart $ORACLE_HOME\n```\n\n`stop_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbshut $ORACLE_HOME\n```\n\n`setenv.sh`：\n\n```\n# Oracle Settings\nexport TMP=/tmp\nexport TMPDIR=$TMP\n\nexport ORACLE_HOSTNAME=tonydb.localdomain\nexport ORACLE_UNQNAME=sampledb\nexport ORACLE_BASE=/home/oracle/app/oracle\nexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1\nexport ORACLE_SID=sampledb\n\nexport PATH=/usr/sbin:/usr/local/bin:$PATH\nexport PATH=$ORACLE_HOME/bin:$PATH\n\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib\nexport CLASSPATH=$ORACLE_HOME/jlib:$ORACLE_HOME/rdbms/jlib\n```\n\n`~/.bash_profile`最后一行追加： \n\n```\n. /home/oracle/scripts/setenv.sh\n```\n\n**6.** 安装`FTP`并上传`zip`文件（这一步有需求就执行，我是通过`windows`传输到`linux`上）\n> 安装包在页面最上方`Resource Link`的第二个`link`，有两个`zip`包，都是需要的，这就是`Oracle Database 11g R2`的包，下面就开始安装数据库了，离成功也就不远了\n\n### Oracle Database 11g R2 Installation\n**参照：** [Oracle Database 11g Release 2 Installation on RHEL/CentOS 6.x/5.x/4.x][7]，以这个`link`的步骤为主，`link1`和`link2`为辅\n\n1. 如果出现`next button`闪退的现象，进入`preference`里面调节屏幕分辨率即可\n\n2. `check`时候报错，如果你在上面安装了那么就`ignore`吧，因为这些包都安装了，只不过版本比它`check`的高：\n```\nlibaio-0.3.105\ncompat-libstdc++-33-3.2.3\nlibaio-devel-0.3.105\nlibgcc-3.4.6\nlibstdc++-3.4.6\nunixODBC-2.2.11\nunixODBC-2.2.11\nunixODBC-devel-2.2.11\nunixODBC-devel-2.2.11\npdksh-5.2.14\n```\n\n### Oracle Listener Configuration\n**参照：** [Oracle Listener][8]\n> 最后`test`不成功也没关系，先保存设置进行下一步，这边把`lsnrcetl`起起来就可以了，先不要进入`oracle`命令行，需要等下一步创建了数据库再进行`startup`等操作\n\n### 创建数据库\n> `dbca`进入创建向导，填写正确的`sid`，`sid`不用填`ORCL`,就用同步生成的就好，不同的数据库可以拥有不同的`sid`，按照默认的配置不断地`next`就好\n\n* 安装后的默认数据库为orcl，如需创建新的数据库\n  - 使用辅助工具(Linux UI mode)\n    > root@oracledb: xhost local:oracledb\n    > root@oracledb: su - oracle\n    > oracle@oracledb: dbca\n  - 使用em管理WebUI\n* 创建表空间\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create tablespace test_space datafile '/opt/oracle/oradata/test_data.dbf' size 500M\n  - 使用em管理WebUI\n* 创建用户    \n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create user test identified by test default tablespace test_space;\n  - 使用em管理WebUI\n* 授权用户权限\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant connect, resource to test;\n  - 使用em管理WebUI\n* 授权DBA权限（RMAN需要dba权限来运行）\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant sysdba to test;\n  - 使用em管理WebUI\n* 数据库启动/关闭\n  * 切换用户\n    > root@oracledb: su - oracle\n  * 启动监听\n    > lsnrctl start\n  * 启动实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > startup\n  * 关闭实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > shutdown immediate\n    > exit\n  * 关闭监听\n    > lsnrctl stop\n* 归档日志模式启动/关闭\n  * 启动归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database archivelog\n    > alter database open\n  * 关闭归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database noarchivelog\n    > alter database open\n\n作者： Tony\n\n日期： 2017/06/26\n\n  [1]: https://pan.baidu.com/s/1pL7ENn9\n  [2]: https://pan.baidu.com/s/1qYufAQo\n  [3]: https://pan.baidu.com/s/1kV3JpCr\n  [4]: https://wenku.baidu.com/view/afd5ca9e5022aaea998f0fa2.html\n  [5]: https://pan.baidu.com/s/1miDQEoC\n  [6]: https://oracle-base.com/articles/linux/automating-database-startup-and-shutdown-on-linux\n  [7]: https://www.tecmint.com/oracle-database-11g-release-2-installation-in-linux/\n  [8]: http://blog.csdn.net/haiross/article/details/13613931\n  [9]: http://ow0mgad6r.bkt.clouddn.com/oracle-eloqua-campaign-manager.png\n","tags":["Oracle 11g R2"],"categories":["tech"]},{"title":"Moodle Local插件","url":"/2017/09/09/Moodle-Local-Plugin/","content":"\n<center>![moodle][8]</center>\n\n`Local`插件在`Moodle`中相比于其他插件算是比较容易上手的一个插件，`Local`插件中前后台分离，不用遨游在杂糅前后端的代码里。学会`Local`插件是上手其他插件的前提，其实一旦学会`Local`插件也会发现其他插件不过是它的一种变种形式罢了，只要举一反三就能很快掌握。\n\n<!--more-->\n\n## Moodle Local 插件架构\n<center>![image][2]</center>\n\n## 前端\n### [Template][3]\n服务器配置了相应的`url`，所以每个插件的加载首页都是`index.php`，但是并不意味着页面的代码就在`index.php`中，首先在`index.php`中调用`renderer.php` 中相应的页面渲染方法，例如  `render_index_page($page) ` 方法：\n\n```\npublic function render_index_page($page) {\n$data = $page->export_for_template($this);\n    render_from_template('local_xxx/index', $data)\n}\n```\n\n* 在这个方法中，首先调用了`index_page`这个类中的 `export_for_template` 方法，获得页面静态加载需要的数据，例如`mustache`模板需要的数据，`data`数组的索引与模板名称一一对应，例如`mustache`文件中若有 &#123;&#123;`#user`&#125;&#125;`hello!`&#123;&#123;`/user`&#125;&#125;，那么对应 class 一定存在 `$data['user'] = xxxx`；\n* 以 `render_from_template('local_xxx/index', $data)` 为例， `local_xxx/index`，`template`目录下必定有文件名为`index.mustache`的文件，这是`moodle` 封装好的命名规范，而`mustache`文件就是页面展示的代码所在了；\n* 通过&#123;&#123;`#js`&#125;&#125;`require xxx`&#123;&#123;`/js`&#125;&#125;引入对应的`JavaScript`文件；\n\n\n### [CSS][4]\n每个 local 插件中都存在一个`style.css`这样的一个`css`样式文件，系统运行时，会将每个模块的`css`文件内容合并到一起，所以如果有两个名称一样的`css`，则会发生覆盖的现象，所以建议起名字统一在最前面加上当前的插件名称，要注意的是，每次新增、修改或者删除`css`文件内容后，如若需要立即查看效果，必要以`admin`角色进入系统，然后**清空缓存**；\n\n### [JavaScript][5] \n目前系统中的`js`是基于`RequireJS`框架，遵循`AMD`开发规范，利用`Grunt`构建的。`amd`目录下有`src`目录和`build`目录，`src`下的`xxx.js`是供开发时编写调试使用，而`build`目录下的`min.js`则是网站实际运行时所使用的`js`文件，经过压缩可以大幅度提升运行速度。所以每次修改`js`完毕后，提交代码切勿忘记生成对应的`min.js`文件并提供，生成命令为 `grunt uglify`；\n\n## 后端\n### [db][6]\n1. access.php\n主要负责记录权限的配置；\n\n2. service.php\n负责接收`ajax`提交，配置对应的后台`API`的类名、路径 和方法名等信息，并将对应的`ajax`方法存入数据库中；\n\n3. install.xml\n用于安装系统时表结构的对应生成；\n\n4. install.php\n用于安装系统时数据库的缺省数据的提供；\n\n5.  upgrade.php [开发规范][7]\n用于系统表结构变化等版本升级事项的实现，由于生产环境中客户已有大量数据存在于数据库中，所以这时候不能采取重装系统的方式更改表结构或者更改数据库等操作，这时候就得借助`upgrade.php`平滑升级。\n\n### 后台 API\n1. externallib.php\n负责编写`service.php`中`ajax`提交方法对应的后台方法，并调用`lib.php`中的`API`实现；\n\n2. lib.php\n负责方法的具体实现，代码与数据库的数据交互\n\n3. lang/en/xxx.php 和 lang/zh_cn/xxx.php\n\n`en`目录下的表示英文翻译语言文件，`zh_cn`目录下的表示中文翻译语言文件，具体调用方法为： \n\n* `php`中： `get_string(字符串名, 插件名)`\n* `mustache`中： &#123;&#123;`#str`&#125;&#125;`字符串名, 插件名`&#123;&#123;`/str`&#125;&#125;\n\n作者：Tony\n2017年02月28日\n\n[1]: https://www.processon.com/view/link/58b50108e4b02ba7e0885137\n[2]: http://on-img.com/chart_image/58b5173ce4b01d80b64d335e.png \n[3]: https://docs.moodle.org/dev/Templates\n[4]: https://docs.moodle.org/dev/CSS\n[5]: https://docs.moodle.org/dev/Javascript_Modules\n[6]: https://docs.moodle.org/dev/Data_definition_API\n[7]: https://note.youdao.com/share/?token=18D5479D0AFA4286A46EACE741345DF5&gid=29389535\n[8]: http://ow0mgad6r.bkt.clouddn.com/moodle-logo-concept.png\n","tags":["PHP"],"categories":["tech"]}]