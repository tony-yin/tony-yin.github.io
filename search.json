[{"title":"玩转 Xshell","url":"/2019/06/20/Best-Practice-Of-Xshell/","content":"\n<center>![Disk Partition][21]</center>\n\n由于笔者算是`Windows`、`Linux`和`OSX`三个系统的雨露均沾者，所以终端工具经常穿梭于`OSX`的`Item2`和`Windows`的`Xshell`之间。最近得空折腾下`Xshell`，分享一些好看的样式和实用小技巧。\n\n<!--more-->\n\n## 主题\n\n`Xshell`内置几种配色方案，原来笔者用的是`Xterm`，样式如下：\n\n<center>![Xshell theme xterm][3]</center>\n\n无论是背景配色、文件配色、文件配色都说不上好看，但是`Xshell`支持导入第三方配色方案的功能：\n\n<center>![Xshell import theme][4]</center>\n\n`Github`上有个[Xshell-ColorScheme][1]的仓库，提供了`250+ Xshell`配色方案，可以将这个项目下载下来，然后导入`.xcs`文件，便可选择自己喜欢的配色方案。\n\n笔者`Item2`用的`theme`是`Dracula`，恰好这里面有，所以`Xshell`也选择了`Dracula`主题，瞬间变身！有木有：\n\n<center>![Xshell theme dracula][5]</center>\n\n除此之外，还有很多好看的主题，比如`hybird`等等，大家各取所需哈。\n\n当然也可以自己编写配色方案，原理就是一个`.xcs`文件，文件内容为一些自定义样式配置：\n\n```\n[mytheme]\ntext(bold)=e9e9e9\nmagenta(bold)=ff00ff\ntext=00ff80\nwhite(bold)=fdf6e3\ngreen=80ff00\nred(bold)=ff0000\ngreen(bold)=3c5a38\nblack(bold)=808080\nred=ff4500\nblue=00bfff\nblack=000000\nblue(bold)=1e90ff\nyellow(bold)=ffff00\ncyan(bold)=00ffff\nyellow=c0c000\nmagenta=c000c0\nbackground=042028\nwhite=c0c0c0\ncyan=00c0c0\n[Names]\ncount=1\nname0=mytheme\n```\n\n## 字体\n\n`Xshell`内置的字体有限，笔者之前一直用的是`Consolas`，凑合着能看，样式如上图。`Xshell`虽然没有提供导入第三方字体的功能，但是它也是可以使用第三方字体的。`Xshell`会默认从`Windows`操作系统上寻找可用的字体，这里要注意的是并不是系统上所有的字体都会支持，`Xshell`只支持**[等宽字体][2]**。\n\n笔者为了和`Item2`同步，选择了`Monaco`字体：\n\n<center>![Monaco Font][6]</center>\n\n配置后，代码字体样式如下：\n\n<center>![font monaco][7]</center>\n\n还有很多好看的等宽字体，比如`FiraCode`也不错，大家看自己喜好挑选即可。\n\n<center>![font fire][8]</center>\n\n* [Monaco Font][9]\n* [FiraCode Font][10]\n\n## 实用小技巧\n\n### 多会话命令\n\n`Xshell`支持在多个会话中并行执行同样的命令，这个功能集群中多个节点执行同样的操作时往往很有用。\n\n在“工具”中开启“发送键输入到所有会话”:\n<center>![enable multi session cmd][18]</center>\n\n多会话执行命令：\n\n<center>![multi session cmd][19]</center>\n\n### 快速命令集\n\n平时开发我们经常会执行一些比较长的命令，比如进入数据库（需要输入复杂的用户名和账号），查看天气等等，通常我们是将这些命令记录在笔记或其他一些地方，然后用到时找到该命令，复制粘贴到命令行执行，这样效率太低了。`Xshell`自带快速命令集的功能，能够将一些命令保存在`Xshell`中，可以达到快速执行的效果。\n\n将查看天气的命令添加到快速命令集中：\n\n<center>![xshell quick cmd][20]</center>\n\n## zsh\n\n`zsh`拥有比传统`bash`更强大的功能，这里首推的配置肯定是[oh-my-zsh][11]，然后主题推荐`powerlevel10k`，在`powerlevel9k`的基础上增加了异步的特性，使得命令行执行更快，解决了之前卡顿的问题。\n\n`powerlevel9k`有一个[Show Off Your Config][13]，里面有大量的主题配置，下面便是其中的一个：\n\n<center>![zsh theme powerlvel9k][12]</center>\n\n直接选择喜欢的配置，粘贴到`.zshrc`中然后`source`一下即可生效。\n\n\n## tmux\n\n`tmux`是终端复用工具，主要有以下作用：\n\n1. 在发生断电等终端退出的情况，依然可以重新连接回终端\n2. 多会话\n3. 多窗口\n4. 分屏切割\n\n`Github`上有一个[oh-my-tmux][14]，配置很强大，可以自行去项目主页了解下，下面是笔者的`tmux`效果图：\n\n<center>![my tmux][15]</center>\n\n除了`oh-my-tmux`，`tmux-themepack`也不错，它打包了一系列的主题，贴个紫色的主题：\n\n<center>![Powerline Magenta][16]</center>\n\n## vim\n\n`vim`是`Linux`下经典的一个编辑器，笔者一直使用它，省去了经常换编辑器的烦恼。`vim`也一直与时俱进，现在已经出了`vim8`和`neovim`等强大的产品，至于配置和插件笔者就不详细赘述了，这个要说的话能扯上几天几夜，网上教程也很多，自己去找适合自己的配置和插件。\n\n下面为笔者的`vim`效果图：\n\n<center>![vim][22]</center>\n\n## 总结\n\n本文主要介绍了`Xshell`的一些配置，让`Xshell`更加炫酷，毕竟我们大部分时间是在敲代码，所以一个好看的编辑器可以让人心情愉悦。接着又介绍了`Xshell`一些实用小技巧，说实话个人感觉在实用性上面`Xshell`是要比`Item2`强大不少的，最后我们又引申出`zsh`、`tmux`和`vim`这些好用的工具，可以说笔者已经将自己的开发环境生态圈介绍了个遍，还是那句话，工具选择很重要，但是不要花费过多的时间折腾，把主要精力放在技术上才是王道。\n\n\n[1]: https://github.com/netsarang/Xshell-ColorScheme\n[2]: https://zh.wikipedia.org/wiki/%E7%AD%89%E5%AE%BD%E5%AD%97%E4%BD%93\n[3]: http://cdn.tony-yin.site/Xshell_theme_xterm.jpg\n[4]: http://cdn.tony-yin.site/Xshell_import_theme.jpg\n[5]: http://cdn.tony-yin.site/Xshell_dracula_theme.jpg\n[6]: http://cdn.tony-yin.site/xshell_font_set.jpg\n[7]: http://cdn.tony-yin.site/xshell_font_mocano.jpg\n[8]: http://cdn.tony-yin.site/xshell_font_fira.jpg\n[9]: https://github.com/todylu/monaco.ttf\n[10]: https://github.com/tonsky/FiraCode\n[11]: https://github.com/robbyrussell/oh-my-zsh\n[12]: http://cdn.tony-yin.site/zsh_theme_powerlevel9k.png\n[13]: https://github.com/bhilburn/powerlevel9k/wiki/Install-Instructions#step-1-install-powerlevel9k\n[14]: https://github.com/gpakosz/.tmux\n[15]: http://cdn.tony-yin.site/xshell_tmux.jpg\n[16]: http://cdn.tony-yin.site/tmux_theme_pack.png\n[17]: https://github.com/jimeh/tmux-themepack\n[18]: http://cdn.tony-yin.site/xshell_enable_multi_session.jpg\n[19]: http://cdn.tony-yin.site/xshell_multi_session.gif\n[20]: http://cdn.tony-yin.site/xshell_quick_cmd.gif\n[21]: http://cdn.tony-yin.site/xshell.png\n[22]: http://cdn.tony-yin.site/vim.jpg\n","tags":["Xshell"],"categories":["tech"]},{"title":"Redfish 详解","url":"/2019/04/20/Redfish_Introduction/","content":"\n<center>![DMTF Redfish][8]</center>\n\n本文介绍一下`redfish`，被称为“下一代数据中心管理标准”，非常强大的一个玩意！\n\n<!--more-->\n\n## 概述： Redfish 是什么？\n\n\n`DMTF`的`Redfish®`是一个标准`API`，旨在为融合、混合`IT`和软件定义数据中心（`SDDC`）提供简单和安全管理。\n\n在`Redfish`出现之前，现代数据中心环境中缺乏互操作管理标准。随着机构越来越青睐于大规模的解决方案，传统标准不足以成功管理大量简单的多节点服务器或混合基础设施。`IPMI`是一种较早的带外管理标准，仅限于“最小公共集”命令集（例如，开机/关机/重启、温度值、文本控制台等），由于供应商扩展在所有平台上并不常见，导致了客户常用的功能集减少。许多用户开发了自己的紧密集成工具，但是也不得不依赖带内管理软件。\n\n而对于企业级用户来说，设备都是上千台，其需要统一的管理界面，就要对接不同供应商的`API`。当基本`IPMI`功能已经不太好满足大规模`Scale-out`环境时，如何以更便捷的方式调用服务器高级管理功能就是一个新的需求。\n\n为了寻求一个基于广泛使用的工具来加快发展的现代接口，现如今，客户需要一个使用互联网和`web`服务环境中常见的协议、结构和安全模型定义的`API`。\n\n`Redfish`可扩展平台管理`API`（`The Redfish Scalable Platforms Management API`）是一种新的规范，其使用`RESTful`接口语义来访问定义在模型格式中的数据，用于执行带外系统管理 （`out of band systems management`）。其适用于大规模的服务器，从独立的服务器到机架式和刀片式的服务器环境，而且也同样适用于大规模的云环境。\n\n`Redfish`的第`1`版侧重于服务器，为`IPMI-over-LAN`提供了一个安全、多节点的替代品。随后的`Redfish`版本增加了对网络接口(例如`NIC`、`CNA`和`FC HBA`)、`PCIe`交换、本地存储、`NVDIMM`、多功能适配器和可组合性以及固件更新服务、软件更新推送方法和安全特权映射的管理。此外，`Redfish`主机接口规范允许在操作系统上运行应用程序和工具，包括在启动前（固件）阶段-与`Redfish`管理服务沟通。\n\n在定义`Redfish`标准时，协议与数据模型可分开并允许独立地修改。以模式为基础的数据模型是可伸缩和可扩展的，并且随着行业的发展，它将越来越具有人类可读性定义。\n\n## 应用场景\n\n可能上面那一大段晦涩的介绍描述让你难以理解`Redfish`的用处，简而言之，如今大多数服务器厂商（比如`Supermicro`、`Dell`等等）都会自带`IPMI`系统，这个系统是独立装在主板芯片上，用户可以通过浏览器远程控制服务器，比如开关、装系统、进入服务器终端等等，而不用跑到机房忍受过高或过低的温度、忍受服务器嘈杂的声音、忍受蹩脚的显示器……\n\n传统的`IPMI`是带来了很多的便捷，但是当服务器规模变大后，单一服务器的管理已经满足不了需求，用户期望能够批量管理服务器，比如笔者想一次性给`100`个服务器安装系统，并且这`100`个服务器并不都是同一厂商，不同厂商的`IPMI`操作都不一样，比如`Dell`是`iDRAC`，你还需要专门学习`iDRAC`使用和各种对接，这会带来很多困扰。而`Redfish`标准的出现彻底改变这种情况，它是凌驾于所有服务器之上的一个标准，对服务器的基本操作都是统一的，并且是基于`Restful API`的方式实现。所以客户可以利用`ansible`等工具轻松实现自动化批量管理大规模服务器。\n\n服务器供应商都希望成为`Redfish`生态系统的一份子，但他们不想放弃对自家的服务器的管控机制。因此，他们会在`Redfish`规定范围之外，持续强化其`API`支持的功能，比如`Dell`的`Redfish`接口就要比`Supermicro`多很多，强大很多。\n\n## API 实践\n\n### 环境信息\n\n* `Dell iDRAC v3.30.30.30`（版本>=`v3.30.30.30`才能使用`mount`相关`API`）\n\n\n### 查看CD挂载信息\n\n`Request`：\n\n```\ntype：GET\nheaders: {'Content-Type': 'application/json'}\nurl: https://192.168.224.1/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD\nbasic auth: username/password\n```\n\n`Response`:\n\n不存在挂载信息：\n\n```\n{\n    \"@odata.context\": \"/redfish/v1/$metadata#VirtualMedia.VirtualMedia\",\n    \"@odata.id\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD\",\n    \"@odata.type\": \"#VirtualMedia.v1_2_0.VirtualMedia\",\n    \"Actions\": {\n        \"#VirtualMedia.EjectMedia\": {\n            \"target\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.EjectMedia\"\n        },\n        \"#VirtualMedia.InsertMedia\": {\n            \"target\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia\"\n        }\n    },\n    \"ConnectedVia\": \"NotConnected\",\n    \"Description\": \"iDRAC Virtual Media Services Settings\",\n    \"Id\": \"CD\",\n    \"Image\": null,\n    \"ImageName\": null,\n    \"Inserted\": false,\n    \"MediaTypes\": [\n        \"CD\",\n        \"DVD\"\n    ],\n    \"MediaTypes@odata.count\": 2,\n    \"Name\": \"Virtual CD\",\n    \"WriteProtected\": null\n}\n```\n\n存在挂载信息：\n\n```\n{\n    \"@odata.context\": \"/redfish/v1/$metadata#VirtualMedia.VirtualMedia\",\n    \"@odata.id\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD\",\n    \"@odata.type\": \"#VirtualMedia.v1_2_0.VirtualMedia\",\n    \"Actions\": {\n        \"#VirtualMedia.EjectMedia\": {\n            \"target\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.EjectMedia\"\n        },\n        \"#VirtualMedia.InsertMedia\": {\n            \"target\": \"/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia\"\n        }\n    },\n    \"ConnectedVia\": \"URI\",\n    \"Description\": \"iDRAC Virtual Media Services Settings\",\n    \"Id\": \"CD\",\n    \"Image\": \"http://192.168.232.1/centos7/CentOS-7-x86_64.iso\",\n    \"ImageName\": \"CentOS-7-x86_64.iso\",\n    \"Inserted\": true,\n    \"MediaTypes\": [\n        \"CD\",\n        \"DVD\"\n    ],\n    \"MediaTypes@odata.count\": 2,\n    \"Name\": \"Virtual CD\",\n    \"WriteProtected\": true\n}\n```\n\n### 挂载CD：HTTP Attach（挂载后不会自动重启）\n\n```\nmethod: POST\nurl: /redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia/\nheaders: {'Content-Type': 'application/json'}\npayload: {\n    \"Image\": \"http://192.168.232.1/centos7.4/CentOS-7-x86_64.iso\",\n    \"Inserted\":true,\n    \"WriteProtected\":true\n}\nAuthorization: Basic Auth - username/password\n```\n\n### 卸载CD：HTTP Detach\n\n```\nmethod: POST\nurl: /redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.EjectMedia/\nheaders: {'Content-Type': 'application/json'}\npayload: {}\nAuthorization: Basic Auth - username/password\n```\n\n### NFS Attach（挂载后自动重启）\n\n```\nmethod: POST\nurl: /redfish/v1/Dell/Systems/System.Embedded.1/DellOSDeploymentService/Actions/DellOSDeploymentService.BootToNetworkISO\nheaders: {'content-type': 'application/json'}\npayload: {\"IPAddress\": \"192.168.232.51\", \"ShareType\": \"NFS\", \"ShareName\": \"/mnt/test\", \"ImageName\": \"custom_cloudos7.1.iso\"}\nAuthorization: BasicAuth - username/password\n```\n\n### CIFS Attach（挂载后自动重启）\n\n```\nmethod: POST\nurl: /redfish/v1/Dell/Systems/System.Embedded.1/DellOSDeploymentService/Actions/DellOSDeploymentService.BootToNetworkISO\nheaders: {'content-type': 'application/json'}\npayload: {\"IPAddress\": \"192.168.232.51\", \"ShareType\": \"CIFS\", \"ShareName\": \"cifstest\", \"ImageName\": \"custom_cloudos7.1.iso\", \"UserName\": \"user1\", \"Password\": \"123\"}\nAuthorization: BasicAuth - username/password\n```\n\n### 引导选项（以CD方式引导）\n\n```\nmethod: PATCH\nurl: /redfish/v1/Systems/System.Embedded.1/\nheaders: {'content-type': 'application/json'}\npayload: {\"Boot\": {\"BootSourceOverrideTarget\": \"Cd\"} }\nAuthorization: BasicAuth - username/password\n```\n\n### 获取网卡信息：\n\n```\nmethod: GET\nurl: /redfish/v1/Systems/System.Embedded.1/EthernetInterfaces/NIC.Mezzanine.1-1-1\nheaders: {'Content-Type': 'application/json'}\nbasic auth: username/password\n```\n\n需要了解更多`Redfish API`，查看官网文档即可。`Dell`官方在`Github`上开源了一个项目：[iDRAC-Redfish-Scripting][7]，这个项目通过`python`代码实现`Redfish API`的调用，很多官方文档中没提到的细节和接口都可以通过阅读源码寻找帮助。\n\n## Refer\n\n1. [Redfish实现跨厂商的服务器管理，让采购新硬件再无后顾之忧][3]\n2. [Redfish：下一代数据中心管理标准][4]\n3. [Redfish 白皮书][5]\n4. [Redfish –融合、混合IT的简单和安全管理][6]\n3. [Supermicro Redfish API][1]\n4. [Dell iDRAC Redfish API][2]\n\n## 总结\n\n`Redfish`真是一个强大的存在，可以帮助我们远程管理服务器，甚至可以实现自动化、批量化的操作，这给大规模或者云应用提供了巨大帮助。想想看，一键式安装系统、部署，并且是分布式的，是不是很酷呢？\n\n[1]: http://cdn.tony-yin.site/RedfishRefGuide.pdf\n[2]: http://cdn.tony-yin.site/Implementation%20of%20the%20DMTF%20Redfish%20API%20on%20Dell%20EMC%20PowerEdge%20Servers.pdf\n[3]: https://searchdatacenter.techtarget.com.cn/9-30148/\n[4]: http://www.voidcn.com/article/p-qucxxbba-qh.html\n[5]: https://www.dmtf.org/sites/default/files/DSP2044%20Redfish%20%E7%99%BD%E7%9A%AE%E4%B9%A6%201.0.0.pdf\n[6]: https://www.dmtf.org/sites/default/files/Redfish_tech_note_converged_hybrid_it.pdf\n[7]: https://github.com/dell/iDRAC-Redfish-Scripting\n[8]: http://cdn.tony-yin.site/DMTF_Redfish_logo_R.jpg\n","tags":["Redfish"],"categories":["tech"]},{"title":"Supermicro IPMI License 反向破解","url":"/2019/04/10/Supermicro_IPMI_license_reverse/","content":"\n<center>![reverse][15]</center>\n\n调研`redfish`时，发现`supermicro IPMI`由于`license`没激活导致该功能使用不了，于是找厂商询问，得到的答复居然是他们也没有这个`license`，并且表示这个`license`需要另外付费购买，所以一般情况下也没有客户购买。笔者心里不禁`MMP`，按道理来说，购买服务器已经付过费了，不应该连基本功能都用不了。一方面让厂商向总部购买`license`，另一方面想办法搞到一个`license`先用起来，毕竟笔者只是用于调研，并不一定以后就会用到这个`license`，甚至这款服务器。本文主要分享一下笔者破解超威服务器`IPMI license`的方法和过程。\n\n<!--more-->\n\n## 概述\n\n`IPMI`是一种远程管理服务器的机制，嵌入到一个芯片中，该芯片与操作系统可访问的典型资源分离。它允许远程管理服务器，即使关闭了它。当您的服务器没有响应，您不想或无法亲自到那里进行故障排除时，它非常有用。你甚至可以通过`IPMI`安装操作系统，启动服务器，甚至进入`BIOS`。\n\n本文大部分参考[Peter分享][1]，在此基础之上进行翻译、完善、实践和优化。感谢`Peter`的分享和推特上耐心的回复。大家有兴趣深入的，可进入[原文][1]详细阅读。\n\n## 方法\n\n破解了`license`的生成算法，**基于`MAC`地址，做`SHA1`，`SECRET KEY`为`85 44 E3 B4 7E CA 58 F9 58 30 43 F8`。**\n\n```\n下面会介绍几种破解方法，但都基于以上的算法和SECRET KEY。\n```\n### 方法1\n\n大家可以进入https://cryptii.com/，最左边选择`Bytes`，填写`IPMI`的`MAC`地址，中间选择`HMAC`，然后填写上述的`SECRET KEY`，算法选择`SHA-1`，`license`的结果就会在最右边出现！！！\n\n<center>![generate license][2]</center>\n\n这种方式生成的`license`在`supermicro` `2013~2018`期间的主板上测试都成功了，说明`supermicro`内部没有改变算法，并且一直用的同样的`SECRET KEY`。填写`6`组`key`来激活`license`如下图：\n\n<center>![license key target][3]</center>\n\n### 方法2\n\n一个推特用户`@astraleureka`分享了他通过`perl`脚本生成`license`的方式，脚本代码如下：\n\n```\n#!/usr/bin/perl\nuse strict;\nuse Digest::HMAC_SHA1 'hmac_sha1';\nmy $key  = \"\\x85\\x44\\xe3\\xb4\\x7e\\xca\\x58\\xf9\\x58\\x30\\x43\\xf8\";\nmy $mac  = shift || die 'args: mac-addr (i.e. 00:25:90:cd:26:da)';\nmy $data = join '', map { chr hex $_ } split ':', $mac;\nmy $raw  = hmac_sha1($data, $key);\nprintf \"%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX\\n\", (map { ord $_ } split '', $raw);\n```\n\n但是经笔者实践发现脚本执行后第五行报错，稍作修改第五行如下（`MAC地址就采用举例中的`）：\n\n```\n#!/usr/bin/perl\nuse strict;\nuse Digest::HMAC_SHA1 'hmac_sha1';\nmy $key  = \"\\x85\\x44\\xe3\\xb4\\x7e\\xca\\x58\\xf9\\x58\\x30\\x43\\xf8\";\nmy $mac  = shift || '00:25:90:cd:26:da';\nmy $data = join '', map { chr hex $_ } split ':', $mac;\nmy $raw  = hmac_sha1($data, $key);\nprintf \"%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX-%02lX%02lX\\n\", (map { ord $_ } split '', $raw);\n```\n\n修改了脚本还不够，还需要在环境上安装`perl-Digest-HMAC`来支持破解算法\n\n```\nyum install -y perl-Digest-HMAC\n```\n\n执行结果如下：\n\n```\n➜  ~ ./test.pl \n857A-B7A9-4A4C-103E-3A8C-C044\n```\n\n### 方法3\n\n还有`shell`脚本方式，通过`openssl`来实现，脚本代码如下：\n\n```\necho -n 'bmc-mac' | xxd -r -p | openssl dgst -sha1 -mac HMAC -macopt hexkey:8544E3B47ECA58F9583043F8 | awk '{print $2}' | cut -c 1-24\n```\n\n执行结果如下：\n\n```\n➜  ~ echo -n '00:25:90:cd:26:da' | xxd -r -p | openssl dgst -sha1 -mac HMAC -macopt hexkey:8544E3B47ECA58F9583043F8 | awk '{print $2}' | cut -c 1-24\n857ab7a94a4c103e3a8cc044\n```\n\n## 过程\n\n破解过程相比上面的破解方法要复杂的多，建议非专业人员跳过以下环节。\n\n### 步骤1：下载和解压固件\n\n`Supermicro`在[其网站][5]上免费提供了`IPMI`升级文件，你只需要根据自己的主板类型选择对应的升级文件进行下载。解压后会发现有一个很大的固件二进制文件，比如`REDFISH_X10_366.bin`如下图：\n\n<center>![firmware][4]</center>\n\n然后将该文件上传至`linux`环境中，通过[binwalk][6]工具扫描二进制文件并且寻找已知格式的签名：\n\n```\n➜  redfish binwalk REDFISH_X10_380.bin \nDECIMAL       HEXADECIMAL     DESCRIPTION\n--------------------------------------------------------------------------------\n103328        0x193A0         CRC32 polynomial table, little endian\n4194304       0x400000        CramFS filesystem, little endian, size: 15200256, version 2, sorted_dirs, CRC 0x96C173E0, edition 0, 8603 blocks, 1099 files\n20971520      0x1400000       uImage header, header size: 64 bytes, header CRC: 0xCD918E89, created: 2019-02-14 08:41:03, image size: 1537490 bytes, Data Address: 0x40008000, Entry Point: 0x40008000, data CRC: 0xB8D13E99, OS: Linux, CPU: ARM, image type: OS Kernel Image, compression type: gzip, image name: \"21400000\"\n20971584      0x1400040       gzip compressed data, maximum compression, has original file name: \"linux.bin\", from Unix, last modified: 2019-02-14 08:18:16\n24117248      0x1700000       CramFS filesystem, little endian, size: 7376896, version 2, sorted_dirs, CRC 0xE5B38117, edition 0, 3029 blocks, 445 files\n```\n\n可以看出该二进制文件主要包括两个`cramfs`文件系统，这里面包含的就是`IPMI`系统文件。原文用的是`HxD`工具来解析，笔者试了不好用，就直接采用`dd`的方式将两个`cramfs`写到两个单独文件，然后再以`cramfs`的方式挂载出来。`skip`表示起始值，对应上面的`DECIMAL`一列，`count`表示大小，对应上面的`size`。(参考[Getting a root shell on the IPMI][14])\n\n```\n➜  dd if=REDFISH_X10_380.bin bs=1 skip=4194304 count=15200256 of=cramfs1\n➜  dd if=REDFISH_X10_380.bin bs=1 skip=24117248 count=7376896 of=cramfs2\n➜  mkdir mnt1 mnt2\n➜  mount -o loop -t cramfs cramfs1 mnt1\n➜  mount -o loop -t cramfs cramfs2 mnt2\n```\n\n到这一步。恭喜你获得了`IPMI`系统的所有文件！\n\n<center>![IPMI Files][7]</center>\n\n### 步骤2：反编译\n\n查看前端页面操作接口对应的`HTML/JS`代码是非常容易的：通过浏览器内置的开发者工具（`F12`）去寻找前端代码，然后再去上面的`IPMI`文件系统中的代码文件寻找对应的后端代码。\n\n如你所见，`IPMI`调用`/cgi/ipmi.cgi`来坚持`license key`是否合法\n\n<center>![font code][8]</center>\n\n以下是从网站上找到的一些信息:\n\n<center>![payload][9]</center>\n\n响应是`XML`，如果无效检查设置为`0`，如果有效检查设置为`1`(奇怪的是，他们没有使用`JSON`):\n\n<center>![check response][10]</center>\n\n接下来，我们需要使用`IDA Pro`打开文件`ipmi.cfg`。它存储在`IPMI`文件系统中，我们在前面的步骤中提取了它。下面你可以看到处理`license`检查的代码。通过阅读这段代码，您可以看到`license`应该是什么样子的。第一个循环是`hex-decoding`输入,即文本`1234-00-FF-0000-0000-0000-0000`变成二进制(`12`字节)：`12 34 00 FF 00 00 00 00 00 00 00 00`。\n\n<center>![ipmi.cfg code][11]</center>\n\n`license`的实际检查在另一个文件`libipmi.so`中完成。它实现了引用的函数`oob_format_license_activate`:\n\n<center>![libipmi.so code][12]</center>\n\n您可以在这里看到实际引用的`license`密钥算法- `HMAC_SHA1`。注意函数调用中的`12`是很重要的，这意味着`96`比特。`96`比特正好是键的长度，用十六进制表示给最终用户。\n\n有趣的是，有一个函数`oob_format_license_create`，它创建`license`，并且更容易阅读。您可以直接看到对私钥的引用。`oob`的意思是`out-of-band`，表示`OEM`，在这里的意思是远程购买的`license`密钥。\n\n<center>![secret key][13]</center>\n\n`Supermicro Secret Key`:\n\n`HSDC`私钥:`39 CB 2A 1A 3D 74 8F F1 DE E4 6b87`\n\n`OOB`私钥:`85 44 E3 B4 7E CA 58 F9 58 30 43 F8`\n\n在这篇博客文章的开头，解释了如何轻松地使用它来创建自己的超微`license`。\n\n## Refer\n\n1. [binwalk Firmware Analysis Tool][6]\n2. [Getting a root shell on the IPMI][14]\n\n## 总结\n\n破解步骤概括一下：\n\n第一步：通过固件解压获取二进制文件，然后通过`binwalk`扫描获取两个`cramfs`文件系统，再通过`dd`得到两个文件，之后通过挂载获取文件系统中的所有文件。\n\n第二步：通过浏览器上请求信息，得到请求的后端代码文件`/cgi/ipmi.cgi`，通过`IDA Pro`反编译后端代码文件，通过逻辑判断找到算法和`secret key`。\n\n最后感慨一下，`twitter`上都能学到这么多，太强了！\n\n[1]: https://peterkleissner.com/2018/05/27/reverse-engineering-supermicro-ipmi/\n[2]: http://cdn.tony-yin.site/generate_license.jpg\n[3]: http://cdn.tony-yin.site/License-Key-Target.jpg\n[4]: http://cdn.tony-yin.site/firmware.jpg\n[5]: https://www.supermicro.com/support/resources/bios_ipmi.php\n[6]: https://github.com/ReFirmLabs/binwalk\n[7]: http://cdn.tony-yin.site/Files-IPMI.jpg\n[8]: http://cdn.tony-yin.site/IPMI_font_code.jpg\n[9]: http://cdn.tony-yin.site/IPMI_payload.jpg\n[10]: http://cdn.tony-yin.site/IPMI_license_check.jpg\n[11]: http://cdn.tony-yin.site/IPMI_cgi_file.jpg\n[12]: http://cdn.tony-yin.site/IPMI_so_file.jpg\n[13]: http://cdn.tony-yin.site/IPMI_secret_key.jpg\n[14]: https://michael.stapelberg.ch/posts/2014-01-27-supermicro_ipmi_openvpn/\n[15]: http://cdn.tony-yin.site/reverse.png\n","tags":["IPMI"],"categories":["tech"]},{"title":"利用软 RAID 实现系统盘高可用","url":"/2019/03/10/ssd_ha/","content":"\n<center>![HA RAID][19]</center>\n\n一套完善的系统理论上是不应该存在任何的单点故障，但是系统盘往往被人所忽略，而系统盘又恰恰是最重要的一个点。本文主要讲解如何利用软`RAID`实现系统盘高可用，并且实现自动换盘、自动告警和自动恢复。\n\n<!--more-->\n\n## 系统盘组成\n\n所有挂载点全部采用`RAID1`方式保证数据冗余，即使其中一块盘损坏，也不会影响操作系统的正常运行，只需要替换一块新盘，即可重新进行数据同步。\n\nMount Point | Raid | Size\n:--:|:----:|:----:|\n/ | Raid1 | 100 GB\n/boot | Raid1 | 512 MB\n/boot/efi | Raid1 | 200 MB\nswap | Raid1 | 50 GB\n/var/log | Raid1 | 50 GB\n\n## 系统盘软 RAID 配置\n\n进入引导页面，选择`UEFI`安装方式，因为传统的`BIOS`方式在容量和分区上都存在限制，具体请阅读[【聊聊 BIOS、UEFI、MBR、GPT、GRUB……】][3]。\n\n<center>![load uefi][1]</center>\n\n配置软`raid`阶段，`UEFI`存在一个`ESP`（`EFI system partition`）， 即`/boot/efi`分区，`RAID`等级设置为`raid1`。\n\n<center>![uefi partition][2]</center>\n\n其他的挂载点也都选择`RAID1`即可。\n\n```\nsda             8:0    0 447.1G  0 disk\n├─sda4          8:4    0   201M  0 part\n│ └─md123       9:123  0   201M  0 raid1 /boot/efi\n├─sda2          8:2    0  50.1G  0 part\n│ └─md127       9:127  0  50.1G  0 raid1 [SWAP]\n├─sda5          8:5    0    50G  0 part\n│ └─md124       9:124  0    50G  0 raid1 /var/log\n├─sda3          8:3    0   513M  0 part\n│ └─md126       9:126  0 512.4M  0 raid1 /boot\n├─sda1          8:1    0  100.1G  0 part\n  └─md125       9:125  0    100G  0 raid1 /\nsdb             8:0    0 447.1G  0 disk\n├─sdb4          8:4    0   201M  0 part\n│ └─md123       9:123  0   201M  0 raid1 /boot/efi\n├─sdb2          8:2    0  50.1G  0 part\n│ └─md127       9:127  0  50.1G  0 raid1 [SWAP]\n├─sdb5          8:5    0    50G  0 part\n│ └─md124       9:124  0    50G  0 raid1 /var/log\n├─sdb3          8:3    0   513M  0 part\n│ └─md126       9:126  0 512.4M  0 raid1 /boot\n├─sdb1          8:1    0  100.1G  0 part\n  └─md125       9:125  0    100G  0 raid1 /\n```\n \n## 换盘流程\n\n以系统盘为`sda`和`sdb`，并且`sdb`为更换的硬盘为例。\n\n### 拔盘 & 插盘\n\n因为所有挂载点都是`RAID1`，软`RAID`中拥有数据冗余的阵列是允许其中一块盘丢失的，所以不会存在磁盘占用的问题，进而不会导致磁盘乱序的问题。所以可以直接热插拔换盘。\n\n### 克隆磁盘分区表信息\n\n新插入的`sdb`理论上是没有分区的，需要将`sda`上面的分区完完全全的克隆过来。`gpt`磁盘分区表的磁盘应该采用`parted`或`sgdisk`工具。\n\n```\n# 将sda的分区信息克隆到sdb上\nsgdisk -R /dev/sdb /dev/sda\nsleep 5\n```\n\n最好`sleep`几秒，因为它克隆后底层同步工作并没有立即完成。\n\n### 生成新的GUID\n\n克隆分区信息后，为`sdb`生成新的`GUID`，否则会因为克隆分区表导致`sdb`和`sda`的`GUID`一样。\n\n```\nsgdisk -G /dev/sdb\n```\n\n### 内核重新加载分区表\n\n```\npartprobe /dev/sdb\n```\n\n### 复制引导程序\n\n**注意：**\n\n```\n这一步非常关键，可以说这是所有流程中最关键并且最容易忽略的一个步骤。因为软RAID数据冗余，并不会对操作系统引导程序有效，即RAID1不会对BIOS中的MBR做冗余，也不会对UEFI中的ESP分区做冗余。这里说的不冗余是指软RAID不会对其做数据冗余，需要另外做冗余。\n```\n\n如果引导方式是传统的`bios`，则需要复制`MBR`，即硬盘的前`512`字节。\n\n```\n[root@ ~]# dd if=/dev/sda of=/dev/sdb bs=512 count=1 \n```\n\n需要注意的是，我们这里采用了`UEFI`的引导方式，跟`BIOS`是完全不一样的，所以如果同样拷贝硬盘的前`512`字节是不会生效的。`UEFI`的引导程序在`ESP`中，需要复制整个`ESP`分区。\n\n```\n[root@ ~]# dd if=/dev/sda of=/dev/sdb\n```\n\n你以为这就结束了吗？\n\n`UEFI`引导方式仅仅复制`ESP`分区还不够，还需要将系统盘添加到启动项中。因为当一块磁盘拔了再插上后，**原先这块盘就会从启动项中移除**，插盘后需要将新盘再添加到启动项中。\n\n```\n[root@ ~]# efibootmgr -c -g -d /dev/sdb -p 1 -L \"Centos #2\" -l '\\EFI\\centos\\grubx64.efi'\n```\n\n想对`efibootmgr`有更深入了解，可以阅读：\n\n* [Efibootmgr wiki][4]\n* [用efibootmgr管理UEFI启动项，添加丢失的启动项][5]\n\n### 数据同步\n\n将替换的磁盘的分区对应加入`RAID1`中，这样就可以将`sda`中的数据同步至`sdb`中，同步完成后，所有阵列又将拥有数据冗余的效果。\n\n```\n[root@ ~]# mdadm /dev/md123 -a /dev/sdb4\n[root@ ~]# mdadm /dev/md124 -a /dev/sdb5\n[root@ ~]# mdadm /dev/md125 -a /dev/sdb1\n[root@ ~]# mdadm /dev/md126 -a /dev/sdb3\n[root@ ~]# mdadm /dev/md127 -a /dev/sdb2\n```\n\n### 同步配置文件\n\n每次修改软`RAID`后，都要实时更新配置文件，方便查看`RAID`配置或利用配置文件重新组装阵列。\n\n```\n[root@ ~]# mdadm -Ds > /etc/mdadm.conf\n```\n\n### 获取进度值\n\n通过查看`/proc/mdstat`查看`RAID`当前信息，如果存在数据同步，会有`recovery`的字样，并且`[1/2]`表示还未同步，`[_U]`表示前面一个设备不是活跃状态，后一个设备为活跃状态。所以`recovery`同行的进度值并不是整体`RAID`同步进度值，只是当前的`RAID`的进度，所有阵列的同步进度值可以通过 `Finish Blocks / All Blocks`来计算。\n\n```\n[root@ ~]# cat /proc/mdstat\nPersonalities : [raid0] [raid1]\nmd123 : active raid1 sdb4[1] sda4[0]\n      205760 blocks super 1.0 [1/2] [_U]\n      bitmap: 0/1 pages [0KB], 65536KB chunk\n\nmd124 : active raid1 sda5[0] sdb5[1]\n      20971520 blocks super 1.2 [1/2] [_U]\n      bitmap: 1/1 pages [4KB], 65536KB chunk\n\nmd125 : active raid1 sda1[0] sdb1[1]\n      83886080 blocks 64K chunks 2 near-copies [1/2] [_U] [=======>........]  recovery = 35.6% (29863444/83886080) finish=0.1min speed=93472K/sec\n      bitmap: 1/1 pages [4KB], 65536KB chunk\n\nmd126 : active raid1 sdb3[1] sda3[0]\n      524736 blocks super 1.2 [1/2] [_U]\n      bitmap: 0/1 pages [0KB], 65536KB chunk\n\nmd127 : active raid1 sda2[0] sdb2[1]\n      104923136 blocks super 1.2 [1/2] [_U]\n      bitmap: 1/1 pages [4KB], 65536KB chunk\n\nunused devices: <none>\n```\n\n## 监控 & 告警\n\n要达到对系统盘更好的维护，监控和告警是必不可少的。\n\n### 磁盘健康告警\n\n可以通过`smartctl`工具获取磁盘健康状态，不同型号的磁盘获取到的健康信息可能会不一致，如果磁盘状态健康，一般会返回`PASSED`或`OK`，如果状态不健康，直接调用邮件接口即可。\n\n```\n[root@ ~]# smartctl -H /dev/sda\nsmartctl 6.2 2017-02-27 r4394 [x86_64-linux-4.14.78-201.1.el7.x86_64] (local build)\nCopyright (C) 2002-13, Bruce Allen, Christian Franke, www.smartmontools.org\n\n=== START OF READ SMART DATA SECTION ===\nSMART STATUS RETURN: incomplete response, ATA output registers missing\nSMART overall-health self-assessment test result: PASSED\nWarning: This result is based on an Attribute check.\n```\n\n### 磁盘拔插告警\n\n通过`udev`的机制，编写`add`和`remove`两个`action`的`rules`文件即可监听磁盘拔出或插入的事件，然后调用告警接口即可。\n\n```\n[root@ ~]# cat /etc/udev/rules.d/50-ssd-monitor.rules\nKERNEL==\"sd[a-z]+$\", ACTION==\"remove\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /usr/lib/python2.7/site-packages/disk_watcher/os_disk.py %k pullout\"\nKERNEL==\"sd[a-z]+$\", ACTION==\"add\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /usr/lib/python2.7/site-packages/disk_watcher/os_disk.py %k insert\"\n```\n## 总结\n\n本文主要介绍了如何利用软`RAID`实现系统盘高可用，在其中一块系统盘损坏后如何换盘并数据同步做了详细描述，同时也对监控告警做了讲解。总体来说，整个流水线基本上覆盖到了，具体细节部分还需多实践。\n\n\n## Refer\n\n1. [How To Copy a GPT Partition Table to Another Disk using sgdisk][3]\n2. [SuSE的软Raid中一块硬盘坏掉后的修复方法][6]\n3. [How to install Ubuntu 14.04/16.04 64-bit with a dual-boot RAID 1 partition on an UEFI/GPT system?][17]\n3. [mdadm: device or resource busy][7]\n4. [Linux硬盘盘符分配][8]\n5. [UEFI via software RAID with mdadm in Ubuntu 16.04][9]\n6. [What's the difference between creating mdadm array using partitions or the whole disks directly][10]\n7. [XenServer 6.2 with Software RAID][11]\n8. [UEFI boot fails when cloning image to new machine][12]\n9. [How to correctly install GRUB on a soft RAID 1?][13]\n10. [How to boot after RAID failure (software RAID)?][14]\n11. [mdadm raid 1 grub only on sda][15]\n12. [Can the EFI system partition be RAIDed?][16]\n13. [Partitioning EFI machine with two SSD disks in mirror][18]\n\n\n\n\n[1]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/load_uefi.png\n[2]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/uefi_partition.png\n[3]: https://tecadmin.net/copy-a-gpt-partition-table-to-new-disk/\n[4]: https://wiki.gentoo.org/wiki/Efibootmgr\n[5]: https://www.cnblogs.com/pipci/p/8862292.html\n[6]: https://support.huawei.com/enterprise/zh/knowledge/EKB1000033953\n[7]: http://dev.bizo.com/2012/07/mdadm-device-or-resource-busy.html\n[8]: http://ilinuxkernel.com/?p=958#comment-76766\n[9]: https://implement.pt/2018/08/uefi-via-software-raid-with-mdadm-ubuntu-16-04/\n[10]: https://unix.stackexchange.com/questions/320103/whats-the-difference-between-creating-mdadm-array-using-partitions-or-the-whole\n[11]: https://djlab.com/2014/03/xenserver-6-2-with-software-raid/#comments\n[12]: https://askubuntu.com/questions/380447/uefi-boot-fails-when-cloning-image-to-new-machine\n[13]: https://unix.stackexchange.com/questions/230349/how-to-correctly-install-grub-on-a-soft-raid-1\n[14]: https://unix.stackexchange.com/questions/83787/how-to-boot-after-raid-failure-software-raid\n[15]: https://serverfault.com/questions/483141/mdadm-raid-1-grub-only-on-sda\n[16]: https://askubuntu.com/questions/66637/can-the-efi-system-partition-be-raided\n[17]: https://askubuntu.com/questions/660023/how-to-install-ubuntu-14-04-16-04-64-bit-with-a-dual-boot-raid-1-partition-on-an\n[18]: https://unix.stackexchange.com/questions/69214/partitioning-efi-machine-with-two-ssd-disks-in-mirror\n[19]: http://cdn.tony-yin.site/ssd_ha.png\n","tags":["RAID"],"categories":["tech"]},{"title":"软 RAID 详解","url":"/2019/02/10/software_raid/","content":"\n<center>![RAID][17]</center>\n\n这篇文章聊聊软`RAID`。\n\n<!--more-->\n\n## RAID & 软 RAID\n\n### RAID\n\n随着数据日益增长，单块硬盘往往因为容量小而不能满足大部分人的需求，于是`RAID`就应运而生。`RAID`（ `Redundant Array of Independent Disks`）即独立磁盘冗余阵列，简称磁盘阵列。简单地说，`RAID`是由多个独立的高性能磁盘驱动器组成的磁盘组，从而提供比单个磁盘更高的存储性能和数据冗余的技术。\n\n#### Tips：JBOD\n\n这里要提一下`JBOD`（`Just a Bunch of Disks`）。`JBOD`将多个物理磁盘串联起来，提供一个巨大的逻辑磁盘。`JBOD`的数据存放机制是由第一块磁盘开始按顺序往后存储，当前磁盘存储空间用完后，再依次往后面的磁盘存储数据。`JBOD`存储性能完全等同于单块磁盘，而且也不提供数据冗余。它只是简单提供一种扩展存储空间的机制，`JBOD`可用存储容量等于所有成员磁盘的存储空间之和，即拥有容量叠加的作用。目前`JBOD`常指磁盘柜，而不论其是否提供`RAID`功能。\n\n#### 关键技术\n\n`RAID`中主要有三个关键概念和技术：镜像（`Mirroring`）、数据条带（`Data Stripping`）和数据校验（`Data parity`）。\n\n* **镜像**，将数据复制到多个磁盘，一方面可以提高可靠性，另一方面可并发从两个或多个副本读取数据来提高读性能。显而易见，镜像的写性能要稍低，确保数据正确地写到多个磁盘需要更多的时间消耗。\n* **数据条带**，将数据分片保存在多个不同的磁盘，多个数据分片共同组成一个完整数据副本，这与镜像的多个副本是不同的，它通常用于性能考虑。数据条带具有更高的并发粒度，当访问数据时，可以同时对位于不同磁盘上数据进行读写操作，从而获得非常可观的`I/O`性能提升。\n* **数据校验**，利用冗余数据进行数据错误检测和修复，冗余数据通常采用海明码、异或操作等算法来计算获得。利用校验功能，可以很大程度上提高磁盘阵列的可靠性、鲁棒性和容错能力。不过，数据校验需要从多处读取数据并进行计算和对比，会影响系统性能。\n\n不同等级的`RAID`采用一个或多个以上的三种技术，来获得不同的数据可靠性、可用性和`I/O`性能。至于采用何种模式的`RAID`，需要在深入理解系统需求的前提下进行合理选择，综合评估可靠性、性能和成本来进行折中的选择。\n\n<center>![raid models][5]</center>\n\n#### 优势\n\n随着处理器、内存、计算机接口等技术的不断发展，`RAID`不断地发展和革新，在计算机存储领域得到了广泛的应用，从高端系统逐渐延伸到普通的中低端系统。`RAID`技术如此流行，源于其具有显著的特征和优势，基本可以满足大部分的数据存储需求。总体说来，`RAID`主要优势有如下几点\n\n1. **大容量**，`RAID`扩大了磁盘的容量，由多个磁盘组成的`RAID`系统具有海量的存储空间。现在单个磁盘的容量就可以到`10TB`以上，这样`RAID`的存储容量就可以达到`PB`级，大多数的存储需求都可以满足。一般来说，`RAID`可用容量要小于所有成员磁盘的总容量。不同等级的`RAID`算法需要一定的冗余开销，具体容量开销与采用算法相关。如果已知`RAID`算法和容量，可以计算出`RAID`的可用容量。通常，`RAID`容量利用率在`50%` ~ `90%`之间。\n2. **高性能**，`RAID`的高性能受益于数据条带化技术。单个磁盘的`I/O`性能受到接口、带宽等计算机技术的限制，性能往往很有 限，容易成为系统性能的瓶颈。通过数据条带化，`RAID`将数据`I/O`分散到各个成员磁盘上，从而获得比单个磁盘成倍增长的聚合`I/O`性能。\n3. **可靠性**，可用性和可靠性是`RAID`的另一个重要特征。从理论上讲，由多个磁盘组成的`RAID`系统在可靠性方面应该比单个磁盘要差。这里有个隐含假定：单个磁盘故障将导致整个`RAID`不可用。`RAID`采用镜像和数据校验等数据冗余技术，打破了这个假定。镜像是最为原始的冗余技术，把某组磁盘驱动器上的数据完全复制到另一组磁盘驱动器上，保证总有数据副本可用。 比起镜像`50%`的冗余开销，数据校验要小很多，它利用校验冗余信息对数据进行校验和纠错。`RAID`冗余技术大幅提升数据可用性和可靠性，保证了若干磁盘出错时，不会导致数据的丢失，不影响系统的连续运行。\n4. **可管理性**，实际上，`RAID`是一种虚拟化技术，它对多个物理磁盘驱动器虚拟成一个大容量的逻辑驱动器。对于外部主机系统来说，`RAID`是一个单一的、快速可靠的大容量磁盘驱动器。这样，用户就可以在这个虚拟驱动器上来组织和存储应用系统数据。从用户应用角度看，可使存储系统简单易用，管理也很便利。由于`RAID`内部完成了大量的存储管理工作，管理员只需要管理单个虚拟驱动器，可以节省大量的管理工作。`RAID`可以动态增减磁盘驱动器，可自动进行数据校验和数据重建，这些都可以大大简化管理工作。\n\n<center>![raid advantages][4]</center>\n\n由于本篇文章主要讲解的是软`RAID`，所以就不展开赘述，对`RAID`想要深入了解或对`RAID`各种模式感兴趣的，可以阅读以下链接：\n\n* [RAID 有哪几种？有什么区别？][1]\n* [RAID 技术详解][2]\n* [RAID 及 mdadm 命令][3]\n\n\n### 软 RAID\n\n\n`RAID`分为软件`RAID`和硬件`RAID`。\n\n### 硬 RAID\n\n硬件磁盘阵列（`hardwareRAID`）是通过**磁盘阵列卡**（`RAID`卡）来创建磁盘阵列的。在`RAID`卡上会有一个专门的芯片来处理`RAID`任务，可以大大减轻原来系统的计算压力和`I/O`压力，效率也会得到很好的提升；同时现在大部分的`RAID`卡都支持热拔插，所以在更换损坏磁盘时是非常方便的。但是不好的一点是`RAID`卡**比较贵**，特别是那些好一点的`RAID`卡就会特别贵，而且功能还不能保证特别齐全，所以在这种情况下就出现了软件`RAID`，来模拟硬件`RAID`。\n\n\n### 软 RAID\n\n软件`RAID`是一种模拟硬件`RAID`的产物，运行在操作系统上面，所有任务的处理都由`CPU`来完成，所以需要占用一定的系统资源，特别是计算和`I/O`资源，所以性能不如硬`RAID`，但软`RAID`实现简单，不需要额外的硬件设备。。虽然说现在的机器计算速度都非常快了，但是对于一些要求较高的场景或企业，一般最好还是尽量不要选择软件`RAID`的，毕竟在总的使用成本上来说硬件`RAID`性价比还是蛮高的（硬`RAID`通过`RAID`卡解决了很多管理问题，大大地降低了后期运维成本）。\n\n### 硬软 RAID 优劣比较\n\n硬`RAID`优势：\n\n* **性能**，所有`RAID`操作都由`RAID`控制芯片完成，不占用任何`CPU`和内存资源，而且`RAID`卡上一般还有额外的`cache`进一步提升性能，同时拥有独立的电池对缓存进行供电保护，系统重启/电源恢复的时候可以把缓存中未写入硬盘的数据写入硬盘。\n* **功能强大**，硬`RAID`可以支持所有与`RAID`相关的功能和所有`RAID`级别，而软`RAID`只能支持几种基础的`RAID`级别，并且不具有硬`RAID`的许多高级功能，比如点灯、缓存、热插拔等等。\n* **兼容性**，硬`RAID`独立于系统，例如`windows/linux`都可以用，本身拥有`CPU`、内存、电池等。软`RAID`依赖于操作系统，如果操作系统出问题，软`RAID`就挂了。硬`RAID`的兼容性要好很多，万一系统出了问题，也可以很方便地做系统维护和数据恢复。\n\n软`RAID`优势：\n\n* **免费！**\n* **免费！**\n* **免费！**\n\n重要的事情说三遍，软`RAID`最大的优势就是便宜！！！\n\n总的来说，硬`RAID`功能上是完爆软`RAID`的，但是硬`RAID`比较昂贵，一般的`RAID`都需要几千元。企业级的核心应用推荐采用硬`RAID`的方案，花钱解决一切烦恼。而一些不是特别核心的应用或者个人推荐采用软`RAID`，软`RAID`目前大部分操作系统都支持，并且对基础的`RAID`的功能支持的也不错，能够满足大部分需求，最关键的还是不要钱！！！\n\n## mdadm\n\n`mdadm`是`Linux`操作系统上开源的软`RAID`用户层管理工具，允许用户创建和管理软`RAID`磁盘阵列。\n\n目前，`mdadm`支持大部分常见的阵列类型，如`JBOD`、`RAID0`、`RAID1`、`RAID4`、`RAID5`、`RAID6`、`MULTIPATH`、`RAID10`等，创建和管理`RAID`也十分的方便，同时，`mdadm`使用非常灵活，磁盘或者分区都可以作为`RAID`的成员盘来创建软`RAID`。主要有`7`种使用模式：\n\n模式名字| 主要功能\n:--:|:---:|\nCreate | 使用空闲的设备创建一个新的阵列，每个设备具有元数据块\nAssemble | 将原来属于一个阵列的每个块设备组装为阵列\nBuild | 创建或组装不需要元数据的阵列，每个设备没有元数据块\nManage | 管理已经存储阵列中的设备，比如增加热备磁盘或者设置某个磁盘失效，然后从阵列中删除这个磁盘\nMisc | 报告或者修改阵列中相关设备的信息，比如查询阵列或者设备的状态信息\nGrow | 改变阵列中每个设备被使用的容量或阵列中的设备的数目\nMonitor | 监控一个或多个阵列，上报指定的事件\n\n### 创建阵列\n\n`mdadm`使用`--create`(或其缩写`-C`)参数来创建新的阵列，并且将一些重要阵列的标识信息作为元数据可以写在每一个底层设备的指定区间。`--level`(或者其缩写`-l`)表示阵列的`RAID`级别，`--chunk`(或者其缩写`-c`)表示每个条带单元的大小，以`KB`为单位，默认为`64KB`，条带单元的大小配置对不同负载下的阵列读写性能有很大影响。`--raid-devices`(或者其缩写`-n`)表示阵列中活跃的设备个数，而`--spare-devices`(或者其缩写`-x`)表示阵列中热备盘的个数，一旦阵列中的某个磁盘失效，`MD`内核驱动程序自动用将热备磁盘加入到阵列，然后重构丢失磁盘上的数据到热备磁盘上。\n\n```\n创建RAID0：\nmdadm --create /dev/md0 --level=0 --chunk=32 --raid-devices=3 /dev/sd[i-k]1\n创建RAID1：\nmdadm -C /dev/md0 -l1 -c128 -n2 -x1 /dev/sd[i-k]1\n创建RAID5：\nmdadm -C /dev/md0 -l5 -n5 /dev/sd[c-g] -x1 /dev/sdb \n创建RAID6：\nmdadm -C /dev/md0 -l6 -n5 /dev/sd[c-g] -x2 /dev/sdb /dev/sdh\n创建RAID10：\nmdadm -C /dev/md0 -l10 -n6 /dev/sd[b-g] -x1 /dev/sdh\n```\n\n如果一个块设备元数据中含有其他旧阵列信息，这时候会需要交互式命令确认`yes/no`，如果想自动化，通过管道符的方式即可：\n\n```\n采用missing作为其中一个设备，相当于创建单盘raid1\nyes|mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdb1 missing\n```\n\n`Build`模式可以用来创建没有元数据（即没有`superblock`）的`RAID0/1`设备，不能创建`RAID4/5/6/10`等设备。由于不存在元数据块，很多区分和检查工作无法进行，需要对使用设备特别清楚，这种模式不常用，常用的还是`Create`模式，毕竟元数据块还是拥有很大的作用。\n\n```\n[root@test ~]# ./mdadm -BR /dev/md0 -l0 -n6 /dev/sd[b-g]\nmdadm: array /dev/md0 built and started. \n[root@test ~]# ./mdadm -BR /dev/md0 -l1 -n2 /dev/sd[b-c]\nmdadm: array /dev/md0 built and started.\n[root@test ~]# ./mdadm -BR /dev/md0 -l5 -n6 /dev/sd[b-g]\nmdadm: Raid level 5 not permitted with --build.\n[root@test ~]# ./mdadm -BR /dev/md0 –l6 -n6 /dev/sd[b-g]\nmdadm: Raid level 5 not permitted with --build.\n[root@test ~]# ./mdadm -BR /dev/md0 –l10 -n6 /dev/sd[b-g]\nmdadm: Raid level 10 not permitted with --build.\n```\n\n### 查询阵列\n\n#### 通过命令查询阵列信息\n\n通过`mdadm`命令查看指定阵列的**简要**信息(使用`--query`或者其缩写`-Q`)和**详细**信息(使用`--detail`或者其缩写`-D`) 详细信息包括`RAID`的版本、创建的时间、`RAID`级别、阵列容量、可用空间、设备数量、超级块状态、更新时间、`UUID`信息、各个设备的状态、`RAID`算法级别类型和布局方式以及块大小等信息。设备状态信息分为`active`，`sync`，`spare`，`faulty`，`rebuilding`，`removing`等等。\n\n```\n查询简要信息\n[root@test ~]# mdadm --query /dev/md0\n/dev/md0: 2.100GiB raid10 6 devices, 1 spare. Use mdadm --detail for more detail.\n查询详细信息\n[root@test ~]# ./mdadm --detail /dev/md0\n/dev/md0:\n        Version : 00.90.03\n  Creation Time : Sun Aug 22 17:49:53 2018\n     Raid Level : raid10\n     Array Size : 3145536 (3.00 GiB 3.22 GB)\n  Used Dev Size : 1048512 (1024.11 MiB 1073.68 MB)\n   Raid Devices : 6\n  Total Devices : 7\nPreferred Minor : 0\n    Persistence : Superblock is persistent\n \n    Update Time : Sun Aug 22 21:55:02 1999\n          State : clean\n Active Devices : 6\nWorking Devices : 7\n Failed Devices : 0\n  Spare Devices : 1\n \n         Layout : near=2, far=1\n     Chunk Size : 64K\n \n           UUID : 0cabc5e5:842d4baa:e3f6261b:a17a477a\n         Events : 0.122\n \n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       1       8       32        1      active sync   /dev/sdc\n       2       8       48        2      active sync   /dev/sdd\n       3       8       64        3      active sync   /dev/sde\n       4       8       80        4      active sync   /dev/sdf\n       5       8       96        5      active sync   /dev/sdg\n \n       6       8      112        -      spare   /dev/sdh\n```\n\n#### 通过命令查询阵列中设备信息\n\n上面是查询阵列里面的详细信息，我们也可以查询阵列中具体设备的详细信息，使用命令`--examine`(或者其缩写`-E`)来检测当前的块设备上是否有阵列的元数据信息，包括设备`RAID`级别，设备`UUID`，阵列`UUID`等等。\n\n```\n[root@test ~]# mdadm -E /dev/sdb1\n/dev/sdb1:\n          Magic : a92b4efc\n        Version : 1.2\n    Feature Map : 0x1\n     Array UUID : df3ceb66:95a8c503:9c6c41b2:db583c57\n           Name : host244:root\n  Creation Time : Wed Apr 17 18:33:11 2019\n     Raid Level : raid1\n   Raid Devices : 2\n\n Avail Dev Size : 167772160 (80.00 GiB 85.90 GB)\n     Array Size : 83886080 (80.00 GiB 85.90 GB)\n    Data Offset : 131072 sectors\n   Super Offset : 8 sectors\n   Unused Space : before=130992 sectors, after=0 sectors\n          State : clean\n    Device UUID : 8054a66a:962c52e8:be0c8425:db207be4\n\nInternal Bitmap : 8 sectors from superblock\n    Update Time : Sat May  4 16:13:56 2019\n  Bad Block Log : 512 entries available at offset 16 sectors\n       Checksum : 6af807c8 - correct\n         Events : 207\n\n\n   Device Role : Active device 1\n   Array State : AA ('A' == active, '.' == missing, 'R' == replacing)\n```\n\n#### 通过文件查询阵列信息\n\n我们还可以通过`cat /proc/mdstat`查看所有运行的`RAID`阵列的状态，在第一行中首先是`MD`的设备名，`active`和`inactive`选项表示阵列是否能读写，接着是阵列的`RAID`级别，后面是属于阵列的块设备，方括号`[]`里的数字表示设备在阵列中的序号，`(S)`表示其是热备盘，`(F)`表示这个磁盘是`faulty`状态。在第二行中首先是阵列的大小，单位是`KB`，接着是`chunk-size`的大小，然后是`layout`类型，不同`RAID`级别的`layout`类型不同，`[6/6]`和`[UUUUUU]`表示阵列有`6`个磁盘并且`6`个磁盘都是正常运行的，而`[5/6]`和`[_UUUUU]`表示阵列有`6`个磁盘中`5`个都是正常运行的，下划线对应的那个位置的磁盘是`faulty`状态的。\n\n```\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid6] [raid5] [raid4] [raid1]\nmd0 : active raid5 sdh[6](S) sdg[5] sdf[4] sde[3] sdd[2] sdc[1] sdb[0]\n      5242560 blocks level 5, 64k chunk, algorithm 2 [6/6] [UUUUUU]\n \nunused devices: <none>\n[root@test ~]# mdadm /dev/md0 -f /dev/sdh /dev/sdb\nmdadm: set /dev/sdh faulty in /dev/md0\nmdadm: set /dev/sdb faulty in /dev/md0\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid6] [raid5] [raid4] [raid1]\nmd0 : active raid10 sdh[6] sdb[7](F) sdc[0] sdg[5] sdf[4] sde[3] sdd[2]\n      3145536 blocks 64K chunks 2 near-copies [6/5] [U_UUUU]\n      [=======>........]  recovery = 35.6% (373888/1048512) finish=0.1min speed=93472K/sec\nunused devices: <none>\n```\n\n并且还会展示阵列中一些状态，比如`recvoery`，`resync`等等，并且还有详细包括进度值、速度、完成块大小等。\n\n#### 通过文件系统查询\n\n`Linux`系统目前支持`sysfs`也可以访问`/sys/block/md0`目录查询阵列信息。\n\n```\n[root@test ~]# ls -l /sys/block/md0/\ncapability  holders  range      size    stat       uevent\ndev         md       removable  slaves  subsystem\n[root@test ~]# ls /sys/block/md0/md/\narray_state      dev-sdg           rd1               suspend_lo\nbitmap_set_bits  dev-sdh           rd2               sync_action\nchunk_size       layout            rd3               sync_completed\ncomponent_size   level             rd4               sync_speed\ndev-sdb          metadata_version  rd5               sync_speed_max\ndev-sdc          mismatch_cnt      reshape_position  sync_speed_min\ndev-sdd          new_dev           resync_start\ndev-sde          raid_disks        safe_mode_delay\ndev-sdf          rd0               suspend_hi\n[root@test ~]# ls /sys/block/md0/slaves/\nsdb  sdc  sdd  sde  sdf  sdg  sdh\n```\n\n### 停止 & 删除阵列\n\n当阵列没有文件系统或者其他存储应用以及高级设备使用的话，可以使用`--stop`(或者其缩写`-S`)停止阵列；如果命令返回设备或者资源忙类型的错误，说明`/dev/md0`正在被上层应用使用，暂时不能停止，必须要首先停止上层的应用，这样也能保证阵列上数据的一致性。\n\n```\n[root@test ~]# mdadm --stop /dev/md0\nmdadm: fail to stop array /dev/md0: Device or resource busy\n[root@test ~]# umount /dev/md0\n[root@test ~]# mdadm --stop /dev/md0 \nmdadm: stopped /dev/md0\n```\n\n上面只是停止阵列，阵列其中的设备还存在着阵列元数据信息，如果需要彻底删除阵列，还需要`--zero-superblock`将阵列中所有设备的元数据块删除。\n\n```\n[root@test ~]# mdadm --zero-superblock /dev/sdb\n```\n\n### 组装阵列\n\n模式`--assemble`或者其缩写(`-A`)主要是检查底层设备的元数据信息，然后再组装为活跃的阵列。如果我们已经知道阵列由那些设备组成，可以指定使用那些设备来启动阵列。\n\n```\n[root@test ~]# mdadm -A /dev/md0 /dev/sd[b-h]\nmdadm: /dev/md0 has been started with 6 drives and 1 spare.\n```\n\n可以通过配置文件`/etc/mdadm.conf`重新组装，`mdadm`先检`查mdadm.conf`中的`DEVICE`信息，然后从每个设备上读取元数据信息，并检查是否和`ARRAY`信息一致，如果信息一致则启动阵列。\n\n```\nmdadm -As /dev/md0\n```\n\n如果没有配置`/etc/mdadm.conf`文件，而且又不知道阵列由那些磁盘组成，则可以使用上面提到的查询命令`--examine`(或者其缩写`-E`)来检测当前的块设备上是否有阵列的元数据信息，找到阵列的唯一标识`UUID`和阵列包含的设备名字，然后再使用上面的命令来组装阵列，也可以使用`UUID`标识来组装阵列。没有一致的元数据的信息设备(例如`/dev/sda`和`/dev/sda1`等)`mdadm`程序会自动跳过。\n\n```\n[root@test ~]# ./mdadm -Av --uuid=0cabc5e5:842d4baa:e3f6261b:a17a477a /dev/md0 /dev/sd*\nmdadm: looking for devices for /dev/md0\nmdadm: no recogniseable superblock on /dev/sda\nmdadm: /dev/sda has wrong uuid.\nmdadm: no recogniseable superblock on /dev/sda1\nmdadm: /dev/sda1 has wrong uuid.\nmdadm: no RAID superblock on /dev/sdi\nmdadm: /dev/sdi has wrong uuid.\nmdadm: /dev/sdi1 has wrong uuid.\nmdadm: no RAID superblock on /dev/sdj\nmdadm: /dev/sdj has wrong uuid.\nmdadm: /dev/sdj1 has wrong uuid.\nmdadm: no RAID superblock on /dev/sdk\nmdadm: /dev/sdk has wrong uuid.\nmdadm: /dev/sdk1 has wrong uuid.\nmdadm: /dev/sdb is identified as a member of /dev/md0, slot 0.\nmdadm: /dev/sdc is identified as a member of /dev/md0, slot 1.\nmdadm: /dev/sdd is identified as a member of /dev/md0, slot 2.\nmdadm: /dev/sde is identified as a member of /dev/md0, slot 3.\nmdadm: /dev/sdf is identified as a member of /dev/md0, slot 4.\nmdadm: /dev/sdg is identified as a member of /dev/md0, slot 5.\nmdadm: /dev/sdh is identified as a member of /dev/md0, slot 6.\nmdadm: added /dev/sdc to /dev/md0 as 1\nmdadm: added /dev/sdd to /dev/md0 as 2\nmdadm: added /dev/sde to /dev/md0 as 3\nmdadm: added /dev/sdf to /dev/md0 as 4\nmdadm: added /dev/sdg to /dev/md0 as 5\nmdadm: added /dev/sdh to /dev/md0 as 6\nmdadm: added /dev/sdb to /dev/md0 as 0\nmdadm: /dev/md0 has been started with 6 drives and 1 spare.\n```\n\n#### 配置文件：\n\n`/etc/mdadm.conf`作为默认的配置文件，主要作用是方便跟踪软`RAID`的配置，尤其是可以配置监视和事件上报选项。`Assemble`命令也可以使用`--config`(或者其缩写`-c`)来指定配置文件。我们通常可以如下命令来建立配置文件。\n\n```\n[root@test ~]# mdadm -Ds > /etc/mdadm.conf\n[root@test ~]# cat /etc/mdadm.conf\n# mdadm.conf written out by anaconda\nMAILADDR root\nAUTO +imsm +1.x -all\nARRAY /dev/md/1 level=raid1 num-devices=2 UUID=fe796549:cdbe506c:51e5404e:fa40fee6\nARRAY /dev/md/2 level=raid1 num-devices=2 UUID=60cecdc1:f115d9e5:f9ddfb52:c3c035ab\nARRAY /dev/md/3 level=raid1 num-devices=2 UUID=df3ceb66:95a8c503:9c6c41b2:db583c57\nARRAY /dev/md/4 level=raid0 num-devices=2 UUID=7c5f42c1:9b013ec0:b17b7337:33df22ca\nARRAY /dev/md/5 level=raid1 num-devices=2 UUID=4cbb3226:c5acf9e3:fb156e23:9aa56a48\n```\n\n使用配置文件启动阵列时，`mdadm`会查询配置文件中的设备和阵列内容，然后启动运行所有能运行`RAID`阵列。如果指定阵列的设备名字，则只启动对应的阵列。\n\n```\n[root@test ~]# mdadm -As\nmdadm: /dev/md1 has been started with 3 drives.\nmdadm: /dev/md0 has been started with 6 drives and 1 spare.\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid0] [raid10]\nmd0 : active raid10 sdb[0] sdh[6](S) sdg[5] sdf[4] sde[3] sdd[2] sdc[1]\n      3145536 blocks 64K chunks 2 near-copies [6/6] [UUUUUU]\n \nmd1 : active raid0 sdi1[0] sdk1[2] sdj1[1]\n      7337664 blocks 32k chunks\n \nunused devices: <none>\n[root@test ~]# mdadm -S /dev/md0 /dev/md1\nmdadm: stopped /dev/md0\nmdadm: stopped /dev/md1\n[root@test ~]# mdadm -As /dev/md0\nmdadm: /dev/md0 has been started with 6 drives and 1 spare.\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid0] [raid10]\nmd0 : active raid10 sdb[0] sdh[6](S) sdg[5] sdf[4] sde[3] sdd[2] sdc[1]\n      3145536 blocks 64K chunks 2 near-copies [6/6] [UUUUUU]\n \nunused devices: <none>\n```\n\n**注意：**\n这个配置文件不是随着阵列信息随时变化的，而是可以手动修改的。所以根据配置文件组装未必百分百成功，因为之前可能阵列被手动修改过，所以建议在每一次改变阵列信息后，最好同步一下当前的阵列信息到配置文件中。\n\n```\n[root@test ~]# mdadm -Ds > /etc/mdadm.conf\n```\n\n### 管理阵列\n\n`mdadm`可以在`Manage`模式下，对运行中的阵列进行添加及删除磁盘。常用于标识`failed`磁盘，增加`spare`(热备)磁盘，以及从阵列中移走已经失效的磁盘等等。 使用`--fail`(或者其缩写`-f`)指定磁盘损坏。\n\n```\n[root@test ~]# mdadm /dev/md0 --fail /dev/sdb\nmdadm: set /dev/sdb faulty in /dev/md0\n```\n\n当磁盘已经损坏时，使用`--remove`(或者其缩写`-f`)参数将这个磁盘从磁盘阵列中移走；但如果设备还正在被阵列使用（即不处于失效状态），则不能从阵列中移走。\n\n```\n[root@test ~]# mdadm /dev/md0 --remove /dev/sdb\nmdadm: hot removed /dev/sdb\n[root@test ~]# mdadm /dev/md0 --remove /dev/sde\nmdadm: hot remove failed for /dev/sde: Device or resource busy\n```\n\n如果阵列带有`spare`磁盘，那么自动将损坏磁盘上的数据重构到新的`spare`磁盘上\n\n```\n[root@test ~]# mdadm -f /dev/md0 /dev/sdb ; cat /proc/mdstat\nmdadm: set /dev/sdb faulty in /dev/md0\nPersonalities : [raid0] [raid10]\nmd0 : active raid10 sdh[6] sdb[7](F) sdc[0] sdg[5] sdf[4] sde[3] sdd[2]\n      3145536 blocks 64K chunks 2 near-copies [6/5] [U_UUUU]\n      [=======>........]  recovery = 35.6% (373888/1048512) finish=0.1min speed=93472K/sec\n \nunused devices: <none>\n```\n\n还可以使用`--add`(或者其缩写`-a`)参数增加磁盘：\n\n```\n[root@test ~]# mdadm /dev/md0 --add /dev/sdh\nmdadm: added /dev/sdh\n```\n\n### 扩展阵列\n\n如果在创建阵列时不想使用整个块设备，可以指定用于创建`RAID`阵列每个块设备使用的设备大小。\n\n```\n[root@test ~]# mdadm -CR /dev/md0 -l5 -n6 /dev/sd[b-g] -x1 /dev/sdh --size=102400\n```\n\n然后在阵列需要扩展大小时，使用模式`--grow`(或者其缩写`-G`)以及`--size`参数(或者其缩写`-z`)再加上合适的大小数值就能分别扩展阵列所使用每个块设备的大小。\n\n```\n[root@test ~]# mdadm -Q /dev/md0\n/dev/md0: 500.00MiB raid5 6 devices, 1 spare. Use mdadm --detail for more detail. \n[root@test ~]# mdadm --grow /dev/md0 --size=204800\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid0] [raid10] [raid6] [raid5] [raid4]\nmd0 : active raid5 sdh[6](S) sdg[5] sdf[4] sde[3] sdd[2] sdc[1] sdb[0]\n      1024000 blocks level 5, 64k chunk, algorithm 2 [6/6] [UUUUUU]\n      [============>......]  resync = 69.6% (144188/204800) finish=0.0min speed=10447K/sec\n \nunused devices: <none>\n[root@test ~]# mdadm -Q /dev/md0\n/dev/md0: 1000.00MiB raid5 6 devices, 1 spare. Use mdadm --detail for more detail.\n```\n\n如果上面是文件系统，在设备大小扩展后，文件系统也要同时扩展。\n\n`mdadm`还提供增加或减少阵列中设备个数的功能(`reshape`)，使用模式`—grow`和`--raid-disks`(或者其缩写`-n`)参数再加上合适的设备个数。扩展后阵列中原来的热备盘变为活跃磁盘，所以阵列的设备个数增加，阵列的大小也相应增加。\n\n```\n[root@test ~]# ./mdadm -D /dev/md0\n/dev/md0:\n        Version : 00.90.03\n  Creation Time : Sun Aug 22 22:16:19 1999\n     Raid Level : raid5\n     Array Size : 1024000 (1000.17 MiB 1048.58 MB)\n  Used Dev Size : 204800 (200.03 MiB 209.72 MB)\n   Raid Devices : 6\n  Total Devices : 7\nPreferred Minor : 0\n    Persistence : Superblock is persistent\n \n    Update Time : Sun Aug 22 22:23:46 1999\n          State : clean\n Active Devices : 6\nWorking Devices : 7\n Failed Devices : 0\n  Spare Devices : 1\n \n         Layout : left-symmetric\n     Chunk Size : 64K\n \n           UUID : 53e6395c:1af16258:087cb2a0:b66b087f\n         Events : 0.12\n \n    Number   Major   Minor   RaidDevice State\n       0       8       16        0      active sync   /dev/sdb\n       1       8       32        1      active sync   /dev/sdc\n       2       8       48        2      active sync   /dev/sdd\n       3       8       64        3      active sync   /dev/sde\n       4       8       80        4      active sync   /dev/sdf\n       5       8       96        5      active sync   /dev/sdg\n \n       6       8      112        -      spare   /dev/sdh\n[root@test ~]# ./mdadm --grow /dev/md0 --raid-disks=7\nmdadm: Need to backup 1920K of critical section..\nmdadm: ... critical section passed.\n[root@test ~]# cat /proc/mdstat\nPersonalities : [raid0] [raid10] [raid6] [raid5] [raid4]\nmd0 : active raid5 sdh[6] sdg[5] sdf[4] sde[3] sdd[2] sdc[1] sdb[0]\n      1024000 blocks super 0.91 level 5, 64k chunk, algorithm 2 [7/7] [UUUUUUU]\n      [===>.............]  reshape = 19.4% (40256/204800) finish=0.7min speed=3659K/sec\n \nunused devices: <none>\n```\n\n### 监控阵列\n\n`mdadm`还可以对`RAID`阵列进行监控，监控程序定时查询指定的事件是否发生，然后根据配置来妥善处理。例如当阵列中的磁盘设备出现问题的时候，可以发送邮件给管理员；或者当磁盘出现问题的时候由回调程序来进行自动的磁盘替换，所有监控事件都可以记录到系统日志中。目前`mdadm`支持的事件有：\n\n* `RebuildStarted`\n* `RebuildNN`(`NN is 20, 40, 60, or 80`)\n* `RebuildFinished`\n* `Fail`\n* `FailSpare`\n* `SpareActive`\n* `NewArray`\n* `DegradedArray`\n* `MoveSpare`\n* `SparesMissing`\n* `TestMessage`\n\n如果配置每`300`秒`mdadm`监控进程查询`MD`设备一次，当阵列出现错误，会发送邮件给指定的用户，执行事件处理的程序并且记录上报的事件到系统的日志文件。使用`--daemonise`参数(或者其缩写`-f`)使程序持续在后台运行。如果要发送邮件需要`sendmail`程序运行，当邮件地址被配置为外网地址应先测试是否能发送出去。\n\n```\n[root@test ~]# mdadm --monitor --mail=root@localhost --program=/root/md.sh --syslog --delay=300 /dev/md0 --daemonise\n```\n查看系统日志信息，可以看到哪个阵列或者阵列中的哪个设备发生过的哪些事件。\n\n```\n[root@test ~]# mdadm -f /dev/md0 /dev/sdb\nmdadm: set /dev/sdb faulty in /dev/md0\n[root@test ~]# tail –f /var/log/messages\nAug 22 22:04:12 fc5 mdadm: RebuildStarted event detected on md device /dev/md0\nAug 22 22:04:12 fc5 kernel: md: using maximum available idle IO bandwidth (but not more than 200000 KB/sec) for recovery.\nAug 22 22:04:12 fc5 kernel: md: using 128k window, over a total of 1048512 blocks.\nAug 22 22:04:14 fc5 mdadm: Fail event detected on md device /dev/md0, component device /dev/sdb\nAug 22 22:04:14 fc5 mdadm: Rebuild80 event detected on md device /dev/md0\nAug 22 22:04:16 fc5 mdadm: RebuildFinished event detected on md device /dev/md0\nAug 22 22:04:16 fc5 mdadm: SpareActive event detected on md device /dev/md0, component device /dev/sdh\nAug 22 22:04:16 fc5 kernel: md: md0: recovery done.\n```\n\n回调程序从`mdadm`程序接受两个或者三个参数：事件名字，监控阵列的名字和特殊事件可能使用到的底层块设备名字。上面的事件返回的信息如下：\n\n```\nEventname: RebuildStarted  Device: /dev/md0  next:\nEventname: Fail  Device: /dev/md0  next: /dev/sdb\nEventname: Rebuild80  Device: /dev/md0  next:\nEventname: RebuildFinished  Device: /dev/md0  next:\nEventname:SpareActive  Device: /dev/md0  next: /dev/sdh\n```\n\n## 总结\n\n本文讲述了`RAID`的特点和优势，然后引入硬`RAID`和软`RAID`，并分析这两者的优劣势；最后着重对软`RAID`的创建，删除，停止和扩展等操作进行了介绍和实践。\n\n## Refer\n1. [RAID 有哪几种？有什么区别？][1]\n2. [RAID 技术详解][2]\n3. [磁盘阵列（RAID）的原理与搭建][15]\n4. [Linux 中软件 RAID 的使用][11]\n3. [RAID 及 mdadm 命令][3]\n4. [RAID 方案总结][12]\n1. [软件磁盘阵列（Software RAID）介绍][6]\n2. [软 RAID 和硬 RAID 各有什么优劣？][7]\n3. [硬RAID、软RAID的区别详解][8]\n4. [三种 Linux RAID 简要说明][9]\n6. [XenServer 6.2 with Software RAID][10]\n8. [How to fix linux mdadm inactive array][13]\n9. [针对mdadm的RAID1失效测试][14]\n10. [mdadm wiki][16]\n\n\n[1]: https://www.zhihu.com/question/20131784\n[2]: https://www.cnblogs.com/efforeffor/p/6767161.html\n[3]: https://www.jianshu.com/p/acf495b6a21a\n[4]: http://cdn.tony-yin.site/raid_advantages.png\n[5]: http://cdn.tony-yin.site/raid_models.jpg\n[6]: https://blog.csdn.net/AhhSong/article/details/77898687\n[7]: https://www.zhihu.com/question/19613331\n[8]: http://www.ha97.com/4753.html\n[9]: http://www.jinbuguo.com/storage/raid_types.html\n[10]: https://djlab.com/2014/03/xenserver-6-2-with-software-raid\n[11]: https://www.ibm.com/developerworks/cn/linux/l-cn-raid/index.html\n[12]: https://blog.csdn.net/lcy4599/article/details/51870809\n[13]: http://fibrevillage.com/storage/676-how-to-fix-linux-mdadm-inactive-array\n[14]: http://www.cnblogs.com/felix-zp/p/9738724.html\n[15]: https://www.jianshu.com/p/e8e4c768de97\n[16]: http://www.cnblogs.com/baiyw/p/3470211.html\n[17]: http://cdn.tony-yin.site/raid.png\n","tags":["RAID"],"categories":["tech"]},{"title":"聊聊 BIOS、UEFI、MBR、GPT、GRUB……","url":"/2019/01/10/Talk_BIOS_UEFI_MBR_GPT_GRUB/","content":"\n<center>![bios][32]</center>\n\n最近一直在搞`Linux`系统启动方面的事情，频繁地接触到`BIOS`、`UEFI`、`MBR`、`GPT`、`GRUB`等诸多概念，一下子有些混淆不清，本文就这些概念进行总结，尽量通过容易理解的表达进行讲解，对这些概念有一个清楚的认识。\n\n<!--more-->\n\n## 简介\n\n`BIOS`（`Basic Input/Output System`）和`UEFI`（`Unified Extensible Firmware Interface `）是不同的计算机启动固件（`Fireware`），需要硬件（通常为主板）支持，相互代替的，其中`UEFI` 是比较新的方式。可以通过下面图片对这两个概念先有个直观了解：\n\n`BIOS`:\n\n<center>![bios][1]</center>\n\n`UEFI`:\n\n<center>![uefi][2]</center>\n\n## BIOS\n\n`BIOS（Basic Input/Output System）`：在`IBM PC`兼容系统上使用，是一种业界标准的固件接口。这种主板类型大家应该比较熟悉，老一点的机器都是使用`BIOS`主板。对于支持`BIOS`的主板来讲，`BIOS`只能完成非常基本的硬件检测和初始化，之后的事情就都由`bootloader`来接手。\n\n```\n一句话概括：BIOS只认识设备，不认识分区、不认识文件。\n```\n\n`BIOS`启动的时候，会按照`CMOS`设置里的顺序，挨个查看存储设备的前`512`字节是不是以`0x55 0xAA`结尾，如果不是，那就跳过找下一个设备；如果是的话，则表示这个磁盘可以启动，加载这`512`字节里的代码，执行之后，后面的事，几乎就跟`BIOS`没啥关系了。\n\n至于加载引导代码后面启动什么系统，取决于这`512B`（字节）里存了什么系统的代码。这个代码是各家系统安装程序写进去的，目的是启动自家系统。比如安装`Windows`，这里面就变成了`Windows`的启动代码；\n安装了`Linux`，这里面就会变成`Grub`的启动代码。\n\n## MBR\n\n那么这`512`字节里面到底什么东西呢，看起来如此重要？这就是`MBR`（`Master Boot Record`），主分区引导记录。\n\n### 硬盘分区表 DPT（Disk Partition Table）\n\n一个磁盘抛开物理组成部分的话，它就是一个地址序列，这个序列从`0~n-1`，每个地址包含`512B`（字节）的空间。一般我们将这些地址称作逻辑块地址（`LBA`），每块由`512B`组成。分区表告诉操作系统，磁盘的分区有几个，从哪里开始到哪里结束。当将一个磁盘插入已经含有操作系统的机器上时，操作系统会检索这个磁盘的分区表，并正确认识它的分区结构。一个磁盘是先有分区表，后有分区，然后才有文件系统，有了文件系统才能被操作系统读写删。磁盘分区表格式目前主要有两种，分别是`MBR`分区表和`GPT`分区表。\n\n```\n这边有个概念需要注意下，避免混淆：MBR有两个意思，一个表示主引导记录，一个表示分区表类型，具体区别请看下文。\n```\n\n* `MBR`：主引导记录位置在磁盘的第一个逻辑扇区，即`LBA0`的位置。一个逻辑扇区仅有`512B`（字节） ，`MBR`引导代码占`446B`，`MBR`分区表占`64B`，最后的`magic number`占`2B`（即上述以`0x55 0xAA`结尾的标识符）。因为每个分区只有`16B`大小的分区表记录，所以寻址最大只能到`2.2TB`，并且由于分区表总共只占`64B`，所以`MBR`分区表最多`4`个分区。下图的绿色部分。\n* `PBR`：即`Partition Boot Record`，对应主引导记录，每个分区都存在引导记录，位置在每个磁盘分区的开始部分，占用扇区不定。这个扇区一般保存着操作系统引导程序的所在位置。下图的红色部分。\n* `File System`：操作系统对磁盘的所有操作都需要经过文件系统，删除文件指的是在文件系统里删除文件的索引条目，创建文件就是在在文件系统里添加索引条目并将具体数据写入磁盘。常见的文件系统格式有：`FAT`、`ext4`、`NTFS`。文件系统位置一般在`PBR`之后。下图黄色部分。\n\n<center>![mbr][3]</center>\n\n## BIOS 引导流程\n\n开机后，固化在`ROM`里的`BIOS`就会被加载到内存运行，`BIOS`自检完毕以后加载`COMS`的参数，通过`COMS`的参数，`BIOS`程序加载启动磁盘的`MBR`到内存里运行，运行`MBR`的引导代码，这段代码会查找活动分区（`BIOS`不认识活动分区，但这段代码认识活动分区）的位置，加载并执行活动分区的`PBR`（另一段引导程序），与`MBR`类似，`PBR`在运行后加载操作系统的引导程序到内存运行，例如`Windows`的`bootmgr`或`Linux`的`grub`。当引导程序运行后，操作系统内核就被加载运行，完成从`BIOS`程序中接手的引导流程，整体流程如下图：\n\n<center>![bios loader][5]</center>\n\n`Windows`的`PBR`认识`FAT32`和`NTFS`两种分区，找到分区根目录的`bootmgr`文件，加载、执行`bootmgr`。\n`bootmgr`没了`MBR`和`PBR`的大小限制，**可以做更多的事**。它会加载并分析`BCD`启动项存储。而且`bootmgr`可以跨越磁盘读取文件了。所以无论你有几个磁盘，你在多少块磁盘上装了`Windows`，一个电脑只需要一个`bootmgr`就行了。`bootmgr`会去加载某磁盘某`NTFS`分区的`\\Windows\\System32\\WinLoad.exe`，后面启动`Windows`的事就由`WinLoad.exe`来完成了。\n\n<center>![bios loader partition][6]</center>\n\n**为什么图中有两组虚线？**\n\n因为“启动磁盘”和“装系统的磁盘”可以是同一个磁盘，也可以不是同一个。“启动分区”和“系统分区”可以是不同磁盘的不同分区，也可以是相同磁盘的不同分区，也可以是同一个分区。\n\n这就解释了，为什么有的时候，`Windows`装在磁盘`2`上，却要在`BIOS`里选磁盘`1`启动了。因为`bootmgr`可能在磁盘`1`上。\n\n## UEFI\n\n`UEFI`，全称`Unified Extensible Firmware Interface`，即“统一的可扩展固件接口”，是一种详细描述全新类型接口的标准，是适用于电脑的标准固件接口，旨在代替`BIOS`。此标准由`intel`公司带头组织`UEFI`联盟中的`140`多个技术公司共同创建，其中包括微软公司。`UEFI`旨在提高软件互操作性和解决`BIOS`的局限性。作为传统`BIOS`的继任者，`UEFI`拥有前辈所不具备的诸多功能，比如图形化界面、多种多样的操作方式、允许植入硬件驱动等等。这些特性让`UEFI`相比于传统`BIOS`更加易用、更加多功能、更加方便。而`Windows 8`在发布之初就对外宣布全面支持`UEFI`，这也促使了众多主板厂商纷纷转投`UEFI`，并将此作为主板的标准配置之一。`Intel`更是宣布在`2020`之前实现在芯片上将`BIOS`[全部替换][8]为`UEFI`。对`UEFI`的由来感兴趣的可以阅读一下[【UEFI背后的历史】][7]。\n　　\n`BIOS`只能在`16`位模式下运行，而`UEFI`可以在`32`位或`64`位模式下运行，并且具有比`BIOS`更多的可寻址地址空间，这意味着引导过程更快。这也意味着`UEFI`设置屏幕可以比`BIOS`设置屏幕更光滑，包括图形和鼠标光标支持。但是这不是强制性的，许多`pc`机仍然带有文本模式的`UEFI`设置界面，这些界面看起来和工作起来都像一个旧的`BIOS`设置屏幕。老一点的机器都是使用`BIOS`主板，现在大部分新机器都是采用`UEFI`主板，`UEFI`采用了向后兼容，所以`BIOS`切换为`UEFI`不可以，而`UEFI`切换为`BIOS`是可以的。\n\nUEFI还包含了其他功能，它支持安全引导，这意味着可以检查操作系统的有效性，以确保没有恶意软件篡改引导过程。它可以在`UEFI`固件本身中支持联网特性，这有助于远程故障排除和配置。对于传统`BIOS`，必须坐在物理计算机前才能配置它。虽然目前[有厂商利用这一特性尝试垄断市场][9]，但这不意味着这不是一个很好的功能。\n\n它也不仅仅是`BIOS`替换。`UEFI`本质上是一个运行在`PC`固件之上的微型操作系统，它可以做的比`BIOS`多得多。它可以存储在主板的闪存中，也可以在启动时从硬盘或网络共享加载。一般的`UEFI`系统仅能识别`FAT32`，不同于`BIOS`只能识别固定位置的磁盘引导块。这意味着只要将引导程序放到`FAT32`分区里，`UEFI`系统就能通过分区表的指引找到这个保存着引导程序的`FAT32`分区。`UEFI`系统是由模块化的`C`语言程序写出来的，所以通过添加或修改程序模块就能获得更多的功能。例如支持更多的文件系统，图形界面，甚至能使用浏览器浏览网页。\n\n上面提到`BIOS`分区表类型为`MBR`，只能管理最大`2.2T`的硬盘，并且分区数上限最大为`4`个，尽管后来为了支持更多的分区，引入了扩展分区及逻辑分区的概念，但是换汤不换药，满足不了硬盘容量和分区数目急速增长的需求。而`UEFI`采用`GPT`分区表的方式后，硬盘容量和分区数目几乎没有上限（目前`windows`支持最大`128`个分区）。\n\n对`UEFI`架构感兴趣的可以关注[UEFI架构详解][31]\n\n## GPT\n\n`GPT`，即`Globally Unique Identifier Partition Table`，全局唯一标识码分区表，简称`GPT`或`GUID`分区表，它是`UEFI`规范的一部分。由于`MBR`分区表 的局限性（还有`BIOS`的），而`UEFI BIOS`的推广也为`GPT`的实现打下了坚实的技术基础，`GPT`应运而生。`GPT`由`GPT`头和`GPT`主体，`GPT`备份组成。起始于磁盘`LBA1`的位置，相对的`LBA0`仍然为`MBR`，但是这个`MBR`是被保护的，没有引导代码，仅仅有一个被标识为未知的分区，当支持`GPT`分区表的操作系统检索到这个`MBR`后就会自动忽略并跳到`LBA1`读取`GPT`分区表。如下图，由于篇幅的关系，表的长度比例不等同与实际在磁盘地址里的实际比例。\n\n<center>![gpt][18]</center>\n\n`PMBR`存在的意义是当不支持`GPT`的分区工具试图对硬盘进行操作时（例如`MS-DOS`和Linux的`fdisk`程序），它可以根据这份`PMBR`以传统方式启动，过程和`MBR+BIOS`完全一致，极大地提高了兼容性。而支持`GPT`的系统在检测`PMBR`后会直接跳到GPT表头读取分区表。和`MBR`类似，分区表中存储了某个分区的起始和结束位置及其文件系统属性信息，而分区才是实际存在的物理磁盘的一部分。\n\n`GPT`表头，主要定义了分区表中分区数及每个分区大小，还包含硬盘的容量信息。分区表头还记录了这块硬盘的`GUID`，分区表头位置（总是`LBA1`）和大小，也包含了备份分区表头和分区表的位置和大小信息（`LBA-1~LBA-34`）。同时还储存着它本身和分区表的`CRC32`校验。固件、引导程序和操作系统在启动时可以根据这个校验值来判断分区表是否出错，如果出错，可以使用软件从硬盘最后的备份`GPT`中恢复整个分区表，如果备份`GPT`也校验错误，硬盘将不可使用。\n\n分区表包含分区的类型`GUID`（如：`EFI`系统分区的`GUID`类型是`{C12A7328-F81F-11D2-BA4B-00A0C93EC93B}`），名称，起始终止位置，该分区的`GUID`以及分区属性。\n\n相较于`MBR`，`GPT`具有以下优点：\n\n* 得益于`LBA`提升至`64`位，以及分区表中每项`128`位设定，`GPT`可管理的空间近乎无限大，假设一个扇区大小仍为`512`字节，可表示扇区数为，算下来，可管理的硬盘容量=`18EB`(`1EB`=`1024PB`=`1,048,576TB`)，`2.2T`在它面前完全不在话下。按目前的硬盘技术来看，确实近乎无限。\n\n* 分区数量几乎没有限制，由于可在表头中设置分区数量的大小，如果愿意，设置分区也可以（有人愿意管理这么多分区吗），不过，目前`windows`仅支持最大`128`个分区。\n\n* 自带保险，由于在磁盘的首尾部分各带一个`GPT`表头，任何一个受到破坏后都可以通过另一份恢复，极大地提高了磁盘的抗性。\n\n* 循环冗余检验值针对关键数据结构而计算，提高了数据崩溃的检测几率。\n\n* 尽管目前分区类型不超过百数，`GPT`仍提供了`16`字节的`GUID`来标识分区类型，使其更不容易产生冲突。\n\n* 每个分区都可以拥有一个特别的名字，最长`72`字节，足够满足种奇葩命名需求。\n\n* 完美支持`UEFI`，毕竟它就是`UEFI`规范的衍生品。在将来全行业`UEFI`的情境下，`GPT`必将更快淘汰`MBR`。\n\n## UEFI 引导流程\n\n整个`UEFI`的启动详细流程描述如下：\n\n* 开机后，固化在`ROM`里的`UEFI BIOS`就会被加载到内存运行。\n* `UEFI BIOS`将引导`EFI`系统进行运行。\n* 在`EFI`系统启动后，`GUID`分区表就会被识别，之后`EFI`系统就会通过`.efi`文件启动`Boot Loader`程序加载操作系统内核。\n\n如下图：\n\n<center>![uefi loader][19]</center>\n\n对于分区表格式为`MBR`分区表的磁盘，`UEFI`系统会 先启动`CSM`兼容模式后按传统`BIOS`的步骤加载操作系统的内核。如下图：\n\n<center>![uefi mbr loader][20]</center>\n\n## 如何使用 GPT 方式安装系统\n\n由于`MBR`方式限制重重，所以大部分情况下我们都会选择使用`GPT`分区方式安装系统。通过上面的讲解，我们应该知道`UEFI`方式安装的系统是肯定是使用`GPT`分区方式；而`BIOS`方式安装的系统默认使用`MBR`分区方式。\n\n看下图，这是一个主板支持`UEFI`的机器，最下面有一个启动设备是`UEFI:`开头的，即表示通过`UEFI`方式安装系统，这样安装默认支持使用`GPT`分区方式：\n\n<center>![uefi support][25]</center>\n\n再看下图，这是一个主板不支持`UEFI`的机器，最下面没有启动设备是`UEFI:`开头的，只有`ATEN`开头的，`ATEN`就是默认地通过`BIOS`方式安装系统：\n\n<center>![uefi no support][26]</center>\n\n这样安装默认不支持使用`GPT`分区方式，不过我们可以通过一些手段强制以`GPT`分区方式安装系统，\n\n在安装开机后第一个界面，按`tab`键，在行末尾，空格，输入`inst.gpt`，如下图所示:\n\n<center>![force gpt][27]</center>\n\n以`GPT`方式安装系统具体方案可以参考之前一篇文章：[Centos7系统安装设置gpt分区][33]\n\n## GRUB\n\n### GRUB 是什么\n\n`GRUB`（`Grand Unified Boot loader`）是硬盘中的软件，引导器（`loader`）的一种。目前主流版本是 `GRUB2`，可以看[GRUB2][22]中文介绍。\n\n`Wikipedia`这样介绍`GURB`：\n\n`GNU GRUB`（简称`GRUB`）是一个来自`GNU`项目的启动引导程序。`GRUB`是多启动规范的实现，它允许用户可以在计算机内同时拥有多个操作系统，并在计算机启动时选择希望运行的操作系统。`GRUB`可用于选择操作系统分区上的不同内核，也可用于向这些内核传递启动参数。\n\n<center>![grub loader][21]</center>\n\n对于只支持`BIOS`的主板，可以使用`grub`进入`grub`命令行，然后`为grub`添加第三方的`EFI shell`模块，然后就可以启动那些使用`EFI`启动的`OS`了。对于只有`EFI`的主板，当然可以写个`.efi`文件，然后转去读取`MBR`传统引导。\n\n`GRUB`用于从多操作系统的计算机中选择一个系统来启动，或从系统分区中选择特殊的内核配置。\n\n如图：第一个选项和最后一个选项是选择不同的操作系统；第一个选项和第二个选项是选择不同的内核配置。\n\n### GRUB位置\n\n\n其启动代码（`boot.img`）直接安装在`MBR`中，然后执行`GRUB`内核镜像（`core.img`），最后从`/boot/grub`中读取配置和其他功能代码。\n\n`BIOS`引导方式中，`MBR`分区表和`GPT`分区表的[GRUB引导文件所放分区不同][24]：\n\n<center>![grub location][23]</center>\n\n如图，`GRUB`的执行顺序为`boot.img` –> `core.img` –> `/boot/grub/`。\n\n* 在`MBR`分区表中，`boot.img`和`core.img`都在`MBR`中。`MBR`虽然只占用一个扇区(`512Byte`)，但是其所在的磁道是空闲的，不会用于分区，可以放下`core.img`。\n\n```\nSome MBR code loads additional code for a boot manager from the first track of the disk, which it assumes to be “free” space that is not allocated to any disk partition, and executes it. – MBR\n```\n\n* 在`GPT`分区表中，`MBR`为`protected MBR`（为兼容`MBR`，在硬盘起始位置保留的空间），后面并没有空间放`core.img`，需要建一个专门的分区来放，称为`BIOS boot partition`，该分区的文件类型为`unformatted`，`flag`为`BOIS_grub`，该`flag`用于标识`core.img`所要安装到的分区。若果使用`UEFI`引导，`GRUB`读取的是`ESP`分区中的数据，不需要`flag`为`BIOS_grub`的分区。\n\n## 总结\n\n本文依次对`BIOS`、`UEFI`、`MBR`、`GPT`、`GRUB`等概念进行了介绍和讲解，相信看完本文后，应该对`BIOS` + `MBR`和`UEFI` + `GPT`这两种方式有了充分的了解。本文做了一个比较详细的总结，旨在让读者对这些概念有一个入门的认识，当你不断研究，深入下去之后会发现这些概念只是冰山一角，还有很多知识等我们去探索，比如`boot loader`、`boot manager`、`FAT`等等。\n\n## Refer\n\n1. [What Is UEFI, and How Is It Different from BIOS?][10]\n2. [Intel to Remove Legacy BIOS Support from UEFI by 2020][11]\n3. [UEFI是什么？与BIOS的区别在哪里？][12]\n4. [Ubuntu16.10 迁移到 SSD][13]\n5. [现代计算机 Boot 相关知识][14]\n6. [科普贴：BIOS和UEFI的启动项][15]\n6. [Mac 系统引导过程概述 & BootCamp 的秘密][16]\n6. [UEFI背后的历史][7]\n7. [UEFI架构][31]\n6. [MBR与GPT][17]\n7. [关于UEFI启动+GPT分区的一些经验][30]\n7. [CentOS 7 安装强制使用GPT分区][28]\n8. [反Secure Boot垄断：兼谈如何在Windows 8电脑上安装Linux][9]\n\n\n[1]: http://cdn.tony-yin.site/bios.png\n[2]: http://cdn.tony-yin.site/uefi.jpg\n[3]: http://cdn.tony-yin.site/mbr.png\n[4]: http://cdn.tony-yin.site/gpt.png\n[5]: http://cdn.tony-yin.site/bios_loader.png\n[6]: http://cdn.tony-yin.site/bios_partition.jpg\n[7]: https://zhuanlan.zhihu.com/p/25281151\n[8]: https://www.anandtech.com/show/12068/intel-to-remove-bios-support-from-uefi-by-2020\n[9]: http://www.ruanyifeng.com/blog/2013/01/secure_boot.html\n[10]: https://www.howtogeek.com/56958/htg-explains-how-uefi-will-replace-the-bios/\n[11]: https://www.anandtech.com/show/12068/intel-to-remove-bios-support-from-uefi-by-2020\n[12]: https://www.hack520.com/uefi.html\n[13]: https://cntchen.github.io/2017/02/11/Ubuntu16-10-%E8%BF%81%E7%A7%BB%E5%88%B0-SSD/\n[14]: https://zhuanlan.zhihu.com/p/33438994\n[15]: https://zhuanlan.zhihu.com/p/31365115\n[16]: https://bbs.feng.com/read-htm-tid-6890655.html\n[17]: https://zhuanlan.zhihu.com/p/26098509\n[18]: http://cdn.tony-yin.site/gpt.png\n[19]: http://cdn.tony-yin.site/efi_loader.png\n[20]: http://cdn.tony-yin.site/uefi_mbr_loader.png\n[21]: http://cdn.tony-yin.site/grub-loader.jpg\n[22]: https://my.oschina.net/guol/blog/37373\n[23]: http://cdn.tony-yin.site/grub_location.png\n[24]: https://en.wikipedia.org/wiki/BIOS_boot_partition\n[25]: http://cdn.tony-yin.site/uefi_support.png\n[26]: http://cdn.tony-yin.site/uefi_no_support.png\n[27]: http://cdn.tony-yin.site/bios_force_uefi.png\n[28]: https://blog.51cto.com/isenknows/2053969\n[30]: https://zhuanlan.zhihu.com/p/37977078\n[31]: https://zhuanlan.zhihu.com/p/25941528\n[32]: http://cdn.tony-yin.site/bios_article.jpg\n[33]: https://www.tony-yin.site/2018/12/10/Centos7-GPT/\n","tags":["GRUB"],"categories":["tech"]},{"title":"Centos7系统安装设置gpt分区","url":"/2018/12/10/Centos7-GPT/","content":"\n硬盘分区最常见的类型为`msdos`和`gpt`，前者表示`MBR`分区，而后者表示`GPT`分区。系统安装默认分区方式为`MBR`，本文讲解如何设置`GPT`分区方式安装系统，这个看起来没什么技术含量，但是笔者实践过程中还是遇到了不少阻力，并且发现相关文章甚少，故做此记录。\n\n<center>![centos7 gpt][13]</center>\n\n<!--more-->\n\n## 概述\n\n`MBR`分区表(即主引导记录) 最大只支持`2TB`的分区，最多只支持`4`个主分区或`3`个主分区加一个扩展分区，信息只存储在一个区域；`GPT`（即`GUID`分区表），是源自`EFI`标准的一种较新的磁盘分区表结构的标准，是未来磁盘分区的主要形式，与`MBR`分区方式相比，突破`4`个主分区限制，每个磁盘最多支持`128`个分区，支持大于`2T`的分区，最大卷可达 `18EB`，信息存储在多个区域，当一部分受损后可修复。\n\n设置`GPT`方式有`UEFI`和`BIOS`两种方式，`UEFI`启动是一种新的主板引导项，与传统的`Bios`方式相比可以提高开机后操作系统的启动速度。如今很多主板已经直接支持`UEFI`，这种场景下可以直接通过`UEFI`的方式设置`GPT`方式，如若主板不支持，那么还得通过传统的`Bios`方式强制设置`GPT`方式。\n\n## 软件环境\n\n```\n操作系统： centos7\n软raid配置：/分区为raid1，boot分区为raid1\n```\n\n## UEFI方式\n\n### 引导 \n\n进入引导页面，如果存在`UEFI`安装方式，则选择`UEFI`方式安装\n\n<center>![load uefi][1]</center>\n\n### 分区\n\n配置软`raid`阶段，`UEFI`安装方式的必须存在一个`UEFI`分区，分区方式为 `raid1`，`/boot/efi`分区必须也设置为`raid1`，否则会有报错。\n\n<center>![uefi partition][2]</center>\n\n\n## BIOS Boot方式\n\n### 引导\n\n进入引导页面，如果不存在`UEFI`安装方式，如果存在则选择默认的`BIOS Boot`方式安装\n\n<center>![load bios boot][3]</center>\n\n进入系统安装页面，强制设置GPT方式安装：\n\n<center>![centos7 install][8]</center>\n\n点击`tab`键，在末尾输入`inst.gpt`\n\n<center>![centos7 inst gpt][9]</center>\n\n### 分区\n\n配置软raid阶段，`BIOS Boot`安装方式的必须存在一个`bios`分区，分区方式设置为`raid1`会报错如下：\n\n<center>![bios boot raid1 partition error][4]</center>\n\n然后系统也不能继续安装下去：\n\n<center>![Can not continue to install][5]</center>\n\n需要将分区方式改成`standard`：\n\n<center>![BIOS Boot Standard partition][6]</center>\n\n可以继续安装系统，但是过程中会出现`boot loader install failed`的报错，直接忽略即可\n\n<center>![Boot loader install failed][7]</center>\n\n## 总结\n\n总体看来`GPT`分区俨然成为`MBR`的替代者，`fdisk`只能操作`MBR`方式的磁盘，而`parted`工具能够操作`GPT`或`MBR`中任意方式的磁盘。\n\n## Refer\n\n1. [使用parted划分GPT分区][10]\n2. [Linux磁盘分区总结][11]\n3. [CentOS 7 安装强制使用GPT分区][12]\n\n[1]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/load_uefi.png\n[2]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/uefi_partition.png\n[3]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/load_biosboot.png\n[4]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/biosboot_partition_error.png\n[5]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/biosboot_cannot_install.png\n[6]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/biosboot_standard_partition.png\n[7]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/boot_loader_install_error.png\n[8]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/centos7_install.png\n[9]: https://raw.githubusercontent.com/tony-yin/Picture-Lib/master/lib/centos7_inst_gpt.png\n[10]: https://my.oschina.net/guol/blog/61424\n[11]: https://www.xncoding.com/2017/03/14/ceph/disk-partition.html\n[12]: http://blog.51cto.com/isenknows/2053969\n[13]: http://cdn.tony-yin.site/lazycat.png\n\n\n","tags":["Linux"],"categories":["tech"]},{"title":"ES告警详解之Sentinl","url":"/2018/12/01/ES-Sentinl/","content":"\n<center>![sentinl][8]</center>\n\n上一篇文章详细讲解了`ElastAlert`这款告警组件，今天我们聊聊另一个`ES`开源告警组件`Sentinl`。\n\n<!--more-->\n\n## 概述\n\n`sentinl`是基于`javascript`开发的`kibana`插件，拥有告警和报表两大功能，分别作为`X-Pack` 的`Alert`和`Reporting`的替代品，即插即用，它将前端`UI`、`webserver`和告警逻辑代码都集成在一个项目中，这一点要比上篇文章提到同为`kibana`插件的`elastalert-kibana-plugin`要强大不少，并且可以说`sentinl`的`UI`无论是美观程度还是操作友好程度都完全秒杀`elastalert-kibana-plugin`。说了这么多是不是`sentinl`就很完美呢？请看下文慢慢讲述。\n\n## 软件环境\n\n```shell\nelasticsearch: 6.4.2\nkibana: 6.4.2\nsentinl: 6.4.2\n```\n\n## 安装\n\n安装环节很简单，只需要进行通用的`kibana`安装插件的方式安装即可。\n\n```shell\n$ /usr/share/kibana/bin/kibana-plugin install https://github.com/sirensolutions/sentinl/releases/download/tag-6.4.2-0/sentinl-v6.4.2.zip\n```\n\n安装完成以后，重启`kibana`服务即可在`kibana`页面上看到`sentinl`。\n\n## Alert配置\n\n告警配置环节其实也很简单，因为`sentinl`大部分配置都是通过`UI`创建告警的时候配置的，需要手动配置的地方并不多，只有告警方式相关的。\n\n和上节一样，众多告警方式我们就选择最普遍的邮件告警进行讲解\n\n```shell\n$ vim /etc/kibana/kibana.yml\n```\n\n在文件最下方添加以下配置：\n\n```shell\nsentinl:\n  settings:\n    email:\n      active: true          // 开启email方式\n      host: 'smtp.163.com'  // smtp server\n      user: 'xxx@163.com'   // 发送邮箱账号\n      password: 'xxx'       //发送邮箱客户端授权码或密码\n      timeout: 10000        // 连接smtp server的最大timeout\n```\n\n配置完成需要重启`kibana`服务才可以生效。这里需要注意的是`host`、`user`和`password`三个选项，`host`表示`smtp`主机地址，`user`表示发送邮箱的账号，`password`表示客户端授权码，如果没有授权码才是密码。\n\n如果这样配置了，还是无法发送邮件成功，可以通过查看`kibana`的日志定位原因，大部分情况都是由于`smtp server`连接失败导致，可以通过本地邮件客户端进行测试：\n\n```shell\n$ mailx -S smtp=<smtp-server-address> -r <from-address> -s <subject> -v <to-address> < body.txt\n```\n\n## Report配置\n\n`sentinl`除了提供告警功能，还提供了一个类似`X-Pack Reporting`的报表功能。\n\n同样这个功能需要开启和配置，不得不说这个功能和相关文档还是存在着不少的问题，比如尽管文档声称默认`engine`是`horseman`，但是现在默认的是`puppeteer`。 \n\n### 问题1\n\n官方文档提供的[配置例子][1]是这样的:\n\n```shell\nsentinl:\n  settings:\n    report:\n      active: true\n      executable_path: '/usr/bin/chromium' # path to Chrome v59+ or Chromium v59+\n```\n\n`chromium`环境中不存在的话，手动下载后同步配置中对应的路径：\n\n```shell\nsentinl:\n  settings:\n    active: true\n      engine: 'puppeteer'\n      executable_path: '/usr/bin/chromium-browser'\n```\n\n运用上述配置会有以下报错：\n\n```\nOption \"report.executable_path\" was deprecated. The path is handled automatically!\n```\n\n报错中显示`executable_path`这个选项已经被弃用，这个路径会被自动解析，一脸懵逼，文档中丝毫没提及。。。\n\n## 问题2\n\n当时笔者也很奇怪，如何自动解析？如何知晓`chromium`的路径呢？这个下面再讲，然后就先注释掉该选项配置：\n\n```\nsentinl:\n  settings:\n    active: true\n      engine: 'puppeteer'\n      # executable_path: '/usr/bin/chromium-browser'\n```\n\n页面换了个报错：\n\n```\nActionError: report action: execute: run 'puppeteer' report: puppeteer work\n```\n\n如下图所示：\n\n<center>![error1][2]</center>\n\n这个时候笔者就通过关键字谷歌搜索，发现`sentinl`里面有一个[issue 535][3]，里面提到查看日志里面是否存在类似下面信息：\n\n```\n....\nserver    log   [07:52:27.332] [info][Sentinl][init] Chrome bin found at: /media/trex/safe1/Development/siren/kibi-internal/plugins/sentinl/node_modules/puppeteer/.local-chromium/linux-564778/chrome-linux/chrome\n...\nserver    log   [07:52:27.428] [info][Sentinl][init] PhantomJS bin found at: /media/trex/safe1/Development/siren/kibi-internal/plugins/sentinl/phantomjs/phantomjs-2.1.1-linux-x86_64/bin/phantomjs\n...\n```\n\n笔者去日志里面查了查，还真发现类似信息：\n\n```\n{\"type\":\"log\",\"@timestamp\":\"2018-11-23T10:27:44Z\",\"tags\":[\"error\",\"Sentinl\",\"init\"],\"pid\":8219,\"message\":\"setting puppeteer report engine: Error: user has no permissions to make file executable: /usr/share/kibana/plugins/sentinl/node_modules/puppeteer/.local-chromium/linux-564778/chrome-linux/chrome\"}\n```\n\n如下图所示\n\n![puppeteer permission][9]\n\n这就是为什么可以自动解析`chrome`路径了，因为`sentinl`内置了`chrome`，所以`sentinl`默认解析的是其内置`chrome`路径而非其他第三方下载的，所以文档中的`/usr/bin/chromium`也有点误导的感觉。\n\n这一段日志提示`chrome`没有执行权限，于是查看权限：\n\n```shell\n$ ll /usr/share/kibana/plugins/sentinl/node_modules/puppeteer/.local-chromium/linux-564778/chrome-linux/chrome\n-rw-r--r-- 1 root root 206915904 Nov 22 10:16 /usr/share/kibana/plugins/sentinl/node_modules/puppeteer/.local-chromium/linux-564778/chrome-linux/chrome\n```\n\n果然是不存在执行权限，然后手动添加权限：\n\n```\n$ chmod +x /usr/share/kibana/plugins/sentinl/node_modules/puppeteer/.local-chromium/linux-564778/chrome-linux/chrome\n```\n\n### 问题3\n\n更改之后这个报错没有了，但是又有了新的报错（真是醉了）：\n\n```\nFailed to load url\n```\n\n如下图所示\n\n![load url error][10]\n\n然后继续谷歌大法，在 [issue 495][4] 中看到了相关信息：\n\n```\nfor the error \"Failed to load url\" I just replaced the action's url with a valide link from a chosen dashboard \n```\n\n于是返回告警配置界面，发现有一个`url`选项，笔者也没填（因为当时也不知道这个`url`是干嘛的）。结合上面这哥们描述的，该`url`应该是`kibana`上面`dashbord`模块中某个已经存在的`dashbord`的链接，然后根据该`dashbord`生成报表。\n\n于是手动创建了一个`dashbord`，然后将该`dashbord`的`url`更新至配置中的`url`。\n\n### 问题4\n\n这次没报错了，`report`也发送到了指定邮箱，但是`report`是空的，这是因为新创建的`dashbord`并没有导入任何可视化图表，所以将`url`切换成了已经存在可视化图表的`dashbord`的`url`，这下可以发现发送的`report`是存在图表的。\n\n结合之前`monitoring`的了解，突然想到这个`url`可能不是`dashbord`直接显示的`url`，而应该是通过`share`功能提供的`link`。\n\n### 问题5\n\n上面`engine`选择`puppeteer`，笔者又尝试将`engine`切换成`horseman`：\n\n```\nsentinl:\n  settings:\n    report:\n      active: true\n      engine: 'horseman'\n```\n\n换了`engine`，依然报错不停：\n\n```\nActionError: report action: execute: run 'horseman' report: spawn EACCES\n```\n\n如下图所示\n\n![eacces error][11]\n\n其实这个问题，上文提到的 [issue 535][3] 中也给出了答案，`horseman engine`依赖的是`phantomjs`库，`puppeteer`依赖的是`chrome`库，问题`2`是`chrome`缺少可执行权限，而`spawn EACCES`这样的关键字则表示`phantomjs`库无法使用，根据上文`chrome`的路径笔者找到`phantomjs`的路径并检查权限：\n\n\n```shell\n$ ll /usr/share/kibana/plugins/sentinl/node_modules/phantomjs-prebuilt/bin/phantomjs \n-rw-r--r-- 1 root root 1050 Nov 22 10:15 /usr/share/kibana/plugins/sentinl/node_modules/phantomjs-prebuilt/bin/phantomjs\n```\n\n发现`phantomjs`也缺少可执行权限，添加权限：\n\n```shell\n$ chmod +x /usr/share/kibana/plugins/sentinl/node_modules/phantomjs-prebuilt/bin/phantomjs \n```\n\n但是`UI`上还是有相同的报错，只能继续查看日志，发现报错：\n\n```shell\n{\"type\":\"log\",\"@timestamp\":\"2018-11-22T02:27:28Z\",\"tags\":[\"info\",\"Sentinl\",\"init\"],\"pid\":21134,\"message\":\"PhantomJS bin found at: /usr/share/kibana/plugins/sentinl/phantomjs/phantomjs-2.1.1-linux-x86_64/bin/phantomjs\"}\n```\n\n原来`sentinl`解析库不是上面`node_modules`目录的那个路径，而是在`phantomjs`这个目录下，查看权限情况：\n\n```shell\n$ ls -lh /usr/share/kibana/plugins/sentinl/phantomjs/phantomjs-2.1.1-linux-x86_64/bin/phantomjs\n-rw-r--r-- 1 root root 65M Nov 22 10:15 /usr/share/kibana/plugins/sentinl/phantomjs/phantomjs-2.1.1-linux-x86_64/bin/phantomjs\n```\n\n果然还是缺少执行权限，添加权限：\n\n```shell\n$ chmod +x /usr/share/kibana/plugins/sentinl/phantomjs/phantomjs-2.1.1-linux-x86_64/bin/phantomjs\n```\n\n终于不报错了。。。\n\n### 官方回应\n\n这一系列问题，笔者也给`sentinl`提了 [issue][5]，作者也表示了这个权限问题之后会通过脚本等方式自动处理，然后表示[这个文档][6]没有同步更新，可以看[最新同步文档][7]。\n\n## Sentinl & ElastAlert\n\n与`elastalert`相比较，就告警功能而言的话，如果要求不高，推荐使用`sentinl`，因为安装容易并且配置简单；但如果有复杂的告警场景或独立的告警方式，那推荐选择`elastalert`。\n\n整理一个对比表格，想必这样看起来更直观：\n\n|比较 |Sentinl|ElastAlert|\n|:---:|:-----:|:-----:|\n| 安装 | 简单 | 一般复杂 |\n| 配置 | 简单 | 一般复杂 | \n| UI | 简单美观 | 不友好 | \n| 告警规则数量 | 1个 | 11个 |\n| 告警方式数量 | 6个 | 11个 | \n| 告警方式隔离| 不独立 | 独立 |\n| 开发语言| javascript | python |\n| 后端配置透明度| 不透明 | 透明 | \n| 成熟度 | 700+ star | 5000+ start |\n| 开发者数量 | 28 | 160 | \n| commit数| 1500+ | 1800+ |\n| 开发周期 | 2016/08 | 2015/11 | \n\n根据上述表格，我们可以看出各有各的优势，`sentinl`更加偏向于集成进`kibana`的一种页面展示插件，而`elastalert`则是更偏向与后端告警功能的打磨。单从告警功能来看，`elastalert`看起来更强大一些，但是`sentinl`也有它的优势，`UI`非常地棒，其次还支持`report`功能，虽然现在还存在很多问题，但相信之后肯定越来越好，而且目前通过开发者数量和代码提交量可以看出有后来居上的感觉。就笔者而言，喜爱`sentinl`的`UI`,喜爱`elastalert`的告警`rule`。\n\n## 总结\n\n`sentinl`作为一个开源组件，提供了`X-Pack`两款收费组件：`Alert`和`Reporting`的替代功能，着实不错，告警功能配置简单、页面美观、操作友好；但是`Report`功能就显得问题较多，并且生成的报表也有很多瑕疵。希望之后可以不断优化和强大吧！\n\n\n\n[1]: https://sentinl.readthedocs.io/en/latest/Config-Example/\n[2]: http://cdn.tony-yin.site/puppeteer_execute.png\n[3]: https://github.com/sirensolutions/sentinl/issues/535\n[4]: https://github.com/sirensolutions/sentinl/issues/495\n[5]: https://github.com/sirensolutions/sentinl/issues/619\n[6]: https://sentinl.readthedocs.io/en/docs/Config-Example/\n[7]: https://sentinl.readthedocs.io/en/latest/\n[8]: http://cdn.tony-yin.site/sentinl.png\n[9]: http://cdn.tony-yin.site/puppeteer_permission.PNG\n[10]: http://cdn.tony-yin.site/load_url_error.PNG\n[11]: http://cdn.tony-yin.site/eacces_error.PNG\n","tags":["Monitor"],"categories":["tech"]},{"title":"ES告警详解之ElastAlert","url":"/2018/11/15/ES-ElastAlert/","content":"\n<center>![alert][13]</center>\n\n今天聊聊`ES`的告警，`X-Pack`提供了报警组件`Alert`，但是这个功能是需要付费，在寻求其他方案的时候，发现了`ElastAlert`，可以说这是一款为`ES`量身定制的告警组件，能够完美替代`Alert`提供的所有功能。今天就`ElastAlert`强大的告警功能和笔者实践过程中遇到的一些问题进行分享。\n\n<!--more-->\n\n## 概述\n\n`ElastAlert`是基于`python2`开发的一个告警框架，它主要有以下特点：\n\n* 成熟，[Github][1]上已有`5k+ star`\n* 开源，[文档][2]很全面\n* 开发语言为`python`，研究和扩展源码较为容易\n* 告警规则丰富\n* 告警方式丰富\n* `SMTP`以每个告警为单位，相对独立\n\n网上已经有了相当多的基础介绍文章，但是笔者发现大多数文章的内容都是过时的，甚至官方文档经常还会展示一些弃用配置；还有虎头蛇尾的通病，往往不全面，经常对一些关键性的细节不提及；再者一些地方解释地不够清晰，导致歧义。笔者在搭建和测试过程中同时借鉴多篇文章，然后在反复尝试中最后才成功，这其中失败了很多次，浪费了很多时间，所以这篇文章借鉴了上面提到的种种问题，保证本文的全面性、细节性以及具体性。\n\n## 软件环境\n\n```\nCentos7\nElasticsearch 6.4.2\nKibana 6.4.2\n```\n\n## ElastAlert安装\n\n安装较为简单，但为了不虎头蛇尾还是做一个完整的步骤介绍：\n\n### 克隆代码\n\n```shell\n$ git clone https://github.com/Yelp/elastalert.git\n```\n\n### 安装依赖\n\n```shell\n$ cd elastalert\n$ python setup.py install\n$ pip install -r requirements.txt\n```\n\n### 整体配置\n\n```shell\n$ cp config.yaml.example config.yaml    // 根据模板生成配置文件\n$ vim config.yaml   // 修改配置\n```\n\n主要修改几个必需的选项，比如`rules_folder`、`es_host`、`es_port`等，那些非必需没有特殊需求就不用更改了：\n\n```shell\n# 用来加载rule的目录，默认是example_rules\nrules_folder: example_rules\n\n# 用来设置定时向elasticsearch发送请求\nrun_every:\n  minutes: 1\n  \n# 用来设置请求里时间字段的范围\nbuffer_time:\n  minutes: 15\n  \n# elasticsearch的host地址\nes_host: 192.168.232.191\n\n# elasticsearch 对应的端口号\nes_port: 9200\n\n# 可选的，es url前缀\n#es_url_prefix：elasticsearch\n\n# 可选的，查询es的方式，默认是GET\n#es_send_get_body_as：GET\n\n# 可选的，选择是否用SSL连接es，true或者false\n#use_ssl: True\n\n#可选的，是否验证TLS证书，设置为true或者false，默认为- true\n#verify_certs: True\n\n# es认证的username和password\n#es_username: someusername\n#es_password: somepassword\n\n# elastalert产生的日志在elasticsearch中的创建的索引\nwriteback_index: elastalert_status\n\n# 失败重试的时间限制\nalert_time_limit:\n  days: 2\n```\n\n详情请参考文档：http://elastalert.readthedocs.io/en/latest/ruletypes.html#rule-configuration-cheat-sheet\n\n### 创建ElastAlert索引\n\n可以在`/usr/bin/`目录下看到以下四个命令：\n\n```shell\n$ ll /usr/bin/elastalert*\n-rwxr-xr-x 1 root root 399 Nov 20 16:39 /usr/bin/elastalert\n-rwxr-xr-x 1 root root 425 Nov 20 16:39 /usr/bin/elastalert-create-index\n-rwxr-xr-x 1 root root 433 Nov 20 16:39 /usr/bin/elastalert-rule-from-kibana\n-rwxr-xr-x 1 root root 419 Nov 20 16:39 /usr/bin/elastalert-test-rule\n```\n\n* `elastalert-create-index`会创建一个索引，`ElastAlert` 会把执行记录存放到这个索引中，默认情况下，索引名叫 `elastalert_status`。其中有`4`个`_type`，都有自己的`@timestamp` 字段，所以同样也可以用`kibana`来查看这个索引的日志记录情况。\n* `elastalert-rule-from-kibana`从`Kibana3`已保存的仪表盘中读取`Filtering` 设置，帮助生成`config.yaml`里的配置。不过注意，它只会读取 `filtering`，不包括`queries`。\n* `elastalert-test-rule`测试自定义配置中的`rule`设置。\n\n执行`elastalert-create-index`命令在`ES`创建索引，这不是必须的步骤，但是强烈建议创建。因为对于审计和测试很有用，并且重启`ES`不影响计数和发送`alert`.\n\n```shell\n$ elastalert-create-index\n```\n\n具体参见文档： [setting-up-elasticsearch][3]\n\n## Rule配置\n\n`rule`配置算是`ElastAlert`最核心的功能了，支持`11`种告警规则，就不一一介绍了，选用一个最为普遍使用的告警规则`frequency`，告警方式也选用最普遍的`email`。\n\n```\n# Alert when the rate of events exceeds a threshold\n\n# (Optional)\n# Elasticsearch host\nes_host: 192.168.232.191\n\n# (Optional)\n# Elasticsearch port\nes_port: 9200\n\n# (OptionaL) Connect with SSL to Elasticsearch\n#use_ssl: True\n\n# (Optional) basic-auth username and password for Elasticsearch\n#es_username: someusername\n#es_password: somepassword\n\n# (Required)\n# Rule name, must be unique\nname: Example frequency rule\n\n# (Required)\n# Type of alert.\n# the frequency rule type alerts when num_events events occur with timeframe time\ntype: frequency\n\n# (Required)\n# Index to search, wildcard supported\nindex: metricbeat-*\n\n# (Required, frequency specific)\n# Alert when this many documents matching the query occur within a timeframe\nnum_events: 5\n\n# (Required, frequency specific)\n# num_events must occur within this amount of time to trigger an alert\ntimeframe:\n  hours: 4\n\n# (Required)\n# A list of Elasticsearch filters used for find events\n# These filters are joined with AND and nested in a filtered query\n# For more info: http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/query-dsl.html\nfilter:\n- query_string:\n    query: \"system.process.cpu.total.pct: >10%\"     // field支持嵌套\n\nsmtp_host: smtp.163.com\nsmtp_port: 25\nsmtp_auth_file: /opt/elastalert/smtp_auth.yaml\n#回复给那个邮箱\nemail_reply_to: xxx@163.com\n##从哪个邮箱发送\nfrom_addr: xxx@163.com\n# (Required)\n# The alert is use when a match is found\nalert:\n- \"email\"\n\n# (required, email specific)\n# a list of email addresses to send alerts to\nemail:\n- \"yyy@qq.com\"\n```\n\n上述配置表示选择`metricbeat`作为告警索引，在`4`小时内将匹配过滤条件，当`CPU`使用百分比的值为`10%`超过`5`次后，即满足告警条件，然后发送邮件。\n\n## 邮件配置\n\n上述配置中已经展示了一部分邮件配置，主要有`smtp host`、`smtp port`、`from addr`和`to_addr`等。这里笔者选择一个网易`163`的邮箱作为发送邮箱，一个`QQ`邮箱作为接收邮件进行测试，所以`smpt host`应该为`smtp.163.com`。\n\n```shell\nsmtp_host: smtp.163.com\nsmtp_port: 25\nsmtp_auth_file: /opt/elastalert/smtp_auth.yaml\n#回复给那个邮箱\nemail_reply_to: xxx@163.com\n##从哪个邮箱发送\nfrom_addr: xxx@163.com\n# (Required)\n# The alert is use when a match is found\nalert:\n- \"email\"\n# (required, email specific)\n# a list of email addresses to send alerts to\nemail:\n- \"yyy@qq.com\"\n```\n\n还有一个`smtp_auth.yaml`文件，这个里面记录了发送邮箱的账号和密码，`163`邮箱有授权码机制，所以密码处应该填写授权码（没有的话则需要开启）。\n\n```shell\n#发送邮件的邮箱\nuser: xxx@163.com\n##不是邮箱密码，是设置的POP3密码\npassword: xxx\n```\n\n网易授权码设置如下图：\n\n<center>![163 client password][14]</center>\n\n## 高级配置\n\n### 避免重复告警\n\n避免一定时间段中重复告警，可以配置`realert`和`exponential_realert`这两个选项：\n\n```\n# 5分钟内相同的报警不会重复发送\nrealert:\n  minutes: 5\n\n# 指数级扩大 realert 时间，中间如果有报警，\n# 则按照5->10->20->40->60不断增大报警时间到制定的最大时间，\n# 如果之后报警减少，则会慢慢恢复原始realert时间\nexponential_realert:\n  hours: 1\n```\n\n### 聚合相同告警\n\n```\n# 根据报警的内，将相同的报警安装 name 来聚合\naggregation_key: name\n\n# 聚合报警的内容，只展示 name 与 message\nsummary_table_fields:\n  - name\n  - message\n```\n\n### 告警内容格式化\n\n可以自定义告警内容，内部是使用`Python`的`format`来实现的。\n\n```\nalert_subject: \"Error {} @{}\"\nalert_subject_args:\n  - name\n  - \"@timestamp\"\n\nalert_text_type: alert_text_only\nalert_text: |\n  ### Error frequency exceeds\n  > Name: {}\n  > Message: {}\n  > Host: {} ({})\nalert_text_args:\n  - name\n  - message\n  - hostname\n  - host\n```\n\n当然还有更多高级配置，详情请参考文档。\n\n## 测试Rule\n\n可以在运行`rule`之前先通过`elastalert-test-rule`命令来测试一下\n\n```\n$ elastalert-test-rule ~/elastalert/example_rules/example_frequency.yaml\n```\n\n详情参考文档：http://elastalert.readthedocs.io/en/latest/running_elastalert.html#testing-your-rule\n\n## 运行Rule\n\n启动`elastalert`服务，监听`es`，这里加了`--rule example_frequency.yaml`表示只运行`example_frequency.yaml`这一个`rule`文件，如果不加该选项则会运行`rules_folder`下所有`rule`文件，上面配置中的`rules_folder`为默认的`example_rules`。\n\n```\n$ python -m elastalert.elastalert --verbose --rule example_frequency.yaml\n```\n\n为了让服务后台运行并且可以达到守护进程的效果，在生产环境中笔者建议使用`supervisor`管理。\n\n邮件效果图如下：\n\n<center>![elastalert email][15]</center>\n\n## 其他Rule\n\n* `any`：只要有匹配就报警；\n* `blacklist`：`compare_key`字段的内容匹配上`blacklist`数组里任意内容；\n* `whitelist`：`compare_key`字段的内容一个都没能匹配上`whitelist`数组里内容；\n* `change`：在相同`query_key`条件下，`compare_key`字段的内容，在 `timeframe`范围内 发送变化；\n* `frequency`：在相同`query_key`条件下，`timeframe` 范围内有`num_events`个被过滤出 来的异常；\n* `spike`：在相同`query_key`条件下，前后两个`timeframe`范围内数据量相差比例超过`spike_height`。其中可以通过`spike_type`设置具体涨跌方向是- `up`、`down`、`both`。还可以通过`threshold_ref`设置要求上一个周期数据量的下限，`threshold_cur`设置要求当前周期数据量的下限，如果数据量不到下限，也不触发；\n* `flatline`：`timeframe`范围内，数据量小于`threshold`阈值；\n* `new_term`：fields字段新出现之前`terms_window_size`(默认`30`天)范围内最多的`terms_size`(默认`50`)个结果以外的数据；\n* `cardinality`：在相同 `query_key`条件下，`timeframe`范围内`cardinality_field`的值超过 `max_cardinality`或者低于`min_cardinality`\n\n摘自：[ElastAlert介绍和安装－1][4]\n详细请参考文档：https://elastalert.readthedocs.io/en/latest/ruletypes.html#rule-types\n\n## 其他告警方式\n\n除了`email`，还有`jira`、`webhook`等内置告警方式，由于笔者没有实践，就不一一赘述了。\n\n第三方的微信和钉钉：\n\n* 微信：https://github.com/anjia0532/elastalert-wechat-plugin\n* 钉钉：https://github.com/xuyaoqiang/elastalert-dingtalk-plugin\n\n也可以根据文档自己实现：https://elastalert.readthedocs.io/en/latest/recipes/adding_alerts.html\n\n## elastalert-kibana-plugin\n\n`elastalert-kibana-plugin`是围绕`elastalert`做的一个`kibana`展示插件，可以在`kibana`上创建、编辑和删除告警，但是说实话这个插件还不是很好用，首先配置就有点麻烦，其次展示效果并不友好，提供配置`rule`的方式太专业化了，对小白或者一般用户来说要求稍高。\n\n### 下载安装包\n\n下载`6.4.2`的`release`安装包\n\n```shell\n$ wget https://github.com/bitsensor/elastalert-kibana-plugin/releases/download/1.0.1/elastalert-kibana-plugin-1.0.1-6.4.2.zip\n```\n\n### 本地安装插件\n\n`Kibana`插件本地安装\n\n```shell\n$ /usr/share/kibana/bin/kibana-plugin install file:///root/elastalert-kibana-plugin-1.0.1-6.4.2.zip\n```\n\n本地安装前面需要加上`file://`，否则会默认为在线资源去解析`url`并下载\n\n**Unix：**\n\n```\n$ sudo bin/elasticsearch-plugin install file:///path/to/plugin.zip\n```\n\n**Windows：**\n\n假定需要安装的插件本地地址为`C:\\path\\to\\plugin.zip`\n\n```\n$ bin\\elasticsearch-plugin install file:///C:/path/to/plugin.zip\n```\n\n### 安装Server\n\n上面安装的只是`kibana`的一个展示插件，插件内部并没有集成`server`，所以还需要再安装一个`server`，笔者之前因为没有做这一步，一直卡着，页面显示报错`502 Bad Gateway`，关键是官方文档也没说清楚一定要装这个。。\n\n#### 克隆仓库\n\n```shell\n$ git clone https://github.com/bitsensor/elastalert.git elastalert-server\n$ cd elastalert-server\n```\n\n这边我们先不用官网说的`docker`运行的方式，先用本地`npm`起服务的方式运行。\n\n#### 下载指定版本的`npm`\n\n```\n$ nvm install \"$(cat .nvmrc)\"\n```\n\n#### 安装依赖\n\n```\n$ npm install\n```\n\n#### 修改配置\n\n这一步很重要，因为很多地方没有说的很清楚，包括`docker`运行方式在这一块也没说清楚。\n\n```\n$ vim config/config.json\n```\n\n默认的配置需要修改，尤其是`elastalertPath`和`rulesPath`中的`path`选项\n\n`elastalertPath`表示的是我们最初安装的`elastalert`仓库的目录，也就是说`elastalert-kibana-plugin`运行需要三个仓库，分别是`elastalert`、`elastalert-kibana-plugin`、和`elastalert-server`，分别对应的是后端代码、前端代码、`webserver`，这也就是笔者之前提到的安装提到的安装麻烦所在了；\n\n其次`rulesPath`中`path`选项表示运用`elastalert-kibana-plugin`插件创建告警后`rule`文件存放的目录，上面笔者在`elastalert`配置的`rules_folder`为`example_rules`，这里配置的`path`为`rules`，主要是因为`elastalert-server`目录下用的是这个，笔者也在`elastalert`项目中创建了个`rules`的目录，并将`rules_folder`配置进行同步，这个看个人喜好自定义即可。\n\n```\n{\n  \"appName\": \"elastalert-server\",\n  \"port\": 3030,\n  \"elastalertPath\": \"/root/elastalert\",\n  \"verbose\": false,\n  \"es_debug\": false,\n  \"debug\": false,\n  \"rulesPath\": {\n    \"relative\": true,\n    \"path\": \"/rules\"\n  },\n  \"templatesPath\": {\n    \"relative\": true,\n    \"path\": \"/rule_templates\"\n  },\n  \"es_host\": \"192.168.232.191\",\n  \"es_port\": 9200,\n  \"writeback_index\": \"elastalert_status\"\n}\n\n```\n\n#### 起服务\n\n```\nnpm start\n```\n\n#### 容器方式\n\n官网提供的命令依旧是很模糊，很多同学直接运行了，也没报错，但是也没正常运行，这是因为跟上面一样，下面这些目录都要对应修改，具体参考上面配置文件即可，最重要的还是要明白整体架构，三个项目各自的作用，知道原理就一目了然了，但不得不说如果官方文档描述地详细一点，大家也许会更容易地搞成功。\n\n```\ndocker run -d -p 3030:3030 \\\n    -v `pwd`/config/elastalert.yaml:/opt/elastalert/config.yaml \\\n    -v `pwd`/config/config.json:/opt/elastalert-server/config/config.json \\\n    -v `pwd`/rules:/opt/elastalert/rules \\\n    -v `pwd`/rule_templates:/opt/elastalert/rule_templates \\\n    --net=\"host\" \\\n    --name elastalert bitsensor/elastalert:latest\n```\n\n## 总结\n\n本文从`elastalert`的安装讲起，接着涉猎`rule`配置、`email`配置等环节，然后通过测试和运行来对`rule`文件进行验证，最后再详细介绍了`elastalert-kibana-plugin`的安装和用法。\n\n总的来说，`elastalert`围绕`es`所提供的告警功能是很强大的，文中提供的案例只是冰山一角，大家感兴趣的可以多看看官方文档，`elastalert`的官方文档还是很全的。\n\n至于`elastalert-kibana-plugin`这个插件，笔者认为一般般，配置过程稍显麻烦，其次功能很弱，跟后端手动修改配置文件没什么两样，也没有同名校验这些机制，相比而言，`sentinl`的`UI`就显得简单美观了，请听下回分解。\n\n\n## refer\n\n1. [ElastAlert监控日志告警Web攻击行为][5]\n2. [ElastAlert：『Hi，咱服务挂了』][6]\n3. [ElastAlert介绍和安装－1][7]\n4. [elastalert的简单运用][8]\n5. [ElastAlert 基于Elasticsearch的监控告警][9]\n6. [elastAlert之kibana的插件使用][12]\n6. [Install plugins][10]\n7. [plugin management custom url][11]\n\n[1]: https://github.com/Yelp/elastalert\n[2]: https://elastalert.readthedocs.io/en/latest/index.html\n[3]: http://elastalert.readthedocs.io/en/latest/running_elastalert.html#setting-up-elasticsearch\n[4]: https://segmentfault.com/a/1190000008227486\n[5]: https://www.freebuf.com/articles/web/160254.html\n[6]: https://blog.xizhibei.me/2017/11/19/alerting-with-elastalert/\n[7]: https://segmentfault.com/a/1190000008227486\n[8]: https://www.jianshu.com/p/f82812e0a743\n[9]: https://anjia0532.github.io/2017/02/14/elasticsearch-elastalert/#&gid=1&pid=1\n[10]: https://www.elastic.co/guide/cn/kibana/current/installing_plugins.html\n[11]: https://www.elastic.co/guide/en/elasticsearch/plugins/current/plugin-management-custom-url.html\n[12]: https://my.oschina.net/qixiaobo025/blog/1932752\n[13]: http://cdn.tony-yin.site/alert.png\n[14]: http://cdn.tony-yin.site/163_pwd.png\n[15]: http://cdn.tony-yin.site/elastalert_email.png\n","tags":["Monitor"],"categories":["tech"]},{"title":"如何避免 Cronjob 重复运行","url":"/2018/11/05/How-To-Avoid-Cronjob-Duplicate-Running/","content":"\n\n<center>![cronjob][4]</center>\n\n`Cronjob`使用中有很多问题需要注意，前段时间写了一篇文章[《为什么 Cronjob 不执行》][1]，里面谈到了各种会导致`cronjob`不执行的因素和解决方案，而本文就`cronjob`重复运行的场景，对技术手段、技术方案、具体代码和相互优劣展开详细讲解。\n\n<!--more-->\n\n## 引子\n\n之前写过一篇文章[《Ctdb Rados（二）：多场景断网高可用》][2]，文中提到支持秒级的定时任务的方法，因为`cronjob`本身最小只支持分钟级别的定时任务，所以笔者在`cronjob`定时脚本中通过`for`循环来达到秒级定时的目的。\n\n然而这种定时间隔很短的任务是很容易出现重复运行的问题的。正常情况下脚本执行时间是很短的，但是一旦遇到`IO`阻塞等问题，会出现多个任务同时运行的情况，这种情况往往不是我们所期望的，可能会导致意想不到的问题。\n\n即使不是秒级的定时任务，只要任务执行时间超过定时间隔都会出现重复运行的问题，比如每分钟运行的定时任务，而其执行时间需要三分钟等等\n\n例子如下：\n\n```shell\n$ ps -elf | grep forever\n4 S vagrant   4095  4094  0  80   0 -  1111 wait   21:59 ?        00:00:00 /bin/sh -c /var/tmp/forever.sh\n0 S vagrant   4096  4095  0  80   0 -  2779 wait   21:59 ?        00:00:00 /bin/bash /var/tmp/forever.sh\n4 S vagrant   4100  4099  0  80   0 -  1111 wait   22:00 ?        00:00:00 /bin/sh -c /var/tmp/forever.sh\n0 S vagrant   4101  4100  0  80   0 -  2779 wait   22:00 ?        00:00:00 /bin/bash /var/tmp/forever.sh\n4 S vagrant   4130  4129  0  80   0 -  1111 wait   22:01 ?        00:00:00 /bin/sh -c /var/tmp/forever.sh\n0 S vagrant   4131  4130  0  80   0 -  2779 wait   22:01 ?        00:00:00 /bin/bash /var/tmp/forever.sh\n4 S vagrant   4135  4134  0  80   0 -  1111 wait   22:02 ?        00:00:00 /bin/sh -c /var/tmp/forever.sh\n0 S vagrant   4136  4135  0  80   0 -  2779 wait   22:02 ?        00:00:00 /bin/bash /var/tmp/forever.sh\n```\n\n## 解决方案\n\n### 方案1：进程数\n\n这是笔者第一时间自己想的方式，通过进程数来判断当前定时脚本同时执行的数量，比如执行的脚本名为`/opt/test.sh`，当有一个任务在运行的时候：\n\n```shell\n[root@tony ~]# ps -ef | grep /opt/test.sh\nroot      1107 25880  0 23:26 pts/0    00:00:00 /usr/bin/bash /opt/test.sh\nroot      1305  1175  0 23:27 pts/5    00:00:00 grep --color=auto /opt/test.sh\n```\n\n此时通过`ps -ef | grep /opt/test.sh | wc -l`得到的数量应该是`2`，如果定时间隔完毕后又刷新了一轮，总进程数则会变成`3`。\n\n所以我们可以在`/opt/test.sh`中加入进程数的判断，如果进程数大于`2`，就说明存在已有任务在运行，此时应该退出执行\n\n```shell\ncount=$(ps -ef | grep /opt/test.sh | wc -l)\nif [ $count -gt 2 ]; then\n    echo \"Exist job running!\"\n    exit 1\nfi\ndo something\n```\n\n但是事与愿违，当我们在`/opt/test.sh`中通过`ps`命令获取定时任务运行数量的时候发现，如果只存在当前的任务运行时，得到的进程数是`3`，如果有其他一个已在运行，则进程数是`4`，以此类推。这是为什么呢？\n\n经过一番研究发现，当只存在当前任务运行时，如果脚本里面是直接运行`ps`命令，得到的进程数是`2`，如下所示：\n\n```shell\nps -ef | grep /opt/test.sh | wc -l\n```\n\n不难看出这是`$()`的原因，它在`shell`中起了一个子`shell`，所以在子`shell`执行`ps`的同时多了一个当前脚本任务运行的进程，所以比正常进程数多`1`，所以上面代码我们需要改为：\n\n```shell\ncount=$(ps -ef | grep /opt/test.sh | wc -l)\nif [ $count -gt 3 ]; then\n    echo \"Exist job running!\"\n    exit 1\nfi\ndo something\n```\n\n### 方案2：普通文件锁\n\n可以通过一个文件来标识当前是否存在任务在运行，具体做法为当运行任务时，先检查是否存在文件锁，如果存在则表示上个任务还没有运行结束，则退出；如果不存在文件锁，则新创建一个文件锁，然后执行任务，最后执行完毕后删除文件锁。\n\n具体代码如下：\n\n```shell\nfile_lock=/opt/test.lock\nif [ -f file_lock ]; then\n    echo \"Exist job running!\"\n    exit 1\nfi\ntouch file_lock\ndo something\nrm -f file_lock\n```\n\n### 方案3：进程号文件锁\n\n所谓进程号文件锁，相比于方案`2`的普通文件锁不同的地方就是会把当前运行任务对应的进程号写入锁文件中，其优势在于除了可以通过检查文件是否存在来判断是否存在已经运行的任务，还可以再通过锁文件里面的进程号来做第二次确认。\n\n也许有人会问这个二次确认有啥用？你还别说，这个还真有用，很多时候进程意外终止或者被手动杀掉后，文件锁依然存在，那么使用普通文件锁的结果就是其实并没有正在运行的任务，但是由于存在文件锁，之后所有的任务都不会再运行。而进程号文件锁则可以在文件锁判断之外，再对锁文件中的进程号进行判断是否还在运行，具体代码如下：\n\n```shell\n#!/bin/bash\n\nPIDFILE=/opt/test.pid\nif [ -f $PIDFILE ]\nthen\n  PID=$(cat $PIDFILE)\n  ps -p $PID > /dev/null 2>&1\n  if [ $? -eq 0 ]\n  then\n    echo \"Exist job running!\"\n    exit 1\n  else\n    echo $$ > $PIDFILE\n    if [ $? -ne 0 ]\n    then\n      echo \"Could not create PID file!\"\n      exit 1\n    fi\n  fi\nelse\n  echo $$ > $PIDFILE\n  if [ $? -ne 0 ]\n  then\n    echo \"Could not create PID file!\"\n    exit 1\n  fi\nfi\n\ndo something\nrm $PIDFILE\n```\n\n虽然此方案看起来很完美，但是还是有一个场景没有考虑到，那就是如果正在运行任务的进程被`kill`掉，然后另一个进程使用了和被`kill`进程相同的`pid`，这样也会导致其实任务并没有在运行，由于存在锁文件和对应进程号的进程在运行，之后所有的任务不再运行。虽然这种场景很极端，但是也是有可能出现的，不过没关系，下面的方案会帮你解决这个问题。\n\n### 方案4：flock 锁\n\n`linux flock`锁有区别于一般的锁，它不仅仅是检查文件是否存在，它会一直存在直到进程结束，所以可以直接地知道进程是否真的执行结束了。\n\n格式：\n\n```shell\nflock [-sxun][-w #] fd#\nflock [-sxon][-w #] file [-c] command\n```\n\n选项：\n\n```shell\n -s, --shared:    获得一个共享锁 \n -x, --exclusive: 获得一个独占锁 \n -u, --unlock:    移除一个锁，脚本执行完会自动丢弃锁 \n -n, --nonblock:  如果没有立即获得锁，直接失败而不是等待 \n -w, --timeout:   如果没有立即获得锁，等待指定时间 \n -o, --close:     在运行命令前关闭文件的描述符号。用于如果命令产生子进程时会不受锁的管控 \n -c, --command:   在shell中运行一个单独的命令 \n -h, --help       显示帮助 \n -V, --version:   显示版本 \n```\n\n锁类型：\n\n```\n共享锁：多个进程可以使用同一把锁，常被用作读共享锁\n独占锁：同时只允许一个进程使用，又称排他锁，写锁。\n```\n\n这里由于我们只允许同时存在一个任务运行，所以选择独占锁，然后需要在脚本执行完丢弃锁：\n\n```shell\n* * * * *  flock -xn /opt/test.lock -c /opt/test.sh\n```\n\n### 方案5：solo 程序\n\n[Solo][3]是一个`Perl`脚本，它的工作原理与`flock`类似，但它并不依赖于锁文件，因为`Solo`程序是通过绑定端口来实现。\n\n```shell\n$ ./solo -port=6000 /opt/test.sh &\n[1] 7503\n$ ./solo -port=6000 /opt/test.sh\nsolo(6000): Address already in use\n```\n\n执行`solo`时，将绑定指定的端口并执行后面指定的命令。一旦命令完成，就会释放端口，允许任务的下一个调用正常执行。\n\n`solo`的优势在于没有人能够通过删除一个文件并意外地导致任务重复运行。即使使用`flock`命令，如果锁文件被删除，也可以启动第二个作业。由于`solo`绑定了一个端口，所以不可能出现这种情况。\n\n## 总结\n\n上面提到了五种方案，第一种方案略显粗糙，但是缺陷相对来说较少；第二种方案存在锁文件被意外删除或者进程被`kill`的风险；第三种方案存在锁文件被意外删除和新进程占用相同进程号的问题；第四种方案还是存在意外删除锁文件的问题；第五种方案则不需要担心锁文件被删除导致任务重复运行的问题。\n\n目前看起来第五种方案是最优的，不存在缺陷。不过还是得看具体场景，笔者认为第三种、第四种、第五种方案都是有可取之处的，大家还是根据各自的场景选择最适合自己的方案吧。\n\n## Refer\n\n* https://blog.csdn.net/phachon/article/details/52106409\n* http://bencane.com/2015/09/22/preventing-duplicate-cron-job-executions/\n\n[1]: https://www.tony-yin.site/2018/10/29/Why-Crontab-Not-Work/\n[2]: https://www.tony-yin.site/2018/05/20/Ctdb-Rados-2/\n[3]: http://timkay.com/solo/\n[4]: http://cdn.tony-yin.site/cronjob.png\n\n","tags":["Cronjob"],"categories":["tech"]},{"title":"为什么 crontab 不执行","url":"/2018/10/29/Why-Crontab-Not-Work/","content":"\n<center>![Why Crontab Not Work][5]</center>\n\n作为`linux`中的定时任务工具，`crontab`被广大开发者所热爱和使用。该技术由来已久，相当成熟，但是在真正使用的时候会时不时地发现为什么`crontab`脚本没有按照预期那样执行？本文以本周笔者遇到一个`crontab`不能运行的问题为引子，详细地介绍为什么`crontab`不运行的各种原因。\n\n<!--more-->\n\n## 引子\n\n本周遇到一个`crontab`不能执行的问题，发现原因后觉得甚是有趣。\n\n笔者通过一个`python`脚本向`/etc/cron.d`目录下的一个文件写入定时任务命令，每分钟调用一个脚本，调用的这个脚本是个`python`文件，然后发现`cron`并没有按照预期每分钟执行一次。然后笔者就将原定时任务脚本`aaa`拷贝了一份，并重新命名为`bbb`，然后将定时任务中调用脚本改成了执行一个简单的`echo`命令，然后保存退出，发现`bbb`是可以正常定时运行的，这时候，笔者就通过`file`命令想比较一下这两个文件有何不同：\n\n```\n[root@tony cron.d]# file *\naaa:     ASCII text, with no line terminators\nbbb:     ASCII text\n```\n\n这个时候我们可以发现`aaa`文件出现了比较奇怪的标识：\n\n```\nwith no line terminators\n```\n\n显而易见，这是在说`cron`脚本中定时命令没有行终止符，导致这个问题是因为该`cron`脚本由`python`代码生成时没有添加换行符：\n\n```\nwith open('/etc/cron.d/aaa', 'w') as f:\n    f.write('xxx')\n```\n\n然后笔者尝试性地在`aaa`文件中在定时命令下新增一行后，发现定时任务可以正常运行了。不得不说，这是一个很有意思的问题，`crontab`居然会因为一个换行符导致定时任务的不运行，后来`google`了一下发现，`crontab`的确存在这个机制，具体解释下面会提到。\n\n在`google`的同时，在`ask unbuntu`上发现了这篇文章：[《Why crontab scripts are not working?》][3]，里面很多开发者罗列了他们遇到`cron`不能正常运行的各种因素，笔者大致浏览了下，发现有遇到过，也有很多并不知道的，所以想把这些因素和解决方案一一罗列下来。\n\n## 因素\n\n### 因素1：环境变量\n\n#### 场景及原因\n\n`cron`中的环境变量和系统的环境变量是不一样的，我们可以通过设置定时脚本将`cron`中的环境变量打印出来：\n\n```\n* * * * * env > /tmp/env.output\n```\n\n可以看到`cron`中的环境变量：\n\n```\nXDG_SESSION_ID=12952\nSHELL=/bin/sh\nUSER=root\nPATH=/usr/bin:/bin\nPWD=/root\nLANG=en_US.UTF-8\nSHLVL=1\nHOME=/root\nLOGNAME=root\nXDG_RUNTIME_DIR=/run/user/0\n_=/usr/bin/env\n```\n\n查看系统的环境变量：\n\n```\n[root@tony cron.d]# env\nXDG_SESSION_ID=1140\nHOSTNAME=tony\nTERM=xterm-256color\nSHELL=/bin/bash\nHISTSIZE=1000\nUSER=root\nPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/root/bin:/root/bin\nMAIL=/var/spool/mail/root\nPWD=/etc/cron.d\nLANG=en_US.UTF-8\nTMUX_PANE=%18\nHISTCONTROL=ignoredups\nSHLVL=2\nHOME=/root\nLOGNAME=root\n_=/usr/bin/env\nOLDPWD=/root\n```\n\n我们可以看到`cron`中的环境变量很多都和系统环境变量不一样（`cron`会忽略`/etc/environment`文件），尤其是`PATH`，只有`/usr/bin:/bin`，也就是说在`cron`中运行`shell`命令，如果不是全路径，只能运行`/usr/bin`或`/bin`这两个目录中的标准命令，而像`/usr/sbin`、`/usr/local/bin`等目录中的非标准命令是不能运行的。\n\n这个问题笔者也遇到很多次，所以很多非标准命令都选择了全路径，但是这个方法也有问题，因为不同环境的命令所存在的目录是不一样的。\n\n#### 解决方案\n\n**方案1：**\n\n在`cron`脚本文件头部声明`PATH`\n\n```\n#!/bin/bash\nPATH=/opt/someApp/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n# rest of script follows\n```\n\n**方案2：**\n\n在定时脚本调用的脚本头部声明`PATH`\n\n```\nPATH=/opt/someApp/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n\n15 1 * * * backupscript --incremental /home /root\n```\n\n\n### 因素2：换行符\n\n#### 场景及原因\n\n这个因素就是笔者引子中提到的，官方解释（`man crontab`）如下：\n\n```\nAlthough cron requires that each entry in a crontab end in a newline character, neither the crontab command nor the cron daemon will detect this error. Instead, the crontab will appear to load normally. However, the command will never run. The best choice is to ensure that your crontab has a blank line at the end.\n\n4th Berkeley Distribution      29 December 1993     CRONTAB(1)\n```\n\n简单翻译一下就是：\n\n尽管`crontab`要求`cron`中的每个条目都要以换行符结尾，但`crontab`命令和`cron`守护进程都不会检测到这个错误。相反，`crontab`将正常加载。然而，命令永远不会运行。最好的选择是确保您的`crontab`在末尾有一个空白行。\n\n#### 解决方案\n\n给`cron`中每个条目下面添加一个空行\n\n**注意：**\n\n除了没了换行符会导致`cron`中的命令不会运行，即引子中所标识：\n\n```\nwith no line terminators\n```\n\n但是因为非`linux`操作系统导致的非`\\n`换行符同样会导致该问题，比如`windows`的`^M`、`mac`的`\\r`等\n\n```\nwith CR line terminators\n```\n\n**解决方案：**\n\n`windows`的话就通过`dos2unix`命令转换；而`mac`则可以通过`mac2unix`来转换，`mac2unix`也是`dos2unix`软件中的一部分\n\n#### Refer\n\n* [remove CR line terminators][4]\n\n### 因素3：crond 服务\n\n#### 场景及原因\n\n很多时候`crond`服务未开启，也会导致定时任务不会正常执行。\n\n#### 解决方案\n\n查看服务是否运行，如果未运行，启动`crond`服务即可。\n\n查看方式有两种：\n\n1.通过进程查看\n\n`pgrep`相当于`ps -ef | grep`\n\n```\npgrep cron\n```\n\n2.通过`service`查看\n\n```\nservice crond status\n```\n\n启动服务：\n\n```\nservice crond start\n```\n\n### 因素4：shell 解释器\n\n#### 场景及原因\n\n从因素`1`就知道`cron`环境变量中的`SHELL`是`sh`而不是`bash`，我们知道很多`shell`命令是可以在`bash`中正常运行，但是不能在`sh`中运行的，所以这个因素也会影响定时任务的正常运行。\n\n#### 解决方案：\n\n**方案1：**\n\n将`cron`中需要执行的命令在`sh`中执行确认\n\n**方案2：**\n\n将`cron`中需要执行的命令外面加一个`bash shell`的封装：\n\n```\nbash -c \"mybashcommand\"\n```\n\n**方案3：**\n\n修改`cron`中的`SHELL`环境变量的值，让所有命令都用`bash`解释器：\n\n```\nSHELL=/bin/bash\n```\n\n**方案4：**\n\n如果定时任务执行的命令是`shell`脚本，只要在脚本内添加`bash`解释器：\n\n```\n#!/bin/bash\n```\n\n### 因素5：时区\n\n#### 场景及原因\n\n当修改系统时区后，无论是之前已经存在的`cron`还是之后新创建的`cron`，脚本中设置的定时时间都以旧时区为准，比如原来时区是`Asia/Shanghai`，时间为`10:00`，然后修改时区为`Europe/Paris`，时间变为`3:00`，此时你设置`11:00`的定时时间，`cron`会在`Asia/Shanghai`时区的`11:00`执行。\n\n#### 解决方案：\n\n**方案1：**\n\n重启`crond`服务\n\n```\nservice crond restart\n```\n\n**方案2：**\n\n`kill crond`进程，因为`crond`进程是可重生的\n\n### 因素6：百分号%\n\n#### 场景及原因\n\n当`cron`定时执行命令中，有百分号并且没有转义的时候，`cron`执行会出错，比如执行以下`cron`：\n\n```\n0 * * * * echo hello >> ~/cron-logs/hourly/test`date \"+%d\"`.log\n```\n\n会有如下报错：\n\n```\n/bin/sh: -c: line 0: unexpected EOF while looking for matching ``'\n/bin/sh: -c: line 1: syntax error: unexpected end of file\n```\n\n有的日志也会有如下报错：\n\n```\n(echo) ERROR (getpwnam() failed)\n```\n\n`crontab manpage`中解释：\n\n```\nThe \"sixth\" field (the rest of the line) specifies the command to be run. The entire command portion of the line, up to a newline or % character, will be executed by /bin/sh or by the shell specified in the SHELL variable of the cronfile. Percent-signs (%) in the command, unless escaped with backslash (\\), will be changed into newline characters, and all data after the first % will be sent to the command as standard input.\n```\n\n即`cron`中换行符或`%`前的命令会被`shell`解释器执行，但是`%`会被认为新一行的字符，并且`%`后所有的数据都会以标准输出的形式发送给命令。\n\n#### 解决方案\n\n为百分号做转义，即在`%`前添加反斜杠`\\`\n\n#### Refer\n\n* [Cron and Crontab usage and examples][1]\n* [How can I execute date inside of a cron tab job?][2]\n\n\n### 因素7：密码过期\n\n#### 场景及原因\n\n`Linux`下新建用户密码过期时间是从`/etc/login.defs`文件中`PASS_MAX_DAYS`提取的，普通系统默认就是`99999`，而有些安全操作系统是`90`。更改此处，只是让新建的用户默认密码过期时间变化，已有用户密码过期时间仍然不变。\n\n当用户密码过期也会导致`cron`脚本执行失败。\n\n#### 解决方案\n\n将用户密码有效期设置成永久有效期或者延长有效期\n\n**方案1：**\n\n```\nchage -M <expire> <username>\n```\n\n**方案2：**\n\n```\npasswd -x -1 <username>\n```\n\n**方案3：**\n\n手动修改`/etc/login.defs`文件中`PASS_MAX_DAYS`的值\n\n### 因素8：权限\n\n#### 场景及原因\n\n很多时候解决方案都是采用`root`用户执行`cron`，但是有时候这并不是一个很好的方式。如果采用非`root`用户执行`cron`，需要注意很多权限问题，比如`cron`用户对操作的文件或目录是否存在权限等。\n\n如果权限不够，`cron`会拒绝执行：\n\n```\nsudo service cron restart\ngrep -i cron /var/log/syslog|tail -2\n2013-02-05T03:47:49.283841+01:00 ubuntu cron[49906]: (user) INSECURE MODE (mode 0600 expected) (crontabs/user)\n```\n\n#### 解决方案\n\n```\n# correct permission\nsudo chmod 600 /var/spool/cron/crontabs/user\n# signal crond to reload the file\nsudo touch /var/spool/cron/crontabs\n```\n\n### 因素9：不同平台\n\n#### 场景及原因\n\n一些特殊选项各个平台支持不一样，有的支持，有的不支持，例如`2/3`、`1-5`、`1,3,5`\n\n#### 解决方案\n\n需要针对不同平台做兼容性测试\n\n### 因素10：不同 cron\n\n#### 场景及原因\n\n将之前运行的`Crontab Spec`在从一个`Crontab`文件移动到另一个`Crontab`文件时可能会崩溃。有时候，原因是你已经将`Spec`从系统`crontab`文件转移到用户`crontab`文件，反之亦然。\n\n`cron`分为系统`cron`和用户`cron`，用户`cron`指`/var/spool/cron/username`或`/var/spool/crontabs/crontabs/username`，系统`cron`指\n`/etc/crontab`以及`/etc/crontab`，这两者是存在部分差异的。\n\n系统`crontab`在命令行运行之前有一个额外的字段`user`。这会导致一些错误，比如你将`/etc/crontab`中的命令或者`/etc/cron.d`中的文件移动至用户`crontab`会报错如下：\n```\ngeorge; command not found\n```\n相反，当发生相反的情况时，`cron`将显示`/usr/bin/restartxyz is not a valid username`之类的错误。\n\n#### 解决方案\n\n当共享系统`cron`或用户`cron`时，注意用户的添加和删除。\n\n### 因素11：crontable 变量\n\n#### 场景及原因\n\n虽然你可以在`crontable`里面声明环境变量，但是在下面这种情况定时任务是不会执行的：\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=${SOME_LOG}/some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=${BIN_DIR}/some_executable_file\n\n0 10 * * * ${MY_EXE} some_param >> ${MY_LOG_FILE}\n```\n\n这是因为在`crontable`里面只能声明变量，不能对变量进行操作或者执行其他任何`shell`命令的，所以上述的`shell`字符串拼接是不会成功的，所以只能声明变量，然后在命令中引用变量。\n\n#### 解决方案：\n\n**方案1：** \n\n直接声明变量\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=/var/log/some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=/usr/local/bin/some_executable_file\n\n0 10 * * * ${MY_EXE} some_param >> ${MY_LOG_FILE}\n```\n\n**方案2：**\n\n声明多个变量，在命令中引用拼接\n\n```\nSOME_DIR=/var/log\nMY_LOG_FILE=some_file.log\n\nBIN_DIR=/usr/local/bin\nMY_EXE=some_executable_file\n\n0 10 * * * ${BIN_DIR}/${MY_EXE} some_param >> ${SOME_DIR}/${MY_LOG_FILE}\n```\n\n### 因素12：GUI\n\n#### 场景及原因\n\n如果你的`cronjob`调用了相关`GUI`应用时，你需要告诉它们应该使用什么`DISPLAY`环境变量，从因素`1`我们可以知道`cron`中的环境变量是和系统环境变量不一样的，`DISPLAY`同样如此，比如\n\n```\nFirefox launch with cron.\n```\n\n#### 解决方案\n\n声明`DISPLAY=:0`\n\n```\n* * * * * export DISPLAY=:0 && <command>\n```\n\n## 总结\n\n目前主要总结了影响`cron`运行的`12`种因素，当然肯定还存在其他影响因素，本文将持续更新，希望这些坑能够被广大开发者所熟知。\n\n大家如果有上述以外导致`cron`不能正常运行的因素可以在博客下方留言，或者在`Github`上面提`pr`，笔者已经将本文在`Github`上面创建了一个仓库，让我们一起不断完善吧 -。-\n\n`Github`仓库地址：https://github.com/tony-yin/Why-Cronjob-Not-Work\n\n\n[1]: http://www.pantz.org/software/cron/croninfo.html\n[2]: https://unix.stackexchange.com/questions/29578/how-can-i-execute-date-inside-of-a-cron-tab-job\n[3]: https://askubuntu.com/questions/23009/why-crontab-scripts-are-not-working?page=1&tab=votes#tab-top\n[4]: https://stackoverflow.com/questions/14080306/remove-cr-line-terminators\n[5]: http://cdn.tony-yin.site/Why-Crontab-Not_Work.png\n","tags":["Crontab"],"categories":["tech"]},{"title":"Daily Article Vol7(2018/6/1 ~ 2018/6/30)","url":"/2018/07/01/Daily-Article-Vol7/","content":"\n<center>![6][51]</center>\n\n这是`Daily Article`系列文章的第七篇，主要记录了`2018/6/1 ~ 2018/6/30`之间的阅读记录。\n\n<!--more-->\n\n六月份主要做了几件事，一个是基于`python`实现一个守护进程，从`unix`实现守护进程的底层原理撸了一遍，加深了对`linux`底层进程一些概念的了解，正好也接触了“骑着企鹅采树莓”系列文章，讲的很不错，通俗易懂；然后把之前基于`pci path`寻找逻辑磁盘的物理位置整理了下，研究了一波块设备持久化命名；然后学习了下`kvm`虚拟化技术，并在此之上研究了下虚拟机迁移技术，这其中包括虚拟机到虚拟机的迁移，`kvm`虚拟机到`vmware`虚拟机的迁移，虚拟机到物理机的迁移，物理机到物理机的迁移等等；最后研究了下多路径设备，概念有点混乱，不断地啃资料终于有点明白了。不得不说，这些底层的知识真是又多又复杂，只能慢慢啃，加油咯。\n\n1. [understanding /dev/disk/by- folders][1](6/1) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n2. [PERSISTENT NAMING][2](6/2) <i class='fa fa-star'></i>\n3. [Understanding Device Names Managed by the udev mechanism][3](6/3) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n4. [Persistent block device naming][4](6/4) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n5. [udev (简体中文)][5](6/9) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n6. [How to find WWN for /dev/sdc][6](6/9)\n7. [SSO单点登录的实现原理是怎样的][7](6/10)\n8. [可爱的 Python：Curses 编程][8](6/10)\n9. [Django cookie和session][9](6/10)\n10. [这也许是你看到过的最通俗易懂的ElasticSearch文章了（理论篇）][10](6/10) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n11. [饿了么在ELasticsearch自动化运维平台和监控平台的应用实践][11](6/10)\n12. [ACCIDENTALLY ADDING A GIT SUBMODULE][14](6/11)\n12. [聊聊MySQL、HBase、ES的特点和区别][12](6/12) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n13. [行存储和列存储][13](6/13)\n14. [【极客时间：左耳听风】洞悉PaaS平台的本质][15](6/13) <i class='fa fa-star'></i>\n15. [【CoolShell】关于高可用的系统][16](6/13) <i class='fa fa-star'></i>\n16. [【极客时间：左耳听风】编程范式游记（1）- 起源][17](6/14)<i class='fa fa-star'></i>\n17. [python fork][19](6/15)\n18. [Python实例浅谈之五Python守护进程和脚本单例运行][20](6/16) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n17. [Python实现守护进程][18](6/18) <i class='fa fa-star'></i>\n18. [在Unix系统上面启动守护进程][21](6/19) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n19. [《APUE》读书笔记—第十三章守护进程][22](6/19) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n20. [【骑着企鹅采树莓】Linux进程基础][23](6/20) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n21. [【骑着企鹅采树莓】Linux进程关系][24](6/20) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n22. [【骑着企鹅采树莓】树莓派：一个关于教育的故事][25](6/21)\n23. [【骑着企鹅采树莓】树莓派心脏：ARM的逆袭][26](6/21)  <i class='fa fa-star'></i>\n23. [【骑着企鹅采树莓】树莓派的大脑：Linux][27](6/22)\n24. [Linux策略路由][28](6/22)\n26. [【迁移Linux系统1】如何迁移备份和裸机恢复Linux系统][30](6/24)\n27. [【迁移Linux系统2】利用 KIWI Imaging System 定制裸机恢复光盘）][40](6/26)\n27. [【虚拟机迁移技术漫谈1】如何在虚拟机和物理机以及虚拟机和虚拟机之间的迁移系统][31](6/24)\n25. [【虚拟机迁移技术漫谈2】KVM虚拟机在物理主机之间迁移的实现][29](6/24) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n26. [【虚拟机迁移技术漫谈3】迁移 VMware 虚拟机到 KVM][32](6/25) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n27. [【虚拟机迁移技术漫谈4】KVM 虚拟机在 IBM System x 上应用][33](6/25) <i class='fa fa-star'></i><i class='fa fa-star-half-empty'></i>\n28. [【虚拟机迁移技术漫谈5】在IBM集成虚拟化管理器上进行动态分区迁移][34](6/26)\n29. [利用Qemu Guest Agent (Qemu-ga) 实现 Openstack 监控平台][36](6/26)\n30. [利用qemu-guest-agent冻结文件系统][37](6/26)\n31. [kvm 快照][38](6/26)\n32. [How to enable QEMU guest agent in KVM][39](6/26)\n33. [DDOS 攻击的防范教程][45](6/27)\n34. [Libvirt 虚拟化库剖析][35](6/27) <i class='fa fa-star'></i>\n35. [IBM 动态分区迁移(Live Partition Mobility)技术-概述][46](6/27)\n36. [【骑着企鹅采树莓】Linux信号基础][47](6/27) <i class='fa fa-star'></i>\n37. [DM-MULTIPATH 概述][48](6/28)\n37. [Linux 内核中的 Device Mapper 机制][43](6/28) <i class='fa fa-star'></i>\n38. [Multipath 多路径配置实践心得][41](6/28)\n39. [multipath多路径实验01-构建iSCSI模拟环境][42](6/30) <i class='fa fa-star'></i>\n40. [multipath多路径实验02-配置多路径软件][49](6/30)\n41. [multipath 路径切换][50](6/30) <i class='fa fa-star'></i>\n\n[1]: https://unix.stackexchange.com/questions/86764/understanding-dev-disk-by-folders\n[2]: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/online_storage_reconfiguration_guide/persistent_naming\n[3]: http://fibrevillage.com/storage/578-understanding-device-names-managed-by-the-udev-mechanism\n[4]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)\n[5]: https://wiki.archlinux.org/index.php/Udev_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)#.E8.AE.BE.E7.BD.AE.E9.9D.99.E6.80.81.E8.AE.BE.E5.A4.87.E5.90.8D\n[6]: https://www.linuxquestions.org/questions/linux-server-73/how-to-find-wwn-for-dev-sdc-917269/\n[7]: http://www.cnblogs.com/songyaqi/p/4553069.html9.html/l-anaconda/index.html\n[8]: https://www.ibm.com/developerworks/cn/linux/sdk/python/python-6/index.html\n[9]: http://blog.51cto.com/830909/1751106\n[10]: https://www.jianshu.com/p/40ec55c6e614?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[11]: https://elasticsearch.cn/slides/109#page=21\n[12]: https://www.jianshu.com/p/4e412f48e820?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[13]: http://www.cnblogs.com/qiaoyihang/p/6262806.html\n[14]: https://humansky.com/2011/12/accidentally-adding-a-git-submodule/\n[15]: https://time.geekbang.org/column/article/1610\n[16]: https://coolshell.cn/articles/17459.html\n[17]: https://time.geekbang.org/column/article/301\n[18]: https://www.jianshu.com/p/e3f3d49093ca\n[19]: https://blog.csdn.net/asware/article/details/4159366\n[20]: https://blog.csdn.net/taiyang1987912/article/details/44850999\n[21]: http://python3-cookbook.readthedocs.io/zh_CN/latest/c12/p14_launching_daemon_process_on_unix.html#id1\n[22]: https://yq.aliyun.com/articles/41477?spm=5176.100240.searchblog.39.HyGU9I\n[23]: http://www.cnblogs.com/vamei/archive/2012/09/20/2694466.html\n[24]: http://www.cnblogs.com/vamei/archive/2012/10/07/2713023.html\n[25]: http://www.cnblogs.com/vamei/p/6415434.html\n[26]: http://www.cnblogs.com/vamei/p/6424666.html\n[27]: http://www.cnblogs.com/vamei/p/6718942.html\n[28]: https://my.oschina.net/guol/blog/156607\n[29]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm2/index.html\n[30]: https://www.ibm.com/developerworks/cn/linux/l-cn-linux-br1/index.html\n[31]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm1/index.html\n[32]: https://www.ibm.com/developerworks/cn/linux/l-cn-mgrtvm3/index.html\n[33]: https://www.ibm.com/developerworks/cn/linux/l-cn-kvm/\n[34]: https://www.ibm.com/developerworks/cn/aix/library/0808_shenlf_ivm/\n[35]: https://www.ibm.com/developerworks/cn/linux/l-libvirt/index.html\n[36]: http://www.cnblogs.com/biangbiang/p/3222458.html\n[37]: http://www.zoues.com/2015/10/13/qemu-guest-agent/\n[38]: http://blog.51cto.com/aishangwei/2124944\n[39]: https://access.redhat.com/solutions/732773\n[40]: https://www.ibm.com/developerworks/cn/linux/l-cn-linux-br2/index.html\n[41]: https://wsgzao.github.io/post/multipath/\n[42]: http://www.cnblogs.com/jyzhao/p/7200585.html\n[43]: https://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\n[44]: https://www.jianshu.com/p/3461c568b82f?utm_campaign=maleskine&utm_content=note&utm_medium=seo_notes&utm_source=recommendation\n[45]: http://www.ruanyifeng.com/blog/2018/06/ddos.html\n[46]: https://www.ibm.com/developerworks/cn/aix/redbooks/live-mobility/\n[47]: http://www.cnblogs.com/vamei/archive/2012/10/04/2711818.html\n[48]: https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/dm_multipath/mpio_description\n[49]: http://www.cnblogs.com/jyzhao/p/7208620.html\n[50]: https://blog.csdn.net/jiaping0424/article/details/51591257\n[51]: http://cdn.tony-yin.site/6.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"块设备持久化命名和多路径","url":"/2018/06/20/Block-device-persistent-name-and-multipath/","content":"\n<center>![persistent name][12]</center>\n\n众所周知，我们看到的磁盘通常是`/dev/sda`，`/dev/sdb`这样的名称，我们可以通过这些磁盘名称对磁盘进行各种操作，例如挂载，分区和格式化等等。但是，如果你的机器上有不止一个`SATA`，`SCSI`或`IDE`磁盘控制器，那么它们所对应的设备节点将会依随机次序添加。这样就可能导致每次引导时设备的名字如`/dev/sda`与`/dev/sdb`互换了，再比如硬盘拔插导致磁盘乱序等等，最终导致系统不可引导、`kernel panic`、或者设备不可见。持久化命名可以解决这些问题。\n\n<!--more-->\n\n## 概述\n\n持久化命名，顾名思义即一次性或者是短暂的命名，它是一种长久的并且稳定靠谱的命名方案。与之形成鲜明对比的就是`/dev/sda`这种非持久化命名，这两种命名方案各有各的用处，本文着重对持久化命名进行介绍。持久化命名方案有四种：`by-label`、`by-uuid`、`by-id`和`by-path`。对于那些使用`GUID`分区表（`GPT`）的磁盘，还有额外的两种方案：`by-partlabel`和`by-partuuid`。你也可以使用`Udev`静态设备名方案，这个我们就不作详细解释。下面我将对每种持久化命名方案进行详细的介绍和讲解，不难发现上面所提到的命名方式在`/dev`目录下都存在一个与之名字对应的文件夹：\n\n## by-label\n\n`label`表示标签的意思，几乎每一个文件系统都有一个标签。所有有标签的分区都在`/dev/disk/by-label`目录中列出。这个目录随着分区标签的变动而被动态地创建和销毁。\n\n```sh\n$ ls -l /dev/disk/by-label\n \ntotal 0\nlrwxrwxrwx 1 root root 10 May 27 23:31 DATA -> ../../sda3\nlrwxrwxrwx 1 root root 10 May 27 23:31 SYSTEM -> ../../sda2\n```\n\n标签必须是唯一的，标签是文件系统的一个属性，所以无法持久地表示单一磁盘阵列设备。\n\n**注意：**`label`是通过从设备中的内容（即数据）获取，所以如果将该内容拷贝至另一个设备中，我们也可以通过`blkid`来获取磁盘的`label`。\n\n## by-uuid\n\n`UUID`是给每个文件系统唯一标识的一种机制，这个标识是在分区格式化时通过文件系统工具生成，比如`mkfs`，这个唯一标识可以起到解决冲突的作用。所有`GNU/Linux`文件系统（包括`swap`和原始加密设备的`LUKS`头）都支持`UUID`。`FAT`和`NTFS`文件系统并不支持`UUID`，但是在`/dev/disk/by-uuid`目录下还是存在着一个更为简单的`UID`（唯一标识）。\n\n```\n$ ls -l /dev/disk/by-uuid/\ntotal 0\nlrwxrwxrwx 1 root root 10 May 27 23:31 0a3407de-014b-458b-b5c1-848e92a327a3 -> ../../sda2\nlrwxrwxrwx 1 root root 10 May 27 23:31 b411dc99-f0a0-4c87-9e05-184977be8539 -> ../../sda3\nlrwxrwxrwx 1 root root 10 May 27 23:31 CBB6-24F2 -> ../../sda1\nlrwxrwxrwx 1 root root 10 May 27 23:31 f9fe0b69-a280-415d-a03a-a32752370dee -> ../../sda4\n```\n\n使用`UUID`方法的优点是，名称冲突发生的可能性大大低于使用`Label`的方式。更深层次地讲，它是在创建文件系统时自动生成的。例如，即使设备插入到另一个系统(可能有一个标签相同的设备)，它仍然是唯一的。\n\n缺点是`uuid`使得许多配置文件(例如`fstab`或`crypttab`)中的长代码行难以读取和破坏格式。此外，每当一个分区被调整大小或重新格式化时，都会生成一个新的UUID，并且必须(手动)调整配置。\n\n## by-path\n\n该目录中的条目提供一个符号名称，该符号名称通过用于访问设备的**硬件路径**引用存储设备，首先引用`PCI hierachy`中的存储控制器，并包括`SCSI host`、`channel`、`target`和`LUN`号，以及可选的分区号。虽然这些名字比使用`major`和`minor`号或`sd`名字更容易，但必须使用谨慎以确保`target`号不改变在光纤通道`SAN`环境中(例如，通过使用持久绑定)，如果一个主机适配器切换到到一个不同的`PCI`插槽的话这个路径也会随之改变。此外，如果`HBA`无法探测，或者如果驱动程序以不同的顺序加载，或者系统上安装了新的`HBA`，那么`SCSI`主机号都有可能会发生变化。附带路径清单的一个例子是:\n\n```\n/dev/disk/by-path/pci-0000:03:00.0-scsi-0:1:0:0\n```\n\n也许还会带着分区号：\n\n```\n/dev/disk/by-path/pci-0000:03:00.0-scsi-0:1:0:0-part1\n```\n\n**注意：** 上面说了很多种情况都会导致`by-path`的值可能发生变化，但是在同一时间来说，`by-path`的值是和物理设备是唯一对应的，也就是说不管怎么说`by-path`是对应物理机器上面的某个位置的，根据`by-path`可以获取对应物理位置的设备。（此前`megaraid`通过逻辑磁盘获取物理磁盘位置就是根据这个原理）\n\n对于`iSCSI`设备，路径/名称映射从目标名称和门户信息映射到`sd`名称。\n应用程序通常不适合使用这些基于路径的名称。这是因为这些路径引用可能会更改存储设备，从而可能导致将不正确的数据写入设备。基于路径的名称也不适用于多路径设备，因为基于路径的名称可能被误认为是单独的存储设备，导致不协调的访问和数据的意外修改。\n\n此外，基于路径的名称是特定于系统的。当设备被多个系统访问时，例如在集群中，这会导致意外的数据更改。\n\n## by-id\n\n此目录中的条目提供一个符号名称，该符号名称通过唯一标识符(与所有其他存储设备不同)引用存储设备。标识符是设备的属性，但不存储在设备的内容(即数据)中。例如:\n\n```\n/dev/disk/by-id/scsi-3600508e000000000ce506dc50ab0ad05\n/dev/disk/by-id/wwn-0x600508e000000000ce506dc50ab0ad05\n```\n\n该`id`从设备的全局`ID`（`WWID`）或设备序列号中获取。`/dev/disk/by-id`条目也可能包含一个分区号。例如:\n\n```\n/dev/disk/by-id/scsi-3600508e000000000ce506dc50ab0ad05-part1\n/dev/disk/by-id/wwn-0x600508e000000000ce506dc50ab0ad05-part1\n```\n`World Wide Identifier`（`WWID`）可用于可靠的识别设备。`SCSI`标准要求所有`SCSI`设备提供一个持久的、系统无关的`ID`。`WWID`标识符保证对每个存储设备都是唯一的，并且独立于用于访问设备的路径。\n\n这个标识符可以通过发出`SCSI`查询来获取设备标识重要厂商数据(第`0x83`页)或单位序列号(第`0x80`页)。从这些`wwid`到当前`/dev/sd`名称的映射可以在`/dev/disk/by-id/`目录中维护的符号链接中看到。\n例如，具有页`0x83`标识符的设备将具有:\n\n```\nscsi-3600508b400105e210000900000490000 -> ../../sda\n```\n\n或者，具有页`0x80`标识符的设备将具有:\n\n```\nscsi-SSEAGATE_ST373453LW_3HW1RHM6 -> ../../sda\n```\n\n`Red Hat Enterprise Linux 5`自动维护从基于`wwid`的设备名称到系统上当前`/dev/sd`名称的正确映射。应用程序可以使用`/dev/ disk/by-id/`的链接引用磁盘上的数据，即使设备的路径改变，甚至当从不同系统访问该设备时都是如此。\n\n但是当设备被插入到硬件控制器的端口时，而这个端口又受另一个子系统控制（即多路径），`by-id`的值也会改变。多路径设备会在下面详细讲解。\n\n## by-partlabel && by-partuuid\n\n这两个和上面提到的`by-label`和`by-uuid`类似，只不过是在`GPT`磁盘上。\n\n## 多路径设备\n\n所谓多路径设备指的是从一个系统到一个设备存在多个路径，这种现象主要出现在光纤网络的`SAN`下，主要是做数据链路冗余以达到高可用的效果，**即对应底层一个物理设备，可能存在多个路径表示它**，具体参考`refer`下面有关文章。\n\n如果从一个系统到一个设备有多个路径，那么` device-mapper-multipath`使用`WWID`来检测它。然后在`/dev/mapper/wwid`中显示一个“伪设备”，例如`/dev/ mapper/3600508b400105df70000000ac0000`。\n\n`Device-mapper-multipath`显示映射到非持久标识符：`Host:Channel:Target:LUN`， `/dev/sd`名称，以及`major:minor`号。\n\n\n```\n3600508b400105df70000e00000ac0000 dm-2 vendor,product \n[size=20G][features=1 queue_if_no_path][hwhandler=0][rw] \n\\_ round-robin 0 [prio=0][active] \n \\_ 5:0:1:1 sdc 8:32  [active][undef] \n \\_ 6:0:1:1 sdg 8:96  [active][undef]\n\\_ round-robin 0 [prio=0][enabled] \n \\_ 5:0:0:1 sdb 8:16  [active][undef] \n \\_ 6:0:0:1 sdf 8:80  [active][undef]\n```\n \n`Device-mapper-multipath`在系统上自动维护每个基于`wwid`的设备名称和其对应的`/dev/sd`名称的正确映射。这些名称即使是在路径发生改变时也是持久的，并且当从不同的系统访问设备时它们仍然是一致的。\n\n## 总结\n\n为了方便管理和使用设备，`linux`操作系统给我们提供了上面这么多持久化命名方式。它们各自有各自的优势和使用场景。`by-label`和`by-uuid`都和文件系统相关，`by-label`是通过读取设备中的内容获取，`by-uuid`则是随着每次文件系统的创建而创建，所以`by-uuid`的持久化程度更高一些；持久化程度最高的要属`by-path`和`by-id`了，因为它们都是根据物理设备的位置或者信息而和链接做对应的，`by-path`会因为路径的变化而变化；而`by-id`则不会因为路径或者系统的改变而改变，它只会在多路径的情况下发生改变。这两个在通过虚拟设备名称寻找物理设备的场景下都十分有用。\n\n多路径设备则帮助我们在`SAN`等场景下提高了数据传输的可用性，目前由于网络带宽的发展，它在`iscsi`场景下也频繁亮相。\n\n## Refer\n\n* [Persistent block device naming][1]\n* [Understanding Device Names Managed by the udev mechanism][2]\n* [Udev][3]\n* [PERSISTENT NAMING][4]\n* [How to find WWN for /dev/sdc][5]\n* [understanding /dev/disk/by- folders][6]\n* [multipath 路径切换][7]\n* [multipath多路径实验01-构建iSCSI模拟环境][8]\n* [multipath多路径实验02-配置多路径软件][9]\n* [Linux 内核中的 Device Mapper 机制][10]\n* [DM-MULTIPATH 概述][11]\n\n[1]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_%28简体中文%29\n[2]: http://fibrevillage.com/storage/578-understanding-device-names-managed-by-the-udev-mechanism\n[3]: https://wiki.archlinux.org/index.php/Udev_%28简体中文%29#.E8.AE.BE.E7.BD.AE.E9.9D.99.E6.80.81.E8.AE.BE.E5.A4.87.E5.90.8D\n[4]: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/online_storage_reconfiguration_guide/persistent_naming\n[5]: https://www.linuxquestions.org/questions/linux-server-73/how-to-find-wwn-for-dev-sdc-917269/\n[6]: https://unix.stackexchange.com/questions/86764/understanding-dev-disk-by-folders\n[7]: https://blog.csdn.net/jiaping0424/article/details/51591257\n[8]: http://www.cnblogs.com/jyzhao/p/7200585.html\n[9]: http://www.cnblogs.com/jyzhao/p/7208620.html\n[10]: https://www.ibm.com/developerworks/cn/linux/l-devmapper/index.html\n[11]: https://access.redhat.com/documentation/zh-cn/red_hat_enterprise_linux/6/html/dm_multipath/mpio_description\n[12]: http://cdn.tony-yin.site/fork-in-the-road.png\n","tags":["Disk"],"categories":["tech"]},{"title":"Python Snack 最佳实践","url":"/2018/06/03/Python-Snack-Best-Practice/","content":"\n<center>![python snack][3]</center>\n\n  了解`linux`的人应该听说过`Newt`，`Newt`是一个为`RedHat`安装程序而设计的基于文本的窗口开发工具，它是由`c`语言编写并不依赖`X`包，`linux`下的`dialog`和`whiptail`都是基于它。而我们今天讨论的`snack`则是`Newt`提供的`python`接口，`redhat`的系统都自带这个模块，本文就如何使用`snack`制作伪终端页面展开讲解，并配合代码展示实现效果。\n\n<!--more-->\n  \n  \n## 用餐介绍\n  \n为啥说是最佳实践呢？因为我使用`snack`的过程中，上网查阅相关资料，发现有关信息甚少。偶尔几篇文章都是处于`API`或者`Demo`的级别，并且讲的都不全，更别说高级扩展功能了。我正好工作需要给我们的一个系统做一个终端部署控制台`UI`，所以我就使用了`python snack`来实现，期间不断新需求，不断迭代，从基本页面到增删改查，再到校验、再到配置导入、再到进度条等等，不断的迭代开发让我对`snack`不断地加深认知，它支持的或不支持的我都想办法一一解决，所以在这把我这段时间的收货进行总结并分享给需要的人。\n\n## 上菜单\n\n本文实践的需求是做一个部署控制台工具，该工具主要分为三个阶段：基础配置、高级配置和部署进度。基础配置页面需要我们创建一些主机，填写一些主机的信息，比如`IP`、`Hostname`和`Password`，然后高级配置我们也需要创建一些主机，不过我们可以复用基础设置的主机，所以我们的工具要支持在高级配置中导入基础配置的功能，在高级配置中我们还有一个全局配置，也就是不限于单个主机的配置（其中具体部署原理和是非，我就不多展开赘述，这不是本文的重点）。最后就是进度条页面，我们可以展示部署的过程阶段和相关时间信息。\n\n<center>![deploy console][1]</center>\n\n> 项目地址：https://github.com/tony-yin/Best-practice-of-python-snack\n\n## 开胃凉菜\n\n### 远古时代\n\n先上几道凉菜，给大家开开胃。所谓的凉菜就是介绍一下`python snack`的基础组件，基础组件很多，类似`html`，主要有：\n\n* Textbox\n* TextboxReflowed\n* Button\n* Compactbutton\n* Checkbox\n* Listbox\n* SingleRadioButton\n* Scale\n* Entry\n\n然后就是一些组合组件，也就是基于上述基础组件封装而得到，主要有：\n\n* RadioBar\n* ButtonBar\n* CheckboxTree\n\n上面这些组件就是所有的基础组件（组合组件也算基础组件），这些组件最终呈现还需要`grid`和`form`这两个组件，`grid`表示“网格”的意思，跟`html`中的`table`类似，由行和列组成，我们的基础组件需要放在网格中来实现页面布局；而`form`也类似“表单”，我们需要把`grid`填充到`form`中，运行后，就可以看到图形化页面了。\n\n### 工业革命\n\n经过上面基础组件的介绍，想必你对`snack`的组件有了充分的了解，这时候你可以参考文末的`refer`做几个小`demo`，做了之后你会发现页面是出来了，emmm... 可是感觉好繁琐哦，很多重复性的代码，而且页面布局也怪怪的，如果要把布局搞好，又需要加很多代码。\n\n我们把用基础组件的阶段称之为“远古时代”，每做一个`window`，都得一瓦一砖地慢慢堆砌，这样效率太低了，所以我们急需一波“工业革命”来提高生产力。\n\n`python snack`似乎考虑到了这个问题，它在上述基础组件之外还提供了`dialog`相关组件，`dialog`组件即集大成者，一个`dialog`组件就是一个`window`，也就是我们上面所说的`form`，并且该`form`中填充了必需的各种基础组件，`dialog`组件主要有：\n\n* ListboxChoiceWindow\n* ButtonChoiceWindow\n* EntryWindow\n\n### 返璞归真\n\n当今社会，大家吃惯了大鱼大肉，反而更是想念农村的野味。同理，我们用惯了“工业革命”的产物，发现虽然可用，但是仅仅停留在基础可用级别上，想换换样式，加加自己的定制化需求，都是有限的，完全达不到新需求的技术实现要求。所以，我们不能只知道用别人实现的现成的产物，我们可以尝试着“返璞归真”一下，回归最初的“远古时代”，自己实现一把“工业革命”。所谓的“dialog”组件无非也就是基础组件的封装而已，我们也可以自己实现一套自己的组件库，这个在前端是非常流行的，例如`font-awesome`、`iview`、`ant-design`等等。这里我们自己实现了以下`dialog`：\n\n* ExtButtonChoiceWindow\n* ExtAlert\n* ExtCheckboxWindow\n* ExtListboxChoiceWindow\n* ExtEntryWindow\n* ExtPwdEntryWindow\n* ExtProgressWindow\n* ExtTextWindow\n\n扩展的功能主要有：\n\n* 热键支持扩展\n* 按钮样式扩展\n* 布局大小自动化扩展\n* 暗文输入框扩展\n* 弹出窗口扩展\n* 进度条窗口信息展示扩展\n* 动态展示扩展\n\n> 扩展组件库地址：[widget extend library][2]\n\n## 管饱正菜\n\n凉菜不够，正菜来凑。上面就是把`python snack`的`API`罗列了一下，做个小`Demo`还行，但是距离产品化还很远，接下来我结合我做部署控制台工具的实践经历分享一下几个“正菜”，必须够硬，不接受反驳，不接受批评， O(∩_∩)O ~\n\n### 热键\n\n`python snack`提供了两种帮助用户使用的途径，一种是窗口下方的操作提示栏，另一个就是热键了。热键就是快捷键，比如我们可以敲击键盘上面的`ESC`键实现页面的返回。我们可以通过调用`grid`的`runOnce`接口获取热键的输入，例如`hotkey = g.runOnce()`，然后我们根据`hotkey`的值进行判断并执行对应的操作。\n\n### 页面切换\n\n当我们存在多个页面的时候，我们需要页面切换的功能，翻阅文档，并没有发现提供类似的功能。在我们的工具中，页面切换主要有两种方式，一种是点击`button`，一种是热键，既然没有原生的页面切换接口，我们就根据触发方式手动切换页面。比如我们想实现页面`1`点击`next`按钮想跳转页面`2`，那我们只需要获取`button`的返回值，判断是否为`next`，如果是`next`，直接调用页面`2`的方法即可，热键同理，即判断热键内容是否为对应热键。\n\n```\nret, button, lb = ExtListboxChoiceWindow(\n    screen, \n    'Distribute Storage Config',\n    'Distribute Storage Config',\n    ips,\n    buttons=(\"prev\", \"next\", \"exit\"),\n    width=50,\n    height=5,\n)                                                                                                                                        \nif button == \"exit\" or ret == \"ESC\":\n    screen.finish()\nelif button == \"prev\":\n    Welcome_Deploy_Window()\nelif button == \"next\":\n    Additional_Config_Window()\nelif lb is not None:\n    Basic_Host_Window(lb)\n```\n\n### 增删改查\n\n增删改查永远是一个软件系统绕不开的基础功能。\n\n**“查”：**\n\n首先是整体查看，我们可以通过一个列表展示所有信息，这时候我们可以用`ExtListboxChoiceWindow`组件来实现；然后就是单个查看了，我们可能有多条信息，我们想查看单个信息的详细内容时，我们可以通过点击具体的`item`进入详细信息的`dialog`，如何实现呢？`listbox`中有一个`current`的概念，也就是`listbox`中每个`li`的唯一标识，我们可以用列表的`index`来填充，因为往往列表页面的信息也无非是数组或者是列表的方式，我们获取到当前的`current`，即获取到数组的索引，然后就是根据索引查值了，我们再调用新增页面，将查到的值赋值到`Textbox`即可，`Textbox`有一个`setText`就是做这个事情的。当然我们的`ExtEntryWindow`组件也可以做到赋值填充。请参考上述代码中的`lb`，其实就是`listbox`的`li.current()`接口。\n\n**“增”：**\n\n我们可以通过一个新增按钮或者`listbox`中的一个`li`作为新增按钮来触发新增操作，点击后出现一个`dialog`，`dialog`中有一些`Textbox`、`Radio`、`Checkbox`等。\n\n```\ndef Basic_Host_Window(current, data=None):\n    buttons = [ 'save', 'cancel', 'exit']\n    if not data:\n        data = ['IP Address:', 'Hostname:', 'Password:']\n        if current != 'add':\n            data = get_format_data(Basic_Config[current], BASIC_TYPE)\n            buttons.insert(1, 'Delete')\n\n    host = ExtEntryWindow(\n        screen,\n        '{} host'.format('Add' if current == 'add' else 'Edit'),\n        'Please fill storage host info.',\n        data,\n        width = 40, \n        entryWidth = 40, \n        buttons = buttons\n    )                            \n```\n\n**“改”：**\n\n修改操作的方法是在`list`页面选中需要修改的项，然后进入详情页面，可以查看之前创建时填写的信息，也就是我们在“查”中查看单个信息提到的方式，我们所要做的就是在用户点击`save`按钮的时候，获取用户编辑后的数据，再进行一次修改即可，在我们工具中，此操作就是根据索引修改数组中对应索引的数据而已。\n\n**“删”：**\n\n有增就有删，这边我暂时还没实现批量删除的功能，一方面`python snack`的支持功能有限，一方面时间有限，所以我只实现了单个删除的功能，在新增和编辑的页面添加一个`delete`按钮即可，为了提醒用户错删，我们还要加上一个确认提示框。\n\n```\nif host[1] == \"delete\":\n    button = ExtButtonChoiceWindow(\n        screen,\n        'Delete host',\n        'Are you sure to delete current host?'\n    )\n    if button == \"ok\":\n        del(Basic_Config[current])\n    else:\n        Basic_Host_Window(current)\n```\n\n### 组件扩展\n\n构建自己的组件库真的很有必要，对于默认的`button`样式，我真是吐槽到不想再吐槽，它居然还认为自己的`border`很`nice`？！所以最终构建自己的组件库的初衷就是想把各个`dialog`中的`button`改为`compactbutton`，没办法，默认的`dialog`组件不给改呀，所以我们得自己返璞归真一下。\n\n当然我们做扩展组件库，也不是仅仅因为一个`button`样式，还有很多新需求都要依赖自己扩展的组件。比如热键，原生`dialog`无法支持热键；还有进度条的进度时间和任务信息展示；还有`Gridform`的动态布局等等。具体就不一一介绍了，想深入了解的直接看代码，做个小`Demo`，一目了然。\n\n## 爽口甜菜\n\n充实的正菜吃饱了，是时候来一波甜菜漱漱口，解解渴了。\n\n在做进度条页面的时候，想除了显示进度任务完成信息之外，还想显示一下开始时间和花费时间。发现`python`的`time`模块比较坑爹，对于时间差的转换支持不行，查阅资料只发现`datetime`可以将时间差转换为微秒、秒和小时三个单位，但是我想实现时间差的自动转换，也就是`60s`自动转换为`1min`，`60min`转为`1h`，`24h`转为`1d`，超越天为单位的我就不进行转换了，逻辑不难，只是拿出来分享给有需要的人，不必重复造轮子罢了。\n\n```\ndef get_time_interval(start_time):\n    start_time = datetime.fromtimestamp(start_time)\n    now_time = datetime.fromtimestamp(time.time())\n    interval = (now_time - start_time).seconds\n    format_interval = get_format_interval(interval)                                                                                          \n    return format_interval\n\n\ndef get_format_interval(interval):\n    if interval < 60:\n        format_interval = \"{}s\".format(str(interval))\n    elif 60 <= interval < 60*60:\n        format_interval = \"{}min {}s\".format(\n            str(interval/60), str(interval%60))\n    elif 60*60 <= interval < 60*60*24:\n        format_interval = \"{}h {}min {}s\".format(\n            str(interval/(60*60)),\n            str(interval%(60*60)/60),\n            str(interval%(60*60)%60)\n        )\n    elif 60*60*24 <= interval:\n        format_interval = \"{}d {}h {}min {}s\".format( \n\t\t\tstr(interval/(60*60*24)),\n            str(interval%(60*60*24)/60*60),\n            str(interval%(60*60)/60),\n            str(interval%(60*60)%60)\n        )\n        \n    return format_interval\n```\n\n## 用餐总结\n\n原本只是想做一个终端图形化的进度条页面，但是后续需求越来越多，导致做成了一个部署控制台工具，整个工程开发和优化花了大约两个星期的时间，项目中遇到的很多难点和问题很多都与`python snack`无关，所以没有做详细解释，就比如上述的甜菜，大家有兴趣的自行翻阅代码即可。\n\n`python snack`还有很多未知的我没有使用，比如`checkbox tree`等，但我相信万变不离其宗，有了这次实践，其他组件的使用和扩展应该不会花很多时间，其实做这个东西，我最深的感触就是前端发展的迅速，`python snack`是`2000`年初的产物了，很多页面逻辑跟`jQuery`比起来要弱的多，更别说现在的`angular`，`vue`等等了，但是领域不同，毕竟是伪终端页面，能做成这样已经不错了。如果是真正的桌面图形化界面（`GUI`），有`pyqt`这种神器，功能貌似很强大。\n\n我在之前的一个项目中，就使用过`python snack`做的控制台，当然当时不知道是用这个技术做的，当时觉得蛮牛的，尝试过修改终端文字成汉子，后来没有成功，便不了了之。这次机缘巧合，工作需要做这么一个控制台，在工作中学习和使用自己感兴趣的技术的感觉真是爽呀。工作中运用技术和自己业余时间学习新技术并做个小`Demo`完全是不一样的，工作中运用会不断有新需求，不断精益求精，不断深入。所以以工作作为平台，实现自己的技术价值，会有很大的成就感，与大家共勉咯。(#^.^#)\n\n> 完整项目代码地址：https://github.com/tony-yin/Best-practice-of-python-snack\n\n[1]: http://cdn.tony-yin.site/deploy_console.gif\n[2]: https://github.com/tony-yin/Best-practice-of-python-snack/blob/master/widget_extlib.py\n[3]: http://cdn.tony-yin.site/python_snack.jpg\n","tags":["Console"],"categories":["tech"]},{"title":"Daily Article Vol6(2018/5/1 ~ 2018/5/31)","url":"/2018/06/01/Daily-Article-Vol6/","content":"\n<center>![May][26]</center>\n\n五月份主要基于`python sna`实现了一个终端伪图形化界面的部署控制台，提供了填写配置信息、校验信息、导入信息和进度监控等功能，详情请见[【Python Snack 最佳实践】][27]。这是我之前一直感兴趣想做的事情，这次正好借助工作的契机对其有了深入的了解和学习，收获颇多。五月份假期多，团建，还有工作进度赶，导致自己的阅读时间受到了影响，大多数阅读都是围绕着工作内容，极客时间这些`APP`、技术博客和微信公众号的很多技术资源，自己都没有规律地学习。这些都不是借口啦，主要还是陷入吃鸡，不能自拔，哈哈哈。（已经卸了，我这个人容易上瘾然而又有点嫌麻烦，这对我来说是最简单有效的方式）\n\n<!--more-->\n\n加入公司两个多月了，工作内容不断增多，难度也不断加大，由起初的合作工作到现在的独立负责某个`task`，也渐渐地忙了起来，总体来说还是游刃有余，能把自己的工作量`cover`住，希望自己继续加油。不过，这段时间大多数工作内容跟`ceph`关系不大，比如`ctdb rados`、`megaraid location`、`deploy console`等，毕竟是做产品，工作内容还是依赖产品需求，不过我还是不希望以后工作内容长期游离于`ceph`之外，因为今天的期望就是深入`ceph`，深入存储，如果工作上满足不了我的话，自己课余时间要挤一点时间做做研究了。不过好在现在项目上除了`ceph`也有很多东西是目前的我所不懂的，有学习的机会也不错，只要每天都能学习到不熟悉的东西便是极好的。除了存储，我们组也在往大数据和检索的方向靠，这两个我也都蛮感兴趣的，最近也稍微了解了下`elaticsearch`，目前好像很火的样子，六月底南京有一个`ES`线下沙龙，已经报名参加了，吼吼。\n\n加油咯 ~\n\n1. [Django实现CAS+OAuth2][2](5/5) <i class='fa fa-star'></i>\n2. [NVMe over Fabric][3](5/6) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n3. [解析Ceph: Librbd–块存储库][1](5/7)\n4. [Docker 核心技术与实现原理][4](4)(5/11 ~ 5/13) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n5. [【Django】@login_required用法简介][6](5/14)\n6. [request.META里包含了哪些数据？][9](5/15)\n7. [CTDB代码流程简要梳理][11](5/16) <i class='fa fa-star'></i>\n7. [ctdb原理介绍][10](5/17~5/18) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n6. [MegaCli:如何使用命令行监控RAID卡状态][8](5/18)\n5. [Python实现控制台中的进度条][5](5/21)\n6. [用PYTHON写NCURSES UI][7](5/22)\n7. [NEWT 程序设计指南][11](5/23)\n8. [Quick Guide to Python's Snack Module][13](5/23)\n9. [python在linux制作图形界面（snack）][15](5/23)\n9. [python在linux(anaconda)的图形界面（snack）][14](5/24)\n10. [An introduction to npyscreen][16](5/24)\n11. [Creating UI Controls under Linux Shell Console using whiptail Utility][17](5/25)\n12. [Dialog: An Introductory Tutorial][18](5/25)\n13. [Create a progress bar in bash][19](5/25)\n14. [How to Monitor Progress of (Copy/Backup/Compress) Data using ‘pv’ Command][20](5/25)\n15. [Python中如何写控制台进度条的整理][21](5/26) <i class='fa fa-star'></i>\n16. [可爱的 Python：Curses 编程][22](5/26)\n17. [PYTHON读取修改INI配置文件[CONFIGPARSER]][23](5/31)\n18. [Python 解析配置模块之ConfigParser详解][24](5/31) <i class='fa fa-star'></i>\n19. [Python(2.7.6) ConfigParser - 读写配置文件][25](5/31)\n\n[1]: http://www.wzxue.com/ceph-librbd-block-library/\n[2]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[3]: https://weibo.com/p/1001603934517592239583?mod=zwenzhang#_loginLayer_1525517479194\n[4]: https://draveness.me/docker\n[5]: http://www.cnblogs.com/lustralisk/p/pythonProgressBar.html\n[6]: https://blog.csdn.net/feng88724/article/details/7262514\n[7]: https://coolshell.cn/articles/677.html\n[8]: https://opstrip.com/2017/06/14/howto-monitor-raidCard-status-with-commandline-MegaCli/\n[9]: http://www.nowamagic.net/academy/detail/1318909\n[10]: https://blog.csdn.net/liuying_1001/article/details/77994642\n[11]: https://blog.csdn.net/naipeng/article/details/77162764\n[12]: https://www.ibm.com/developerworks/cn/linux/guitoolkit/newt/index.html\n[13]: http://www.wanware.com/tsgdocs/snack.html  \n[14]: https://blog.csdn.net/taiyang1987912/article/details/49273601\n[15]: http://www.cnblogs.com/Xjng/p/3869869.html\n[16]: https://npyscreen.readthedocs.io/introduction.html#example-code\n[17]: https://helloacm.com/creating-ui-controls-under-linux-shell-console-using-whiptail-utility/\n[18]: https://www.linuxjournal.com/article/2807\n[19]: https://askubuntu.com/questions/747143/create-a-progress-bar-in-bash\n[20]: https://www.tecmint.com/monitor-copy-backup-tar-progress-in-linux-using-pv-command/\n[21]: http://blog.ihipop.info/2010/10/1736.html\n[22]: https://www.ibm.com/developerworks/cn/linux/sdk/python/python-6/index.html\n[23]: http://wklken.me/posts/2012/02/19/python-ini-configparser.html\n[24]: https://www.jianshu.com/p/4202a2051668\n[25]: http://www.cnblogs.com/huey/p/4334152.html\n[26]: http://cdn.tony-yin.site/5-2.png\n[27]: http://www.tony-yin.site/2018/06/03/Python-Snack-Best-Practice/\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Ctdb Rados（二）：多场景断网高可用","url":"/2018/05/20/Ctdb-Rados-2/","content":"\n<center>![ping][2]</center>\n\n之前写过一篇文章【[Ctdb Rados方式导致All Banned的问题][1]】，谈到了当`ctdb`将`recovery lock`设置成`rados`的方式后，断网`master`节点会造成所有`ctdb`节点`All Banned`，主要原因是`master`意外断网没有释放锁，其他节点无法获取到锁，当时的解决方案是每`5`分钟检查一次`ctdb`状态，如果连续两次发生了`All Banned`的情况，则手动删除`lock`，这种做法在最近的测试中遇到了一些问题，本文对这些问题进行剖析并对相应的解决方案进行分享。\n\n<!--more-->\n\n> 完整代码地址：https://github.com/tony-yin/Ctdb-Rados-Monitor\n\n## 场景一\n\n如果基于原来的做法，`ctdb`发生`All Banned`的情况，需要十分钟的监控时间加上两分钟左右的`recovery`时间，也就是说大概需要十二分钟才能恢复`ctdb`服务，这样看来高可用有点名实其副了，这个也会明显地影响存储业务的正常运行。后来，我们讨论出新的方案：每`5s`检查一次`ctdb`的状态，`All Banned`的次数累计到`5`次才确定为该故障场景，然后手动删除`lock`，最终要保证`ctdb`能够在`2min`内完成恢复。\n\n### 问题1\n\n`cron tab`最短周期只支持分钟级别，所以如何`5s`检查一次便是一个问题。\n\n代码是死的，人是活的，虽然`cron tab`只支持分钟级别，但是我们可以每分钟调用一个脚本，然后在这个脚本中遍历`12`次，每次调用`ctdb monitor`脚本，然后`sleep 5s`，这样就可以达到每`5s`检查一次`ctdb`的效果了。\n\n```shell\n# ctdb_monitor\n* * * * * root /etc/ctdb/cron-seconds\n\n# cron-seconds\n#!/bin/bash\nfor((i=1;i<=12;i++));do\n    ../monitor_ctdb\nsleep 5\ndone\n```\n\n这样检查到`ctdb`发生`All Banned`情况，只需要花费`25s`，剩下的就是`recovery`的时间了。\n\n## 问题2\n\n当`ctdb master`节点的`network`服务断掉，其他两个节点（我的开发环境是三节点的虚拟机环境）便会选举一个为`master`节点，然后去获取`lock`，因为原`master`没有释放锁，导致所有节点`All Banned`，即使我们手动删除了锁，但是这时候其他两个节点仍然处于`Banned`的情况，需要等到`Ban Timeout`才会再次尝试获取锁并开始恢复过程，这个`timeout`的时间是`300s`，即`5min`，这显然是我们不能接受的，所以我们要在删除`lock`后，重启所有节点的`ctdb`服务。\n\n不过该如何触发该重启操作呢？\n\n我们在删除`lock`后将`ctdb`所有节点的`ip`作为对象存进`rados`中，然后在每`5s`监控的脚本中，查看`rados`中是否存在本节点的`ip`对象，如果有，则尝试重启`ctdb`操作，重启后便删除该对象。\n\n```shell\nfunction save_nodes_ip() {\n    nodes=$(ctdb listnodes)\n    for node in $nodes; do\n        echo \"$node\" > $node\n        rados -p rbd put $node $node\n        rm -f $node\n    done\n}\n\nfunction get_current_node_ips() {\n    ips=$(/usr/sbin/ip addr | grep \"inet \" | awk '{print $2}')\n    echo $ips\n}\n\nfunction monitor_nodes_ip_in_rados() {\n    ips=$(get_current_node_ips)\n    for ipinfo in $ips; do\n        ip=${ipinfo%/*}\n        if $(timeout 10 rados -p rbd ls | grep \"$ip\" -qw); then\n            systemctl restart ctdb\n            rados -p rbd rm $ip\n        fi\n    done\n}\n```\n\n至于为什么三个节点的`ip`都要存入`rados`，这个是因为原`master`节点恢复网络后，`ctdb`服务的状态为`failed`，同样需要重启`ctdb`服务才能正常恢复原`master`节点。 \n\n**注意：**\n\n这边有两个问题，当时浪费了我不少时间，问题不是多么深奥，但是不易发现。。。\n\n第一个问题便是`ips=$(/usr/sbin/ip addr | grep \"inet \" | awk '{print $2}')`这行代码，原来的写法是`ips=$(ip addr | grep \"inet \" | awk '{print $2}')`，当时发现`ip`总是获取不到，然后无论是命令行还是脚本运行都可以正常获取到，后来还是同事提醒才发现在`crontab`脚本中，`shell`命令默认是`/usr/bin/`下的，而`ip`命令则是`/usr/sbin/`下，所以这里的命令我们需要全路径。（这个需要格外注意！！！被坑的不要不要的。。。）\n\n第二个问题便是`rados -p rbd ls | grep \"$ip\" -qw`这行代码，当时没注意写成了`rados -p rbd ls | grep \"$ip\" -w`，发现`if`判断时常有问题，一开始还以为不能`grep`数字什么的，后来才发现没有加`q`，`q`表示安静模式，不打印任何标准输出，如果有匹配的内容则立即返回状态值0。\n\n## 场景二\n\n“断网”这个词不够具体，在实际生产环境中，一个集群中，一般都会有多个网络，就拿本人的`ceph`集群环境来说（物理机环境，并非前文提及的虚拟机开发环境），`ceph`有个`public network`和`cluster network`，而`ctdb`也有它的`node network`和`public network`，`ceph`的`public`和`ctdb`的`public`是同一网段，`ceph`的`cluster`是单独网段，`ctdb`的`node`是单独的网段。所以`ctdb master`断网可以分为三种情况：\n\n* 拔掉`ctdb master node`网段网线\n* 拔掉`ctdb master public`网段网线\n* 断掉`ctdb master network`服务\n\n当拔掉`ctdb master public`网段网线，这没有什么好说的，`ctdb master`节点服务还存在，只是`master`节点上的`public address`不可用了，会漂移到其他节点上。\n\n### 问题1\n\n当拔掉`ctdb master node`网段网线后，`master`节点仍然有`public`网卡，（**这里注意**）它仍然可以获取其他`ctdb`节点的状态，而其他节点却不可以获取它的状态，因为`master`的`node`节点`ip`不存在。所以造成的结果就是原`master`节点还默认自己是`master`节点，而其他的节点却又选举出了新的`master`，我们的脚本因为`All Banned`手动删除了`lock`，这时候其他节点可以正常恢复`ctdb`服务，但是当`ctdb master`节点断网再恢复后，它还以为自己是`master`，会不断去获取锁，而原来的锁已经被我们手动删除，这时候新的锁被新的`master`掌握，所以此时产生脑裂，我们要牺牲原`master`节点，也就是断网节点，所以需要重启它。这个重启触发机制我们是通过在每次删除`lock`之后在`rados`中存入`ctdb`所有节点的`ip`作为`object`（这就是为什么要存入所有节点的`ip`），然后只要发现有这个`object`便执行`ctdb`重启操作，然后便删除这个对象。至于为什么要存所有对象是因为除了原`master`需要重启之外，另外两个正常节点发生`All Banned`的情况，默认`timeout`时间是`300s`（这个上面也提到过），我们为了减少恢复时间，直接在删除`lock`后重启`ctdb`；\n\n## 问题2\n\n由于现在`ctdb`的锁是放在`rados`中，而不是以前的`cephfs`的方式了。所以当`master`断网再恢复时，它会不断地去`rados`获取他原来的锁，这是获取锁的进程越来越多，会阻塞住`rados`服务，我们可以通过`ps -ef | grep rados_helper`看到进程不断变多，那么`rados`服务不能正常读写就影响到我们上一条的机制，不能读`rados`中是否含有本节点`ip`的对象，就没办法进行重启操作，那么这样它就会不断地继续获取`lock`，所以我们在这里又加了一个机制，如果`ps -ef | grep rados_helper`的数目超过`6`个，就默认启动重启`ctdb`服务。\n\n```shell\nfunction monitor_get_lock_timeout() {\n    count=$(ps -ef | grep rados_helper | wc -l)\n    if [ $count -ge $RADOS_HELPER_PROCESS_MAX ]; then\n        systemctl restart ctdb\n        update_last_ctdb_restart_time\n    fi\n}\n```\n\n## 问题3\n\n`ctdb`目前重启的机制有点多，有自身自带的故障重启，也有我们监控脚本的异常情况，很容易发生重复重启，还有可能`rados_helper`堆积的进程很多，比如`20`个，我们的脚本是`5s`一次，也许`20`个的时候重启了，过`5s`，进程释放也需要时间，可能此时还有`10`个，那么大于我们规定的`6`个，就会继续重启，这种重复重启没有必要，所以我们要加上`ctdb`重启的周期限定`2min`。\n\n```shell\nfunction get_ctdb_restart_interval() {\n    last_time=$(get_ctdb_restart_last_time)\n    if [ -z \"$last_time\" ]; then\n        interval=$(expr $RESTART_CTDB_INTERVAL_MAX + 1)\n    else\n        current_time=$(date +%s)\n        interval=$(expr $current_time - $last_time)\n    fi\n    echo $interval\n}\n```\n\n考虑并解决以上提到的问题，基本上可以覆盖以上三种断网的场景了，在监控和管理`ctdb`的过程中，一定要小心，不能影响到业务正常运行。\n\n## 总结\n\n生产环境网络结构错综复杂，往往在虚拟机上开发的功能当时好好的，到了物理机上面测试会发生各种问题，此时，我们首先要搞清楚网络拓扑结构，熟悉硬件配置，各网段的作用和相互之间的关联，这样遇到问题我们可以顺藤摸瓜，同样`ctdb`的原理也需要掌握才能了解它各种行为的触发机制，才能更好的定制化监控和管理。之后我会花点时间好好地研究一下`ctdb`，然后再单独做分享。\n\n> 完整代码地址：https://github.com/tony-yin/Ctdb-Rados-Monitor\n\n[1]: http://www.tony-yin.site/2018/04/20/Ctdb-Rados-All-Banned/\n[2]: http://cdn.tony-yin.site/ping-600x450.png\n","tags":["Ctdb"],"categories":["tech"]},{"title":"Megaraid 磁盘定位","url":"/2018/05/12/Megaraid_Location/","content":"\n<center>![mega drive][2]</center>\n\n早前写过一篇【[利用Raid卡工具获取逻辑盘是否为SSD][1]】的文章，大概讲述了如何通过`raid`卡工具判断一个逻辑磁盘对应物理磁盘是否为`SSD`，当时主要提到了`megacli`和`sas3ircu`这两种工具，核心是如何通过`raid`卡工具定位到逻辑磁盘对应的物理磁盘的位置，当时的方式现在看来在有些场景会存在缺陷。\n\n<!--more-->\n\n当时的方案主要是先通过`lspci`获取`raid`卡型号，然后找到对应的`raid`卡型号，紧接着通过`lsscsi`命令获取逻辑磁盘的`targetid`，再通过`raid`卡工具根据`targetid`定位到对应的物理盘。当时的方案在多`controller`的场景下存在问题，可能会出现重复`target id`的情况，所以这时候只能再借助`controller id`来定位唯一的磁盘了。总而言之，想真正定位逻辑磁盘对应的物理磁盘，就必须要获取到磁盘的`controller id`，`enclosure id`和`slot number`，有了这三个参数，便可以获取该磁盘的信息，或者对该物理磁盘进行点灯、响音和做`raid`等操作。\n\n那么，具体如何定位逻辑磁盘的物理位置呢？且看下文分析\n\n## 获取 raid 卡信息\n\n通过`lspci`命令可以获取到操作系统上所有`raid`卡信息，我们可以看到每个`raid`卡最前面都有一串数字，比如第一行是`02:00.0`，第二行是`03:00.0`，这里的`02`和`03`表示的是`raid`卡的`busid`，即`raid`卡控制器在`pci`总线上的`id`。\n\n```shell\n[root@tony ~]# lspci | grep \"LSI Logic\"\n02:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3008 [Fury] (rev 02)\n03:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3108 [Invader] (rev 02)\n```\n\n## 获取磁盘 pcipath\n\n在`linux`中，一切皆文件，每个文件都有自己的唯一标识，对于磁盘而言，`pcipath`就是它的唯一标识，`pci`总线上面有很多控制器，比如`scsi`控制器，而磁盘又存在于`scsi`控制器上，所以我们可以在`lsscsi`命令获取到的`scsi`设备列表中查看到操作系统上的磁盘信息。\n\n以`sda`为例，我们可以在`/dev/disk/by-path`目录下查看到磁盘的`pcipath`\n\n```shell\n[root@tony ~]# ll /dev/disk/by-path/\ntotal 4\nlrwxrwxrwx 1 root root   9 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0 -> ../../sda\nlrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part1 -> ../../sda1\nlrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part2 -> ../../sda2\nlrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:1:0 -> ../../sdb\nlrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:10:0 -> ../../sdk\n```\n\n由于在`linux`中，`udev`是用户态的设备管理，所以我们也可以通过`udev`获取。\n\n```shell\n[root@tony ~]# udevadm info --query=symlink --name=sda\ndisk/by-id/scsi-36509a4c0ac86790022337b9105005435 disk/by-id/wwn-0x6509a4c0ac86790022337b9105005435 disk/by-path/pci-0000:02:00.0-scsi-0:2:0:0\n```\n\n这边我们可以得到磁盘`sda`的`pcipath`为`pci-0000:02:00.0-scsi-0:2:0:0`，`02`就是磁盘的`raid`卡的`bus id`，后面的`00`表示`channel id`，再后面的`0:2:0:0`就和`lsscsi`获取的一样了，其中`2`就表示`target id`。\n\n所以通过`bud id`，我们可以获取到磁盘对应的`raid`卡型号，根据对应的`raid`卡工具操作磁盘。这边我们只讨论`megaraid`，所以工具也就是`megacli`了。\n\n## 获取 controller id\n\n上面我们获取到了磁盘的`target id`和对应`raid`卡的`bus id`，而对于`megacli`工具而言，每个`raid`卡都有一个与之对应的`controller`。\n\n```shell\n[root@tony ~]# /opt/MegaRAID/MegaCli/MegaCli64 -AdpGetPciInfo -aall -NoLog\n\nPCI information for Controller 0\n--------------------------------\nBus Number      : 2\nDevice Number   : 0\nFunction Number : 0\n\nPCI information for Controller 1\n--------------------------------\nBus Number      : 3\nDevice Number   : 0\nFunction Number : 0\n\n\nExit Code: 0x00\n```\n\n这边我们可以看到`megacli`获取到了两个`controller`，也就对应上面`lspci`获取到的两张`raid`卡。细心的朋友可以发现这边有一个`Bus Number`，分别为`2`和`3`，而我们上面获取到了`raid`卡的`bus id`分别为`02`和`03`，没错，这边的`Bus Number`和`bus id`是对应的，只是`Bus number`没有自动填补成两位数，所以我们可以通过`bus id`得到`sda`所对应的`controller`为`0`。\n\n> **注意：**\n> 原本系统中版本`8.07.07`的`megacli`工具获取`raid`卡信息的时候会存在问题，每次`Bus Number`都会变化，我们只要升级`megacli`即可，我这边是把`megacli`升级到了`8.07.14`版本。\n> \n> 安装包地址：https://github.com/tony-yin/Megaraid_location/blob/master/MegaCli-8.07.14-1.noarch.rpm\n\n## 获取磁盘组\n\n此时，我们拥有了`controller id`，可以获取该`controller`下所有的磁盘组信息。\n\n```shell\n[root@tony ~]# /opt/MegaRAID_new/MegaCli/MegaCli64  -LdPdInfo -a0 -NoLog\nAdapter #0\n\nNumber of Virtual Disks: 13\nVirtual Drive: 0 (Target Id: 0)\nName                :\nRAID Level          : Primary-1, Secondary-0, RAID Level Qualifier-0\nSize                : 558.375 GB\nSector Size         : 512\nIs VD emulated      : No\nMirror Data         : 558.375 GB\nState               : Optimal\nStrip Size          : 64 KB\nNumber Of Drives    : 2\nSpan Depth          : 1\nDefault Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU\nCurrent Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU\nDefault Access Policy: Read/Write\nCurrent Access Policy: Read/Write\nDisk Cache Policy   : Disk's Default\nEncryption Type     : None\nDefault Power Savings Policy: Controller Defined\nCurrent Power Savings Policy: None\nCan spin up in 1 minute: Yes\nLD has drives that support T10 power conditions: Yes\nLD's IO profile supports MAX power savings with cached writes: No\nBad Blocks Exist: No\nIs VD Cached: No\nNumber of Spans: 1\nSpan: 0 - Number of PDs: 2\n\nPD: 0 Information\nEnclosure Device ID: 32\nSlot Number: 12\nDrive's position: DiskGroup: 0, Span: 0, Arm: 0\nEnclosure position: 1\nDevice Id: 12\nWWN: 50000398181A974C\nSequence Number: 2\nMedia Error Count: 0\nOther Error Count: 0\nPredictive Failure Count: 0\nLast Predictive Failure Event Seq Number: 0\nPD Type: SAS\n...\n...\n```\n\n然后我们可以根据`target id`获取对应的磁盘组信息，`target id`与上面的`Target Id`所对应，这样我们可以过滤得到唯一的磁盘组信息。这边我们可以看到`sda`对应`Target Id`为`0`的磁盘组，该`raid`类型为`raid1`，虚拟磁盘组中有两块物理盘，然后我们可以获取这两块物理盘的`enclosure id`和`slot number`，这样再加上前文的`controller id`，我们就可以完完全全地定位到具体一块磁盘的物理位置。\n\n## 一键定位\n\n针对这种需求，本人根据以上逻辑写了一个简单的脚本可以一键获取磁盘的定位。\n\n```shell\n[root@tony ~]# ./get_disk_location.py sda\n['0:32:12', '0:32:13']\n```\n\n这边`0:32:12`分别表示磁盘的`controller id`，`enclosure id`和`slot number`。\n\n> 完整代码地址：https://github.com/tony-yin/Megaraid_location/\n\n## 总结\n\n之前的做法大部分场景可行，但是在一些场合偶尔会发现问题，总感觉还是不够靠谱，身边的人还有通过`sda`，`sdb`这种排列顺序来查找和`megacli`中显示磁盘的对应关系的，就更不靠谱了。`linux`操作系统是可以识别到具体硬件设备的，所以是肯定存在方法识别硬件对应的逻辑设备的，本文通过`pcipath`获取到设备的唯一标识，然后根据`pcipath`中的`bus id`和`megacli`中的`cobtroller`建立连接，最后通过`target id`锁定唯一磁盘组中的磁盘信息。\n\n通过这种方式，我们不需要肉眼判断，也不需要顾虑部分场景方案不适用，这完全就是操作系统使用的方式，使用这种最基础，最底层的方式实现，真是让人豁然开朗。这跟看源码类似，了解一个功能的背后具体实现，你才知道最正确的姿势，不用去碰，去凑，这种感觉真好。\n\n给大家推荐一本书《`Linux`设备驱动程序》，这本书详细讲解了`linux`中各种设备与驱动的细节，很底层也很枯燥，不过看完后应该会很有收获。希望大家在使用各种已有工具和框架的基础上，多去了解背后的实现机制，这样可以帮助我们更好地实现更深层次的需求。\n\n## Refer\n\n1. [Persistent block device naming][3]\n2. [hwraid][4]\n3. [showtools][5]\n4. [Linux SCSI 子系统剖析][6]\n\n[1]: http://www.tony-yin.site/2018/01/05/RaidCardToolUtils/\n[2]: http://cdn.tony-yin.site/mega_drive-600x450.png\n[3]: https://wiki.archlinux.org/index.php/Persistent_block_device_naming_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)\n[4]: https://github.com/eLvErDe/hwraid\n[5]: https://github.com/louwrentius/showtools\n[6]: https://www.ibm.com/developerworks/cn/linux/l-scsi-subsystem/\n","tags":["Megaraid"],"categories":["tech"]},{"title":"Django CAS Token 解决方案","url":"/2018/05/02/Django_CAS_Token_Solution/","content":"\n<center>![cas][6]</center>\n\n`CAS`单点登录主要是为了解决主系统和子系统的统一登录问题，能够做到任意一个子系统登录成功后，再登录其他子系统后不再需要认证，让用户不用重复地进行登录认证。`CAS`单点登录的方案很多，并且大多数都是采用`session`的方式，而本文结合个人实践，着重讨论`django cas token`的解决方案。\n\n<!--more-->\n\n本方案中，`cas`客户端和服务端都采用了开源项目，服务端是[django-mama-cas][1]，而客户端是[django-cas-ng][2]。\n\n## CAS Server\n\n服务端相比于客户端要简单地多，根据`github`步骤一步步下载和配置就好。\n\n### 下载\n\n```\npip install django-mama-cas\n```\n\n### 配置\n\n```python\n# settings.py\nINSTALLED_APPS = (\n    'mama_cas',\n)\n\n# 重要！，service是client的IP，是个数组，可以在后面添加SERVICE的HOST:PORT。\nMAMA_CAS_SERVICES = [\n    {\n        'SERVICE': 'http://127.0.1.1:8000',\n        'CALLBACKS': [\n            'mama_cas.callbacks.user_model_attributes',     # 返回除了password的所有Field\n            # 'mama_cas.callbacks.user_name_attributes', # 只返回 username\n        ],\n        'LOGOUT_ALLOW': True,\n        'LOGOUT_URL': 'http://127.0.1.1:8000/accounts/callback',\n    },\n]\n\n# urls.py\nurl(r'', include('mama_cas.urls')),\n```\n\n## Client\n\n首先是一些基本的客户端配置，比如`server ip`等，但是`django-cas-ng`默认是通过`session`的方式认证的，而我们需要通过`token`的方式认证，所以如果想继续用`django-cas-ng`来解决问题，那要么查看它是否有原生支持的接口，要么改源码。改源码可能不大友好，所以我优先研究了一下`django-cas-ng`的原生支持，无意中发现[view-wrappers-example][3]可以继承它原生的登录接口做一些封装，而我们完全通过继承原生的登录方法，然后加入我们的`token`相关代码。所以`urls.py`里面登录的方法我们写的是我们写在`view.py`中封装的登录方法，而并非默认的。\n\n```python\n# settings.py\nINSTALLED_APPS = (\n    # ... other installed apps\n    'django_cas_ng',\n)\n\nAUTHENTICATION_BACKENDS = (\n    'django_cas_ng.backends.CASBackend',\n)\n\n# 注意：这是cas server的地址\nCAS_SERVER_URL = 'http://127.0.0.1:8000'\n\n# 存入所有CAS 服务端返回的user数据。\nCAS_APPLY_ATTRIBUTES_TO_USER = True\n\n# urls.py\nimport view import *\nurl(r'^accounts/login$', cas_login, name='cas_login'),\n\n#view.py\nfrom django_cas_ng import views as baseviews\nfrom django.views.decorators.csrf import csrf_exempt\n\n@csrf_exempt\ndef cas_login(request, **kwargs):\n    r = baseviews.login(request, **kwargs)\n    if not request.user.is_anonymous():\n        token = get_token(request)\n        if token:\n            r.set_cookie('token', token)\n        else:\n            print 'Get token error'\n    else:\n        print('User is anonymous')\n    return r\n\ndef get_token(request, *args, **kwargs):\n    user = request.user\n    try:\n        request_hash = AuthToken.get_request_hash(request)\n        try:\n            token = generate_token()    # function used to geneate token, this place won't show more detail codes\n            token.refresh()\n        except IndexError:\n            pass\n    except Exception as e:\n        print e\n        return False\n    return token.key\n```\n\n生成`token`的方法我就不详细描述了，这边主要提供了一个思路，我们将`django-cas-ng`原生的登录方法进行了继承，然后生成`token`并放到了`session`当中。\n\n## 总结\n\n本文主要为`CAS Token`方案提供一个思路，如果集成进已有项目中，肯定会遇到很多细节问题，不过万变不离其宗，我们首先要熟悉手中运用的工具，然后要善于在此基础之上根据自己的定制需求进行开发，多看看文档和源码，每一次可能都会有新的发现。\n\n## Refer\n\n1. [使用django-mama-cas快速搭建CAS服务][4]\n2. [Django实现CAS+OAuth2][5]\n\n\n[1]: https://github.com/jbittel/django-mama-cas\n[2]: https://github.com/mingchen/django-cas-ng\n[3]: https://github.com/mingchen/django-cas-ng#view-wrappers-example\n[4]: https://www.jianshu.com/p/d97a3d367037\n[5]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[6]: http://cdn.tony-yin.site/bottle.png\n","tags":["CAS"],"categories":["tech"]},{"title":"Daily Article Vol 5 - (2018/4/1 ~ 2018/4/30)","url":"/2018/05/01/Daily-Article-Vol5/","content":"\n<center>![April][45]</center>\n\n这是`Daily Article`系列的第五篇，罗列了`2018`年`4`月的阅读清单。\n\n<!--more-->\n\n本月是在新公司工作的第一个月，主要做了利用`python`搭建`NAS`服务稳定性测试框架、`CTDB`使用`rados object`作为`lock file`、利用`django`做`CAS`单点登录。首先`python`搭建`NAS`服务稳定性测试框架的工作就是通过`python`对`NAS`服务进行连接，并进行读写操作，该框架不需要任何挂载操作便可以对`NAS`服务进行连接和读写操作，目前支持`NFS`、`CIFS`和`FTP`，具体请参考[基于Python的NAS稳定性测试框架][44]；然后就是`CTDB`使用`rados object`作为`lock\nfile`这种方案的实践，期间也遇到了`Nodes All Banned`这种问题并解决了；最后就是采用`django-mama-cas`作为服务端，`django-cas-ng`作为客户端，搭建了一套`CAS`环境，并对目前的项目进行集成。\n\n个人课余时间的话，一方面关注的是`python`和`django`有关的知识，因为工作中要用到，其次就是围绕着`ceph`和存储相关知识的了解和学习了。上周在扫盲区的过程中，看到了很久之前了解的`nvme over fabric`这个概念，然后又去深入地了解了一下。这一了解又引申出了无数的盲区，比如`RDMA`、`nvme`、`nvme over\nip`、`FC`等等，而这些技术也很新，文档不多，并且大多数都是很枯燥的，只能逼着自己慢慢地啃下去。感慨技术变革真是日新月异的同时，也感慨扫盲区的速度远远赶不上盲区生成的速度，每天都会有好多自己不知道或者不是特别了解原理的概念，等待着自己去学习和深入。不过呢，也正如之前一位前辈讲过，很多人都觉得这个技术牛逼，那个技术牛逼，不是那么容易学会的，也正是因为这种心态，很多人都不去学习，而很多学习的人即使没有到达最顶峰，比不学的人多了解一点就强一些。所以人生苦短，没事多看看总没坏处。\n\n碎片时间也会看极客时间上面的专栏，我已经买了两个专栏。从目前的观看效果来说，首先我觉得还是不错的，但是很多内容都是概念上面的东西，仿佛是为了写文章而写文章，缺少落地的东西，经常围绕着一个概念反复强调，我觉得还不如多讲讲具体的场景更容易让别人接受一些。对于我而言，分布式，服务治理，服务分发这些概念和场景我都有经历过，所以我还蛮能接受的，我想很多没有经历过这些开发的童鞋直接看也许会感觉到云里雾里的。但是还是那句话，多看一点总是有好处的，因为很多东西你看了，你没到一定层次是不会了解的，你提前看了，也许之后某一天就会有一种豁然开朗的感觉。但是我们还是要讲究方法的，时间是有限的，我们要在有限的时间学习更多的知识。我认为看这些文章，目的不是为了一步登天，获得什么实质效应，事实上他也不会给你这种回报，我们要做的就是从上面获取一些技术概念、原理和本质，由此作为一个引子，自己找项目，找场景去实践，去深入，光说不练假把式，只有实践了才能深入，然后最好的就是能够在产品中不断运用和学习相关技术了，这样才是最好的学习和掌握。因为很多时间如果固步自封，很多概念你都不知道，更别谈去学习了。最后我会专门整理一篇文章，把我看过的极客时间的文章分享出来，这样可以让一些童鞋免费了解和学习，我这边用的是极客时间提供的分享的功能，也不算违规，每次分享只有十个名额，所以大家先到先得。emmm，看很多同学买课很积极，很多同学却很消极，我觉得前者起码有学习的想法，后者就不谈了，但是针对前者我想说的是，不是花的钱越多就有用的，而是学到手，记到脑子里才属于自己的，希望大家都能不断学习，越变越好，happy everyday！\n\n\n1. [django2.0入门教程第一节][2](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n2. [django2.0入门教程第二节][3](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n3. [django2.0入门教程第三节][4](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n4. [django2.0入门教程第四节][5](4/2) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n2. [【极客时间-左耳听风】：分布式系统关键技术：服务调度][1](4/3)\n3. [数据分析入门：初识数据埋点（一）][6](4/7) <i class='fa fa-star'></i>\n4. [docker初体验][7](4/7)\n5. [构建FTP文件传输服务器][8](4/8)\n6. [FTP文件传输协议][9](4/8)\n7. [ftp实现原理以及抓包分析][10](4/8)\n8. [Linux下ftp服务搭建之小试牛刀][11](4/8)\n9. [python 操作samba文件服务器][13](4/10) <i class='fa fa-star'></i>\n10. [佛系程序员的月薪五万指南][14](4/12) <i class='fa fa-star'></i>\n11. [关于python中的setup.py][15](4/13) <i class='fa fa-star'></i>\n9. [【极客时间-左耳听风】：分布式系统关键技术：流量与数据调度][12](4/16) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n10. [分布式高可用CTDB方案][16](4/16)\n11. [CTDB使用rados object作为lock file][17](4/16) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n12. [关于CTDB][18](4/17)\n13. [Cephfs的文件存到哪里了][23](4/21)\n13. [最佳日志实践][19](4/22) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n14. [最佳日志实践（v2.0）][20](4/23) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n15. [使用日志系统graylog获取Ceph集群状态][21](4/23) <i class='fa fa-star'></i>\n16. [Cephfs 操作输出到日志查询系统][22](4/23) <i class='fa fa-star'></i>\n17. [bluestore调研][24](4/23) <i class='fa fa-star'></i>\n18. [Python打包时添加非代码文件的坑][25](4/24)\n19. [cephfs介绍和功能测试][26](4/24)\n20. [块存储的世界][27](4/24) <i class='fa fa-star'></i>\n21. [python virtualenv][28](4/24) <i class='fa fa-star'></i>\n22. [rpmbuild实战][29](4/25) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n23. [使用RPM方式安装Linux软件][30](4/25)\n24. [RPM 打包技术与典型 SPEC 文件分析][31](4/25) <i class='fa fa-star'></i>\n25. [RDMA(远程直接内存访问)技术浅析][32](4/26) <i class='fa fa-star'></i>\n26. [RDMA技术][33](4/26) <i class='fa fa-star'></i>\n27. [Fabric是否代表网络架构的未来？][34] <i class='fa fa-star-half-full'></i>\n28. [2017下一代数据中心网络研究报告][35](4/27)\n29. [统一Fabric和互联云][36](4/27)\n30. [浅谈数据中心网络架构的发展][37](4/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n31. [为了部落：NVMe over Fabric诞生记][38] <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n32. [NVMe over Fabrics：概念、应用和实现][39](4/28)\n33. [NVME概述][40](4/28)\n34. [详谈NVMe over Fabric技术发展简史][41](4/29) <i class='fa fa-star'></i>\n35. [Django实现CAS+OAuth2][42] <i class='fa fa-star'></i>\n36. [NVMe over Fabric][43] <i class='fa fa-star'></i><i class='fa fa-star'></i>\n\n[1]: https://time.geekbang.org/column/article/ed937b37244d4db63f60e5f00be38fce/share\n[2]: https://www.jianshu.com/p/6f67a4b9dad3\n[3]: https://www.jianshu.com/p/b8d73d39f184\n[4]: https://www.jianshu.com/p/2004b8dbebb4\n[5]: https://www.jianshu.com/p/e2a09d2a4a2f\n[6]: http://www.woshipm.com/data-analysis/872543.html\n[7]: https://www.jianshu.com/p/5b33bbd61c48\n[8]: https://www.jianshu.com/p/f43659a58d71\n[9]: https://blog.csdn.net/bear_huangzhen/article/details/41806903?from=singlemessage\n[10]: https://www.jianshu.com/p/05212313d0e2\n[11]: https://www.jianshu.com/p/e99519739b5e\n[12]: https://time.geekbang.org/column/article/b307919cc599a82c542ec39e7aa3ddc7/share\n[13]: https://blog.csdn.net/u014245412/article/details/72286348\n[14]: https://yq.aliyun.com/articles/578927?utm_content=m_45816\n[15]: https://lingxiankong.github.io/2013-12-23-python-setup.html\n[16]: http://blog.sina.com.cn/s/blog_8c243ea30102uxaw.html\n[17]: https://ceph.com/planet/ctdb使用rados-object作为lock-file/\n[18]: https://blog.csdn.net/naipeng/article/details/75045177\n[19]: http://blog.jobbole.com/56574/\n[20]: https://zhuanlan.zhihu.com/p/27363484\n[21]: http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/\n[22]: http://www.zphj1987.com/2017/07/13/CEPHFS-op-to-graylog/\n[23]: http://www.zphj1987.com/2017/04/20/where-is-cephfs-data-store/\n[24]: https://blog.csdn.net/xiaqunfeng123/article/details/56675696?locationNum=1&fps=1\n[25]: https://zhuanlan.zhihu.com/p/24312755\n[26]: https://www.jianshu.com/p/d910a70dfee7\n[27]: http://xiaqunfeng.cc/2017/01/20/%E5%9D%97%E5%AD%98%E5%82%A8%E7%9A%84%E4%B8%96%E7%95%8C/#more\n[28]: https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001432712108300322c61f256c74803b43bfd65c6f8d0d0000#0\n[29]: http://stackeye.com/2014/08/rpmbuild-in-action/\n[30]: https://zhuanlan.zhihu.com/p/28492389\n[31]: https://www.ibm.com/developerworks/cn/linux/l-rpm/index.html\n[32]: http://mp.weixin.qq.com/s?src=11&timestamp=1524706491&ver=839&signature=nEMLgYwV4GI9Pd19glqGgJS6DR7sfrZesONMDXvpIWz34-KhzcYGK8h*WE7DjAFpXU9NYztsheG7Doy29A2zEC-V2bw*zFNgXbyy5VK2vRBR83sVVudSgxEsbAN0R8iW&new=1\n[33]: https://blog.csdn.net/chenhaifeng2016/article/details/78072498?locationNum=4&fps=1\n[34]: https://www.csdn.net/article/1970-01-01/302809\n[35]: http://dy.163.com/v2/article/detail/CDG9CD7G05179LAH.html\n[36]: http://book.51cto.com/art/201105/266135.htm\n[37]: http://net.zol.com.cn/459/4598330.html\n[38]: https://blog.csdn.net/memblaze_2011/article/details/51820631\n[39]: http://www.cnblogs.com/rodenpark/p/6220519.html\n[40]: https://blog.csdn.net/u010616442/article/details/70773956\n[41]: https://mp.weixin.qq.com/s?src=11&timestamp=1524808998&ver=841&signature=56U4bzWNuWUqlDANFQNBFPkSc2dX-R*HeLL9y7vN*ha-Ph0PrOfhkEPUfB8R*Gy-SLlwFeMqCUKDneUkeDPfpgT-igBpw77SSCr6Hhl9Ul29OsEMtzY1B2-JPQbBd1vH&new=1\n[42]: http://www.voidcn.com/article/p-yvycalqd-brm.html\n[43]: https://weibo.com/p/1001603934517592239583?mod=zwenzhang#_loginLayer_1525517479194\n[44]: http://www.tony-yin.site/2018/04/08/python_nas_stable_test/\n[45]: http://cdn.tony-yin.site/hello_april_600x450.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Ctdb Rados方式导致All Banned的问题","url":"/2018/04/20/Ctdb-Rados-All-Banned/","content":"\n<center>![lock object][2]</center>\n\n`ctdb`最近专门为`ceph`提供了一种`raods object`作为文件锁的方式，`lock file`可以放在对象存储中，而不是`cephfs`，从而大大降低了系统宕机的延时。在此方案的实践中，我们发现`master`节点宕机会导致严重的`All Banned`的问题，本文则围绕该问题展开讨论和提供本人的解决方案。\n\n<!--more-->\n\n很多系统都在用`ctdb`做`HA`，今天我们讨论的是基于`cephfs`的`ctdb HA`方案。`ctdb`的作用是在一个共享文件系统中，当所有节点都访问同一个文件时，`ctdb`会选举出一个`master`节点获得`lock`，我们之前的做法是把这个`lock file`放在`cephfs`的共享目录中，但是当其中某个节点`down`了之后，会导致`cephfs`这个目录卡死，进一步导致`lock file`在其他节点都获取不到，只有等到锁超时了之后才能获取到，而这个超时时间默认是`300s`，再加上`ctdb`的监控检测和恢复的时间，切换的时间少则十几分钟，多则几十分钟，这对于高可用场景来说无疑是灾难级的。\n\n## 具体场景\n\n`ctdb`的编译和安装我就不说了，大家可以参考磨渣的文章：[CTDB使用rados object作为lock file][1]。在`ceph`集群中所有节点安装好`ctdb`后，起服务后通过`systemctl status ctdb`可以发现`reclock`是通过`ctdb_mutex_ceph_rados_helper`的方式，就说明`ctdb rados`的方式配置成功了。\n\n然后我们可以通过`rados -p rbd ls`也可以看到自己配置的锁存在于`rbd pool`中。这时我们断电一个`slave`节点，一分钟左右后可以实现节点切换。但是我们的测试发现当断网`master`节点的时候，就会造成长时间的卡住，且节点并不会切换。详细查看可以发现断网后，`master`节点没有释放`lock`，然后其他的集群节点选举出了`master`节点后，试图获取锁，但是由于之前的`master`节点一直没有释放，所以一直获取不到，然后就不停的去获取，`ctdb`的机制是如果有不断的这种行为，就会让所有节点`All Banned`。因为`slave`节点并不拥有锁，所以不存在之前的问题。\n\n这个问题是比较严重的，因为不存在超时机制，拥有锁的节点断网或者断电，所以不会因为超时就释放锁。所以就会一直就卡着，并且一直实现不了切换节点。这就意味着一旦这种情况发生，客户的业务就会发生中断，这是无法接受的。并且我们也发现了如果使用原来将`lock file`放在`cephfs`目录的方式，断网或者断电主节点并不会发生这种情况，后来大概看了下源码大概是因为`cephfs`自己的机制会强制释放共享目录中文件的锁。\n\n具体报错如下：\n\n```shell\n[root@tony ~]# ctdb status\nWarning: All nodes are banned.\n```\n\n## 解决方案\n\n我们的解决方案没有尝试着修改`ctdb`的源码，而是通过定时监控`ctdb`的状态。如果是主节点上面的`ctdb`，并且如果是`rados`方式的话，每`3`分钟查看一下`ctdb status`的状态，如果有连续两次的状态都是`All Banned`的话，我们就认为目前主节点发生了不释放锁的问题，我们就主动地删除`lock object`。部分代码如下：\n\n```shell\n#! /bin/bash\n\nfunction check_if_master() {\n    MASTER_PNN=$(ctdb recmaster)\n    CURRENT_PNN=$(ctdb pnn)\n    if [ $MASTER_PNN -eq $CURRENT_PNN ]; then\n        echo true\n    else\n        echo false\n    fi  \n}\n\nfunction get_lock_name() {\n    LOCK_INFO=$(grep rados $CTDB_CONFIG_FILE | awk '{print $5}')\n    LOCK_NAME=${LOCK_INFO:0:-1}\n    echo $LOCK_NAME\n}\n\nfunction monitor_lock() {\n    STATUS_FILE=/etc/ctdb/status.txt\n    CTDB_STATUS=$(ctdb status 2>&1)\n    ALL_BANNED=\"Warning: All nodes are banned.\"\n\n    if [ ! -f \"$STATUS_FILE\" ]; then\n        echo \"$CTDB_STATUS\" > $STATUS_FILE\n    else\n        if [ \"$CTDB_STATUS\" = \"$ALL_BANNED\" ]; then\n            LAST_CTDB_STATUS=$(cat $STATUS_FILE)\n            if [ \"$LAST_CTDB_STATUS\" = \"$ALL_BANNED\" ]; then\n                LOCKNAME=$(get_lock_name)\n                echo $(date)\" Ctdb all nodes banned: Second time\" >> /var/log/monitor_ctdb.log\n                echo $(date)\" Remove ctdb rados lock: \"$LOCKNAME >> /var/log/monitor_ctdb.log\n                rados -p rbd rm $LOCKNAME \n                echo -n \"\" > $STATUS_FILE\n            else\n                echo $(date)\" Ctdb all nodes banned: First time\" >> /var/log/monitor_ctdb.log\n                echo \"$ALL_BANNED\" > $STATUS_FILE\n            fi\n        else\n            echo -n \"\" > $STATUS_FILE\n        fi\n    fi\n}\n\nCTDB_CONFIG_FILE=/etc/sysconfig/ctdb\nif $(grep rados $CTDB_CONFIG_FILE -q); then\n    if $(check_if_master); then\n        monitor_lock\n    fi\nfi\n```\n\n> 完整代码地址：`https://github.com/tony-yin/Ctdb-Rados-Monitor`\n\n## 总结\n\n也许我的这种做法不是最优方案，希望遇到同样问题的同学可以一起讨论，拥有更好解决方案的可以一起分享。\n\n[1]: http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/\n[2]: http://cdn.tony-yin.site/lock.jpg\n","tags":["HA"],"categories":["tech"]},{"title":"基于Python的NAS稳定性测试框架","url":"/2018/04/08/python_nas_stable_test/","content":"\n<center>![NAS Stable Test][6]</center>\n\n最近公司有个集群一直在跑着，领导想要测测它上面`NAS`服务的稳定性，也就是看看正常持续的读写会不会导致`NAS`服务异常，这个其实通过`fio`或者`cosbench`这类的工具测试起来很容易，但是这样一是没有挑战性，二是比较机械，可扩展性低。比如并行测试、进程保护和异常通知等等这些是机械地运用工具测试所做不到的，所以我们尝试做了一套基于`NAS`稳定性测试的框架。\n\n<!--more-->\n\n## 概述\n\n整个测试框架打包和发布都是通过RPM的方式，方便测试人员一键部署。部署之后测试工作由`supervisor`管理，实时监控后台进程的运行状态，发生异常时可以进行重启等自动化操作。所有读写操作都是通过`python` 连接NAS服务，无需做任何挂载工作。主要测试工作是通过`celery`实现任务调度，支持并行多个NAS服务的读写测试，`broker`和`backend store`都采用了`rabbitmq`。后端注册了`register`、`nfs`、`cifs`和`ftp`四个`job`，定时每`10`分钟执行一次，设置最大开启`worker`数为`5`个。`Job`注册进消息队列中后，`celery worker`会自动去消费，针对服务器中不同的`NAS`服务进行读写操作，每个任务的执行结果最后都会记录在日志中，出了异常通过邮件通知管理员。\n\n## 系统架构\n\n整个项目的框架图如下：\n\n<center>![Nas_Stable_Test架构图][1]</center>\n\n## NAS服务读写流程\n\n由于每个`NAS`服务的测试方式是一致的，所以下面就以单个`NAS`服务的流程来介绍。首先`client`端向`server`的`NAS`服务端口发起连接，`server`端接收到`client`端的请求后建立连接。`Client`在`/tmp`目录下生成固定大小`1G`的文件，并且记录该文件的`MD5`值，然后将该文件上传至远端NAS服务目录（即对`NAS`服务进行写操作），上传完成后将该文件从本地删除。接着对之前上传至`NAS`服务目录的文件进行下载（即对`NAS`服务进行读操作），下载完成后再次记录文件`MD5`值，并删除掉远端`NAS`服务目录对应的文件。最后对两次记录的`MD5`值进行比较，判断上传和下载的文件是否一致，并将比较结果记录在日志中，再次删除本地下载的文件。`NAS`服务读写流程图如下：\n\n<center>![Nas_Stable_Test流程图][2]</center>\n\n## 准备工作\n\n### 安装libnfs\n\n通过`pip`安装：\n\n```\npip install libnfs\n```\n\n一般会报这个错：\n\n```\nlibnfs/libnfs_wrap.c:2969:25: fatal error: nfsc/libnfs.h: No such file or directory\n```\n\n这个错看起来是缺少这个头文件的包，但是通过`yum search libnfs`是找不到相关的包的，所以我们只能去官网下载`rpm`包然后在安装：\n\n```\n## 下载rpm\nwget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-1.9.8-1.el7.nux.x86_64.rpm\nwget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm\n## 安装rpm\nyum localinstall libnfs-1.9.8-1.el7.nux.x86_64.rpm\nyum localinstall libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm\n```\n\n### 安装pysmb\n\n这个比较简单，直接`pip`安装就可以了，也没遇到什么问题。\n\n```\npip install pysmb\n```\n\n## NAS服务相关代码实现\n\n这里只贴出部分`python`连接或者操作具体`Nas`服务的代码实现，如果想要了解或者贡献整个项目，请关注：[Github python_nas项目][3]\n\n### NFS\n\n#### Connect\n\n```\ndef open(self):\n    self.nfs = libnfs.NFS('nfs://{}'.format(self.mount_point))        \n    log.info('nfs connect successfully!')\n```\n\n#### Read\n\n这里有个关键点就是分段读写文件，避免内存溢出。\n\t\n```\ndef read(self):\n\tlog.info('nfs read start...')\n\ta = self.nfs.open('/{}'.format(self.filename), mode='r')          \n\twith open(self.download_path, 'a') as f:                          \n    \twhile True: \n        \tcontent = a.read(1024*1024)                               \n        \tif content == '':\n            \tbreak\n        \tf.write(content)                                          \n\ta.close()\n    log.info('nfs read end...')\n```\n\n#### Write\n\n```\ndef write(self, content):                                             \n\tlog.info('nfs write start...')                                    \n\ta = self.nfs.open('/{}'.format(self.filename), mode='w+')         \n\ta.seek(self.file_size)\n\ta.write(content)\n\ta.close()\n\tlog.info('nfs write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n    log.info('nfs file delete start...')\n\tself.nfs.unlink('/{}'.format(self.filename))\n\tlog.info('nfs file delete end...')\n```\n\n### CIFS\n\n#### Connect\n\n```\ndef open(self):\n    self.smb = SMBConnection(\n        self.username,\n        self.password,\n        self.my_name.encode('utf-8'),\n        self.remote_name.encode('utf-8'),\n        use_ntlm_v2=True\n    )\n    self.smb.connect(self.host, self.port)\n    log.info('cifs connect successfully!')\n```\n\n#### Read\n\n```\ndef read(self):\n    log.info('cifs read start...')\n    file_obj = open(self.download_path, 'wb')\n    self.smb.retrieveFile(\n        self.directory,\n        self.filename,\n        file_obj\n    )\n    file_obj.close()\n    log.info('cifs read end...')\n```\n\n#### Write\n\n```\ndef write(self):\n\tlog.info('cifs write start...')\n\tfile_obj = open(self.client_path, 'rb')\n\tself.smb.storeFile(\n    \tself.directory,\n    \tself.filename,\n    \tfile_obj\n \t)\n\tfile_obj.close()\n\tlog.info('cifs write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n\tlog.info('cifs delete start...')\n\tself.smb.deleteFiles('path3', self.filename)\n\tlog.info('cifs delete end...')\n```\n\n#### Close\n\n```\ndef close(self):\n    self.smb.close()\n```\n\n### FTP\n\n#### Connect\n\n```\ndef open(self):\n    self.ftp = FTP()\n    self.ftp.connect(\n        host=self.host.encode('utf-8'),\n        port=self.port.encode('utf-8')\n    )\n    self.ftp.login(self.username, self.password)\n    log.info('ftp connect successfully!')\n```\n\n#### Read\n\n```\ndef read(self):\n\tlog.info('ftp read start...')                                     \n\tbuff_size = 1024\n\tfp = open(self.download_path, \"wb\")                               \n\tself.ftp.retrbinary(\n    \t\"RETR {}\".format(self.filename),\n    \tfp.write,\n    \tbuff_size                                                     \n\t)   \n\tfp.close()\n\tlog.info('ftp read end...')\n```\n\n#### Write\n\n```\ndef write(self):\n\tlog.info('ftp write start...')                                    \n\tbuff_size = 1024\n \tfp = open(self.client_path, \"rb\")\n\tself.ftp.storbinary(\n    \t\"STOR {}\".format(self.filename),\n    \tfp,\n   \t\tbuff_size\n\t)\n\tfp.close()\n\tlog.info('ftp write end...')\n```\n\n#### Delete\n\n```\ndef delete(self):\n\tlog.info('ftp delete start...')\n\tself.ftp.delete(self.filename)\n\tlog.info('ftp delete end...')\n```\n\n#### Close\n\n```\ndef close(self):\n    self.ftp.quit(self.filename)\n```\n\n## 总结\n\n还有一些其他的`API`接口就不一一赘述了，具体实现细节大家可以查看`github`上面的项目代码，具体地址我会在文末贴出。整个项目的核心是通过`celery`实现任务的调度，还有全程通过`python`连接和操作`nas`服务，后续还会不断完善~~~\n\n> **项目地址：**https://github.com/tony-yin/python_nas\n\n<br/>\n\n> 参考列表：\n> [python 操作samba文件服务器][4]\n> [python libnfs][5]\n\n[1]: http://cdn.tony-yin.site/NAS_Stable_Test%E6%A1%86%E6%9E%B6%E5%9B%BE.png\n[2]: http://cdn.tony-yin.site/NAS_Stable_Test%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%9B%BE.png\n[3]: https://github.com/tony-yin/python_nas\n[4]: https://pypi.org/project/libnfs/\n[5]: https://blog.csdn.net/u014245412/article/details/72286348\n[6]: http://cdn.tony-yin.site/nas.jpg\n","tags":["NAS"],"categories":["tech"]},{"title":"Daily Article Vol 4 - (2018/3/1 ~ 2018/3/31)","url":"/2018/04/01/Daily_Article_Vol4/","content":"\n<center>![Daily Article 3][29]</center>\n\n这个月主要因为离职和入职，花费了大量时间和相关的人沟通，还有办理了很多手续，包括找房子和搬家等等。真是忙的焦头烂额，奢侈地给自己放了一个星期的假期。\n\n<!--more-->\n\n离开了工作了两年的公司，真是感慨万千。还没毕业就到这家公司实习，然后提前试用、转正。师父耐心的指导，无论是技术上还是生活上都受益匪浅，帮我扎实地掌握了一名软件开发工程师应该具备的技能；钱总作为`CTO`仿佛身上一直贴着技术控的标签，殊不知竟各项全能，机缘巧合带我做起了`Ceph`，这一年中不仅从钱总身上学到了很多高新技术，也从每次吃饭饭桌上、下班路上、地铁上倾听他年轻的故事中受益很多，常常听起来热血沸腾。还有很多帮助过我和关心过我的人就不一一点名了。总之，大家都很厉害，也正是因为如此，在我工作的两年中，我能够不断感受到压力，不断进步，并且以后会一直以你们为我的榜样，以后还要多多指教，多交流多切磋。\n\n入职了新的公司，规模和规范都比原来上了一个档次，正式了许多，上下班都要打卡，一开始不习惯，下班总是忘记。。还有经常开会，要写很多设计和方案，感觉不像以后有师父和钱总这样带我走了，哈哈，但也多了很多参与感，大事小事都可以提出自己的想法，也可以从同事的想法中学习很多。也是巧合，之前网上因博客结缘的一位小伙伴现在和我同组，负责带我熟悉各方面，顿时轻松了很多，他人也很`nice`，看的出来很热爱技术，身上有很多值得我学习的点，算是一个挺优秀的同龄人了。总体来说，对新公司的印象还是不错的，好好加油吧！\n\n这个月我买了极客时间上面陈皓老师（左耳朵耗子）的专栏，没买但想看的朋友可以点击我下面分享的链接观看，每个链接都有十个免费的观看圈，先到先到，如果觉得收获很大的话，建议买了看看，受益绝不止这`199`哦~~~\n\n新公司任务调度用到了`celery`框架，感觉还蛮好用的，还在熟悉中...\n\n\n1. [Ceph: manually repair object][1](3/2) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n1. [【分析】Ceph数据一致性检查 - Scrub的介绍][2](3/5)\n1. [【分析】Ceph数据一致性检查 - 端到端的数据校验][3](3/5)\n1. [洞悉技术的本质，享受科技的乐趣][4](3/17)\n1. [程序员如何用技术变现（上）][5](3/17) <i class='fa fa-star'></i>\n1. [程序员如何用技术变现（下）][6](3/17)\n1. [Equifax信息泄露始末][7](3/18)\n1. [从Equifax信息泄露看数据安全][8](3/18)\n1. [何为技术领导力][9](3/18)\n1. [如何拥有技术领导力][10](3/18)\n2. [每个程序员都该知道的事][13](3/19)\n3. [Go语言，Docker和新技术][14](3/19)\n4. [答疑解惑：渴望、热情和选择][15](3/20)\n5. [如何成为一个大家愿意追随的Leader？][16](3/20)\n1. [NFS服务详细分析][11](3/21) <i class='fa fa-star'></i>\n2. [理解Linux系统/etc/init.d目录和/etc/rc.local脚本][12](3/21)\n3. [分布式系统架构的冰与火][17](3/21) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n4. [从亚马逊的实践，谈分布式系统的难点][18](3/22) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n5. [访问网络文件共享服务-CIFS][19](3/23) <i class='fa fa-star'></i>\n6. [任务调度利器：Celery][20](3/26)\n7. [异步任务神器 Celery 简明笔记][21](3/26)\n8. [Python 并行分布式框架：Celery][22](3/26) <i class='fa fa-star'></i>\n9. [Redis实现简单消息队列][23](3/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n10. [cifs协议与samba服务][24](3/28~3/29) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n11. [SELinux 入门][25](3/29)\n12. [Ring Buffer 有什么特别?][26](3/29) <i class='fa fa-star'></i>\n13. [分布式系统的技术栈][27](3/30) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n14. [分布式系统关键技术：全栈监控][28](3/31) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n\n[1]: http://www.sebastien-han.fr/blog/2015/04/27/ceph-manually-repair-object/\n[2]: http://blog.csdn.net/younger_china/article/details/75150261\n[3]: http://blog.csdn.net/younger_china/article/details/75149045\n[4]: https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx5d7aad8e7ec33bfd&redirect_uri=https%3A%2F%2Faccount.geekbang.org%2Faccount%2Foauth%2Fcallback%3Ftype%3Dwechatopen%26ident%3D60f4b1%26redirect%3Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare%253Ffailedurl%253Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare&response_type=code&scope=snsapi_userinfo&state=d2b1c68fbd5a953da5f04515e46f9b48&connect_redirect=1#wechat_redirect\n[5]: https://time.geekbang.org/column/article/e3b6ea4c823d2c319bcbce9a1dc23501/share\n[6]: https://time.geekbang.org/column/article/c260269c38db5276c17f1be49718f74a/share\n[7]: https://time.geekbang.org/column/article/70f255c35f7e58cb008e4410659cb39e/share\n[8]: https://time.geekbang.org/column/article/3aa6350a995faf76c32e61ba6f7db3e9/share\n[9]: https://time.geekbang.org/column/article/66c547af27e04afd4b7c2f8444d0971f/share\n[10]: https://time.geekbang.org/column/article/0294f13512d5b6608115a601eb373287/share \n[11]: http://blog.51cto.com/hongtengfei/1684809\n[12]: http://blog.csdn.net/acs713/article/details/7322082\n[13]: https://time.geekbang.org/column/article/5df06f61f6d635f5fc71b27ccc39902d/share\n[14]: https://time.geekbang.org/column/article/af44ced83be43d287e728d2eaee10afc/share\n[15]: https://time.geekbang.org/column/article/946c705caf3299894fcc991bc59992de/share\n[16]: https://time.geekbang.org/column/article/ef3daa17d473e583835df8248082f6fc/share\n[17]: https://time.geekbang.org/column/article/73253891c16c684d3c9dcfd02dfcb54f/share\n[18]: https://time.geekbang.org/column/article/a3b4de20403567fbdf305bdd4e403026/share\n[19]: http://blog.csdn.net/a18829898663/article/details/71065999\n[20]: https://www.liaoxuefeng.com/article/00137760323922531a8582c08814fb09e9930cede45e3cc000\n[21]: https://www.jianshu.com/p/1840035cb510\n[22]: http://www.open-open.com/lib/view/open1426298834326.html\n[23]: https://www.jianshu.com/p/9c04890615ba\n[24]: https://www.cnblogs.com/davidshen/p/8145984.html\n[25]: https://linuxtoy.org/archives/selinux-introduction.html\n[26]: http://www.cnblogs.com/shanyou/archive/2013/02/04/2891300.html\n[27]: https://time.geekbang.org/column/article/f9126577b469b13317889a99ea70d2f0/share\n[28]: https://time.geekbang.org/column/article/8efeb52c4015735a7ea424e0cff861c1/share\n[29]: http://cdn.tony-yin.site/2018_march.jpg\n","tags":["Daily-Article"],"categories":["read"]},{"title":"硬件环境测试环境模拟","url":"/2018/03/09/Hardware-Test-Tool/","content":"\n<center>![hardware][1]</center>\n\n最近在做一个`feature`，测试的时候需要硬件环境的支撑。一般我们开发环境都是虚拟机，所以针对这种开发工作的自测无法进行，比如虚拟机上没有物理磁盘，没有`raid`卡等，为了一个小功能的测试，需要出`build`，需要硬件环境的部署和安装，这个工作量着实不小。\n\n往往针对这种情况，作为开发人员可以针对硬件环境的具体需求，尽可能在自己的环境上做模拟，也就是我们俗称的“打桩”。\n\n<!--more-->\n\n就拿我这次做的需求来说吧，我想获取磁盘的相关信息，获取方式是通过`raid`卡工具，可能是`megacli`，也可能是`sas3ircu`等等，这个取决于`lspci`查看`raid`卡的型号，然后还要通过`lsblk`和`lsscsi`工具获取相关信息。\n\n我的做法是针对这些工具，自己写一个简易的小工具，就比如`lsblk`我也写一个`lsblk`的脚本，里面的代码也很简单，先找一个硬件环境，将`lsblk`读取的内容重定向到文件中，这时候我们自己写的脚本直接去读这个文件就可以了。务必要保证各个软件工具的一致性。\n\n以`lsblk`为例，我可能需求两种情况`lsblk`和`lsblk -l`：\n\n`lsblk`：\n\n```shell\n#! /bin/bash                                                        \n\nlsblk_path=$(dirname $0)\nif [ $# -eq 0 ]; then\n    echo \"$(cat $lsblk_path\"/lsblk.txt\")\"\nfi\n\nwhile getopts \":l\" opt; do\n    case $opt in  \n        l)  \n            echo \"$(cat $lsblk_path\"/lsblk_list.txt\")\"\n            ;;  \n        \\?) \n            echo \"Invalid option: -$OPTARG\"\n            ;;  \n    esac\ndone\n```\n\n`lsblk.txt`：\n\n```shell\nNAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT \nsda        8:0    0   3.7T  0 disk \n├─sda1     8:1    0  30.5M  0 part ar\n├─sda2     8:2    0 488.3M  0 part ar\n├─sda3     8:3    0  93.1G  0 part /rt\n├─sda4     8:4    0   256G  0 part [SWAP]WA\n└─sda5     8:5    0   3.3T  0 part /data/osd.0sd\nsdb        8:16   0 372.1G  0 disk \n└─sdb1     8:17   0 372.1G  0 part ar\nsdc        8:32   0   2.7T  0 disk \n└─sdc1     8:33   0   2.7T  0 part /data/osd.1sd\nsdd        8:48   0   2.7T  0 disk \n└─sdd1     8:49   0   2.7T  0 part /data/osd.2sd\nsde        8:64   0   2.7T  0 disk \n└─sde1     8:65   0   2.7T  0 part /data/osd.3sd\nsdf        8:80   0   2.7T  0 disk \n└─sdf1     8:81   0   2.7T  0 part /data/osd.11d.\nsdg        8:96   0   2.7T  0 disk \n└─sdg1     8:97   0   2.7T  0 part /data/osd.12d.\nsdh        8:112  0   2.7T  0 disk \n└─sdh1     8:113  0   2.7T  0 part ar\nsdi        8:128  0   2.7T  0 disk \n└─sdi1     8:129  0   2.7T  0 part /data/osd.8sd\nsdj        8:144  0   2.7T  0 disk \n└─sdj1     8:145  0   2.7T  0 part /data/osd.9sd\nsdk        8:160  0   2.7T  0 disk \n└─sdk1     8:161  0   2.7T  0 part /data/osd.10d.\nsdl        8:176  0   2.7T  0 disk \n└─sdl1     8:177  0   2.7T  0 part ar\nsdm        8:192  0   2.7T  0 disk \n└─sdm1     8:193  0   2.7T  0 part ar\nsdn        8:208  0   2.7T  0 disk \n└─sdn1     8:209  0   2.7T  0 part ar\nsdo        8:224  0   2.7T  0 disk \n└─sdo1     8:225  0   2.7T  0 part\nsdp        8:240  0   2.7T  0 disk\n└─sdp1     8:241  0   2.7T  0 part\nsdq       65:0    0   2.7T  0 disk\n└─sdq1    65:1    0   2.7T  0 part\nsdr       65:16   0   2.7T  0 disk\n└─sdr1    65:17   0   2.7T  0 part\nsds       65:32   0   2.7T  0 disk\n└─sds1    65:33   0   2.7T  0 part\nsdt       65:48   0   2.7T  0 disk\n└─sdt1    65:49   0   2.7T  0 part\nsdu       65:64   0   2.7T  0 disk\n└─sdu1    65:65   0   2.7T  0 part\nsdv       65:80   0 744.7G  0 disk\n├─sdv1    65:81   0    50G  0 part\n├─sdv2    65:82   0    50G  0 part\n├─sdv3    65:83   0    50G  0 part\n├─sdv4    65:84   0    50G  0 part\n├─sdv5    65:85   0    50G  0 part\n├─sdv6    65:86   0    50G  0 part\n├─sdv7    65:87   0    50G  0 part\n├─sdv8    65:88   0    50G  0 part\n├─sdv9    65:89   0    50G  0 part\n├─sdv10   65:90   0    50G  0 part\n├─sdv11   65:91   0    50G  0 part\n└─sdv12   65:92   0    50G  0 part\n```\n\n`lsblk_list.txt`：\n\n```shell\nNAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT\nsda      8:0    0   3.7T  0 disk \nsda1     8:1    0  30.5M  0 part \nsda2     8:2    0 488.3M  0 part \nsda3     8:3    0  93.1G  0 part /\nsda4     8:4    0   256G  0 part [SWAP]\nsda5     8:5    0   3.3T  0 part /data/osd.0\nsdb      8:16   0 372.1G  0 disk \nsdb1     8:17   0 372.1G  0 part \nsdc      8:32   0   2.7T  0 disk \nsdc1     8:33   0   2.7T  0 part /data/osd.1\nsdd      8:48   0   2.7T  0 disk \nsdd1     8:49   0   2.7T  0 part /data/osd.2\nsde      8:64   0   2.7T  0 disk \nsde1     8:65   0   2.7T  0 part /data/osd.3\nsdf      8:80   0   2.7T  0 disk \nsdf1     8:81   0   2.7T  0 part /data/osd.11\nsdg      8:96   0   2.7T  0 disk \nsdg1     8:97   0   2.7T  0 part /data/osd.12\nsdh      8:112  0   2.7T  0 disk \nsdh1     8:113  0   2.7T  0 part \nsdi      8:128  0   2.7T  0 disk \nsdi1     8:129  0   2.7T  0 part /data/osd.8\nsdj      8:144  0   2.7T  0 disk \nsdj1     8:145  0   2.7T  0 part /data/osd.9\nsdk      8:160  0   2.7T  0 disk \nsdk1     8:161  0   2.7T  0 part /data/osd.10\nsdl      8:176  0   2.7T  0 disk \nsdl1     8:177  0   2.7T  0 part \nsdm      8:192  0   2.7T  0 disk \nsdm1     8:193  0   2.7T  0 part \nsdn      8:208  0   2.7T  0 disk \nsdn1     8:209  0   2.7T  0 part \nsdo      8:224  0   2.7T  0 disk \nsdo1     8:225  0   2.7T  0 part\nsdp      8:240  0   2.7T  0 disk\nsdp1     8:241  0   2.7T  0 part\nsdq     65:0    0   2.7T  0 disk\nsdq1    65:1    0   2.7T  0 part\nsdr     65:16   0   2.7T  0 disk\nsdr1    65:17   0   2.7T  0 part\nsds     65:32   0   2.7T  0 disk\nsds1    65:33   0   2.7T  0 part\nsdt     65:48   0   2.7T  0 disk\nsdt1    65:49   0   2.7T  0 part\nsdu     65:64   0   2.7T  0 disk\nsdu1    65:65   0   2.7T  0 part\nsdv     65:80   0 744.7G  0 disk\nsdv1    65:81   0    50G  0 part\nsdv2    65:82   0    50G  0 part\nsdv3    65:83   0    50G  0 part\nsdv4    65:84   0    50G  0 part\nsdv5    65:85   0    50G  0 part\nsdv6    65:86   0    50G  0 part\nsdv7    65:87   0    50G  0 part\nsdv8    65:88   0    50G  0 part\nsdv9    65:89   0    50G  0 part\nsdv10   65:90   0    50G  0 part\nsdv11   65:91   0    50G  0 part\nsdv12   65:92   0    50G  0 part\n```\n\n还有一些其他工具我就不详细贴代码了，具体请去`github`上查看。\n\n项目地址：https://github.com/tony-yin/Hardware_Test_Tool\n\n操作步骤：\n\n1.下载代码\n\n```shell\ngit clone git@github.com:tony-yin/Hardware_Test_Tool.git\n```\n\n2.安装工具\n\n```shell\n./build install\n```\n\n3.卸载工具\n\n```shell\n./build uninstall\n```\n\n[1]: http://cdn.tony-yin.site/hardware-600x450.jpg\n","tags":["Test"],"categories":["tech"]},{"title":"Daily Article Vol 3 - (2018/2/1 ~ 2018/2/28)","url":"/2018/03/01/Daily-Article-Vol3/","content":"\n<center>![Daily Article 3][19]</center>\n\n这段期间，一方面是因为过年，另一方面因为换工作，所以诸事缠身，导致每天阅读学习的量严重受影响，不能保持每天都有固定的时间学习和阅读，还是自己的问题，不应该找借口。之后，不管什么事情，必须要保证每天的学习量和阅读量，养成雷打不动的好习惯。\n\n这是`Daily Article`系列的第三篇，罗列了`2018`年`2`月的阅读清单。\n\n<!--more-->\n\n---\n1. [ceph的数据存储之路(10) -----ceph对象存储的ls命令实现及思考][4](2/1 ~ 2/5) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [八大排序算法的 Python 实现][5](2/6)\n1. [Ceph解析-消息处理模块][3](2/7) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [解析Ceph: 网络层的处理][7](2/7)\n1. [ceph存储 ceph集群消息处理][6](2/8)\n1. [ceph的数据存储之路(1) ---rbd设备介绍][1](2/11) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(2) ----- rbd到osd的数据映射][2](2/11)\n1. [张大胖学递归][10](2/16) <i class=\"fa fa-star\"></i>\n1. [张大胖的socket][9](2/17) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(4) ----- rbd client 端的数据请求处理][8](2/18~2/20) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [学习面向对象的令狐冲][11](2/21) <i class=\"fa fa-star\"></i>\n1. [张大胖学数据库][13](2/22)\n1. [数据库村的旺财和小强][14](2/23) <i class=\"fa fa-star\"></i>\n1. [ceph的数据存储之路(5) -----osd数据处理][12](2/24 ~ 2/26) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [ Ceph开发每周谈 Vol 70 | RGW 同步到 AWS S3 | Elastic Search API 整合][15](2/26)\n1. [Ceph OSD从filestore 转换到 bluestore的方法][16](2/27)\n1. [ceph的数据存储之路(6) -----pg的创建][17](2/28)\n1. [XSKY在OpenStack场景下的优势][18](2/28)\n\n\n\n[1]: https://my.oschina.net/u/2460844/blog/531646\n[2]: https://my.oschina.net/u/2460844/blog/531686\n[3]: https://georgezhuo.github.io/georgezhuo.github.io/2015/12/22/ceph-message/\n[4]: https://my.oschina.net/u/2460844/blog/669769\n[5]: http://python.jobbole.com/82270/\n[6]: http://blog.csdn.net/skdkjzz/article/details/41980631\n[7]: http://www.wzxue.com/ceph-network/\n[8]: https://my.oschina.net/u/2460844/blog/532755?p=2&temp=1519178944837#blog-comments-list\n[9]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513387&idx=1&sn=99665948d0b968cf15c5e7a01ffe166c&chksm=80d679e8b7a1f0febad077b57e8ad73bfb4b08de74814c45e1b1bd61ab4017b5041942403afb&scene=21#wechat_redirect\n[10]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513393&idx=1&sn=c1d6caca8ef9972f1105df982f15bb58&chksm=80d679f2b7a1f0e410fd53691d68a9ada158aac2b8814fe329d6dfed86ee54b68a53eb84cdb0&scene=21#wechat_redirect\n[11]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513353&idx=1&sn=a5dc69542fae6aabf0fef9b5f5881a9d&chksm=80d679cab7a1f0dc530bd1745c2c9552b739afc701ecb2f8e1eba8624d1fefc2c3cc64cd1d30&scene=21#wechat_redirect\n[12]: https://my.oschina.net/u/2460844/blog/534390?p=1&temp=1519628036891#blog-comments-list\n[13]: http://mp.weixin.qq.com/s/wyt-0y3lEhewa2cpeILYSQ\n[14]: https://mp.weixin.qq.com/s/tSF_w9xUOj3Q2hmOxJkwLg\n[15]: https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&mid=2651256389&idx=1&sn=e11edcce5722853f442b9a7b8211787e&chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&mpshare=1&scene=23&srcid=0502SazrSPsWnszP3xfdEId4#rd\n[16]: http://ceph.com/planet/ceph-osd%E4%BB%8Efilestore-%E8%BD%AC%E6%8D%A2%E5%88%B0-bluestore%E7%9A%84%E6%96%B9%E6%B3%95/\n[17]: https://my.oschina.net/u/2460844/blog/535007\n[18]: https://www.sogou.com/link?url=6IqLFeTuIyjnBL6rnEdhCp_rFXz42FvmEhOk1arvVrbkYalcctcthKCfjFS1STqz\n[19]: http://cdn.tony-yin.site/february-600x450.jpg\n","tags":["Daily-Article"],"categories":["read"]},{"title":"Daily Article Vol 2 - (2018/1/1 ~ 2018/1/31)","url":"/2018/02/01/Daily-Article-Vol2/","content":"\n<center>![Daily Article Vol2][44]</center>\n\n这是`Daily Article`系列的第二篇，罗列了`2018`年`1`月的阅读清单。\n\n<!--more-->\n\n1. [OLTP vs. OLAP][1](1/2)\n1. [OLTP和OLAP的区别][2](1/2)\n1. [Ceph开发每周谈 Vol 31｜ZetaScale 开源 | DMClock][3](1/2)\n1. [Ceph开发每周谈 Vol 32｜Ceph-osd on 4.x 内核异常][4](1/3)\n1. [Ceph开发每周谈 Vol 33｜Encode 改进方案][5](1/3)\n1. [Ceph开发每周谈Vol 34 | Ceph Days 亚太路演 | Ceph OSD CLASS 支持 LUA 编程][6](1/3)\n1. [Ceph开发每周谈 Vol 35 | Ceph Developer Month][7](1/3)\n1. [Ceph开发每周谈 Vol 36|Ebay 的 CephFS 使用深度报告][8](1/4)\n1. [Ceph开发每周谈 Vol 37｜NVME Over Fabric｜FAST][9](1/4)\n1. [【码农翻身】 浏览器：一个家族的奋斗][10](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>\n1. [【码农翻身】 浏览器家族的安全反击战][11](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [【码农翻身】 黑客三兄弟][12](1/6) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [【码农翻身】 GitHub/Stackoverflow 找工作时有什么用？][13](1/6)\n1. [【码农翻身】 我是一个线程(修订版)][14](1/7) <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>\n1. [Linux下安装编译IPVS内核模块][15](1/8) <i class=\"fa fa-star\"></i>\n1. [How to Repack Deb Files on Debian and Ubuntu][16] <i class=\"fa fa-star\"></i>(1/8)\n1. [什么是DevOps][18] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i></i><i class=\"fa fa-star-half-full\"></i>(1/8)\n1. [我是一个线程][19] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/8)\n1. [Javascript: 一个屌丝的逆袭][20] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/9)\n1. [TCP/IP 之 大明王朝的邮差][21] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/9)\n1. [TCP/IP 之 大明内阁][22] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/9)\n1. [TCP/IP 之 蓟辽督师][23] <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/10)\n1. [CPU 阿甘][24] <i class=\"fa fa-star\"></i>(1/10)\n1. [CPU 阿甘之烦恼][25] <i class=\"fa fa-star\"></i>(1/10)\n1. [开源万岁！2018 年开源技术的 10 大发展趋势][17](1/10)\n1. [面试过阿里等互联网大公司，我知道了这些套路][26](1/11)\n1. [访谈 | 鸟哥惠新宸：程序员应该不断提升自身的不可替代性][27](1/15)\n1. [黑客三兄弟（续）][28] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/19)\n1. [我是一个网卡][30] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/20)\n1. [我是一个路由器][29] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/21)\n1. [我是一个进程][31] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i>(1/22)\n1. [我是一块硬盘（上）][32] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/22)\n1. [我是一块硬盘（下）][33] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/23)\n1. [如何维护一个好的技术博客？][34] <i class=\"fa fa-star\"></i>(1/23)\n1. [算法分析神器—时间复杂度][35] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>(1/23)\n1. [我是一个键盘][36] <i class=\"fa fa-star\"></i><i class=\"fa fa-star\"></i> (1/24)\n1. [PHP 面试题 - 如果没有 mb 系列函数，如何切割多字节字符串][37](1/28)\n1. [ceph读写流程分析][38](1/28)\n1. [Token 认证的来龙去脉][39](1/29) <i class=\"fa fa-star\"></i><i class=\"fa fa-star-half-full\"></i>\n1. [ceph 读流程(1)][42](1/29) <i class=\"fa fa-star\"></i>\n1. [一个项目做完以后，不能就让它这么“完”了][40](1/30)\n1. [科学与星球大战：当科幻遇到现实][41](1/30) <i class=\"fa fa-star\"></i>\n1. [ceph 读流程(2)][43](1/31) <i class=\"fa fa-star\"></i>\n\n\n\n\n[1]: http://datawarehouse4u.info/OLTP-vs-OLAP.html\n[2]: http://blog.csdn.net/zhanghaocore/article/details/9820215\n[3]: https://www.xsky.com/tec/ceph-weekly-vol-31/\n[4]: https://www.xsky.com/tec/ceph-weekly-vol-32/\n[5]: https://www.xsky.com/tec/ceph-weekly-vol-33/\n[6]: https://www.xsky.com/tec/ceph-weekly-vol-34/\n[7]: https://www.xsky.com/tec/ceph-weekly-vol-35/\n[8]: https://www.xsky.com/tec/ceph-weekly-vol-36/\n[9]: https://www.xsky.com/tec/ceph-weekly-vol-37/\n[10]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514131&idx=1&sn=a11640045e1458c0e3ba866d23541526&chksm=80d67cd0b7a1f5c6c4dee2cb64b6f173524d443dae8be471bb901eebeec4db5d1773818080b7&scene=21#wechat_redirect\n[11]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514143&idx=1&sn=28ea209c00309e6b93d8d1f76032d7a4&chksm=80d67cdcb7a1f5ca81d8d454a98af56d58b22f6058f100e21ff30e70867ea6e3e922a4f000bf&scene=21%23wechat_redirect\n[12]: https://mp.weixin.qq.com/s/YvYvL0siJT1UhO0tXnYVNA\n[13]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513220&idx=1&sn=bb9c4df63cf6994d6aab9d77a10fe628&scene=21#wechat_redirect\n[14]: https://mp.weixin.qq.com/s/-BMCUuIWYE3O_oC3ZNNJRg\n[15]: http://www.freeoa.net/osuport/botinstal/instal-compile-linux-ipvs-mod_3170.html\n[16]: https://www.tuicool.com/articles/RfmI3u\n[17]: https://mp.weixin.qq.com/s?__biz=MjM5NTU2MTQwNA==&mid=2650655754&idx=1&sn=0b03b942ecdb25ef45ea3a9cbff686c3&chksm=beffc6d989884fcf49c79c662162ed7852b17e192b7f40fcda040308eb10b26a9b187f675940&scene=0&key=78733623947167cee9eb37fa8148459e73e455140b258998bf251e0d57524ceaaa6187e820f481781748d004b780efc58804a25dfdf285670ca35369d0e62d2d8e7cd8ca84306d36da7105f9c8042683&ascene=0&uin=MjEzMTAwMzgyNQ%3D%3D&devicetype=iMac+MacBookAir7%2C1+OSX+OSX+10.12.6+build(16G29)&version=12020810&nettype=WIFI&lang=zh_CN&fontScale=100\n[18]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665514224&idx=1&sn=5b802dfab658a626a197635cf56c9bac&chksm=80d67cb3b7a1f5a585f778b28ceb88855b140ff090c0d0b00f10e9e9b889c81c20190ba80a06&scene=38#wechat_redirect\n[19]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=416915373&idx=1&sn=f80a13b099237534a3ef777d511d831a&scene=21#wechat_redirect\n[20]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513059&idx=1&sn=a2eaf97d9e3000d15a33681d1b720463&scene=21#wechat_redirect\n[21]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513094&idx=1&sn=a2accfc41107ac08d74ec3317995955e&scene=21#wechat_redirect\n[22]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513375&idx=1&sn=e11745d5cb28fa1f89465f8d0e5fae1a&chksm=80d679dcb7a1f0cadec0a1db45b3fa3f6eabde4a9e56a77acf551fbc1511e6b8f055ee2c97e3&scene=21#wechat_redirect\n[23]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513384&idx=1&sn=82c45e4430618270a744e212d2f57990&chksm=80d679ebb7a1f0fd27fe1814765a0e60b1c293ca4e09403271b4bb3f38f317439956fdca1bc1&scene=21#wechat_redirect\n[24]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513017&idx=1&sn=5550ee714abd36d0b580713f673e670b&scene=21#wechat_redirect\n[25]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513254&idx=1&sn=a4d1912b6259c3e65c0e172fb5a10dbb&scene=21#wechat_redirect\n[26]: https://mp.weixin.qq.com/s?__biz=MzIxMTE0ODU5NQ%3D%3D&mid=2650236979&idx=1&sn=71f07d1741a57f8fd429d76d37fd8a07&chksm=8f5a026fb82d8b7931f95e747224b1049d1b0cf72f72babbf6629e183fa15c72570be4fc7253\n[27]: https://www.oschina.net/question/2928191_2272289\n[28]: https://mp.weixin.qq.com/s/_-7C_ZfFfvNKhBQzSB6j4Q\n[29]: https://mp.weixin.qq.com/s/VyGQ4-Dn4UX2Z0CrCHgUqw\n[30]: https://mp.weixin.qq.com/s/vyHlB9pem4rv4htJS9ca6Q\n[31]: https://mp.weixin.qq.com/s/KtHxMoc1_3sQd4d_MuKKJA\n[32]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513289&idx=1&sn=ab19dcad7b1dc217463f155fe106091a&scene=21#wechat_redirect\n[33]: http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513292&idx=1&sn=5b1c87dc72a20c92883924080174b16b&scene=21#wechat_redirect\n[34]: https://mp.weixin.qq.com/s/gliuPj9tfZkr9oXQn9re2w\n[35]: https://mp.weixin.qq.com/s/643URs9k_EQIMWGmb35QkQ\n[36]: https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665513299&idx=1&sn=264f4d0891e1b96fb5e356dc7b6c91dc&scene=21#wechat_redirect\n[37]: https://mengkang.net/1129.html\n[38]: http://www.quts.me/ceph-readwrite/\n[39]: https://segmentfault.com/a/1190000013010835\n[40]: https://mp.weixin.qq.com/s/gI1TjeAYjqgNo4RKqEqF-Q\n[41]: https://mp.weixin.qq.com/s/hDKQ1ITzTvi20kTA11xa1g\n[42]: https://bean-li.github.io/ceph-read-flow/\n[43]: https://bean-li.github.io/ceph-read-2/\n[44]: http://cdn.tony-yin.site/2018-01-600x450.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"实现RBD导出NFS高可用（二）：提供多虚拟IP访问","url":"/2018/01/28/RBD-HA-2/","content":"\n<center>![RBD-HA-2][2]</center>\n\n之前分享过一篇[【通过 Keepalived 实现 Ceph RBD 的高可用】][1]，主要讲解了将`RBD`导出为`NFS`，然后通过`keepalived`实现高可用，保证当提供虚拟`IP`节点发生故障时，可以自动切换节点，使得业务不发生中断。\n\n这样可以基本使用`RBD`代替`CephFS`对外提供`Ceph`服务，至于为什么不用`CephFS`就不多说了，不清楚的可以去看上一篇。虽然说这样可以保证无单点故障，但是有一点还是不如`CephFS`，那就是`CephFS`可以实现多节点同时提供服务，而`RBD`说白了其实同时只有一个节点能提供服务，当客户端流量高的时候，`RBD`方式的带宽并不能满足需求。就比如都是三个节点，`CephFS`可以将客户端流量分流到三个节点，而`RBD`只能用一个节点，而带宽上限又取决与网卡、磁盘和`IO`等等原因，所以同样的硬件设施`RBD`的带宽性能是跟不上的，本文就多虚拟`IP`暴露访问方式进行分享。\n\n<!--more-->\n\n## CephFS & RBD\n\n此前的文章我们`Ceph`集群只有一个`RBD image`，并且只通过一个`vip`暴露这个`image`让客户端通过`NFS`访问。这与`CephFS`的差距就在没有充分利用每个节点的资源，所以我们可以大胆设想一下是否可以通过`RBD`对外提供多个`vip`，每个节点都能被`NFS`访问呢？理想很美好，现实很残酷。如果一个`RBD`对多个节点同时提供读写的话，会导致不一致的后果，现在`RBD`并不能做到`CephFS`那样多个节点同时提供服务且保证读写一致。那怎么办呢？\n\n虽然一个`RBD image`不能同时被多客户端访问，但是我们是否可以创建多个`RBD image`，然后利用多个`vip`对外提供访问呢？这样听起来貌似可行，但是还是存在诸多问题，比如如何暴露多虚拟`IP`，如何将`IP`绑定到具体的`RBD image`，如何保证多`RBD image`的高可用等等，下文将就这些技术细节进行详细地分析。\n\n## 需求背景\n\n客户端有多种应用场景，对流量要求较高的情况下，我们可以为每一种应用场景都提供一个`vip`用于`NFS`方式访问`Ceph`存储集群。然后每个`vip`各自对应集群中的一个`RBD image`，`RBD image`尽量均匀的分布到各个节点上，这样才能把性能提升到最高，比如集群有三个节点的话，如果暴露三个`vip`，那么必须要分布到三个不同的节点上，如果要提供四个`vip`的话，那么前三个`vip`均匀地分布到三个节点上，第四个`vip`就在第一个节点上暴露，以此类推，这边说的第一个节点只是我们自己将三个节点进行逻辑上的排序，我们需要通过一些算法确保`vip`分布均匀，具体看下文分析。\n\n\n## 整体设计\n\n一般在完成一个`feature`之前，我们往往需要做一个`design`，对要做的事情和流程进行设计和评估，这样不但可以梳理流程，使得之后动手的时候思路清晰，更重要的是可以预见一些问题和难点，尽早与 团队成员进行交流，选择最佳方案，防止真正做的时候走弯路。这边涉及的技术点主要有：\n\n### Keepalived暴露多个VIP\n\n`keepalived`暴露单个`vip`很常见，具体格式网上都有，而暴露多个`vip`就要注意一些细节，比如`router_id`，`ins_name`，`priority`等等，对于一个节点而言，它上面`keepalived`暴露`vip`的情况完全是由配置文件`keepalived.conf`所决定的，而对于`keepalived.conf`而言，一个`vip`其实就是`ins`，而`ins_name`和`router_id`要求同一个`keepalived`组内成员相同，我们这边就默认`router_id`就是`vip`隔着小数点的四位整数相加的和，而`ins_name`则是将`vip`的小数点换成下划线。\n\n\n### VIP动态均匀分布\n\n`vip`均匀分布要保证尽可能的均匀，比如三个节点，如果要提供两个`vip`的话，那就随意挑选两个节点作为`vip`绑定，如果四个`vip`的话，则是三个节点各自绑定一个`vip`后再任意选择一个节点作为第四个`vip`绑定。我们这边的做法是先将所有节点进行排序，将两个节点作为一个`keepalived`组，下两个节点为另外一组，假设有三个节点，我们设为`1, 2, 3`，那么如果要暴露三个`vip`，我们就需要三个`keepalived`组，这边三个组分别是`1, 2`，`3, 1`和`2, 3`，然后组内其中第一个节点为`master`，第二个节点为`backup`。这样可以基本保证所有`vip`的均匀分布，具体算法实现参见下文。\n\n### 多RBD高可用\n\n上一篇文章中只有一个`RBD`，所以高可用就围绕它一个，发生故障后随意切换节点即可，因为我们每个节点都是一个`keepalived`组的成员。但是如果有多个`RBD`的话，我们如果随意切换的话，那么`RBD`分布就会变得不均匀。上文提及的算法可以保证`vip`的均匀分布，两两节点作为一个`keepalived`组，这样我们即使一个节点掉了，切换也只会在当前组内切换，而`vip`一开始绑定节点的时候就根据相应算法保证了每个`RBD`的均匀分布，所以这边组内切换不会影响分布的均匀性。\n\n上一篇文章中提过`keepalived`的机制，当主节点`down`了，主节点会触发我们自己写的`ChangetoBackup.sh`，而副节点则会触发`ChangetoMaster.sh`。之前由于只有一个`RBD`，所以当时做的比较无脑，`ChangetoMaster.sh`直接遍历当前节点上面的所有`RBD`，然后通过之前记录的`RBD`和`UI`上创建的`目录`的映射关系进行挂载，而`ChangetoBackup.sh`也是一样的`umount`所有`RBD`的挂载点。针对目前的多`RBD`的情况，这样的做法肯定是不行的，因为现在我们一个节点可能是一个或多个`vip`的`master`，也可能是另外一个或多个`vip`的`backup`，如果我们还是像之前那样一股脑的全部卸载或者挂载，那么造成的后果显而易见，就是业务中断，暴露服务节点紊乱。所以最合理的应该对号入座，一个`vip`对应一个`RBD image`，哪个`vip`出现了问题，作为该`vip`的`master`节点，应该只`umount`该`vip`绑定`RBD`所对应的目录，而`backup`节点应该只`mount`对应的目录。其他不相关`RBD`和其对应的目录，我们都不应该有所操作。那么我们只有在触发`ChangetoMaster.sh`和`ChangetoBackup.sh`这两个脚本的时候加上“目录”这个参数，具体实现详见下文分析。\n\n### 大容量RBD image的创建和删除\n\n我们系统的实现是`UI`上创建目录，后端`daemon`轮询根据目录信息做对应的事情，比如前端`UI`创建了目录，后端就是在创建`RBD image`，而生产环境上面的容量的要求都是很高的，往往都是几十`T`，甚至上百`T`,但是熟悉`RBD`的朋友都知道创建如此大的`RBD image`是需要很长的时间的，那这样就不但会影响当前目录能够提供服务的时间，也会阻塞住代码，影响之后目录的创建。之前我们的做法是一开始我们可以创建一个比较小的`image`，然后我们后台选择在业务不繁忙的时候进行定时扩容，这也可以算是暂时止血了。但是后来测试发现删除`image`才是真的慢，这边就不像创建那样有曲线救国的方式了，所以这边无论是创建还是删除`RBD image`我们都不能做成同步的方式了，我们采取了另起一个线程单独做这个事情，不影响后端业务的正常处理。\n\n### 快照保证扩容的安全性\n\n在我们的测试过程中，发现对`RBD image`扩容会偶尔发生文件系统出错的情况，这种情况是很危险的，一旦文件系统发生问题，并且用`e2fsck`等工具修复不了的话，那么数据恢复是很困难的，我们必须要保证客户数据的安全性。所以我们用了`RBD`的`snapshot`的功能，在每次扩容之前为`RBD image`做快照，这样即使发生了问题，我们起码可以做到最小程度的损失。\n\n## 具体代码实现\n\n### Keepalived暴露多个VIP\n\n当`UI`创建一个`vip`的时候，我们就要加一个`ins`，以下就是我们添加一个`ins`的`API`，本文所有代码都是`python`写的，大家凑合看吧。（部分代码和接口不是很全，文章尾部将会贴出详细代码的地址）\n\n```python\ndef add_keepalived_ins(self, vip, folder, state):\n    vrrp_ins = \"\"\"\nvrrp_instance VI_{ins_name} {{\n    state {state}\n    interface {pubif}\n    priority {priority}\n    virtual_router_id {router_id}\n    advert_int 1\n    authentication {{\n        auth_type PASS\n        auth_pass 1111\n    }}\n    track_script {{\n        chk_nfs\n    }}\n    notify_master \"/etc/keepalived/ChangeToMaster.sh {folder}\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh {folder}\"\n    virtual_ipaddress {{\n        {vip}\n    }}\n}}\n\"\"\".format(ins_name = vip.replace('.', '_').replace('/', '_'),\n           state = state,\n           priority =  200 if state == \"MASTER\" else 100,\n           router_id = self.get_router_id(vip),\n           pubif = get_public_interface(),\n           folder = folder,\n           vip = vip)\n        return vrrp_ins\n```\n\n这边我们可以看到`ins_name`和`router_id`都是根据`vip`转换成特定格式，标识`ins`的唯一性。而`priority`则是根据`state`来决定，`state`为`master`时，`priority`为`200`，而`backup`的`priority`为`100`。至于如何获取`state`，这个涉及到`vip`均匀算法，后续会讲。\n\n### VIP动态均匀分布\n\n假设三个节点，为`1, 2, 3`，三个`vip`，为`a, b, c`，那么最后`a`对应的节点为`1, 2`，`b`对应的节点为`3, 1`，`c`对应的节点为`2, 3`，具体实现算法是先将所有`vip`进行排序，获取要操作`vip`的`index`，然后获取集群内所有节点，然后将上面获取的`index`乘以`2`，再对所有节点的个数做余数，然后可以获得一个整数，这个整数就是`vip`对应`master`节点在所有节点数组中的`index`，这种算法大家应该很容易从规律中推算出来。\n\n```python\ndef get_my_state(self, vip_idx):\n    nodes = get_all_nodes()\n    nodes.sort()\n    idx = vip_idx * 2 % len(nodes)\n    my_ip = get_public_ip()\n    if my_ip == nodes[idx]:\n        return 'MASTER'\n    elif my_ip == nodes[(idx + 1) % len(nodes)]:\n        return 'BACKUP'\n    else:\n        return None\n```\n\n### 多RBD高可用\n\n我们在创建目录的时候，需要获取当前节点是否为`master`，之前那个只有一个`vip`，所以当前节点要么是`master`，要么是`backup`，但是这边的话，一个节点可能是一个`vip`的`master`的同时也可能是另一个`vip`的`backup`，所以是否为`master`是要根据目录而定的。在这边我们在创建目录、删除目录、创建`vip`和删除`vip`时，更新一个`vip`和目录之间的映射关系。这个`map`我是存在`ceph`的`leveldb`中，至于为什么不存在节点本地，是因为这份数据必须要保证所有节点强一致，放在本地节点，可能会因为一些故障原因导致之后内容不一致的情况。\n\n这边我们要求在创建目录前，必须要存在空闲`vip`可以提供目录绑定。所以当创建一个`vip`时，此时应该没有目录需要绑定，我们建立一个`key`和`value`都是`vip`的字典；当创建一个目录的时候，随机找到一个空闲`vip`进行绑定，建立一个`key`为`vip`，`value`为目录名的字典；当删除`vip`时，肯定是存在其他空闲`vip`的，所以在删除原来对应`map`后，我们要找到其他一个空闲`vip`与之前删除`vip`对应的目录进行绑定；当删除目录时，只要将对应关系中的`value`换成`key`，也就是对应的`vip`了。\n\n有了这个`map`，我们就可以实时获取目录和`vip`的信息和之间的对应关系。\n\n`vip.py`\n\n负责当`vip`发生变化时，更新`ip_folder_map`，以及`ip_folder_map`的读写`API`\n\n```python\ndef get_ip_folder_map():\n    result = {}\n    ip_folder_map = LevelDB(\"ip_folder_map\")\n    result = json.loads(ip_folder_map)[\"ip_folder_map\"]\n    return result\n    \ndef set_ip_folder_map(ip_folder_map):\n    ip_folder_map = LevelDB(\"ip_folder_map\")\n    ip_folder_map.set(json.dumps({\"ip_folder_map\": ip_folder_map}))\n    ip_folder_map.save()\n    \ndef update_ip_folder_map_by_ip(ips):\n    ip_folder_map = get_ip_folder_map()\n    old_ips = ip_folder_map.keys()\n    if len(ips) > len(old_ips):\n        new_ip = list(set(ips) - set(old_ips))[0]\n        ip_folder_map[new_ip] = new_ip\n    else:\n        del_ip = list(set(old_ips) - set(ips))[0]\n        folder = ip_folder_map[del_ip]\n        del ip_folder_map[del_ip]\n        if folder != del_ip:\n            for k, v in ip_folder_map.iteritems():\n                if k == v:\n                    ip_folder_map[k] = folder\n                    break\n    set_ip_folder_map(ip_folder_map)\n```\n\n`folder.py`\n\n负责当`folder`发生变化时，更新`ip_folder_map`\n\n```python\nimport vip\n\ndef update_ip_folder_map_by_folder(folder, type):\n    ip_folder_map = vip.get_ip_folder_map()\n    folder = get_folder_path(folder)\n    if type == \"add\":\n        for k, v in ip_folder_map.iteritems():\n            if k == v:\n                ip_folder_map[k] = folder\n                break\n    elif type == \"delete\":\n        for k,v in ip_folder_map.iteritems():\n            if v == folder:\n                ip_folder_map[k] = k\n                break\n    vip.set_ip_folder_map(ip_folder_map)\n```\n\n上面说了在切换节点的时候，需要传递目录参数，保证只操作对应目录。而脚本是静态的，目录确是动态的，所以我们需要在目录或者`vip`发生变化的时候对原来的`keepalived.conf`进行更新，添加目录参数。也就是说当`vip`发生变化时，我们根据当前`vip`选择添加或者减少`ins`，并且更新每个`ins`调用脚本后面追加的参数；而`folder`发生变化时，`vip`调用脚本后面追加的参数也需要更新，要么是`vip`，要么是`folder`。这边也需要用到上面的`ip_folder_map`，因为每个`ins`就是一个`vip`，而每个`vip`对应一个`folder`。所以我们这边当目录或者`vip`发生变化时，会根据`ip_folder_map`更新`keepalived.conf`，具体实现代码如下：\n\n```python\n    def update_keepalived_conf(self):\n        kconf = \"\"\"global_defs {\n    notification_email {\n    }\n    \n    router_id NFS_HA_112\n}\n\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"\n    interval 2\n}\n\"\"\"\n        vips = self.ip_folder_map.keys()\n        vips.sort()\n        for vip, folder in self.ip_folder_map.items():\n            vip_idx = vips.index(vip)\n            state = self.get_my_state(vip_idx)\n            if state is not None:\n                kconf += self.add_keepalived_ins(vip, folder, state)\n        with open(KEEPALIVED_CONF_PATH, 'w') as f:\n            f.writelines(kconf)\n        do_shell('service keepalived reload')\n```\n\n下面是添加一个`ins`的模板，上面也贴过代码，至于这边再次贴一遍的目的是想侧重展示一下脚本后面参数的动态变化的实现方式。\n\n```python\n    def add_keepalived_ins(self, vip, folder, state):\n        vrrp_ins = \"\"\"\nvrrp_instance VI_{ins_name} {{\n    state {state}\n    interface {pubif}\n    priority {priority}\n    virtual_router_id {router_id}\n    advert_int 1\n    authentication {{\n        auth_type PASS\n        auth_pass 1111\n    }}\n    track_script {{\n        chk_nfs\n    }}\n    notify_master \"/etc/keepalived/ChangeToMaster.sh {folder}\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh {folder}\"\n    virtual_ipaddress {{\n        {vip}\n    }}\n}}\n\"\"\".format(ins_name = vip.replace('.', '_').replace('/', '_'),\n           state = state,\n           priority =  200 if state == \"MASTER\" else 100,\n           router_id = self.get_router_id(vip),\n           pubif = get_public_interface(),\n           folder = folder,\n           vip = vip)\n        return vrrp_ins\n```\n\n触发脚本：\n\n`ChangetoMaster.sh`\n\n```bash\n#!/bin/bash\n\nfolder=\"$(dirname $1)/$(basename $1)\"\nfname=$(basename $folder)\n\nif [ -d $folder ]; then\n    if $(mount | grep -q \"$folder \"); then\n        umount -f $folder > /dev/null\n    fi\n    device=$(rbd showmapped | awk '/image_'$fname' / {print $5}')\n    if [ -b \"$device\" ]; then\n        mount $device $folder\n    fi\nfi\nservice nfs-kernel-server restart\n```\n\n`ChangetoBackup.sh`\n\n```bash\n#!/bin/bash\n\nfolder=$1\nservice nfs-kernel-server stop\nif [ -d $folder ]; then\n    if $(mount | grep -q \"$folder \"); then\n        umount -f $folder > /dev/null\n    fi\nfi\nservice nfs-kernel-server start\n```\n\n### 大容量RBD image的创建和删除\n\n在另外一个端口另起一个线程，通过异步的方式实现，主要利用`python`的`rpyc`模块实现，忧郁项目保密性等原因，只贴上部分关键代码，给大家提供一些思路。\n\n以删除`RBD image`为例，调用`remove_image`方法，进入装饰器，从而在新现成做删除操作，不再阻塞之前进程的流程。\n\n```python\ndef rbd_background():\n    conn = connect('localhost', RBD_PORT)\n    module = conn.modules['rbd_utils']\n    async_func = rpyc.async(getattr(module, func_name))\n    \n    return async_func\n    \n@rbd_utils.rbd_background\ndef remove_image(pool, image):\n    while True:\n        try:\n            logger.info('rbd {} delete start'.format(image))\n            do_shell('rbd rm {}/{} >> /var/log/rbd_rm.log'.format(pool, image))\n            logger.info('rbd {} delete finish'.format(image))\n            break\n        except Exception:\n            logger.error('rbd {} delete error'.format(image))\n            time.sleep(30)\n```\n\n### 快照保证扩容的安全性\n\n首先介绍一下定时扩容的脚本：\n\n`monitor_rbd.sh`：当`RBD image`可利用空间小于`50%`或者小于`50T`时，扩容`50T`\n\n```bash\n#!/bin/bash\n\nfunction convert_to_MB()\n{\n    size=$1\n    unit=${size:(-1):1}\n    nr=${size/$unit/}\n    case $unit in\n        (k|K|\\)) echo \"$nr / 1024\" | bc;;\n        (m|M|\\)) echo \"$nr\";;\n        (g|G|\\)) echo \"$nr * 1024\" | bc;;\n        (t|T|\\)) echo \"$nr * 1024 * 1024\" | bc;;\n        (p|P|\\)) echo \"$nr * 1024 * 1024 * 1024\" | bc;;\n        *) echo \"Error: cannot convert to MB: $size\";;\n    esac\n}\n\nfunction get_available_size()\n{\n    disk=$1\n    unit_size=$(convert_to_MB '50T')\n    \n    disk_size=$(df -h | grep $disk | awk '{print $2}')\n    disk_size=$(convert_to_MB $disk_size)\n    pool=$(rbd showmapped | grep $disk | awk '{print $2}')\n    available_pool_size=$(ceph df | grep $pool | awk '{print $5}')\n    available_pool_size=$(convert_to_MB $available_pool_size)\n    if [ $(echo \"$available_pool_size < $unit_size\" | bc) -eq 1 ]; then\n        new_size=$(echo \"$disk_size + $available_pool_size\" | bc)\n    else\n        new_size=$(echo \"$disk_size + $unit_size\" | bc)\n    fi\n    echo ${new_size%.*}\n}\n\nfunction check_and_enlarge_disk()\n{\n    disk=\"$1\"\n    if [ \"$disk\" = \"\" ]; then\n        echo \"Error: You must specify the disk name\"\n        return 1\n    fi\n    echo \"Checking the disk [/dev/$disk] ...\"\n    if ! rbd showmapped | grep -q $disk; then\n        echo \"Error: Cannot find the disk [$disk]\"\n        return 2\n    fi\n    disk_usage=$(df | grep $disk | awk '{print $5}')\n    available_disk_size=$(df | grep $disk | awk '{print $4}')\n    available_disk_size=$(convert_to_MB \"${available_disk_size}k\")\n    echo \"  The disk use% is ${disk_usage}\"\n    disk_usage=${disk_usage/\\%/}\n    if [ $disk_usage -lt 50 -a $available_disk_size -gt 1024 * 1024 * 50 ]; then\n        echo 'Less then 50% use and more then 50TB available space left, just quit'\n        return 0\n    fi\n    echo 'Enlarging the disk ...'\n    new_size=$(get_available_size $disk)\n    echo \"  the new size is ${new_size}MB\"\n    pool=$(rbd showmapped | grep $disk | awk '{print $2}')\n    image=$(rbd showmapped | grep $disk | awk '{print $3}')\n    rbd resize --size $new_size -p $pool $image\n    sleep 3\n    resize2fs /dev/${disk} \"${new_size}M\"\n    echo \"Done\"\n}\n\ndisks=$(lsblk | grep rbd | awk '{print $1}')\nfor disk in $disks\ndo\necho \"==============================================\"\ncheck_and_enlarge_disk \"$disk\"\necho \"==============================================\"\ndone\n```\n\n这边我们采用`ceph`提供的原生`python`的接口，完成`RBD`的定时快照的创建和删除\n\n```python\n#!/usr/bin/python\n\nimport os\nimport time\nimport rados\nimport rbd\nfrom folder import get_all_folder_info\nfrom vip import get_ip_folder_map\n\nCEPH_CONF = '/etc/ceph/ceph.conf'\nMAX_SNAP_COUNT = 5\n\ndef create_snap(pool, rbd_image):\n    now = time.localtime()\n    snap = time.strftime(\"%Y_%m_%d_%H_%M_%S\", now)\n    with rados.Rados(conffile=CEPH_CONF) as cluster:\n        with cluster.open_ioctx(str(pool)) as ioctx:\n            with rbd.Image(ioctx, rbd_image) as image:\n                image.create_snap(snap)\n                \ndef get_images():\n    pubif = get_public_interface()\n    pub_ips = do_cmd(\"ip addr show {} | awk '/inet/ {{print $2}}'\".format(pubif)).split()\n    vip_folders = get_ip_folder_map(gwgroup)\n    my_folders = []\n    for pip in pub_ips:\n        if pip in vip_folders and pip != vip_folders[pip]:\n            my_folders.append(os.path.basename(vip_folders[pip]))\n    folders = get_all_folder_info()\n    images = []\n    for folder in folders:\n        if folder in my_folders:\n            images.append({\n                'image': 'image_{}'.format(folder),\n                'pool': folders[folder]['pool']\n            })\n    return images\n    \ndef remove_old_snap(pool, rbd_image):\n    with rados.Rados(conffile=CEPH_CONF) as cluster:\n        with cluster.open_ioctx(str(pool)) as ioctx:\n            with rbd.Image(ioctx, rbd_image) as image:\n                snaps = sorted(image.list_snaps(), key=lambda snap: snap['name'])\n                if len(snaps) > MAX_SNAP_COUNT:\n                    for snap in snaps[0:len(snaps)-MAX_SNAP_COUNT]:\n                        image.remove_snap(snap['name'])\n                        \ndef main():\n    images = get_images()\n    for image in images:\n        create_snap(image['pool'], image['image'])\n        remove_old_snap(image['pool'], image['image'])\n        device = do_shell(\"rbd showmapped | awk '/{}[ \\t]*{}/ {{print $5}}'\".format(image['pool'], image['image']))\n        do_shell('/usr/local/bin/monitor_rbd.sh {}'.format(os.path.basename(device)))\n        \nif __name__ == \"__main__\":\n    main()\n```\n\n## 总结\n\n内容和代码都比较多，其实每一个技术点都可以单独拿出来写一篇，但是我觉得这是一个完整`feature`，想让大家能够代入，了解到完成这样一个`feature`周边需要支持的各种技术点和注意点，一个`feature`往往是经过不断迭代和维护，很多实现方法也随着时间和应用场景不断发生变化。\n\n完成这样一个`feature`，我也是反复修改，就比如异步实现`RBD image`的创建和删除，很多场景在生产环境和测试环境中的 情况是完全不一样的，比如我开发的时候创建的`image`都是`1G`的，当然很快，也不能存在什么阻塞的问题，也遇到很多问题和想不通的地方，感谢我的同事和前辈提供的帮助和启发。\n\n最后，衷心希望`ceph`能够早日将`CephFS`完善，保证其在生产环境中的稳定性和性能。这样我们也就不用绞尽脑汁这般曲线救国了，哈哈。\n\n最后的最后，贴上部分代码地址，由于项目保密性等原因，我只能贴出比较关键的代码，大家请见谅，我觉得这些代码应该足够了，足够给大家提供一个思路了，其实往往思路比代码更重要，相信很多人的实现方式要比我更加优秀呢！\n\n> 代码地址：https://github.com/tony-yin/Multi_RBD_HA\n\n如果大家觉得有帮助的话，欢迎`Star`哦 ~\\(≧▽≦)/~\n\n[1]: http://www.tony-yin.site/2017/12/07/RBD-HA/\n[2]: http://cdn.tony-yin.site/rbd2-600x450.jpg\n\n\n","tags":["Keepalived"],"categories":["tech"]},{"title":"scp 免交互式和 ssh 免交互式脚本","url":"/2018/01/10/scp-and-ssh/","content":"\n<center>![Scp-And-Ssh][1]</center>\n\n简单实现了`scp`的免交互式脚本和`ssh`免交互式脚本。\n\n<!--more-->\n\n## scp\n\n场景：\n\n需要将以下源主机上的三个文件拷贝到以下目的主机的对应目录下\n\n> `src host`: 192.168.1.1\n> `dist host`: 192.168.1.2\n> `files`: `/root/1/1.txt`, `/root/2/2.txt`, `/root/3/3.txt`\n\n这时候如果手动做的话，将会很繁琐，所以这时候需要一个脚本能够实现文件的自动复制，并且脚本需要自动把密码验证的步骤也覆盖。\n\n这里的关键就是如何实现`shell`交互式命令行的自动化，这边可以用分界符`EOF`，`EOF`范围中的字符将会被作为命令输入到交互式命令行中，具体脚本如下：\n\n```shell\nSOURCEHOST=192.168.1.1\nDISTHOST=$1\nFILE1=/root/1/1.txt\nFILE2=/root/2/2.txt\nFILE3=/root/3/3.txt\nFOLDER1=/root/1/\nFOLDER2=/root/2/\nFOLDER3=/root/3/\nPASSWORD=123456\nscp FILE1 ${DISTHOST}${Folder1} << EOF\n$PASSWORD\nEOF\nscp FILE2 ${DISTHOST}${Folder2} << EOF\n$PASSWORD\nEOF\nscp FILE3 ${DISTHOST}${Folder3} << EOF\n$PASSWORD\nEOF\n```\n\n调用方式：`sh scp.sh 192.168.1.2`\n\n## ssh\n\n场景：\n\n上面的场景是建立在登陆`192.168.1`主机的基础上，现在我想我在任意主机上都可以实现上面将`192.168.1.1`的上述文件拷贝到`192.168.1.2`的对应目录下\n\n这里的关键是实现`ssh`的免密登陆，这时我们需要用到`sshpass`，具体介绍可以自行搜索下，用法如下：\n\n```shell\n#! /bin/bash\n\nSOURCEHOST=192.168.1.1\nDISTHOST=$1\nFILE1=/root/1/1.txt\nFILE2=/root/2/2.txt\nFILE3=/root/3/3.txt\nFOLDER1=/root/1/\nFOLDER2=/root/2/\nFOLDER3=/root/3/\nPASSWORD=123456\n\nsshpass -p $PASSWORD ssh $SOURCEHOST \\\n    sshpass -p $PASSWORD scp ${FILE1} ${DISTHOST}${FOLDER1} && \\\n    sshpass -p $PASSWORD scp ${FILE2} ${DISTHOST}${FOLDER2} && \\\n    sshpass -p $PASSWORD scp ${FILE3} ${DISTHOST}${FOLDER3}\n```\n\n调用方式：`sh scp.sh 192.168.1.2`\n\n[1]: http://cdn.tony-yin.site/open_ssh-600x450.png\n","tags":["Linux"],"categories":["tech"]},{"title":"利用Raid卡工具获取逻辑盘是否为SSD","url":"/2018/01/05/RaidCardToolUtils/","content":"\n<center>![RaidCardToolUtils][2]</center>\n\n网上很多获取一块盘是否为`SSD`的方式都是不靠谱的，不能覆盖到所有情况。一般我们在操作系统上的硬盘都是虚拟出来的逻辑盘，比如`/dev/sda`这种，它可能对应一块单独的物理硬盘，也有可能对应的是几块盘组成的`raid`。我们有时候想获取一块盘的具体信息，比如磁盘类型、插槽号、序列号等等，这时候我们就得借助对应的`raid`卡工具了，最常见的如`Megacli`，通过逻辑盘找到对应的物理盘，然后读取信息。\n\n<!--more-->\n\n## Raid卡简介 \n\n所谓`raid`卡，就是为了更好的统一管理物理硬盘而存在的，在出现单独的`raid`卡之前，对硬盘做`raid`操作，需要`cpu`完成其中的计算操作，这个会很影响其他依赖`cpu`的应用或进程的性能，后来就将`raid`卡单独提取出来，并且在其之上存在一个小型`cpu`供来完成`raid`相关操作的计算，这其中最常见的`raid`工具应该非`Megacli`莫属了。\n\n为什么说最常见的呢？因为`raid`卡工具对应不同型号的`raid`卡是不一样，`LSI`只是一个半导体厂商，负责提供`raid`芯片，最后还需要集成到服务器厂商的机器上，所以最后的工具还是由厂商决定和提供，也可以理解为特定型号的`raid`对应各自的工具。\n\n## HBA卡简介\n\n近来，又出现了一种`HBA`卡，只从`HBA`的英文解释`HOST BUS ADAPTER`（主机总线适配器）就能看出来，他肯定是给主机用的，一般`HBA`就是给主机插上后，给主机扩展出更多的接口，来连接外部的设备。大多数讲到`HBA`卡都是指光纤的`HBA`卡，给主机提供光纤接口的。也有`ISCSI`的`HBA`卡，链接`ISCSI`设备的，从这种功能上说，我们也可以把独立网卡称为`HBA`卡，通过独立网卡扩展出网口来连接外部网络设备或主机。不过习惯上大部分`HBA`只是称光纤卡或者`iscsi`卡。\n\n简而言之，这种`HBA`卡本身是为了扩展外部连接设备而存在的，但是它具有部分`raid`功能，与`raid`卡相比它的优势在于它价格便宜，性价比高；劣势在于虽然具有`raid`功能，但是都是基础的功能，没有`raid`卡那么完善。\n\n> 这篇文章讲`raid`卡和`HBA`卡讲的挺好的：[HBA卡 和 RAID卡][1]\n\n## 需求和背景\n\n据我所知，这类工具往往是运维人员用的居多，但是往往开发中也会需要用到。本文通过获取逻辑盘对应盘的类型展开描述，并借此讲解获取逻辑盘的一类信息或通过逻辑盘操作对应物理盘。因为这其中的关键就是找到逻辑盘和物理盘之间的对应关系。无论是`raid`卡工具还是`HBA`卡工具都是罗列所有硬盘的信息，所以你要从中找到你选择的逻辑盘所对应的便是重中之重。\n\n逻辑盘对应的物理盘可能为单独的硬盘，也可能是`raid`，单独的可以直接读取硬盘类型，`raid`的话我们认为只会将同样类型的盘做`raid`，混合的情况不考虑。\n\n`raid`卡工具的话，我只对`Megacli`和`Sas3ircu`进行讲解，所以阅读本文前最好有使用以上两个工具的相关经验。首先我会根据目前存在的`raid`卡类型建立一个`map`关系，然后通过`raid`卡类型自动获取对应`raid`卡工具，每个`raid`卡都是一个类，然后里面的方法都是为该工具定制化的操作。\n\n## 获取raid卡工具\n\n目前就考虑两种型号的`raid`卡，以后有新的再往`map`里面填充就好了。`NotSupport`指的是其他不支持型号的`raid`卡和虚拟机。\n\n> `do_shell`是本人封装的一个在`python`中执行`shell`命令的方法，大家可以根据自己的情况对该方法进行转换\n\n通过获取的`card mode`，根据`map`找到对应的`tool`，然后实例化对应的工具类\n\n```python\nclass RaidCardToolFactory():\n    RaidCardMap = {\n        'SAS2208': MegaraidTool,\n        'SAS3008': HBATool,\n        'NotSupport': NotSupport\n    }\n    \n    def getTool(self):\n        card_model = self.get_raidcard_model()\n        tool = self.RaidCardMap[card_model]()\n        return tool\n        \n    def get_raidcard_model(self):\n        card_model = 'NotSupport'\n        card_info = do_shell(\"lspci | grep 'LSI Logic'\")\n        if card_info == '':\n            return card_model\n        card = card_info.strip().splitlines()[0].split()\n        if 'RAID bus controller' in card_info:\n            card_model = card[10] + card[11]\n        elif 'Attached SCSI controller' in card_info:\n            card_model = card[10]\n        return card_model\n```\n\n## Megaraid工具类\n\n1. 先通过`lsscsi`命令获取逻辑盘是否为`raid`；\n2. 如果是`raid`，那么直接根据`lsscsi`获取当前逻辑盘的`target id`，也就是第三个号，然后通过`megacli cfgdsply -aALL`获取所有`raid`信息，根据逻辑盘的`target id`对应物理盘中的`Target Id`找到对应`raid`，然后只要获取`raid`中第一块物理盘的硬盘类型即可，也就是`Media Type`，具体参见下方`API`: `get_ld_type`\n3. 如果不是`raid`，那么直接根据`lsscsi`获取当前逻辑盘的`target id`，也就是第三个号，这边的`target id`直接对应`megacli`中每一块单盘中的`Device Id`字段，所以根据`target id`匹配`megacli pdlist aAll`获取磁盘列表的每一项的`Device Id`便可以找到对应的物理盘，具体参见下方`API`: `get_pd_type`。\n\n```python\nclass MegaraidTool():\n    def get_disk_type(self, disk_name):\n        scsi_info = do_shell(\"lsscsi | grep {} -w\".format(disk_name))\n        target_id = scsi_info.split()[0].split(\":\")[2]\n        serial_nu = scsi_info.split()[3].strip()[2:]\n        if \"LSI\" in scsi_info:\n            disk_type = self.get_ld_type(target_id, serial_nu)\n        else:\n            disk_type = self.get_pd_type(target_id)\n        return disk_type\n        \n    def get_ld_type(self, target_id, serial_nu):\n        disk_type = ''\n        cmd = MEGACLI + ' cfgdsply -aALL -NoLog|grep -E \"Product Name|Target Id|Media Type\"'\n        output = do_shell(cmd)\n        adapters = output.split('Product Name')\n        for adapter in adapters:\n            if serial_nu not in adapter:\n                continue\n            lines = adapter.split('\\n')\n            for line in lines:\n                if \"Target Id: {}\".format(target_id) in line:\n                    index = lines.index(line)\n                    if 'Solid State Device' in lines[index + 1]:\n                        disk_type = \"SSD\"\n                    else:\n                        disk_type = \"HDD\"\n                    break\n            if disk_type != '':\n                break\n        return disk_type\n        \n    def get_pd_type(self, target_id):\n        disk_type = ''\n        cmd = MEGACLI + ' pdlist aAll | grep -E \"Device Id|Media Type\"'\n        output = do_shell(cmd)\n        lines = output.split('\\n')\n        if 'Device Id: {}'.format(target_id) not in lines:\n            return ''\n        index = lines.index('Device Id: {}'.format(target_id))\n        if 'Solid State Device' in lines[index + 1]:\n            disk_type = \"SSD\"\n        else :\n            disk_type = \"HDD\"\n        return disk_type\n```\n\n## HBA工具类\n\n1. `HBA`类用的工具是`sas3ircu`，首先我们需要根据命令`sas3ircu list`获取所有的`controller`，然后每次获取信息都需要遍历所有`controller`；\n2. 第一步依旧是判断逻辑盘是否为`raid`；\n3. 如果是`raid`，获取逻辑盘的`target id`，与之匹配的是`sas3ircu`中的`Initiator at ID`字段，找到对应的`raid`，然后通过获取其下第一个物理盘的类型，这边类型字段变成了`Drive Type`，具体参考下方`API`: `get_ld_type`；\n4. 如果非`raid`，我匹配的是`sas3ircu`中的`Sas Address`字段，那么逻辑盘的`Sas Address`如何获取呢？这边我用的方式是通过`udev`获取逻辑盘的`symlink`，这里面有很多`address`，而我们需要的是`by-path`，我这边就简单做了，看`sas3ircu`每个盘的`Sas Address`是否被`udev`获取的`symlink`包含，如果包含了，那么也就匹配到了，然后直接获取`Drive Type`字段就可以得到磁盘类型类；具体参考下方`API`: `get_pd_type`\n\n```python\nclass HBATool():\n    def get_disk_type(self, disk_name):\n        scsi_info = do_shell(\"lsscsi | grep {} -w\".format(disk_name))\n        if \"LSI\" in scsi_info:\n            target_id = scsi_info.split()[0].split(\":\")[2]\n            disk_type = self.get_ld_type(target_id)\n        else:\n            sas_address = do_cmd('udevadm info --query=symlink --name={}'.format(disk_name))\n            disk_type = self.get_pd_type(sas_address)\n        return disk_type\n        \n    def get_ld_type(self, target_id):\n        disk_type = ''\n        controllers = self.get_controllers()\n        for controller in controllers:\n            cmd = 'sas3ircu {} display|grep -E \"Initiator at ID|Drive Type\"'.format(controller)\n            output = do_shell(cmd)\n            if 'Initiator at ID #{}'.format(target_id) in output:\n                lines = output.splitlines()\n                index = lines.index('Initiator at ID #{}'.format(target_id))\n                if 'HDD' in lines[index + 1]:\n                    disk_type = 'HDD'\n                else:\n                    disk_type = 'SSD'\n                break\n        return disk_type\n        \n    def get_pd_type(self, sas_address):\n        disk_type = ''\n        controllers = self.get_controllers()\n        for controller in controllers:\n            cmd = 'sas3ircu {} display|grep -E \"SAS Address|Drive Type\"'.format(controller)\n            output = do_shell(cmd)\n            lines = output.splitlines()\n            for i in xrange(0, len(lines), 2):\n                address = lines[i].split()[-1].replace('-', '')\n                if address in sas_address:\n                    if 'HDD' in lines[i + 1]:\n                        disk_type = 'HDD'\n                    else:\n                        disk_type = 'SSD'\n                    break\n            if disk_type != '':\n                break\n        return disk_type\n        \n    def get_controllers(self):\n        cmd = 'sas3ircu list | awk \\'{print $1}\\''\n        list = do_shell(cmd).splitlines()\n        index = list.index('Index') + 2\n        controllers = []\n        for i in range(index, len(list) - 1):\n            controllers.append(list[i])\n        return controllers\n\n```\n\n## 调用方式\n\n```python\nfrom mcs3.raidcardutils import RaidCardToolFactory\n\ntool = RaidCardToolFactory().getTool()\ndisk_type = tool.get_disk_type(disk_name)\n```\n\n## 总结\n\n其实这其中的关键就是先找到每一块物理盘的唯一标识，然后我们根据工具获取列表中的唯一标识字段，获取逻辑盘对应的信息，就比如上面的`Device Id`，对应的是逻辑盘的`target id`。\n\n> 完整代码地址：https://github.com/tony-yin/RaidCardTool/\n> 如果有所帮助的话，帮忙`star`一下哦 ^_^\n\n\n[1]: http://www.cnblogs.com/weikunzz/p/6707395.html\n[2]: http://cdn.tony-yin.site/raid-600x450.png\n\n","tags":["Raidcard"],"categories":["tech"]},{"title":"Daily Article Vol 1 - (2017/11/1 ~ 2017/12/31)","url":"/2018/01/01/Daily-Article-2017/","content":"\n<center>![Daily Article 2][62]</center>\n\n书籍可以系统的学习一些知识，并且需要比较长的时间集中注意力学习。而现在网络越来越发达，各种社区的流行，还有开源分享精神的传播，导致现在互联网上很多优秀文章、博客、微信公众号等出现，这些文章贴近热点，往往都很新，并且篇幅有长有短，我们可以利用一些碎片时间来吸收这些知识。优秀文章很多，所以每天读个一两篇文章可以作为一个习惯养成，这样日积月累相信会获益良多。\n\n`Daily Article`系列就是为了记录我每天的阅读历程，以月为单位，每个月出一篇大的总结，一是为了约束自己每天按时按量阅读，也给自己打气，二是给自己每个月的阅读内容做一个总结，用于回头阅读，三是将其中有质量的内容分享给有需要的人。由于我是从`2017`九月才可以记录，所以趁着年底索性把十一和十二月的记录一次性发出来。\n\n目前我采取星星的方式给文章评级，最高三颗星，最低没有星星，一般我放上来的都不会是太水的文章，所以如果是一般的工具类或者没有很大特色的文章我不会进行标记。一颗星表示`good`，即这篇文章有特色，对自己有帮助；两颗星表示`very\ngood`，说明这篇文章内容很好，有深度有广度，是一篇很有质量的文章；三颗星表示`excellent`，说明这篇文章不仅内容技术讲的很到位，文章文笔也很出色，实践结合理论，让人很容易理解，看完后收获很大或者是顿悟，总而言之是一片很优秀的文章。当然我还会在这三个等级中结合半个星星进行调节，反正就是对文章的一个个人看法而已，仁者见仁，智者见智吧。\n\n<!--more-->\n\n## 十一月\n\n1. [大话 Ceph -- CephX 那点事儿][1](11/15-11/17) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n2. [互联网架构为什么要做服务化？][2](11/18) <i class='fa fa-star'></i>\n3. [基于开源软件构建高性能集群NAS系统][3](11/20)\n4. [虚拟IP原理][4](11/21)\n5. [究竟为什么要引入数据库中间件][5](11/27) <i class='fa fa-star'></i><i class='fa fa-star-half-full'></i>\n6. [程序员如何用技术变现（上）][6](11/28)\n7. [Go语言，Docker和新技术][8](11/28)\n8. [秒杀系统架构优化思路][7](11/29) <i class='fa fa-star'></i>\n\n## 十二月\n\n1. [Ceph开发每周谈首发][8](12/1)\n1. [Ceph开发每周谈Vol2][9](12/4)\n1. [Ceph开发每周谈Vol3][9](12/5)\n1. [Ceph开发每周谈Vol4][9](12/5)\n1. [Ext4 Filesystem][13](12/11)\n1. [Formatted 'Ext4' External Hard Disk is Busy][14](12/12)\n1. [Ext4lazyinit detail][15](12/12)\n1. [Ceph开发每周谈Vol5][16](12/13)\n1. [Ceph开发每周谈Vol6][17](12/13)\n1. [Ceph开发每周谈Vol7][18](12/13)\n1. [Ceph开发每周谈Vol8—社区加快开发节奏, CDS 变更为 CDM, Firefly 结束版本支持][19](12/13)\n1. [Ceph开发每周谈Vol9—Ceph开发每周谈 Vol 9—加密、压缩、EC 全场景支持][20](12/13)\n1. [Ceph开发每周谈 Vol 10—NFS 已经被 RadosGW 支持|用户态][21](12/13)\n1. [Ceph开发每周谈 Vol 11—RadosGW 支持 KeyStone V3, AWS v4, 多站点多活][22](12/13)\n1. [Ceph开发每周谈 Vol 12—Scrub 增强，Jewel 小结][23](12/13)\n1. [An introduction to Linux filesystems][24](12/15 ~ 12/17) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [linux lsof命令详解][25](12/18)\n1. [linux中的ldd命令简介][26](12/18)\n1. [linux-->ldd命令的介绍][27](12/18)\n1. [【Linux笔记】ldconfig、ldd][28](12/18)\n1. [Linux 的 EXT4 文件系统的历史、特性以及最佳实践][29](12/18) <i class='fa fa-star'></i><i class='fa fa-star'></i> \n1. [理解inode][30](12/18)\n1. [Ceph开发每周谈 Vol 13 — Cache on SPDK / bufferlist][31](12/19)\n1. [Ceph开发每周谈 Vol 14 — LDAP/ BlueStore SMR][32](12/19)\n1. [Ceph开发每周谈 Vol 15—Unix Socket / BlueStore 压缩和 Checksum][33](12/19)\n1. [Everything is a file][34](12/21) <i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [Overview of the Directory Tree][35](12/21)\n1. [没错，它就是存储界的“大胃王”][36](12/22)\n1. [Ceph开发每周谈 Vol 16—Jewel RC Release!][37](12/22)\n1. [Ceph开发每周谈 Vol 17 — ARM Status | RBD 一致性组合 | 内核模块 转至元数据结尾][38](12/22)\n1. [Ceph开发每周谈 Vol 18 — EXT4 废弃论战? | 去重支持][39](12/22)\n1. [Ceph开发每周谈Vol19 | Ceph Next 2016 闭门会议资讯独家大放送][40](12/26)\n1. [Ceph开发每周谈Vol 20 | NVMe Over Fabric/Kernel Multi Queue][41](12/26)\n1. [Ceph开发每周谈Vol 104 | NFS Ganesha VS Kernel Client][42](12/27)\n1. [Ceph开发每周谈Vol 21 | ZetaScale | CMP/WriteSame][43](12/27)\n1. [Ceph开发每周谈Vol 22 | 全球最大Ceph集群到底有多大？][44](12/28)\n1. [Ceph开发每周谈vol23｜BlueStore新动向][45](12/28)\n1. [Ceph开发每周谈 Vol 24｜Jewel 10.2.1 第一个 Bug 修复版本释出][46](12/28)\n1. [Ceph开发每周谈 Vol 25 | Ceph & DPDK 网络插件开源][47](12/29)\n1. [Ceph开发每周谈Vol 27｜主线分支默认启用 AsyncMessenger][48](12/29)\n1. [Ceph开发每周谈 Vol 28 | OSD 心跳 | Jewel RBD 测试][49](12/30)\n1. [Ceph开发每周谈 Vol 29 — RBD Cache 警告: 数据不一致风险][50](12/30)\n1. [Ceph开发每周谈 Vol 30 — ISA-L 和 BlueStore 性能有哪些进展？][51](12/30)\n1. [用GDB调试程序（一）][52](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（二）][53](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（三）][54](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（四）][55](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（五）][56](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（六）][57](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [用GDB调试程序（七）][58](12/31) <i class='fa fa-star'></i><i class='fa fa-star'></i><i class='fa fa-star'></i>\n1. [rados put striper功能的调试][59](12/31) <i class='fa fa-star'></i>\n1. [准备Ceph开发环境][60](12/31) <i class='fa fa-star'></i>\n1. [ceph编译源码、单机搭建调试环境][61](12/31) <i class='fa fa-star'></i>\n\n[1]: http://www.xuxiaopang.com/08/23/easy-ceph-CephX/\n[2]: http://mp.weixin.qq.com/s/UiDd-1zwrqIsk3-KEcAQaA\n[3]: http://blog.csdn.net/liuaigui/article/details/7163482\n[4]: http://blog.csdn.net/whycold/article/details/11898249\n[5]: http://mp.weixin.qq.com/s/8aI9jS0SXJl5NdcM3TPYuQ\n[6]: https://time.geekbang.org/column/article/183\n[7]: https://h5.ele.me/hongbao/#hardware_id=&is_lucky_group=True&lucky_number=10&track_id=&platform=0&sn=29dad164ef30a0c9&theme_id=1745&device_id=\n[8]: https://time.geekbang.org/column/article/294\n[9]: https://www.xsky.com/tec/ceph-weekly-vol-1/\n[10]: https://www.xsky.com/tec/ceph-weekly-vol-2/\n[11]: https://www.xsky.com/tec/ceph-weekly-vol-3/\n[12]: https://www.xsky.com/tec/ceph-weekly-vol-4/\n[13]: https://www.thomas-krenn.com/en/wiki/Ext4_Filesystem\n[14]: https://www.hecticgeek.com/2015/01/ext4-external-hard-disk-busy-at-idle-fix/\n[15]: https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=bfff68738f1cb5c93dab1114634cea02aae9e7ba\n[16]: https://www.xsky.com/tec/ceph-weekly-vol-5/\n[17]: https://www.xsky.com/tec/ceph-weekly-vol-6/\n[18]: https://www.xsky.com/tec/ceph-weekly-vol-7/\n[19]: https://www.xsky.com/tec/ceph-weekly-vol-8/\n[20]: https://www.xsky.com/tec/ceph-weekly-vol-9/\n[21]: https://www.xsky.com/tec/ceph-weekly-vol-10/\n[22]: https://www.xsky.com/tec/ceph-weekly-vol-11/\n[23]: https://www.xsky.com/tec/ceph-weekly-vol-12/\n[24]: http://link.zhihu.com/?target=https%3A//opensource.com/life/16/10/introduction-linux-filesystems\n[25]: http://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316599.html\n[26]: http://blog.csdn.net/stpeace/article/details/47069215\n[27]: http://blog.csdn.net/qq_26819733/article/details/50610129\n[28]: http://www.cnblogs.com/lyongde/p/4190588.html\n[29]: https://zhuanlan.zhihu.com/p/27875337\n[30]: http://www.ruanyifeng.com/blog/2011/12/inode.html\n[31]: https://www.xsky.com/tec/ceph-weekly-vol-13/\n[32]: https://www.xsky.com/tec/ceph-weekly-vol-14/\n[33]: https://www.xsky.com/tec/ceph-weekly-vol-15/\n[34]: https://opensource.com/life/15/9/everything-is-a-file\n[35]: http://www.tldp.org/LDP/sag/html/dev-fs.html\n[36]: https://www.xsky.com/news/20171218/\n[37]: https://www.xsky.com/tec/ceph-weekly-vol-16/\n[38]: https://www.xsky.com/tec/ceph-weekly-vol-17/\n[39]: https://www.xsky.com/tec/ceph-weekly-vol-18/\n[40]: https://www.xsky.com/tec/ceph-weekly-vol-19/\n[41]: https://www.xsky.com/tec/ceph-weekly-vol-20/\n[42]: http://mp.weixin.qq.com/s/h9mz5gWN8tKRz8by2-Sn6w\n[43]: https://www.xsky.com/tec/ceph-weekly-vol-21/\n[44]: https://www.xsky.com/tec/ceph-weekly-vol-22/\n[45]: https://www.xsky.com/tec/ceph-weekly-vol-23/\n[46]: https://www.xsky.com/tec/ceph-weekly-vol-24/\n[47]: https://www.xsky.com/tec/ceph-weekly-vol-24-2/\n[48]: https://www.xsky.com/tec/ceph-weekly-vol-27/\n[49]: https://www.xsky.com/tec/ceph-weekly-vol-28/\n[50]: https://www.xsky.com/tec/ceph-weekly-vol-29/\n[51]: https://www.xsky.com/tec/ceph-weekly-vol-30/\n[52]: http://blog.csdn.net/haoel/article/details/2879\n[53]: http://blog.csdn.net/haoel/article/details/2880\n[54]: http://blog.csdn.net/haoel/article/details/2881\n[55]: http://blog.csdn.net/haoel/article/details/2882\n[56]: http://blog.csdn.net/haoel/article/details/2883\n[57]: http://blog.csdn.net/haoel/article/details/2884\n[58]: http://blog.csdn.net/haoel/article/details/2885\n[59]: http://www.zphj1987.com/2017/04/26/rados-put-strip-debug/\n[60]: https://ivanjobs.github.io/2016/05/11/prepare-ceph-dev-env.html\n[61]: https://my.oschina.net/u/2460844/blog/515353\n[62]: http://cdn.tony-yin.site/2018-1b.png\n","tags":["Daily-Article"],"categories":["read"]},{"title":"译：一切皆文件","url":"/2017/12/21/Everything-is-a-file/","content":"\n<center>![Everything is a file][7]</center>\n\n这里先提一个技巧性的问题:以下哪一个是文件?\n\n* 目录\n* `Shell`脚本\n* `Office`文档\n* 串行端口（`Serial ports`）\n* 内核数据结构\n* 内核调优参数\n* 硬盘驱动器\n* 分区\n* 逻辑卷（`LVM`）\n* 打印机\n* 套接字（`Sockets`）\n\n也许你不会相信，但是对于`Unix`和`Linux`，它们都是文件。这是最令人惊奇的概念之一——这样做使得许多管理任务可以被一些非常简单但功能强大的方法执行，否则这些任务实现起来可能非常困难甚至不可能。\n\n<!--more-->\n\n## 备份主引导记录\n\n举个简单任务的例子，考虑一下为你的硬盘驱动器地主引导记录（`MBR`）做一个备份工作。有时候我需要恢复或重新创建我的`MBR`，尤其是分区表。从头开始重新创建它是非常困难的。但是从保存好的文件中恢复出来这是非常容易的。`Linux`有一个很强大的`GNU`工具 --- `dd`，它可以实现这个和其他很多功能。\n\n`dd`表示`disk dump`的缩写，意为“磁盘转储”，但是我们很多资深管理员一直认为它是`disk destroyer`的缩写，因为如果你不是很小心的话，这个工具会准确无误地执行你告诉它要做的事情，包括将硬盘上或者分区上所有的数据都破坏掉。\n\n以下命令将会备份你的`MBR`，它必须要是`root`用户执行，因为非`root`用户没有访问`/dev`目录下硬盘驱动器[设备文件][1]的权限。`BS`是`Block Size`缩写，表示块大小，`count`表示从源文件读取的块的个数。这个命令将在`/tmp`目录创建一个`myMBR.bak`的文件。这个文件的大小将为`512`字节，包含了`MBR`的内容，包括引导代码和分区表等。\n\n```\ndd if=/dev/sda of=/tmp/myMBR.bak bs=512 count=1\n```\n\n如果`MBR`被损坏了，就需要引导到一个修复盘并执行下面的命令，这个命令本质上就是上面的反向操作。值得注意的是这条命令没有必要指定块大小和块个数这两个参数，因为`dd`命令将会把备份文件简单地拷贝到硬盘的第一个扇区，并且当它执行到源文件末尾后停止。\n\n```\ndd if=/tmp/myMBR.bak of=/dev/sda\n```\n\n## 都是文件系统的一部分\n\n`Linux`计算机上的所有内容都可以作为文件系统空间的文件被访问。这是非常重要的，这使得我们 可以[使用通用的工具访问不同的东西][2]。\n\n`dd`命令可用于将硬盘的整个分区拷贝到一个文件或者如下所示的其他硬盘。在这里`dd`命令再次将数据拷贝到输入设备的末尾并停止。请确保输出设备的容量要大于输入设备。\n\n```\ndd if=/dev/sdf2 of=/dev/sdg3\n\ndd if=/dev/sda of=/dev/sdg\n```\n\n此外文件系统还有其他工具可以达到此作用。比如，`cat`命令可以用来将任意文件的内容发送到标准输出，这包括分区和整个硬盘。然后，输出还可以被重定向到一个文件。\n\n```\ncat /dev/sda1 > partition1.backup\n```\n\n但是，`cat`命令没有`dd`命令的控制功能。例如，不能指定从源设备或者源文件读取的数据量。\n\n\n下面是一个有趣的实验，它将正面一切皆文件的事实。大多数`Linux`发行版都有多个虚拟控制台，其中`1`到`7`可以用来登录到一个带有`shell`接口的本地控制台会话。可以通过一些组合键访问它们，比如`Ctrl-Alt-F1`是控制台`1`，`Ctrl-Alt-F2`是控制台2，以此类推。\n\n按`Ctrl-Alt-F2`切换到控制台2。在一些发行版中，登录信息包括与此控制台相关的`tty`（`Teletype`）设备，但是也有很多发行版不包括。页面应该显示`tty2`的信息，因为你当前在控制台`2`。\n\n用一个非`root`登录，你可以通过`who am i`这个命令来确定哪一个`tty`设备连接到当前控制台。\n\n在我们实际执行这个实验之前，请看一下`/dev`目录下的`tty2`和`tty3`设备的列表清单。\n\n```\nls -l /dev tty[23]\n```\n\n有大量的定义过的`tty`设备，但是它们其中的大多数我们并不关心，我们只关系`tty2`和`tty3`设备。作为设备文件，它们没有什么特殊之处；它们只是简单的字符类型的设备。我们将用这些设备做这个实验。`tty2`设备连接到虚拟控制台`2`，`tty3`设备连接到虚拟控制台`3`。\n\n按`Ctrl-Alt-F3`组合键切换到控制台`3`，再次以同样的非`root`用户登录。\n\n现在在控制台`3`输入以下命令：\n\n```\necho \"Hello world\" > /dev/tty2\n```\n\n按`Ctrl-Alt-F2`组合键返回控制台`2`。字符串“Hello world”（没有引号）将显示在控制台`2`上。\n\n这个实验也可以在`GUI`桌面的终端模拟器上进行。桌面上的终端会话在`/dev`树中使用伪终端设备，比如`/dev/pts/1`。通过`Konsole`或者`Xterm`开启两个终端会话，确定它们连接到哪个伪终端后，使用其中一个发送消息给另一个。\n\n现在继续试验，使用`cat`命令在不同的终端显示`/etc/fstab`文件。\n\n\n另一个有趣的实验是使用`cat`命令直接将文件打印到打印机上。假设你的打印机设备是`/dev/usb/lp0`，并且你的打印机可以直接打印`PDF`文件，下面的命令将会在你的打印机上打印一个`PDF`文件。\n\n```\ncat test.pdf > /dev/usb/lp0\n```\n\n`dd`命令也可以用来打印一个准备打印的文件。不过，我认为`cat`命令实际上更适合这个任务。\n\n## “一切皆文件”的含义\n\n“一切都是文件”的含义是深远的，远远超过了像这篇文章所列举的那样。你们已经在前面的实验中看到过一些例子，但这里有一个包含这些和更多的简短列表。\n\n1. 克隆硬盘。\n2. 备份分区。\n3. 备份主引导记录(`MBR`)。\n4. 在`u`盘上安装`ISO`镜像。\n5. 与其他终端用户沟通。\n6. 将文件打印到打印机。\n7. 更改`/proc pseudo`文件系统中的某些文件的内容，以修改运行内核的配置参数。\n8. 用随机数据或零覆盖文件、分区或整个硬盘驱动器。\n9. 将不需要的输出重定向到`/dev/null`设备，它将永远不会显示。\n10. 等等，等等，等等。。。\n\n这里有太多的例子，任何一个列表都只是表面的一部分。我相信，你肯定会想出或指出许多比我这里提到更有创造性的方式，来使用`Linux`的这个特性。我很乐意看到你对如何使用“一切都是文件”的评论。\n    \n## 附加信息\n\n有关`/dev/`目录和你可能在那里找到的设备的更多信息，请参阅`Linux Journal`上的[这篇文章][3]。有关单个设备的更详细信息，[Linux文档项目][6]中的[这篇文章][4]和[这篇文章][5]会有所帮助。\n\n> 原文地址：https://opensource.com/life/15/9/everything-is-a-file\n\n[1]: https://en.wikipedia.org/wiki/Device_file\n[2]: http://yarchive.net/comp/linux/everything_is_file.html\n[3]: http://www.linuxjournal.com/article/2597\n[4]: http://www.tldp.org/LDP/sag/html/dev-fs.html\n[5]: http://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/dev.html\n[6]: http://www.tldp.org/\n[7]: http://cdn.tony-yin.site/file-600x450.png\n","tags":["Filesystem"],"categories":["read"]},{"title":"译：Linux 文件系统介绍","url":"/2017/12/17/Linux-Filesystem/","content":"\n<center>![Linux Filesystem][1]</center>\n\n本文旨在对`Linux`文件系统概念进行深层次的讨论。本文既不准备对某个特定类型的文件系统（比如`ext4`）进行基础性的描述，也不打算作为一个讲解文件系统命令的教程。\n\n<!--more-->\n\n每台通用的计算机都需要把各种类型的数据存储在硬盘驱动器(`HDD`)或者一些同样功能的设备上，比如`USB`。存储在这些设备上有几个原因，首先,`RAM`会在计算机电源关闭时丢失内容，虽然也有非易失性类型的内存，可以在电源关闭后维持数据存储不丢失(如`flash`内存也就是闪存使用的`USB`和固态硬盘)，但`flash`内存要比一些标准的、挥发性的内存比如`DDR3`和其他类似的类型昂贵的多。\n\n数据需要存储在硬盘驱动器上的第二个原因是，即使是标准的`RAM`也要比磁盘空间更昂贵。`RAM`和磁盘成本都在迅速下降，但如果按每字节的成本来算的话还是`RAM`更高。我们就基于`16GB RAM`和`2TB`硬盘的成本，快速计算其每个字节的成本，结果显示  `RAM`比硬盘驱动器的价格高约`71`倍。目前，`RAM`的典型成本大约每字节`0.0000000043743750`。\n\n更加直截了当说的话，在计算机的早期，一种内存是基于`CRT`屏幕上的点的，每一比特大约`1`美元，这是非常非常昂贵的!\n\n\n## 定义\n\n你也许会听到人们经常以不同的或者混淆的方式谈论文件系统这个词。这个词本身可能有多重含义，你可能需要从讨论或文档的语境中辨别真正的意思。\n\n我将尝试根据我在不同情况下使用它的方式来定义“文件系统”的各种含义。注意，在试图遵循标准的“official”含义时，我的意图是根据它的各种用法定义术语。在本文后面的小节中，将更详细地讨论这些含义。\n\n1. 整个`Linux`目录结构从顶部(/)根目录开始。\n2. 各种特定类型的数据存储格式，如`EXT3`、`EXT4`、`BTRFS`、`XFS`等。`Linux`支持近`100`种类型的文件系统，包括一些非常古老的文件系统，以及一些最新的文件系统。每个文件系统类型都使用自己的元数据结构来定义如何存储和访问数据。\n3. 一个分区或被格式化为特定类型文件系统的逻辑卷，可以被挂载到`Linux`文件系统上的指定挂载点上。\n\n## 基本的文件系统功能\n\n磁盘存储是必需的，它带来了一些有趣且不可避免的细节。显然，文件系统的设计目的是为数据的非易失性存储提供空间，这是它的最根本的功能。但是它还有许多其他重要的功能满足不同的需求。\n\n所有文件系统都需要提供一个命名空间（`namespace`）——即一个命名和组织的方法。它定义了如何命名文件，具体来说是文件名的长度和可用于文件名的字符的子集，这些字符可以从全部字符集中获取。它还定义了磁盘上数据的逻辑结构，例如使用目录来组织文件，而不是将它们集中在一个单一的、巨大的文件集中。\n\n一旦定义了名称空间，就需要一个元数据结构来为该名称空间提供逻辑基础。其中元数据包括支持分层目录结构所需的数据结构；用于确定磁盘上哪些块空间已经被使用和哪些可用的结构；允许维护文件和目录名称的结构；文件相关的信息，比如它们的大小和时间，比如创建时间、修改时间或最后访问时间等等；以及属于文件的数据在磁盘上面的位置。还有一些其他元数据用于存储关于磁盘划分的高级信息，如逻辑卷和分区。这个更高级别的元数据和它所代表的结构包含了描述存储在驱动器或分区上的文件系统的信息，这些元数据独立于上面提到的一般文件系统元数据。\n\n文件系统还需要`API`接口为系统函数调用提供访问，这些系统函数调用操作文件和目录等文件系统对象。`APIs`提供诸如创建、移动和删除文件之类的接口。它还提供了一些算法来确定文件放置在文件系统上的位置。这些算法还有确定速度或最小化磁盘碎片等作用。\n\n现代文件系统还提供了一个安全模式，它是一个为文件和目录定义访问权限的方案。`Linux`文件系统安全模式有助于确保用户只能访问他们自己的文件，而不是其他人或操作系统本身的文件。\n\n最后的构建块是实现所有这些功能所需的软件。为了改提高系统和程序员效率，`Linux`使用了一种`two-part`的软件实现方式。\n\n<center>![filesystem diagram][2]</center>\n<center>图1：Linux two-part 文件系统软件实现方式</center>\n\n这两部分实现的第一部分是`Linux`虚拟文件系统。这个虚拟文件系统为内核和开发人员提供了访问**所有类型**文件系统的一组命令。虚拟文件系统软件调用特定的设备驱动程序来连接到各种类型的文件系统。文件系统特定的设备驱动程序是实现的第二部分。设备驱动程序将文件系统命令的标准集根据特定分区或逻辑卷上的文件系统类型做转换和解释。\n\n## 目录结构\n\n作为一个通常很有条理的处女座，我喜欢把东西放在那些小而有组织的地方，而不是一个大的桶。使用目录可以帮助我存储和定位我想找的文件。目录也被称为文件夹，因为它们可以被看作实际生活中办公桌上保存文件的文件夹。\n\n在`Linux`和许多其他操作系统中，目录可以以树状的层次结构来构造。`Linux`目录结构在[Linux文件系统层次标准(FHS)][3]中得到了很好的定义和记录。在访问它们时引用这些目录是通过使用由前斜杠(`/`)连接的顺序较深的目录名称(`/`)来实现的，例如`/var/log`和`/var/spool/mail`。这些被称为路径。\n\n下表提供了一个非常简短的标准、众所周知的和定义在顶层上的`Linux`目录及其用途的列表。\n<center>![top level directory][8]</center>\n<center>表1：Linux文件系统层次结构的顶层</center>\n\n表`1`中显示的目录及其子目录及其子目录的子目录，其中背景色为蓝色的目录被认为是根文件系统中不能缺少的组成部分。也就是说，它们不能作为单独的文件系统创建，并且在启动时安装。这是因为它们(特别是它们的内容)必须在引导时出现，以便系统正确引导。\n\n`/media`和`/mnt`目录是根文件系统的一部分，但它们不应该包含任何数据。相反，它们只是临时的挂载点。\n其余的目录，在表`1`中没有背景颜色的目录不需要在引导序列中出现，但是会在以后安装，在启动序列中准备主机来执行有用的工作。\n\n可以通过参考官方的[Linux文件系统层次标准][3]`(FHS)web`页面，以了解这些目录及其许多子目录的详细信息。维基百科对`FHS`的描述也很好。应尽可能密切地遵循这一标准，以确保业务和职能的一致性。不管主机上使用的文件系统类型是什么，这个分层目录结构都是相同的。\n\n## Linux 统一目录结构\n\n在一些非`linux PC`操作系统中，如果有多个物理硬盘或多个分区，每个磁盘或分区都被分配一个驱动器号。想定位到文件或程序所在的硬盘的位置，驱动器号是必需的，比如`C:`或`D:`。然后，以命令的形式发出驱动器字母`D:`例如，要更改到`D:`驱动器，然后使用`cd`命令更改到正确的目录来定位所需的文件。每个硬盘都有自己独立的和完整的目录树。\n\n`Linux`文件系统将所有物理硬盘和分区统一到一个目录结构中。这一切都是从顶部(`/`)目录开始的。所有其他目录及其子目录都位于`Linux`根目录下。这意味着只有一个单独的目录树来搜索文件和程序。\n\n它们之所以能工作,都是因为文件系统,如`/home`、`/tmp`、`/var`、`/opt`、`/usr`可以被创建在单独的物理硬盘上一个不同的分区,或一个不同的逻辑卷`/`(根)文件系统,然后安装在一个挂载点(目录)作为根文件系统树的一部分。即使是可移动的驱动器，如闪存盘或外部`USB`或`ESATA`硬盘驱动器也将安装到根文件系统中，并成为该目录树的一个不可分割的部分。\n\n在从一个版本的`Linux`发行版升级到另一个版本，或者从一个发行版切换成到另一个发行版时，文件系统有一个很好的理由可以做到这一点。一般来说，除了`Fedora`的`dnf`升级之类的升级工具之外，偶尔重新格式化包含操作系统的硬盘驱动器是明智的，因为在升级过程中，硬盘驱动器会清除任何随时间积累的东西。如果`/home`是根文件系统的一部分，它将被重新格式化，然后必须从备份中恢复。通过将`/home`格式化为一个单独的文件系统，那么在根文件系统格式化时它将识别成一个单独的文件系统，并且可以跳过当前步骤。这也适用于数据库、电子邮件收件箱、网站和其他可变用户和系统数据存储的目录`/var`。\n\n维护`Linux`目录树的某些部分作为单独的文件系统还有其他原因。例如，很久以前，当我还没有意识到围绕着所有需要的`Linux`目录都作为`/(root)`文件系统的一部分的潜在问题时，我曾用大量非常大的文件填充了我的主目录。由于`/home`目录和`/tmp`目录都不是独立的文件系统，而只是根文件系统的子目录，所以整个根文件系统都被填满了。操作系统没有空间创建临时文件或扩展现有的数据文件。起初，应用程序开始抱怨没有空间保存文件，然后操作系统本身开始变得非常奇怪。引导到单用户模式，并清除我的主目录中的问题文件，这让我可以重新开始。然后，我使用一个相当标准的多文件系统设置重新安装了`Linux`，并能够防止完全的系统崩溃再次发生。\n\n我曾经还遇到过一个情况，`Linux`主机继续运行，但是阻止用户使用`GUI`桌面登录。我能够使用一个[虚拟控制台][4]本地使用命令行接口(`CLI`)，并远程使用`SSH`。问题是，`/tmp`文件系统已经填满了，而`GUI`桌面所需的一些临时文件在登录时无法创建。由于`CLI`登录不需要在`/tmp`中创建文件，因此缺少空间并没有阻止我使用`CLI`进行登录。在这种情况下，`/tmp`目录是一个单独的文件系统，在卷组中有大量可用的空间，`/tmp`逻辑卷是其中的一部分。我只是将`/tmp`[逻辑卷扩展][5]到一个够大的容量（其实就是`LVM`扩容），以适应我对该主机所需要的临时文件空间数量的新需求，并解决了问题。请注意，此解决方案不需要重新启动，并且当`/tmp`文件系统被放大后，用户可以登录到桌面。\n\n> 逻辑卷扩展也可以参考我之前总结的一篇文章，简洁明了：[LVM动态扩展][6]\n\n另一种情况发生在我在一家大型科技公司做实验室管理员的时候。我们的一个开发人员在错误的位置（`/var`）安装了应用程序（我个人认为不能说是装在错误的位置，只能说装的位置的可用空间不合适）。应用程序崩溃是因为`/var`文件系统已经满了，而存储在`/var/log/`上的日志文件由于缺少空间，不能添加新的消息。但是，由于关键的`/(root)`和`/tmp`文件系统没有填充，系统仍然保持运行。删除违规应用程序并将其重新安装到`/opt`文件系统中解决了这个问题。（其实通过`LVM`动态扩容也是可以解决这个问题，要么扩展空间大小，要么换大空间的文件系统）\n\n## 文件系统类型\n\n`Linux`支持读取大约`100`个分区类型，它只可以在其中的几个而不是所有的分区上创建或写文件。但是，在同一个根文件系统上的不同类型的挂载文件系统是可以做到的，也是非常常见的。在此上下文中，我们讨论的是在硬盘或逻辑卷的分区上存储和管理用户数据所需的结构和元数据。这里提供了`Linux fdisk`命令识别的文件系统分区类型的完整列表，这样您就可以了解`Linux`与许多类型的系统之间的高度兼容性。\n\n```\n0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 <32M 40 Venix 80286 85 Linux extended c7 Syrinx 5 Extended 41 PPC PReP Boot 86 NTFS volume set da Non-FS data 6 FAT16 42 SFS 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux plaintext de Dell Utility 8 AIX 4e QNX4.x 2nd part 8e Linux LVM df BootIt 9 AIX bootable 4f QNX4.x 3rd part 93 Amoeba e1 DOS access a OS/2 Boot Manag 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi ea Rufus alignment e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD eb BeOS fs f W95 Ext'd (LBA) 54 OnTrackDM6 a6 OpenBSD ee GPT 10 OPUS 55 EZ-Drive a7 NeXTSTEP ef EFI (FAT-12/16/ 11 Hidden FAT12 56 Golden Bow a8 Darwin UFS f0 Linux/PA-RISC b 12 Compaq diagnost 5c Priam Edisk a9 NetBSD f1 SpeedStor 14 Hidden FAT16 <3 61 SpeedStor ab Darwin boot f4 SpeedStor 16 Hidden FAT16 63 GNU HURD or Sys af HFS / HFS+ f2 DOS secondary 17 Hidden HPFS/NTF 64 Novell Netware b7 BSDI fs fb VMware VMFS 18 AST SmartSleep 65 Novell Netware b8 BSDI swap fc VMware VMKCORE 1b Hidden W95 FAT3 70 DiskSecure Mult bb Boot Wizard hid fd Linux raid auto 1c Hidden W95 FAT3 75 PC/IX bc Acronis FAT32 L fe LANstep 1e Hidden W95 FAT1 80 Old Minix be Solaris boot ff BBT\n```\n\n拥有支持读取这么多分区类型的能力的主要目的是允许兼容性和与其他计算机系统的文件系统的某些互操作性。使用`Fedora`创建新文件系统时可用的选项如下所示。\n\n* btrfs\n* **cramfs**\n* **ext2**\n* **ext3**\n* **ext4**\n* fat\n* gfs2\n* hfsplus\n* minix\n* **msdos**\n* ntfs\n* reiserfs\n* **vfat**\n* xfs\n\n其他发行版支持创建不同的文件系统类型。例如，`CentOS 6`只支持创建上面列表中粗体显示的文件系统。\n\n## 挂载\n\n在`Linux`中，`to mount`一词指的是早期的计算机中，当一个磁带或可移动的磁盘包需要在适当的驱动器上进行物理安装时。在物理上放置磁盘之后，磁盘包上的文件系统将由操作系统逻辑上挂载，以使操作系统、应用程序和用户能够访问这些内容。\n\n挂载点仅仅是一个目录，就像任何其他的目录一样，它是作为根文件系统的一部分创建的。例如，`home`文件系统安装在目录`/home`上。文件系统可以安装在其他非根文件系统上的挂载点上，但这并不常见。\n`Linux`根文件系统安装在根目录上(`/`)非常早的引导序列。其他文件系统会被安装在后面，由`Linux`启动程序，无论是在`SystemV`下的`rc`，还是在新的`Linux`版本中的`systemd`，在启动过程中挂载文件系统是由`/ etc/fstab`配置文件管理的。一个容易记住的方法是`fstab`表示“文件系统表”，它是要挂载的文件系统的列表，还有它们指定的挂载点，以及特定文件系统可能需要的任何选项。\n\n文件系统安装在现有的目录（挂载点）上，使用`mount`命令。一般来说，任何被用作挂载点的目录都应该是空的，并且没有包含其中的任何其他文件。`Linux`不会阻止用户将一个文件系统安装到一个已经存在的文件系统上，或者在一个包含文件的目录上。如果你在现有的目录或文件系统上安装了一个文件系统，那么原始的内容将会被隐藏，只有新挂载的文件系统的内容才会可见。\n\n## 结论\n\n我希望本文能够把围绕“文件系统”这个术语的一些可能的混淆给理清楚。我花了很长时间才真正理解并理解`Linux`文件系统的复杂性、优雅和功能。\n\n如果你有问题，请把它们加到下面的评论中，我会尽量回答。\n\n## 下个月\n\n另一个重要的概念是，对于`Linux`，一切都是文件。这个概念为用户和系统管理员提供了一些有趣且重要的实际应用程序。我之所以提到这一点，是因为您可能想在我下个月的`/dev`目录下的文章前阅读我的[一切皆文件][7]的文章。\n\n> 原文地址：https://opensource.com/life/16/10/introduction-linux-filesystems\n\n“一切皆文件”这篇文章我也进行了翻译\n\n> 翻译地址：http://www.tony-yin.site/2017/12/21/Everything-is-a-file\n\n[1]: http://cdn.tony-yin.site/filesystem-600x450.png \n[2]: http://cdn.tony-yin.site/filesystem_diagram.png\n[3]: http://www.pathname.com/fhs/\n[4]: https://en.wikipedia.org/wiki/Virtual_console\n[5]: https://opensource.com/business/16/9/linux-users-guide-lvm\n[6]: http://www.tony-yin.site/2017/11/14/LVM-Space-Expansion/\n[7]: https://opensource.com/life/15/9/everything-is-a-file\n[8]: http://cdn.tony-yin.site/linux-filesystaem.png\n","tags":["Filesystem"],"categories":["read"]},{"title":"阅读感悟：《白夜行》","url":"/2017/12/10/Into-Withe-Night/","content":"\n<center>![Into White Night][1]</center>\n\n阅读周期：2017/12/08 ~ 2017/12/10\n阅读评分：4.9\n阅读人群：还是成年的人看着好一点，不大适合未成年人\n\n> 我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借着这份光，我便能把黑夜当成白天。我从来就没有太阳，所以不怕失去。\n\n<!--more-->\n\n之前一直在看技术书，这周末想稍微放松一下自己，翻了翻买了许久却没阅读的《白夜行》，之前听身边很多人称赞过这本书，果然看了就没停的下来，花了两天直接拉满读完。。。\n\n坊间传言这是东野圭吾写的最好的一本书，我只看过《嫌疑人x的献身》，对比那个的话我觉得这一本更加细腻和深远。\n\n看完了这本书，我去看了很多的书评，各有各的见解，并且都很有道理，我想这就是东野圭吾的厉害支持了，真正厉害的推理小说，不是作者在推理，而是最后是读者在推理。\n\n对我个人而言，这本书最大的亮眼之处，就是作者重来不主动把某件事情说明，而是留下种种线索，相关关联，埋下伏笔，这样就导致永远没有一个确定性的答案，作者在意的也许不是具体的场景和手段，他着重刻画的是人性的本质、作恶的动机和灵魂的交融。\n\n回过头一看，全书亮司与雪穗的生活宛如两条平行线，男女主角之间竟然没有任何对白，也没有交待任何他们见面的情节。书最后亮司突然死了，故事也很突然就这样结束了。让人反应不过来，因为她的太阳失去了。亮司承担了所有的罪恶，为了想让她生活在阳光下。\n\n很多人都在讨论桐源和雪穗之间的关系，大多数人都认为他们是深爱着的。我个人觉得他们或许有爱，但是更多的是相互依靠和相互依附，也就是双方都是彼此的灵魂，活下去的羁绊，我觉得这是和单单的爱情是不一样的，否则他们大可以在一起，他们有的是钱，完全可以隐性瞒名的私奔。\n\n我觉得桐源应该更爱雪穗一些，比如一直在他身边守护着她，最后关头为了保住她选择自杀，还有就是只有和她ML的时候才不会迟泄（这个在夕子那个宾馆的章节可以看出），但是桐源尊重雪穗，他只是一直在她身边守护着，永远不会过分打扰她的生活，即使她和别的男人恋爱，即使结婚，即使离婚再结婚，在他心里，雪穗永远是最重要的，满足她想要的和想做的是他一生的追求。桐源一直是作恶的执行者，雪穗背后的指使者，很多读者都为桐源不值，觉得他为了雪穗付出了那么多，最后得到确是冷漠的不回头，其实他并不后悔，看到《嫌疑人x的献身》的同学都知道最后时刻男主多希望女主也能像这边的女主雪穗一样头也不回地离开，这样他的付出才是值得的。桐源对典子应该也是有感情的，从他愿意带她去大阪，跟她讲童年时光，不过这种感情是建立在先利用之后感情培养出来的，但是在桐源心中雪穗永远是最重要的，即使典子获取桐源所有的爱情，当要做出选择的时候，我相信桐源还是选择雪穗吧，因为我觉得在他心中爱情远没有他和雪穗之间的羁绊重要，他们之间的关系关乎到灵魂和生死，他们是对方相互的太阳。\n\n> 曾经拥有的东西被夺走，并不代表就会回到原来没有那种东西的时候。                    ---典子\n\n而雪穗难道对桐源没感情吗？我想肯定有，但是他们俩成为互相羁绊的时间太小了，才十一二岁，在太早的时间双方互相守护秘密，更像是相互保护，少了爱情的起源，所以她们也不知道双方之间是不是爱，雪穗说过她不知道如何去爱一个人，这个人说的可能是高工城，可能是一成，也有可能是桐源，桐源和雪穗经常就是在实行某个行动需要对方配合帮忙的时候才会聚头，比如夕子的那件事，雪穗肯定是通过手或者嘴帮助桐源完成了SJ，但是雪穗从来没有为高工城做过这样的事情，桐源在雪穗的心目中的位置可见一斑。但是也许她自己都不确定这是不是爱情，就像高工城内心的疑问一样，总感觉这份感情里面掺杂了很多其他东西。而雪穗对一诚或许在感情上面和别的人有一些不同，但是这也仅仅是有一点点不同，除了感情我觉得还有一个原因，雪穗不喜欢被别人无视，当大学社团一诚喜欢江利子忽略雪穗时，这让她感觉到嫉妒了，她一直认为自己是完美的，是所有人的焦点所在。\n\n很多人都在愤怒雪穗最后的冷漠，上面我也说了，除了桐源希望用自己的生命保全雪穗之外，雪穗的行为是头也不回的离去，要知道雪穗在此之前无论是生母、养母、前夫等等，都表现出自己的伤心，唯独这次雪穗没有表现正常，也算是她的失误了，我想她不仅因为是失去了她的太阳觉得无助，最重要的是他也不敢面对这个现实，她面对谁都可以表演的很好，但是除了桐源，她做不到，也许她过不了多久也会选择离开这个世界，要么就永远在白夜中度过。。。\n\n店已打烊女主确上楼而不是离开，是否会意味着要与亮一同结束在梦魇开始的地方？\n\n\n[1]: http://cdn.tony-yin.site/white_night.jpg\n","tags":["Read"],"categories":["read"]},{"title":"故障修复：文件系统导致OSD启动失败","url":"/2017/12/08/Filesystem-Bug-Fix/","content":"\n<center>![Bug fix][1]</center>\n\n记一次因为文件系统导致`OSD`无法启动的故障修复。\n\n<!--more-->\n\n## 集群状况：\n\n`Cluster`：\n\n```\nroot@gigabyte:~# ceph -s\n    cluster de3627bb-c748-4623-8cb1-b88c646ff5d5\n     health HEALTH_WARN 42 pgs degraded; 1 pgs down; 1 pgs peering; 17 pgs recovering; 25 pgs recovery_wait; 359 pgs stale; 42 pgs stuck degraded; 1 pgs stuck inactive; 359 pgs stuck stale; 244 pgs stuck unclean; recovery 6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%); 2/23 in osds are down\n     monmap e1: 1 mons at {erxdl=10.16.180.28:6789/0}, election epoch 2, quorum 0 erxdl\n     mdsmap e654: 1/1/1 up {0=irlhy=up:active}\n     osdmap e218: 23 osds: 21 up, 23 in\n      pgmap v8943: 7936 pgs, 16 pools, 639 GB data, 159 kobjects\n            105 GB used, 76827 GB / 76933 GB avail\n            6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%)\n                 359 stale+active+clean\n                 201 active+remapped\n                7333 active+clean\n                  25 active+recovery_wait+degraded\n                  17 active+recovering+degraded\n                   1 down+peering\n```\n\n`OSD`：\n\n```\nroot@test:/data# ceph osd tree\n# id\tweight\ttype name\tup/down\treweight\n-3\t82.29\tpool 4T\n-4\t82.29\t\thost test1\n0\t0\t\t\tosd.0\tup\t    1\t\n8\t3.578\t\t\tosd.8\tup\t    1\t\n17\t3.578\t\t\tosd.17\tup\t    1\t\n5\t3.578\t\t\tosd.5\tup\t    1\t    \n...\n...\n16\t3.578\t\t\tosd.16\tup\t    1\t\n10\t3.578\t\t\tosd.10\tup\t    1\t\n15\t3.578\t\t\tosd.15\tup\t    1\t\n13\t0\t\t\tosd.13\tdown\t1\t\n```\n\n由上可知：`osd.0`和`osd.13`已经被集群剔除，并且权重变为了`0`\n\n## 报错日志：\n\n### 报错1：\n\n```\n2017-12-06 10:18:05.180802 7f4809ae07c0 -1 osd.13 234 set_disk_tp_priority(22) Invalid argument: osd_disk_thread_ioprio_class is  but only the following values are allowed: idle, be or rt\n\n2017-12-06 10:10:44.974634 7f4cbcfff700 -1 os/FileStore.cc: In function 'virtual int FileStore::read(coll_t, const ghobject_t&, uint64_t, size_t, ceph::bufferlist&, bool)' thread 7f4cbcfff700\n\n2017-12-06 10:10:44.972299 os/FileStore.cc: 2851: FAILED assert(allow_eio || !m_filestore_fail_eio || got != -5)\n```\n\n### 报错2:\n\n```\n2017-12-06 10:32:56.602065 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is supported and appears to work\n\n2017-12-06 10:32:56.602095 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is disabled via 'filestore fiemap' config option\n \n2017-12-06 10:32:56.620337 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: syncfs(2) syscall fully supported (by glibc and kernel)\n\n2017-12-06 10:32:56.739219 7f576071b7c0  0 filestore(/ceph/osd.13) limited size xattrs\n```\n\n### 报错3：\n\n```\n2017-12-06 10:16:32.442815 7f06cddf2700 -1 journal aio to 1398235136~434176 got (5) Input/output error\n\n2017-12-06 10:16:32.443787 7f06cddf2700 -1 os/FileJournal.cc: In function 'void FileJournal::write_finish_thread_entry()' thread 7f06cddf2700\n\n2017-12-06 10:16:32.442867 os/FileJournal.cc: 1383: FAILED assert(0 == \"unexpected aio error\")\n```\n\n### 报错4：\n\n```\n2017-12-06 00:52:33.814785 7fb24be877c0 -1 filestore(/ceph/osd.13) FileStore::mount: unable to access basedir '/ceph/osd.0': (30) Read-only file system\n\n2017-12-06 00:52:33.814801 7fb24be877c0 -1 osd.13 0 OSD:init: unable to mount object store\n\n2017-12-06 00:52:33.814806 7fb24be877c0 -1 ^[[0;31m ** ERROR: osd init failed: (30) Read-only file system\n```\n\n## 解决方案\n\n首先我们肯定是要尝试把`OSD`起来嘛，所以要做的就是先给`osd`加权重，接着加入集群，最后再启动。\n\n```\nceph osd crush add osd.0 3.578 host=test1\nceph osd in osd.0\nservice ceph start osd.0\n\nceph osd crush add osd.13 3.578 host=test1\nceph osd in osd.13\nservice ceph start osd.13\n```\n\n这时候发现`OSD`还是起不来，我们就去看`osd.0`和`osd.13`的`log`，也就会发现以上一系列的报错日志，错误较多；\n\n`filesystem`出现较多`limited size xattrs`这一行引起了我的注意，由于此环境`osd`用的文件系统是`ext4`，而`ext4`对存储`xattr`的大小有限制，使得OSD信息不能安全的保存。\n\n所以在`ceph`中如果`osd`采用`ext4`文件系统时，需要在配置项里面加入相关配置实现用`omap`来存储`xattr`，而`xfs`文件系统由于对`xattr`的存储是足够的，所以不存在这个问题。\n\n所以解决这个问题有三个方案：\n\n### 方案1\n\n更改文件系统，将`ext4`改成`xfs`\n\n### 方案2 \n\n文件系统还是采用`ext4`，配置让`Ceph filestore`中的`omap`存储`xattr`，在`/etc/ceph/ceph.conf`中`global section`或`osd section`中插入一行以下配置：\n\n```\nfilestore xattr use omap = true\n```\n\n### 方案3 \n\n限制对象的长度大小，同样是修改`ceph.conf`，在`global section`或者`osd section`中加入以下配置：\n\n```\nosd max object name len = 256 \nosd max object namespace len = 64 \n```\n\n然后再次重启以下对应的`osd`服务就OK了！\n\n\n## 总结\n\n在`Ceph`中如果用`ext4`文件系统的话，一定要注意配置将`xattr`存在`omap`中。\n\n[1]: http://cdn.tony-yin.site/bugfix.png\n","tags":["Bugfix"],"categories":["tech"]},{"title":"通过 Keepalived 实现 Ceph RBD 的高可用","url":"/2017/12/07/RBD-HA/","content":"\n<center>![HA][5]</center>\n\n由于`Cephfs`很不稳定，并且性能差，很难达到用户在性能上的需求，所以`Cephfs`也难以应用于生产环境之上。而`RBD`可以说是一直非常稳定的存储接口方案，用户可以将`image`挂载到客户端进行访问读写，然而很多用户不愿意在本地安装`Ceph`客户端，所以我们常常需要自己封装一层，给客户端暴露出一个通用的接口进行访问，现在一般都是默认用`NFS`，所以本文就`Ceph RBD`如何实现高可用暴露`NFS`给客户端访问进行分享。\n\n<!--more-->\n\n## 环境：\n\n> `Linux Distribution` : `ubuntu`\n> `Ceph` : `Giant`\n> `Keepalived` : `v1.2.2`\n> 集群信息 ：三节点，`IP`分别为`192.168.1.111`，`192.168.1.112`，`192.168.1.113`\n\n## Keepalived 简介\n\n建议先简单了解一些`keepalived`的机制再看下面的内容~\n\n> `Keepalived`的作用是检测集群中服务器的状态，如果有一台服务器死机，或工作出现故障，`Keepalived`将检测到，并将有故障的服务器从集群中剔除，当服务器工作正常后`Keepalived`自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。\n\n下面从网上找了几张图片，方便大家理解一下其原理和机制：\n\n### Keepalived 内部结构：\n\n<center>![keepalive layer][4]</center>\n\n### 双机热备：\n\n<center>![keepalive two host master and backup][3]</center>\n\n### 负载均衡、应用分层：\n\n<center>![keepalive layer][2]</center>\n\n### 配置文件解析\n\n已下摘录自：http://blog.csdn.net/love_is_simple/article/details/47903527\n\n```\nglobal_defs {\n   notification_email {\n     sai@localhost # 报警邮件接收人的地址\n   }\n   notification_email_from root@localhost # 发送报警邮件发件人地址\n   smtp_server 127.0.0.1# 发送邮件的服务器地址\n   smtp_connect_timeout 30# 邮件超时时间(可以根据自己的需求进行设定)\n   router_id LVS_DEVEL# 一个实例的标识地址(可以有多个实例但不能相同)\n}\nvrrp_script monitor_nginx {\n  script “/root/scripts/monitor_nginx.sh”#根据自己的实际路径放置脚本文件\n  interval 1# 脚本执行间隔\nWeight -5#脚本结果导致的优先级变更:5表示优先级加5；-5表示优先级减5\n}\nvrrp_instance VI_1 {# 虚拟路由器自己的名字\n    state MASTER# 设置服务器模式，当前为主节点,master端\n    interface eth0# 实例网卡,也就是提供服务的网卡，来发送vrrp通告\n    virtual_router_id 51# 设置vrid,这里非常重要,相同的vrid为一个组,他决定,它将决定多播的MAC地址.（建议不要使用默认地址,以免发生冲突）\n    priority 100#  设置本节点的优先级,优先级高的为master\n    advert_int 1# 检查间隔,默认为1秒\n    authentication {\n        auth_type PASS# 认证方式,可以是pass或者AH两种认证方式\n        auth_pass 1111# 认证密码\n    }\n    virtual_ipaddress {# 设置vip,虚拟ip地址(实现高可用,转移的vip地址)\n        10.0.1.230# 此地址并不存在,当成为主节点时,此ip地址将会自动生成\n}\nscript_track {\nmonitor_nginx  #跟踪这个monitor_nginx脚本;就是不断去检查这个脚本\n}\n}\n```\n## 原理分析\n\n### RBD 导出 NFS\n\n首先我们要实现`RBD`导出`NFS`功能，毕竟只有先能让客户端通过`NFS`访问后端存储，然后才有必要谈后端存储集群的高可用方案。\n\n我们需要在`Ceph Server`集群创建`RBD image`；然后在三个节点上都建立`RBD Map`关系，最终只会有一个块设备被`mount`，其余两个既用于占位（防止多`image`的情况下造成节点间块设备名称不一致），又是为了作为备机在主机发生故障时转换角色，挂载当前节点的块设备。\n\n接着在三个节点上分别在指定目录下创建目录，本文是在`/vol/`目录下创建目录，比如创建目录`/vol/ec`，这个目录就是块设备对应的挂载目录。\n\n如果有童鞋对`rbd`导出`nfs`过程有兴趣的话，具体请参考：[使用NFS挂载RBD][1]。\n\n### Keepalived 实现 HA\n\n我们后端存储集群最终只会暴露出一个接口或者说是一个`IP`，`keepalived`中有`VIP`这种配置可以支持，所以我们需要在三个节点上配置`keepalived.conf`文件，然后启动`keepalived`所有节点会选举一个`master`节点并暴露虚拟`IP`。\n\n然后我们在`master`节点上将块设备挂载到之前创建的目录`/vol/ec`，同步信息至`/ect/exports`，可以通过`showmount -e vip`可以发现`/vol/ec`已经暴露到了`vip`上，客户端便可以通过`NFS`将上一步创建的目录`/vol/ec`挂载到本地目录，比如`/client_ec`；\n\n这时候客户端已经可以通过虚拟`IP`对`RBD image`进行读写了，但是如果这时候`master`节点`down`了咋办呢？\n\n为了防止集群中主节点不能给`client`提供访问，我们需要实现高可用，也就是当主节点`down`了后，集群自动切换主机，并且针对`RBD`做自动相应挂载操作，让用户无感知访问存储后端。\n\n我们需要配置`keepalived.conf`，当节点角色转为`backup`时，触发停止`NFS`并卸载暴露目录等操作；当节点角色转为`master`时，触发挂载`RBD image`并启动`NFS`等操作；定时检查当前`NFS`，一旦`NFS`服务停止了，尝试重启，如果重启失败，停止`keepalived`服务触发节点角色切换等等。\n\n这些操作对用户来说是无感知的，我们还可以针对`keepalived`做相关邮件配置提醒服务器发生故障等等。\n\n## 具体步骤\n\n### 准备环境\n\n#### 搭建 Ceph 集群\n\n这个就不多说了，基本操作。\n\n#### 安装 Keepalived\n\n本位基于`ubuntu`，`redhat`派可以转换成对应的命令再操作\n\n```\napt-get install libpopt-dev     // 安装依赖\napt-get install keepalived\n```\n\n### RBD 导出\n\n#### 创建 image\n\n这里默认在`test1 pool`中创建`1G`的`image`，请根据自己的场景转换大小，生产环境一般都要几十`T`，甚至上百`T`\n\n```\nrbd create --size 1024 -p test1 image_ec\n```\n\n#### 建立 map 关系\n\n```\nrbd map -p test1 image_ec\n# 输出块设备名称： /dev/rbd0\n```\n\n#### 为块设备格式化文件系统\n\n```\nmkfs.ext4 -i 400000 /dev/rbd0\n```\n\n#### 将块设备挂载到本地目录\n\n```\nmount -o noatime,user_xattr /dev/rbd0 /vol/ec\n```\n\n### 配置 Keepalived\n\n#### 配置 VIP\n\n`VIP`必须是当前集群不存在的`ip`，通过将配置个节点上`keepalived.conf`，为`virtual_ipaddress`选项添加`IP`，我这边用的是`192.168.1.13/24`\n\n```\nvirtual_ipaddress {\n    192.168.1.13/24\n}\n```\n\n#### 配置优先级和网卡\n\n三个节点的角色都配置为`BACKUP`，并且配置`nopreempt`，这样就可以实现不抢占模式，当主节点`down`恢复后不会抢占成为主节点，对我而言哪个是主节点并不重要，频繁切换反而会造成客户端延时。我这边的对外网卡是`eth0`，`priority`是真正决定一开始初始化选举`master`的因素，最大值的节点是`master`节点，一旦切换角色，这个值并不会改变。\n\n```\nvrrp_instance VI_1 {\n    state BACKUP\n    interface eth0\n    priority 100\n    nopreempt\n}\n```\n\n#### 配置角色切换后触发相关操作\n\n当主机`down`之后，如果没有关机，角色转换为`backup`后需要做卸载相关操作；而之前的备机如今成为了主机，也要做挂载等相关操作，这些需求我们可以通过配置`keepalived`，当角色转换时触发相关脚本，这里的配置就表示当节点角色切换为了`master`时则需要执行`/etc/keepalived/ChangeToMaster.sh`，角色切换为`backup`则会执行`/etc/keepalived/ChangeToBackup.sh`：\n\n```\nvrrp_instance VI_1 {\n    notify_master \"/etc/keepalived/ChangeToMaster.sh\" \n    notify_backup \"/etc/keepalived/ChangeToBackup.sh\"\n}\n```\n\n#### 配置定时检测 NFS 状态\n\n如果一旦`NFS`服务断了，我们不及时处理的话，客户端就可以明显地感知到无法读写了。所以我们需要定时不断检测`NFS`的状态，这个也可以通过配置`track_script`选项执行某个脚本并指定间隔时间：\n\n```\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"       # 调用脚本\n    interval 2      # 设置间隔时间为 2s\n}\nvrrp_instance VI_1 {\n    track_script {\n        chk_nfs     # 调用上面的chk_nfs函数\n    }\n}\n```\n\n#### 整个配置文件内容\n\n暂时还是比较精简的，邮件什么的都没配置，`keepalived`还是可以做很多事情的，有兴趣的童鞋可以深入研究\n\n```\nglobal_defs {\n    notification_email {\n    }\n\n    router_id NFS_HA_222\n}\n\nvrrp_script chk_nfs {\n    script \"/etc/keepalived/check_nfs.sh\"\n    interval 2\n}\n\nvrrp_instance VI_1 {\n    #state MASTER\n    state BACKUP\n    interface eth0\n    priority 100\n    virtual_router_id 100\n    advert_int 1\n    authentication {\n        auth_type PASS\n        auth_pass 1111\n    }\n    track_script {\n        chk_nfs\n    }\n    nopreempt\n    notify_master \"/etc/keepalived/ChangeToMaster.sh\"\n    notify_backup \"/etc/keepalived/ChangeToBackup.sh\"\n    virtual_ipaddress {\n        192.168.1.13/24\n    }\n}\n```\n\n#### 相关脚本\n\n> 大家可以手动从下面复制，也可以去我的`github`上面获取，欢迎点赞！\n\n>地址： https://github.com/tony-yin/ceph_scripts#keepalived\n\n这些脚本都是针对我当前环境的，需要针对自己的环境和需求进行相应更改\n\n`ChangeToBackup.sh`：\n\n```\n#!/bin/bash\n\nservice nfs-kernel-server stop\nfor folder in $(ls /vol)\ndo\n    if $(mount | grep $folder -q); then\n        umount -f /vol/$folder\n    fi\ndone\n```\n\n`ChangeToMaster.sh`：\n\n```sh\n#!/bin/bash\nfor folder in $(ls /vol)\ndo\n    if $(mount | grep $folder -q); then\n        umount /vol/$folder > /dev/null\n    fi\n    device=$(grep $folder /etc/block_map -w | awk '{print $1}')\n    mount $device /vol/$folder\ndone\nservice nfs-kernel-server start\n```\n\n`check_nfs.sh`：\n\n```sh\n#!/bin/sh\nvip=$(grep -A 1 virtual_ipaddress /etc/keepalived/keepalived.conf | grep -v virtual_ipaddress | tr -d [:blank:] | cut -d '/' -f 1)\nif ! /sbin/ip addr | grep -q $vip; then\n    exit\nfi\n\n# check nfs service\n/sbin/service nfs-kernel-server status >/dev/null\nif [ $? -ne 0 ]; then\n    # abnormal, try to restart the nfs service\n    /sbin/service nfs-kernel-server restart\n    /sbin/service nfs-kernel-server status >/dev/null\n    if [ $? -ne 0 ]; then\n        # still abnormal\n        for folder in $(ls /vol)\n        do\n            if $(mount | grep $folder -q); then\n                umount -f /vol/$folder\n            fi\n        done\n        # stop keepalived service\n        /sbin/service keepalived stop\n    fi\nfi\n```\n\n配置完后，分别在三个节点上执行`service keepalived restart`重启服务，然后分别在三个节点上执行`ip addr`查看`IP`情况，可以发现`VIP`暴露在了`node2`上，说明我这里`node2`在`keepalived.conf`里面配置`priority`的值是最大的\n\n`node1`：\n\n```\nroot@node1:/etc/keepalived# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n`node2`：\n\n```\nroot@node2:/etc# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n```\n\n`node3`：\n\n```\nroot@node3:~# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n### 客户端通过 NFS 访问 RBD\n\n客户端检查`VIP`对外暴露接口\n\n```\n[root@tony /]# showmount -e 192.168.1.13\nExport list for 192.168.1.13:\n/vol/ec1 *\n```\n\n将`server`端挂载块设备的目录`/vol/ec1`再次挂载到客户端上的`ec1`目录\n\n```\n[root@tony /]# mkdir ec1\n[root@tony /]# mount -o rw,hard,intr -t nfs 192.168.1.13:/vol/ec1 /ec1\n[root@tony /]# cd ec1\n[root@tony ec1]# ls\nlost+found      # 此时是没有数据的\n```\n\n我们可以测试一下读写，先看下读，比如我们在`node2`的`/vol/ec1`目录下写一个文件：\n\n```\nroot@node2:/vol/ec1# ls\nlost+found\nroot@node2:/vol/ec1# echo 'hello' > hello.txt\n```\n\n然后客户端查看`/ec1`目录：\n\n```\n[root@tony ec1]# ls\nhello.txt  lost+found\n[root@tony ec1]# cat hello.txt \nhello\n```\n\n接下来测写，我们可以在客户端写一个文件，然后到服务端查看\n\n```\n[root@tony ec1]# echo 'i am client' > client.txt\n[root@tony ec1]# ls\nclient.txt  hello.txt  lost+found\n```\n\n服务端查看：\n\n```\nroot@node2:/vol/ec1# ls\nclient.txt  hello.txt  lost+found\nroot@node2:/vol/ec1# cat client.txt \ni am client\n```\n\nok，读写正常，目前为止客户端访问后端存储集群一切顺利！\n\n### 测试高可用\n\n分三个测试：\n\n- 手动停止主机`NFS`\n- 手动停止主机`Keepalived`\n- 手动关机主机\n\n#### 手动停止主机 NFS\n\n这个主要是测试`check_nfs.sh`这个脚本是否在实时监控`NFS`状态，可以看到刚`stop`再次查看状态已经是`running`了，本测试通过~\n\n```\nroot@node2:/vol/ec1# service nfs-kernel-server stop\n * Stopping NFS kernel daemon                                                                                                                                                         [ OK ] \n * Unexporting directories for NFS kernel daemon...                                                                                                                                   [ OK ] \nroot@node2:/vol/ec1# service nfs-kernel-server status\nnfsd running\n```\n\n#### 手动停止主机 Keepalived\n\n手动停止主机`node2`的`keepalived`服务，发现`VIP`已经在`node2`上面消失不见\n\n`node2`：\n\n```\nroot@node2:/vol/ec1# service keepalived stop\n * Stopping keepalived keepalived                                                                                                                                                     [ OK ] \nroot@node2:/vol/ec1# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n```\n\n我们可以在`node1`发现了上面消失不见得`VIP`，可知如今角色发生了改变，`node1`已经成为了新的`master`节点\n\n`node1`：\n\n```\n# 出现了 VIP：192.168.1.13/24\nroot@node1:/etc/keepalived# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n# 查看node1的/vol/ec1目录     \nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found\n# 查看mount信息\nroot@node1:/etc/keepalived# mount\n/dev/sda3 on / type ext4 (rw,errors=remount-ro)\n/dev/sdb2 on /data/osd.0 type ext4 (rw,noatime,user_xattr)\nnfsd on /proc/fs/nfsd type nfsd (rw)\n...\n/dev/rbd0 on /vol/ec1 type ext4 (rw)\n```\n\n此时我们可以再次测试一下读写：\n\n```\n# 新的主机读\nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found\n\n# 新的主机写\nroot@node1:/etc/keepalived# echo 'new service 111' > /vol/ec1/new_server.txt\nroot@node1:/etc/keepalived# ls /vol/ec1\nclient.txt  hello.txt  lost+found  new_server.txt\n\n# 客户端读\n[root@tony ec1]# ls\nclient.txt  hello.txt  lost+found  new_server.txt\n[root@tony ec1]# cat new_server.txt\nnew service 111\n\n# 客户端写\n[root@tony ec1]# echo 'hello new server' > hello_new_server.txt\n# 可以看到刚刚客户端写的文件\nroot@node1:/vol/ec1# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt\n```\n\nok，本测试通过~\n\n#### 手动关机主机\n\n关闭主机`node1`，稍等片刻，确定完全关闭再测试\n\n```\nroot@node1:/vol# reboot\n\nBroadcast message from root@node1\n\t(/dev/pts/3) at 9:16 ...\n\nThe system is going down for reboot NOW!\n```\n\n等待完全关闭，我们在`node3`上看到了`VIP`：\n\n```\nroot@node3:~# ip addr\n1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN \n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc mq state UP qlen 1000\n    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff\n    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet 192.168.1.13/24 scope global secondary eth0\n       valid_lft forever preferred_lft forever\n```\n\n客户端读写测试：\n\n```\n# 客户端读\n[root@tony ec1]# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt\n# 客户端写\n[root@tony ec1]# echo 'reboot' >  reboot.txt\n[root@tony ec1]# ls\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt\n```\n\n`node3`:\n\n```\nroot@node3:~# ls /vol/ec1\nclient.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt\nroot@node3:~# cat /vol/ec1/reboot.txt \nreboot\n```\n\nok，本测试通过~\n\n### 测试小结\n\n通过上面三个测试，我们已经基本确保了`keepalived`会保证集群中主机发生异常时还是可以很好地对外提供服务，并且真正地做到了高可用，低延时，高可靠。\n\n\n## QA 环节\n \n因为我实现这个`RBD`高可用是在我们项目中做的，我们项目中`UI`上可以创建共享目录，但是之前都是用的`cephfs`实现的，而我做的就是将`cephfs`方式使用`RBD`替代，大家应该都清楚作为`POSIX`文件接口的`cephfs`内部已经做好了很多事情，它可以将所有节点挂载的目录做到真正的共享，也就是共享目录三个目录都有，改一个其他两个都会随之而改变，而不是像我们`RBD`同时只会针对某一个主机访问。\n\n而`RBD`替换的话必然存在很多困难和问题，在这里我就以`QA`问答的方式分享一下我实现过程中遇到的种种问题和别人提出的需求。\n\n### 问题1\n\n问：如何通过代码实现三个节点都做相关操作的，比如创建`image`和目录等等？\n\n答：我们项目是这样实现的，将前端`UI`的增删改查（比如创建或删除目录等）和后端具体实现共享目录业务分离，也就是说前端只负责做这些信息的增删改查，对应的后端也只是负责这些信息的增删改查，所以用户会即时收到反馈。而真正做事的是在共享业务后端，这个和`UI`对应的后端做事有所区别，这边共享业务后端是做成了一个`Daemon`每隔几秒就会去扫描`UI`后端存储数据是否变化，发生了变化就会做相关操作，比如多了一个文件夹就要创建`image`等，删除一个文件夹也要做一系列的事情。\n\n### 问题2\n\n问：如何判断哪个节点是`master`？\n\n答：这个很简单，就通过`ip addr`命令查找`VIP`就好了，不能通过配置文件中的`priority`来判断，因为即使角色切换，那个值也不会变化的，也就是说即使`priority`是最大值也有可能当前节点不是主节点，这里要注意的是不能仅仅是包含`VIP`，而是要精确匹配才行，比如`VIP`是`192.168.1.12`，如果此时还有个`192.168.1.123`，如果只是字符串包含的话，那这个也会被匹配，所以要精确匹配。\n\n### 问题3\n\n问：创建文件夹后端实现的逻辑是什么样的？\n\n答：后端`Daemon`当扫描存储的目录信息相比于上一次扫描时新增的话，那么后端就会做事情了。首先我们要判断是否为主节点，如果是主节点，那么就创建`image`，然后做`Map`，接着就要`format`文件系统，创建目录，然后再做挂载。这时候要注意其他两个备节点也要做`rbd map`操作，这样做的原因，一是为了占位，比如当创建的项目多了之后，`backup`节点再`map`的时候顺序会乱掉，二是为了当主机`down`，备机转换为`master`后要找同样的块设备挂载，比如都是`/dev/rbd0`\n\n### 问题4\n\n问：删除文件夹后端实现的逻辑是什么样的？\n\n答：扫描当前目录少于上一次扫描的目录，那么就针对这些目录，主节点要先`umount`，再`umap`，然后`rm image`，最后删除目录，对于备节点的话就`umap`，然后再删除目录就好\n\n### 问题5\n\n问：三个节点的`Daemon`可能执行的顺序不一样，不一定是主节点先执行，那么这个时候备节点将无法`map`，同理很有可能在删除`image`的时候，别的节点的`image`都还没`unmap`，这样的话`image`是会删除失败的，这里怎么处理节点间的冲突呢？\n\n答：\n\n首先是创建目录，这时候主节点我们已经做得比较好了，主要担心备节点`map`的时候`image`还没有创建，那么我们这边就要判断一下，如果`image`还不在指定`pool`中，那么就要设置当前目录情况还为上一次的目录信息，这样下一次扫描代码就会又以为有新目录了，那么该段代码就会又执行一次，此时应该成功了，反正只要成功的时候才会把当前目录信息更改为最新的目录信息。\n\n再来说删除目录，这个和创建`map`不同的地方在于，我要删除`image`的时候，我无法知道这个`image`还有没有和其他节点有`map`关系，所以我们只有尝试去删除，这边加一个异常捕获，因为`rm image`报错我们不处理的话会造成代码出错，所以外面包一层异常，这样就可以和上面类似的操作了。这边要注意的是一旦发生异常，我们还必须要在`map`回去，否则我们无法获取`pool`等信息了，因为我们是通过`rbd showmapped`来获取相关信息的。\n\n### 问题6\n\n问：请问`keepalived`如何做自动化的？\n\n答：由于`keepalived`也比较简单，三个配置文件相关配置信息都配一样的，我们要做的就是网卡和`VIP`，网卡的话我们就从项目中获取`public ip`，而`VIP`就是`UI`上面配置的，然后读写文件就好了。\n\n### 问题7\n\n问：一旦节点关机的话，下次开机后块设备就会没了，我们该如何做呢？\n\n答：这个问题的确存在，所以我们要提前将对应关系存在文件中，下次开机的时候根据文件然后对应做`map`工作\n\n### 问题8\n\n问：多个块设备的时候，`keepalived`触发的脚本如何做？\n\n答：其实这个的做法我已经暴露在上面我分享的三个脚本里面了，要做的就是遍历`/vol`目录下的所有目录或者所有`rbd*`，这边要注意的就是`/vol/`或者指定目录下存在的必须只有创建的共享目录。\n\n\n## 总结\n\n本文通过`keepalived`初步实现了`RBD`的高可用，简单地替代了`cephfs`导出`NFS`，针对`cephfs`性能不行的问题，应该有很多小伙伴有这种需求，希望这篇文章能给大家带来一些思路和帮助\n\n之后，我会尝试研究`CTDB`做高可用，因为`keepalived`由于比较简单，所以功能也就比较局限了。然后本文只有`NFS`，后续应该还会有`smb`，`iscsi`等等\n\n通过本文，我对`RBD`和高可用的认识又深入了一些，其实本文涉及到的技术还是比较简单的，还有很多更复杂、更牛逼的高可用方案，这里不禁要说一句，后端还是有意思呀！（相比于前端而言），起码对我来说是这样的，以后会不断精进这些技术，加油！\n\n\n\n[1]: http://www.tony-yin.site/2017/10/31/RBD-Mount-NFS/\n[2]: http://cdn.tony-yin.site/keepalived_layer.png\n[3]: http://cdn.tony-yin.site/keepalived_two_host.png\n[4]: http://cdn.tony-yin.site/keepalived_space.png\n[5]: http://cdn.tony-yin.site/HA.jpg\n","tags":["Keepalived"],"categories":["tech"]},{"title":"Cephx 实战演练","url":"/2017/11/30/Cephx-practice/","content":"\n<center>![cephx][2]</center>\n\n本文就阅读完[徐小胖的大话Cephx][1]后，针对一些猜测和疑惑进行了实战演练，对原文的一些说法和结论进行了验证，并进行了一系列的扩展的思考猜想和总结。最后收获满满，不仅对原文的一些结论进行了验证，也发现了其中的一些问题，更多的是自己动手后一些奇妙的场景和发现。\n\n<!--more-->\n\n本文实战任务和完成情况如下：\n\n- [x] 删除`client.admin.keyring`\n- [x] 修改`cephx`配置\n- [x] 修改`Monitor keyring`\n- [x] 修改`OSD keyring`\n- [x] 修改`client.admin.keyring`，通过`Mon`找回正确的`keyring`\n- [x] `Mon Cap`\n- [x] `OSD Cap`\n- [x] 删除所有`keyring`文件再恢复\n- [x] 删除`ceph.conf`再恢复\n- [ ] 关闭`CephX`后不重启`OSD`\n- [x] 通过`osd.keyring`访问集群\n- [ ] 配置只能访问一个`RBD`的用户权限\n\n## 删除 client.admin.keyring\n\n主节点开始存在`keyring`，可以正常访问集群\n\n```\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n \n  services:\n    mon:     3 daemons, quorum node1,node2,node3\n    mgr:     no daemons active\n    osd:     6 osds: 5 up, 5 in\n    rgw:     1 daemon active\n    rgw-nfs: 1 daemon active\n \n  data:\n    pools:   10 pools, 444 pgs\n    objects: 257 objects, 36140 kB\n    usage:   6256 MB used, 40645 MB / 46901 MB avail\n    pgs:     63.288% pgs not active\n             311/771 objects degraded (40.337%)\n             158 undersized+degraded+peered\n             158 active+undersized+degraded\n             65  down\n             58  incomplete\n             5   active+clean+remapped\n```\n\n将`keyring`文件移动到其他地方，相当于删除了`keyring`，这时访问集群报错\n\n```\n[root@node1 ceph]# mv ceph.client.admin.keyring /tmp/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-mgr.keyring  ceph.bootstrap-osd.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring  rbdmap\n[root@node1 ceph]# ceph -s\n2017-11-23 18:07:48.685028 7f63f6935700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 18:07:48.685094 7f63f6935700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 18:07:48.685098 7f63f6935700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n再拷贝回来又可以访问集群了\n\n```\n[root@node1 ceph]# mv /tmp/ceph.client.admin.keyring ./\n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2, mon.node3\n```\n \n`node3`由于`/etc/ceph/`目录下没有`keyring`文件，所以也无法连接集群\n\n```\n[root@node3 ceph]# ls\nceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# ceph -s\n2017-11-23 17:59:16.659034 7fbe34678700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory\n2017-11-23 17:59:16.659085 7fbe34678700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication\n2017-11-23 17:59:16.659089 7fbe34678700  0 librados: client.admin initialization error (2) No such file or directory\n[errno 2] error connecting to the cluster\n```\n\n**结论：**\n\n> 当`ceph.conf`中的`auth`配置为`cephx`的时候，访问集群是需要秘钥文件的\n\n## 修改 cephx 配置\n\n在`node3`节点上的`/etc/ceph/`目录下操作，首先将`ceph.client.admin.keyring`文件删除，然后将`auth`配置从`cephx`改为`none`，然后先重启`monitor`，再重启`osd`，这时候依然不可以访问集群，因为`cephx`是面向整个集群的，而不是某个节点，接下来需要在其他节点做一样的操作，更改`cephx`为`none`，然后重启`monitor`和`osd`，这时候便可以在没有`keyring`文件的情况下访问集群了。\n\n```\n# 删除keyring文件\n[root@node3 ~]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# mv ceph.client.admin.keyring /tmp/\n# 更改cephx配置\n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# vim ceph.conf \n[root@node3 ceph]# cat ceph.conf \n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = none\nauth_service_required = none\nauth_client_required = none\n\npublic network = 192.168.1.0/24\nmon clock drift allowed = 2\nmon clock drift warn backoff = 30\n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon\nceph-mon@               ceph-mon@node3.service  ceph-mon.target         \n[root@node3 ceph]# systemctl restart ceph-mon.target\n[root@node3 ceph]# systemctl restart ceph-osd.target\n# 更改单个节点配置后依然不可以访问集群\n[root@node3 ceph]# ceph -s\n2017-11-27 23:05:23.022571 7f5200c2f700  0 librados: client.admin authentication error (95) Operation not supported\n[errno 95] error connecting to the cluster\n# 相应的更改其他几个节点并重启，便又可以正常访问集群了\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n**结论：**\n\n> 当`auth`配置为`cephx`的时候访问集群必须要借助秘钥文件，而当`auth`配置为`none`的时候，不再需要秘钥文件就可以访问集群了。（**更改配置需要集群所有节点都做才可以生效，而不是单一节点**）\n\n## 删除monitor秘钥\n\n`/etc/ceph`和`/var/lib//ceph/mon/ceph-node1`各有一个`mon keyring`\n```\n[root@node1 ceph-node1]# cd /etc/ceph/\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring\n[root@node1 ceph]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  keyring  kv_backend  store.db  systemd\n```\n\n先删除`/etc/ceph/ceph-mon.keyring`，还是可以访问集群\n\n```\n[root@node1 ceph]# rm ceph.mon.keyring \nrm: remove regular file ‘ceph.mon.keyring’? y\n[root@node1 ceph]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n            no active mgr\n            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete\n            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized\n            application not enabled on 3 pool(s)\n            clock skew detected on mon.node2\n...\n...\n```\n\n再删除`/var/lib/ceph/mon/ceph-node1/keyring`\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph-node1]# systemctl restart ceph-mon@node1.service \n[root@node1 ceph-node1]# ceph -s\n\n```\n\n访问集群一直`timeount`，查看`log`文件发现`Mon`初始化失败\n\n```\n2017-11-24 00:33:55.812955 7fa16f995e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:33:55.812991 7fa16f995e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:33:55.812999 7fa16f995e40 -1 failed to initialize\n```\n\nok，那我们再试试将`/var/lib/ceph/mon/ceph-node1/keyring`删除，将`etc/ceph/ceph.mon.keyring`拷贝回来，这时候意外发生了，居然`mon`初始化失败\n\n**结论：**\n\n> `Monitor`启动是需要`keyring`文件进行秘钥认证的，并且这个文件必须是`/var/lib/ceph/mon/ceph-node1/`目录下的，`/etc/ceph/`目录下的`ceph.mon.keyring`并不起作用\n\n```\n[root@node1 ceph-node1]# rm keyring \nrm: remove regular file ‘keyring’? y\n[root@node1 ceph]# ls\nceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap\nceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring  \n[root@node1 ceph]# ceph -s\n// timeout\n...\n```\n\n`mon.log`中的现象：\n\n```\n2017-11-24 00:44:26.534865 7ffaf5117e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can't open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory\n2017-11-24 00:44:26.534901 7ffaf5117e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,\n2017-11-24 00:44:26.534916 7ffaf5117e40 -1 failed to initialize\n```\n\n至此，我们可以得出结论`monitor`初始化的时候依赖的文件是`/var/lib/ceph/mon/ceph-node1/keyring`而不是`/etc/ceph/ceph.mon.keyring`\n\n## 修改 Mon keyring\n\n\n### 原始的 keyring\n\n```\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 将中间的五个A替换成了五个C\n\n```\n[root@node1 ceph-node1]# vim keyring \n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n### 重启查看 Mon keyring\n\n理想结果：\n\n```\n[root@node1 ceph-node1]# systemctl restart ceph-mon.target\n[root@node1 ceph-node1]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n令人疑惑的现实：\n\n```\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n[root@node1 ceph]# ceph auth get mon.\nexported keyring for mon.\n[mon.]\n\tkey = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n```\n\n可以看到一会是修改之前的`keyring`，一会是修改之后的`keyring`，那遇到这种问题，我们就通过`log`观察如何获取`keyring`的\n\n`node1`的`mon.log`中日志：\n\n```\n2017-11-24 09:30:08.697047 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:08.697106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1169357136' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:10.020571 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:10.020641 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2455152702' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:11.393391 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:11.393452 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1704778092' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:12.669987 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:12.670049 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/275069695' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:14.113077 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:14.113147 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/3800873459' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:15.742038 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:15.742106 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/1908944728' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:30:17.629681 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:30:17.629729 7f9b73e09700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/2193002591' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n`node2`的`mon.log`中日志：\n\n```\n2017-11-24 09:29:23.799402 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4284881078' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:26.030516 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:26.030588 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4157525590' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n2017-11-24 09:29:38.637677 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command({\"prefix\": \"auth get\", \"entity\": \"mon.\"} v 0) v1\n2017-11-24 09:29:38.637748 7fdb3c0ae700  0 log_channel(audit) log [INF] : from='client.? 192.168.1.58:0/4028820259' entity='client.admin' cmd=[{\"prefix\": \"auth get\", \"entity\": \"mon.\"}]: dispatch\n```\n\n**结论：**\n\n* `Monitor`的秘钥哪怕被修改过了，也不会影响`Monitor`的启动，也就是说`Monitor`启动时只要存在秘钥文件就好，内容忽略并不重要\n* `Monitor`启动的时候读取秘钥文件是随机的，并不一定是当前节点的，具体选择机制需要后期去看源代码了\n\n\n## 修改OSD keyring和修复\n\n`OSD`启动的时候需要秘钥才可以登录集群，这个秘钥会存在`Monitor`的数据库中，所以登录的时候就会拿本地的`keyring`和存在`Monitor`中的`keyring`相匹配，正确的话才可以启动成功。\n\n下面我们将本地的`OSD keyring`故意改错，然后重启`OSD`查看效果\n\n```\n# 更改秘钥文件\n[root@node3 ceph]# cd /var/lib/ceph/osd/ceph-2\n[root@node3 ceph-2]# ls\nactivate.monmap  active  block  bluefs  ceph_fsid  fsid  keyring  kv_backend  magic  mkfs_done  ready  systemd  type  whoami\n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = BBBp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd\nceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     \n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service\n# 重启后发现OSD的状态时down\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2    down  1.00000 1.00000 \n```\n\n查看日志，发现`init`失败，原因是`auth`认证出错\n\n```\n2017-11-27 23:52:18.069207 7fae1e8d2d00 -1 auth: error parsing file /var/lib/ceph/osd/ceph-2/keyring\n2017-11-27 23:52:18.069285 7fae1e8d2d00 -1 auth: failed to load /var/lib/ceph/osd/ceph-2/keyring: (5) Input/output error\n...\n2017-11-27 23:52:41.232803 7f58d15ded00 -1  ** ERROR: osd init failed: (5) Input/output error\n```\n\n我们可以通过查询`Monitor`数据库获取正确的`keyring`，将错误的`keyring`修正过来再重启`OSD`\n\n```\n# 查询Monitor数据库中的osd keyring\n[root@node3 ceph-2]# ceph auth get osd.2\nexported keyring for osd.2\n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n\tcaps mgr = \"allow profile osd\"\n\tcaps mon = \"allow profile osd\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph-2]# vim keyring \n[root@node3 ceph-2]# cat keyring \n[osd.2]\n\tkey = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==\n[root@node3 ceph-2]# systemctl restart ceph-osd@2.service \n# 重启OSD后可以发现osd.2状态已经变为up\n[root@node3 ceph-2]# ceph osd tree | grep osd.2\n 2   hdd 0.00980         osd.2      up  1.00000 1.00000 \n```\n\n**结论：**\n\n> `OSD`启动需要正确的`keyring`，错误的话则无法启动成功，正确的`keyring`会被存在`Monitor`的数据库中\n\n\n## 修改Client keyring和修复\n\n之前我们通过删除`client keyring`验证了当`auth=cephx`的时候，客户端需要`keyring`才可以访问集群，那么它是像`Monitor`一样内容不被`care`还是和`OSD`一样需要精确匹配`keyring`呢？\n\n```\n# 修改ceph.client.admin.keyring\n[root@node3 ceph-2]# cd /etc/ceph/\n[root@node3 ceph]# ls\nceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap\n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = BBBB7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群出错\n[root@node3 ceph]# ceph -s\n2017-11-28 00:06:05.771604 7f3a69ccf700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:06:05.771622 7f3a69ccf700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:06:05.771634 7f3a69ccf700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n```\n\n可以看出访问集群需要正确的`keyring`，这时候如何修复呢？大家应该能够猜到，它和`OSD`的原理是一样的，正确的`keyring`也存在与`Monitor`的数据库\n\n```\n# 直接获取client.admin出错\n[root@node3 ceph]# ceph auth get client.admin\n2017-11-28 00:08:19.159073 7fcabb297700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring\n2017-11-28 00:08:19.159079 7fcabb297700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error\n2017-11-28 00:08:19.159090 7fcabb297700  0 librados: client.admin initialization error (5) Input/output error\n[errno 5] error connecting to the cluster\n# 需要加上monitor的keyring文件才可以获取client.admin.keyring\n[root@node3 ceph]# ceph auth get client.admin --name mon. --keyring /var/lib/ceph/mon/ceph-node3/keyring\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 修正keyring\n[root@node3 ceph]# vim ceph\nceph.client.admin.keyring  ceph.conf                  ceph-deploy-ceph.log       \n[root@node3 ceph]# vim ceph.client.admin.keyring \n[root@node3 ceph]# cat ceph.client.admin.keyring \n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n# 访问集群成功\n[root@node3 ceph]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_WARN\n    ...\n```\n\n出现了令人惊奇的一幕，就是上面通过`ceph auth`获取`OSD`的`keyring`可以正常获取，而获取`client.admin.keyring`却要加上`monitor.keyring`，原因可以从报错信息看出，`ceph auth`需要以客户端连接集群为前提。\n\n结论：\n\n> `Client`访问集群和`OSD`一样，需要正确的`keyring`与存在`Monitor`数据库中对应的`keyring`相匹配，并且当`client.admin.keyring`\n不正确时，通过`ceph auth`读取`keyring`的时候需要加上`monitor keyring`的选项\n\n## Mon Caps\n\n\n### r 权限\n\n`Monior`的`r`权限就是拥有读权限，对应的读权限都有哪些操作？在这里的读权限其实就是拥有读取`Monitor`数据库中信息的权限，`MON`作为集群的状态维护者，其数据库(`/var/lib/ceph/mon/ceph-$hostname/store.db`)内保存着集群这一系列状态图(`Cluster Map`)，这些`Map`包含但不限于：\n\n* `CRUSH Map`\n* `OSD Map`\n* `MON Map`\n* `MDS Map`\n* `PG Map`\n\n所以接下来我们可以创建一个新的只拥有读权限的用户，进行相关操作验证读权限具体拥有哪些权限\n\n```\nceph auth get-or-create client.mon_r mon 'allow r' >> /root/key\n[root@node3 ceph]# ceph auth get client.mon_r\nexported keyring for client.mon_r\n[client.mon_r]\n\tkey = AQABvRxaBS6BBhAAz9uwjYCT4xKavJhobIK3ig==\n\tcaps mon = \"allow r\"\n\t\nceph --name client.mon_r --keyring /root/key -s      // ok\n\nceph --name client.mon_r --keyring /root/key osd crush dump     // ok\nceph --name client.mon_r --keyring /root/key osd getcrushmap -o crushmap.map        // ok\n\nceph --name client.mon_r --keyring /root/key osd dump       // ok\nceph --name client.mon_r --keyring /root/key osd tree       // ok\nceph --name client.mon_r --keyring /root/key osd stat       // ok\n\nceph --name client.mon_r --keyring /root/key pg dump        // ok\nceph --name client.mon_r --keyring /root/key pg stat        // ok\n```\n\n尝试了下两个写操作，都显示报错权限拒绝\n\n```\n[root@node3 ceph]# rados --name client.mon_r --keyring /root/key -p testpool put crush crushmap.map\nerror putting testpool/crush: (1) Operation not permitted\n\n[root@node3 ceph]# ceph --name client.mon_r --keyring /root/key osd out osd.0\nError EACCES: access denied\n```\n\n**注意：**\n\n虽然上面有`osd`和`pg`等信息，但是这些都隶属于`crush map`的范畴中，所以这些状态数据都是从`Monitor`获取的\n\n**结论：**\n\n> `Monitor`的读权限对应的是从`Monitor`数据库获取一系列的`Map`信息，具体的上面也都讲的很详细了，并且该权限只能读取状态信息，不能获取具体数据信息，且不能进行`OSD`等守护进程写操作\n\n### w 权限\n\n`w`权限必须配合`r`权限才会有效果，否则，单独`w`权限执行指令时，是会一直`access denied`的。所以我们在测试`w`权限时，需要附加上`r`权限才行：\n\n```\nceph auth get-or-create client.mon_rw mon 'allow rw' >> /root/key\n```\n\n而`w`权限就可以做一些对组件的非读操作了，比如：\n\n```\n# 踢出OSD\nceph osd out\n# 删除OSD\nceph osd rm \n# 修复PG\nceph pg repair\n# 替换CRUSH\nceph osd setcrushmap\n# 删除MON\nceph mon rm\n...\n# 还有很多操作，就不一一赘述\n```\n\n**结论：**\n\n> `Mon`的`r`权限可以读取集群各个组件的状态，但是不能修改状态，而`w`权限是可以做到的\n\n**注意：**\n\n> 这里的`w`权限能做到的写权限也只是修改组件的状态，但是并不包括对集群对象的读写权限，因为这些组件状态信息是存在`Mon`，而对象信息是存在`OSD`里面的，而这里的`w`权限也只是`Mon`的写权限，所以也很好理解了。\n\n### x 权限\n\n`MON`的`x`权限很局限，因为这个权限仅仅和`auth`相关，比如`ceph auth list`，`ceph auth get` 之类的指令，和`w`权限类似，`x`权限也需要`r`权限组合在一起才能有效力：\n\n```\n# 用上面创建拥有rw权限的用户访问auth list后auth报错\n[root@node3 ~]# ceph --name client.mon_rw --keyring /root/key auth list\n2017-11-28 21:28:10.620537 7f0d15967700  0 librados: client.mon_rw authentication error (22) Invalid argument\nInvalidArgumentError does not take keyword arguments\n# 创建rw权限的用户访问auth list成功\n[root@node3 ~]# ceph --name client.mon_rx --keyring /root/key auth list\ninstalled auth entries:\n\nosd.0\n\tkey: AQDaTgBav2MgDBAALE1GEEfbQN73xh8V7ISvFA==\n\tcaps: [mgr] allow profile osd\n\tcaps: [mon] allow profile osd\n\tcaps: [osd] allow *\n...\n...\n```\n\n这边需要注意的是徐小胖的原文应该是笔误，他是用的`client.mon.rw`访问的，所以说实践可以发现很多光看发现不了的东西\n\n**结论：**\n\n> `x`权限也需要和`r`权限搭配才有效果，该权限只能处理与`auth`相关的操作\n\n### * 权限\n\n这没什么好说的，猜也能猜到了，就是拥有`rwx`所有权限\n\n## OSD Caps\n\n这一章需要研究一波再发出来\n\n## 丢失所有秘钥的再恢复\n\n如果所有秘钥全部删除，是否真的能恢复？所有秘钥包括\n\n* `MON` ： `/var/lib/ceph/mon/ceph-$hostname/keyring`\n* `OSD` ： `/var/lib/ceph/osd/ceph-$hostname/keyring`\n* `Client` ：`/etc/ceph/ceph.client.admin.keyring`\n\n```\n# 删除 mon keyring\n[root@node1 ceph-node1]# mv keyring /root/\n# 删除 ceph.conf\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.conf /root/\n# 删除 client.admin.keyring\n[root@node1 ceph-node1]# mv /etc/ceph/ceph.client.admin.keyring /root\n# 尝试访问集群报错\n[root@node1 ceph-node1]# ceph -s\n2017-11-29 23:57:14.195467 7f25dc4cc700 -1 Errors while parsing config file!\n2017-11-29 23:57:14.195571 7f25dc4cc700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195579 7f25dc4cc700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:14.195580 7f25dc4cc700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n# 尝试获取auth list报错\n[root@node1 ceph-node1]# ceph auth list\n2017-11-29 23:57:27.037435 7f162c5a7700 -1 Errors while parsing config file!\n2017-11-29 23:57:27.037450 7f162c5a7700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037452 7f162c5a7700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory\n2017-11-29 23:57:27.037453 7f162c5a7700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory\nError initializing cluster client: ObjectNotFound('error calling conf_read_file',)\n```\n\nok，下面开始修复：\n\n### 伪造 Mon keyring\n\n在`ceph`中除了`mon.`用户以外的的账户密码都保存在`Mon`的数据库`leveldb`中，但是`mon.` 用户的信息并没有保存在数据库里，而是在`MON`启动时读取`Mon`目录下的`keyring` 文件得到的，这也是我们之前验证后得到的结论。所以，我们可以随便伪造一个`keyring`，放到`Mon` 目录下去。然后同步到各个`Mon`节点，然后重启三个`Mon`。\n\n```\n[root@node1 ceph-node1]# cd /var/lib/ceph/mon/ceph-node1/\n[root@node1 ceph-node1]# ls\ndone  kv_backend  store.db  systemd\n[root@node1 ceph-node1]# vim keyring\n# 伪造 keyring，可以看到里面还有tony的字样，可以看出明显是伪造的\n[root@node1 ceph-node1]# cat keyring \n[mon.]\n\tkey = AQCtonyZAAAAABAAQOysx+Yxbno/2N8W1huZFA==\n\tcaps mon = \"allow *\"\n# 重启 mon\n[root@node1 ceph-node1]# service ceph-mon@node1 restart\nRedirecting to /bin/systemctl restart  ceph-mon@node1.service\n```\n\n可以看到效果：\n\n```\n# monitor log显示mon.node1@0初始化成功，并被选举成了monitor leader\n2017-11-30 00:15:04.042157 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.042299 7f8c4e28a700  1 mon.node1@0(electing).elector(934) init, last seen epoch 934\n2017-11-30 00:15:04.048498 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election\n2017-11-30 00:15:04.048605 7f8c4e28a700  1 mon.node1@0(electing).elector(937) init, last seen epoch 937, mid-election, bumping\n2017-11-30 00:15:04.078454 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1@0 won leader election with quorum 0,1,2\n```\n\n**注意（很重要）：**\n\n> 虽然说`mon`在启动的时候读取对应的`keyring`，不在乎内容的正确性，但是不代表这个`keyring`可以胡乱修改。也就是说这个`keyring`是要**符合某种规范和格式的**，在实践过程我发现`keyring`前三位必须为大写的`AQC`，当然还有其他的格式要求，比如结尾是否必须要是`==`？长度是否是固定的？这个格式要求可能很多，我没有时间一个一个手动无脑验证，这个可以日后查看源码了解实现思路，有兴趣的童鞋可以试试，说不定可以发现很有趣的现象。当然说了这么多是否意味着很难伪造呢？这个我们也不必担心，最好的做法是从别的集群的`Mon keyring`拷贝一份过来就可以了，自己胡乱伪造启动会报错如下：\n\n```\n2017-11-29 23:49:50.134137 7fcab3e23700 -1 cephx: cephx_build_service_ticket_blob failed with error invalid key\n2017-11-29 23:49:50.134140 7fcab3e23700  0 mon.node1@0(probing) e1 ms_get_authorizer failed to build service ticket\n2017-11-29 23:49:50.134393 7fcab3e23700  0 -- 192.168.1.58:6789/0 >> 192.168.1.61:6789/0 conn(0x7fcacd15d800 :-1 s=STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH pgs=0 cs=0 l=0).handle_connect_reply connect got BADAUTHORIZER\n```\n\n\n###　还原 ceph.conf\n\n没有`/etc/ceph/ceph.conf`这个文件，我们是没法执行`ceph`相关指令的，所以我们需要尽可能的还原它。首先`fsid`可以通过去任意`osd`目录（`/var/lib/ceph/osd/ceph-$num/`）读取`ceph-fsid`文件获得，然后`mon_initial_members`和`mon_host`代表着集群每个节点的`hostname`和`ip`，这些都是我们知道的。\n\n```\n# 还原 ceph.conf\n[root@node1 ceph-node1]# cat /var/lib/ceph/osd/ceph-0/ceph_fsid \n99480db2-f92f-481f-b958-c03c261918c6\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.conf\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.conf\n[global]\nfsid = 99480db2-f92f-481f-b958-c03c261918c6\nmon_initial_members = node1, node2, node3\nmon_host = 192.168.1.58,192.168.1.61,192.168.1.62\nauth_cluster_required = cephx\nauth_service_required = cephx\nauth_client_required = cephx\n\npublic network = 192.168.1.0/24\n\n# 通过 mon keyring 访问集群状态成功\n[root@node1 ceph-node1]# ceph -s --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n### 恢复 ceph.client.keyring\n\n有了`Mon keyring`，并且可以执行`ceph`指令，那么我们就可以通过`ceph auth get`去`Monitor leveldb`获取任意`keyring`\n\n```\n# 通过 Mon 获取 client.admin.keyring\n[root@node1 ceph-node1]# ceph --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring auth get client.admin\nexported keyring for client.admin\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n# 创建 /etc/ceph/ceph.client.admin.keyring，并将上面内容更新到该文件\n[root@node1 ceph-node1]# vim /etc/ceph/ceph.client.admin.keyring\n[root@node1 ceph-node1]# cat /etc/ceph/ceph.client.admin.keyring\n[client.admin]\n\tkey = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==\n\tcaps mds = \"allow *\"\n\tcaps mgr = \"allow *\"\n\tcaps mon = \"allow *\"\n\tcaps osd = \"allow *\"\n\n# 用默认 ceph -s 测试一下，发现可以正常访问了\n\n[root@node1 ceph-node1]# ceph -s\n  cluster:\n    id:     99480db2-f92f-481f-b958-c03c261918c6\n    health: HEALTH_OK\n \n  services:\n    mon: 3 daemons, quorum node1,node2,node3\n    mgr: node1_mgr(active)\n    osd: 6 osds: 6 up, 6 in\n```\n\n## 总结\n\n首先感谢徐小胖给我提供了`cephx`方面的思路，希望日后多出好文，我也在不断地拜读这些优质文章。这篇文章花了我很长时间，大家从日志的时间可以看出来，跨度已经有好几天了，很多实践真的不是一蹴而就的，需要反复的尝试和思考才能得到最后的成功。`Ceph`还是要多动手，看别人文章是好事，但是记得要加以实践，否则再好的文章也只是想当然，作者说什么你就跟着他的思路走，你永远不知道别人一句简短的话语和结论的背后花了多少时间去推敲和实践，你看起来一条命令执行成功或者在某一步执行某个命令那也许是别人失败了无数次总结出来的。所以我们要自己实践去验证，除了可以验证原文的观点正确与否，往往可以发现一些其他有用的知识。\n\n经历这次总结，收获满满，我对`cephx`的理解又上了一个层次。本文就`cephx`在不同组件中的角色扮演和依赖关系进行梳理，然后再对各组件的`cap`进行了研究，最后针对各个`keyring`的恢复给出了详细的指南和步骤。然后还剩两项任务没有完成，等有空进行完善！\n\n[1]: http://www.xuxiaopang.com/2017/08/23/easy-ceph-CephX/\n[2]: http://cdn.tony-yin.site/cephx-600x450.jpg\n","tags":["Cephx"],"categories":["tech"]},{"title":"阅读感悟：《Ceph Cookbook》","url":"/2017/11/26/read-ceph-cookbook/","content":"\n<center>![ceph cookbook][1]</center>\n\n阅读周期：2017/10/15 ～ 2017/11/26\n阅读人群：拥有`ceph`基础知识想进阶的同学\n阅读评分：4.8\n\n这本书和之前一本《ceph分布式存储学习指南》是同一个作者：卡兰.辛格。是`ceph`界最先出版的学习书籍了，对他表示崇高的敬意和感谢。这本书绝不是上一本书的复制品，我觉得我的运气很好，先阅读了上一本然后才看的这一本，没错，在我看来这本书是上一本的进阶版，内容更加饱满和细致。很多方面还是讲解的比较全面的，而不是笼统的说那些道理，基本上每个章节都可以让读者进行实战代码演练。这本书让我对`cookbook`系列的书籍有着很好的印象。\n\n<!--more-->\n\n## 第1章 Ceph介绍和其他\n\n第1章没什么好说的，就是传统的介绍和与其他存储做比较。然后第一章还把`ceph`的部署也插入了进来，估计是不想浪费过多的章节讲解这么基础的 东西吧，嗯，很棒。\n\n## 第2章 使用Ceph块存储\n\n哇，我只想说讲的太全面了，第一本讲解的方式永远是三个存储方式放在一起介绍一下，而这本书是每一个存储方式分了一章。从讲解块设备的创建、映射、调整、快照、克隆到和`openstack`的集成，面面俱到，为了阐述`rbd`是`openstack`的最佳匹配，分别就`openstack`的`glance`、`cinder`和`nova`讲解了详细的对接挂载方式，过程是可以自己实操的。\n\n## 第3章 使用Ceph对象存储\n\n这一章除了讲解老一套的`rados`网关配置与`S3`和`Swift`对接之外，还讲了`rados`网关和`openstack keystone`的集成。最大的特色是讲解了`radosgw`多区域网关配置和测试，无限逼近线上大规模生产环境的场景。最后就`RGW`创建文件同步和共享服务进行了介绍，干货多多。\n\n## 第4章 使用Ceph文件系统\n\n借着`cephfs`介绍了`mds`一番，然后讲解了多种访问`cephfs`的方式，然后就是熟悉的将`cephfs`导出为`nfs`，这算是很常见的场景了，但是据我所知目前`ceph`版本的`cephfs`的性能堪忧，以至于用的比较多的还是`rbd`，希望`ceph`可以在之后的版本中将`cephfs`做的越开越好。最后介绍了`ceph-dokan`的`cephfs`的`windows`客户端，很牛逼，还在持续迭代中。还介绍了如何使得`cephfs`替换`hdfs`，这个我之前在`ceph`线下沙龙南京站听那位中兴的工程师讲的`cephfs`和`hadoop`的恋爱史印象非常深刻，讲的蛮好的。\n\n## 第5章 用Calamari监控Ceph集群\n\n先科普了一下各大监控命令和手段，然后介绍了`Calamari`这个工具的编译和搭建，这个工具主要就是一个`dashboard`，并不是那种能在`UI`上操作和管理集群的工具，并且已经很久没再更新了，用于生产环境的请注意，自己玩玩倒是还可以。\n\n## 第6章 操作和管理Ceph集群\n\n先是介绍了两种方式管理各种守护进程和服务。然后讲解了横向扩展和纵向扩展。最后介绍了集群的升级。\n\n## 第7章 深入Ceph\n\n这一章蛮重要的，首先讲解了`crush`的机制和算法，然后介绍了`ceph`的身份验证，也就是`cephx`，最后结合`crush`和`pg`创建了定制化的`osd`和`pool`。\n\n## 第8章 Ceph生产计划和性能优化\n\n这本书在讲解这一部分明显要比上本书要全面的多，分别针对操作系统、`osd`、`filestore`、`cilent`等性能优化给出参数配置建议。然后介绍了纠删码的各种插件，配了结构图很好理解，最后还是缓存分层，做了测试，更能清楚了了解分层的原理。\n\n## 第9章 Ceph虚拟存储管理器（VSM）\n\n这个工具就要比上面提到的`calamari`之类的工具要强得多了，不仅有`dashboard`可以监控集群，还可以操作和管理集群，还有牛逼的是升级`ceph`方面，你也只需要修改配置要升级版本的网址，便可以实现自动升级，这是`intel`开发并提供开源的一款产品，值得去尝试一番。\n\n## 第10章\n\n同样的基准测试，不一样的味道。这一章提供了`n`多种的方式。有`dd`，有`rados bench`，有`rados load-gen`，`rdb bench-write`和`fio`等等。从测试网络，池，块设备和模拟负载应有尽有，关键这些都是`ceph`自带的除了`fio`。然后介绍了`ceph daemon`和`ceph tell`两个高校操作手段，最后介绍了使用`Ansible`部署`Ceph`，它可以通过配置文件配置集群信息而搭建集群，也就是说不需要手动的一个一个创建了，这样就显得很高效了。\n\n## 总结\n\n`ceph`已经看完两本书了，按理来说不应该再是那种一无所知的小白了，我觉得我缺少的还是大量的实践，所以之后我会对着这两本书照着例子再把代码敲一遍加深印象，然后对`ceph`应该会有另一番理解吧。接下来的进阶路线应该是中兴出版的《ceph原理和源码分析》了把，这本今年才出版的书我已经入手了，看了第一章，总体觉得还是蛮深入的，大量的源码，需要一定的知识铺垫，正好之后我也有看`ceph`源码和提供贡献的想法，所以正好那这本书过渡一下，加油！\n\n\n[1]: http://cdn.tony-yin.site/cookbook-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Ceph分布式存储学习指南》","url":"/2017/11/26/read-ceph-guide/","content":"\n<center>![ceph guide][1]</center>\n\n阅读周期：2017/9/5 ～ 2017/10/7\n阅读人群：入门ceph的研发或者运维\n阅读评分：4.7\n\n\n这本书不到两百页，但是也讲解了`ceph`的方方面面。对于入门`ceph`来说，看官网文档是必要的，但是很多时候官方文档太基础，太零散了，而这样一本书籍就可以弥补这样的缺陷，它把所有组件进行总结和概括，并且进行了实战例子的演示。\n\n<!--more-->\n\n## 目录结构\n\n* 第1章 Ceph存储介绍\n* 第2章 Ceph实战部署\n* 第3章 Ceph架构和组件\n* 第4章 Ceph内部构件\n* 第5章 Ceph部署\n* 第6章 Ceph存储配置\n* 第7章 Ceph操作及管理\n* 第8章 监控Ceph集群\n* 第9章 Ceph与Openstack的集成\n* 第10章 Ceph性能调优和基准测试\n\n## 具体分析\n\n### 第1章\n\n第1章主要介绍了一下`ceph`的特点和历史发展，通过与`raid`，其他存储相比较得出自身的优势和特色，然后着眼于未来的一些设计和解决方案。最后针对`ceph`三大存储接口：块存储、文件系统存储和对象存储进行了介绍。\n\n### 第2章\n\n这一章其实就是利用`ceph-deploy`手动搭建集群的一个演示，后续章节还会有相应的补充\n\n### 第3章\n\n第3章重点讲解了`ceph`的几大组件，`rados`存储网关，`osd` 讲的比较多，也是应该的，毕竟是存储的核心，首先对比了目前主流三个文件系统的优劣，然后对日志进行了科普，顺带讲解了一下不推荐在`ceph`集群中采用`raid`的原因。之后就是一些`osd`，`monitor`和`mds`的基础命令了。\n\n### 第4章\n\n如果说上一章主要讲的是`osd`，`monitor`，`mds`的话，那么这一章很符合标题内部构件，对应讲解的是`pg`，`crush`，对象和`pool`。`pg`是对象的载体，`pg`的状态往往决定着集群的状态，徐小胖有一篇文章叫大话`pg`讲的挺简单明了的，其实写一个数据然后查看一下存储目录结构就知道七七八八了。`ceph`池是存储对象的逻辑分区，用户可以根据自己的需求定制池的类型和规则。比如副本池还是`EC`池，也可以为`pool`定制`crush rule`。\n\n### 第5章\n\n这一章也是部署，跟第2章不同的是这一章不用`ceph-deploy`工具而采用了手工部署的方式。这样做可能要做的事情多一点，繁琐一些，但是会更灵活一些，因为`ceph-deploy`这种自动化的工具往往存在局限性，比较适用于测试环境，并不适用于生产环境。往往生产环境需要做很多的定制化的改变。最后还讲解了一些如何进行`ceph`升级。\n\n### 第6章\n\n这一章干货满满，分别就块设备，文件系统和对象存储进行了讲解，还是挺全面的，建议一边看书一边实践，这些命令还是要多用，否则很快也会忘记。目前`rbd`还是`ceph`最稳定的存储方式，相比`cephfs`而言他的性能、稳定和快照复制等功能都做的很全面了。`cephfs`虽然越来越被更多的应用内嵌，但是还是不能应用与生产环境之上。最后介绍了一下`rados`网关配置分别和`s3`，`swift`对接的流程。\n\n### 第7章\n\n这一章主要介绍了如何运行和管理集群里面的守护进程和相关服务。然后就集群扩容和所容等扩展操作进行了讲解。最后重点讲解了如何管理`crushmap`，通过配置确定集群存储的结构，定制`pool`的规则等等。\n\n### 第8章\n\n这一章对应标题讲解了一系列的监控命令，有`osd`，有`pg`，还有`mon`等等。最后引入了一些开源的监控软件，比如`kraken`，`ceph-dash`和`Calamari`，讲的比较简洁，想要具体了解还需要自己动手去搭建，不过貌似有一些已经长时间不维护了，建议动手前看一下。\n\n### 第9章\n\n这一章主要讲`ceph`和`openstack`的完美集成，不过讲的也很简洁，其实就是了解歌大概情况，我觉得这对刚入门的小白来说挺好的，讲得太细也没什么用，反而云里雾里的，其实这本书对我最大的帮助就是了解了存储和`ceph`大概是个什么情况，然后知道了一些命令的用法和应用场景。第一遍熟悉一下就可以了，然后之后可以在深入，否则第一次接触就搞`openstack`这种高级玩意估计吃不消呢。\n\n### 第10章\n\n最后一章先分别对软件和硬件的性能调优提出了建议，这个东西我觉得还是得到生产环境中不断尝试，虚拟机的话没有任何参考建议。然后集群优化分别介绍了`osd`，`filestore`和客户端的相关参数，这个我们可以熟悉一下，自己没事动手改改，看看效果。然后针对`ec`和缓存分层进行了一些介绍，也是局限于基本够用的层面上。最后介绍了`rados bench`这个基准测试工具。\n\n## 总结\n\n这本书专为入门而生，就是在官网文档的基础之上进行规整和总结，如果看不下去官网文档的可以尝试这个，书很薄，很快就可以看完了，看完大概就对`ceph`没那么陌生了，还是一个比较不错的入门书籍。如果已经了解`ceph`一段时间的朋友我就不是很推荐了。\n\n\n[1]: http://cdn.tony-yin.site/book-rocket-600x450.png\n\n\n","tags":["Read"],"categories":["read"]},{"title":"阅读感悟：《Python编程快速上手---让繁琐工作自动化》","url":"/2017/11/26/read-python-automation/","content":"\n<center>![python automation][1]</center>\n\n> 阅读感悟这个系列是针对每次阅读一本书后，先对全本书内容进行整体的总结和评价，然后针对每一章节进行具体分析，分享一下自己从中的收获，这样不但可以在看完整本书后对全书有个完整的加深回顾，还可以对没有阅读过本书的人提供经验和建议或者跟同样阅读过的伙伴相互交流，交换感悟和经验。\n\n- 阅读周期：2017/7/15 ～ 2017/10/7\n- 阅读人群：无语言基础的小白，有其他语言基础让入门`python`的，熟练`python`想深入学习的请勿下手\n- 阅读评分：4.7\n\n<!--more-->\n\n大家都知道其实一本书可能并不是很贵，一般都是几十元，就算是本烂书对大家的经济损失来说其实并不是很严重，最关键的就是时间了，在这个压力山大和节奏飞快的时代，时间无疑是最宝贵的，所以很多时候我买书之前都会很仔细的看下评论，我个人是比较喜欢买书的，一般会选择在亚马逊上面购买，对比京东而言的话我觉得他的优惠策略更加直接，不需要抢什么券什么的，还有他的物流非常快，最多隔天就到了，包装也很精致，很少出现盗版书这种情况，最后也是最重要的是相比京东而言亚马逊书籍评论更加贴切一些，往往京东一本书下面会有上千条评论，你很少会看到负面的评价，而亚马逊就不一样了，一般评论的都是骨灰级读者了，一旦评论都是很认真并且很全面的，所以也很难得在亚马逊上面看到全五星的书籍。\n\n这本书是我入门`python`的第一本书，在此之前我有`C`，`Java`和`PHP`的语言经验。无论是在阅读这本书的过程中还是看完了之后，我都有一种庆幸感，那就是我当初选择这本书真的没错。在阅读前三章的时候，我一度觉得这本书太浅了，这主要可能因为我有其他语言基础，而这本书面对的读者不仅仅有语言基础的，而他这么做对那种小白来说就非常`nice`了。\n\n这本书给我最深的印象可以概括为四个字：深入浅出。就是在讲的很细致的同时，同时也会配有一些深入的内容。全本书的最大特色莫过于插图和`demo`了。一般的书一般是以代码围绕着讲解，而这本书的话往往是通过一些小项目或者小例子讲解一些概念和用法。第一部分主要讲解了语言的基础概念和`python`重要的数据结构：列表和字典。第二部分主要讲解了自动化知识，就比较深入了，具体比如正则表达式、对文件的操作、调试技巧、获取`web`信息、处理`excel`文件、处理`pdf`和`word`文档、处理`CSV`文件和`JSON`数据、计划任务、邮件和短信、操作图像和操作鼠标和键盘。掌握了列表、字典和字符串这些数据结构就可以进行基础的`python`编程了，然后学习了正则表达式和文件操作后，就可以很轻松了写一些自动化脚本了。从`web`抓取信息这一章是专门为喜爱爬虫的小伙伴准备的，提供了很多有用的工具和手段。之后还有对各种类型文件的操作，操作图像这一章我也很喜欢，准备之后做一个小项目，因为我个人对好看的图片很有兴趣。\n\n## 目录结构\n* 第一部分 Python编程基础\n    * 第1章 python基础\n    * 第2章 控制流\n    * 第3章 函数\n    * 第4章 列表\n    * 第5章 字典和结构化数据\n    * 第6章 字符串操作\n* 第二部分\n    * 第7章 模式匹配与正则表达式\n    * 第8章 读写文件\n    * 第9章 组织文件\n    * 第10章 调试\n    * 第11章 从Web抓取信息\n    * 第12章 处理Excel电子表格\n    * 第13章 处理PDF和Word文档\n    * 第14章 处理CSV文件和JSON数据\n    * 第15章 保持时间、计划任务和启动程序\n    * 第16章 发送电子邮件和短信\n    * 第17章 操作图像\n    * 第18章 用GUI自动化控制键盘和鼠标\n\n\n## 章节分析\n\n### 第1章、第2章和第3章\n\n这3章对我来说没什么好说的，就是一些语言基础，比如字符串拼接啊，控制流啊，函数什么的，对没有语言基础的小伙伴来说非常的贴心和容易理解，老司机的话可以快速阅读或者略过。\n\n### 第4章\n\n列表是`python`比较有特色的数据结构了，操作起来非常的方便。这一章分别阐述了获取列表、操作列表（merge、copy、delete）、列表循环和多重赋值技巧。然后就是一些深入的操作技巧了，比如排序、插入和删除。最后引入了字符串和元组和列表加以比较，讲解了可变类型、转换类型、传递引用和深拷贝等，总的来说还是很实用。\n\n### 第5章、第6章\n\n首先围绕字典讲解了一些基本的`API`，然后通过很好的棋盘建模模拟了字典的数据结构，很形象。字符串的话也差不多一样。\n\n### 第7章\n\n相比其他语言的正则来说，我觉得`python`的正则表达式无论是理解还是使用上面还算是比较简单的。先讲解了单个匹配和全局匹配，然后针对邮件，电话等常见场景给出了例子。\n\n\n### 第8章\n\n先讲解了`os`模块很多有用的`API`，大多数都是跟文件和目录有关，然后讲解了文件的读写操作，其中提到了`shelve`模块，这个在需要确保文件保密的场景非常有用。最后通过一个小例子讲解了`pyperclip`模块如何进行剪切板操作。\n\n### 第9章\n\n讲解了通过`shutil`模块复制文件和文件夹、移动、重命名、永久删除和安全删除。然后讲解了围绕`zipfile`模块相应的操作。\n\n### 第10章\n\n断言给我的印象比较深，还有`python`自带的`IDLE`的调试器的使用，我本人平时基本上是在`linux`下`vim`工作，所以这个工具平时也用不到，以后可以在`windows`下尝试一下。\n\n### 第11章\n\n这一章学习到了很多通过`python`取获取`web`信息的手段，这会在爬虫的时候很受用。比如通过`webbrowser`模块启动浏览器；通过`request`模块下载网页；通过`BeautifulSoup`模块解析`html`，这个模块非常强，简直和`js`操作`html`相媲美。\n\n### 第12章、第13章、第14章\n\n这三章分别针对`excel`、`pdf`、`word`、`csv`和`json`类型的文件或数据格式进行操作。对`excel`的支持还蛮强的，但是`pdf`和`word`的话就或多或少可能有问题，这个就不赘述了，用到的时候再查就好了。\n\n### 第15章、第16章、第18章\n\n这几章没什么好说的\n第15章主要讲解了保持时间、计划任务和启动程序等功能\n第16章主要讲解了消息通知，包括了邮件和短信接口\n第18章主要讲解了如何通过`python`控制键盘和鼠标，其中比较有意思的就是控制键盘原生不存在的一些特殊字符\n\n\n### 第17章\n\n`python`操作图像还是蛮好用的，本章主要讲解了对图片的裁剪，旋转，改变像素等，后续我会做一个小项目，通过`python`对图片进行一些操作，顺便深入了解一下这部分的知识。\n\n\n## 总结\n\n第一篇阅读总结感觉写的并不是很好，篇幅虽然还算长，但是感觉内容并不是很饱满，很多地方略显啰嗦。想了一下，之后可以把每一章的具体分析放在平时，每阅读完一章节就可以进行总结了，然后读完全本书就进行一个整体总结就可以了，这样每一章的总结就更加细致了，就不用读完全本书的时候再去回忆，这样效率很低。最后我会在之后有空的情况下分别做一个`python`的爬虫项目和一个操作图像的项目。\n\n\n\n[1]: http://cdn.tony-yin.site/python-automation-600x450.jpg\n","tags":["Python"],"categories":["read"]},{"title":"博客主题 shadow","url":"/2017/11/19/hexo-theme-shadow/","content":"\n<center>![blog theme shadow][4]</center>\n\n使用了[black-blue][1]主题一段时间后，有些地方不符合我个人的审美和习惯，然后自己偶尔改了一些，久而久之还改了不少东西，为了方便存档和以后持续更新，就打造一个自己的专属主题：[hexo-theme-shadow][2]，大家可以去[我的博客][3]看下实际效果\n\n如果大家有任何批评和建议，随时可以在`github`上面提出`PR`，要是觉得主题还不错的话，欢迎留下你的`star`哦 ^_^\n\n<!--more-->\n\n具体如何配置主题和搭建博客这些基础通用的东西我就不讲了，这边我主要罗列一下我已经更新和以后可能要做的功能点\n\n## 已经更新的功能\n\n* 添加左侧栏`segmentfault`小图标\n* 更新`requirejs`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`jquery`源为`https`，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`baidu`分享源到本地，避免`https`网站引入`http`脚本时浏览器告警\n* 更新`markdown code`的风格，具体参见博客\n    * 更改了全局的代码颜色样式\n    * 原来项目编号中的代码样式是白色的，很丑陋，也进行了相应的更新\n* 将头像从懒加载改为了直接加载，懒加载头像需要很长时间，用户体验很差\n* 网站背景音乐支持\n* 代码块样式修改，每个代码块上面有个蓝色的条条很不好看\n* 三级、四级和五级标题样式修改\n* 链接样式修改\n* 畅言评论支持\n* 打赏支持\n* 书签样式修改\n* 代码块样式修改\n* 项目编号样式修改\n* 表格样式修改\n\n## 使用\n\n具体使用教程请参考我主题的`github`地址，在下方有写\n\n如果大家有什么建议或者问题，可以在`github`上面提`issue`或者`PR`，也可以通过邮箱联系到我\n\n`github`地址： `https://github.com/tony-yin/hexo-theme-shadow`\n邮箱地址： `1241484989@qq.com`\n\n[1]: http://geeksblog.cc/\n[2]: https://github.com/tony-yin/hexo-theme-shadow\n[3]: http://www.tony-yin.site/\n[4]: http://cdn.tony-yin.site/do_blog_600x450.png\n","tags":["博客"],"categories":["tech"]},{"title":"LVM动态扩展","url":"/2017/11/14/LVM-Space-Expansion/","content":"\n<center>![LVM Space Expansion][1]</center>\n\n在平时的开发工作中，经常会创建磁盘不足够大的虚拟机，然后往集群里面写一些数据导致磁盘满了。手动编辑虚拟机的磁盘大小是不会文件系统识别的，大多数同学只能无奈的重新装`OS`，这里我介绍一种基于`LVM`实现动态的方式。\n\n<!--more-->\n\n## LVM了解\n\n`LVM`是逻辑盘卷管理（`LogicalVolumeManager`）的简称，它是`Linux`环境下对磁盘分区进行管理的一种机制，`LVM`是建立在硬盘和分区之上的一个逻辑层，来提高磁盘分区管理的灵活性。通过LVM系统管理员可以轻松管理磁盘分区，如：将若干个磁盘分区连接为一个整块的卷组（`volumegroup`），形成一个存储池。管理员可以在卷组上随意创建逻辑卷组（`logicalvolumes`），并进一步在逻辑卷组上创建文件系统。管理员通过`LVM`可以方便的调整存储卷组的大小，并且可以对磁盘存储按照组的方式进行命名、管理和分配。\n\n\n## 查看分区\n\n当前默认只有一个采用`lvm`的分区，一开始`sda`磁盘容量为`16G`，后来发现不够用了，编辑磁盘大小为`50G`，但是可以发现这`50G`并没有起到扩展分区容量的效果。\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       14G  3.4G  9.6G  26% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n└─sda2                           8:2    0 15.5G  0 part \n  ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n  └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n```\n\n## 分区\n\n可以通过新增一块其他磁盘来扩容，我这边采取的是增大当前磁盘的容量实现扩容。\n\n有时候因为系统设备处于繁忙状态，所以分区需要重启后才会生效。\n\n```\n[root@tony-play ~]# fdisk /dev/sda\nWARNING: DOS-compatible mode is deprecated. It's strongly recommended to\n         switch off the mode (command 'c') and change display units to\n         sectors (command 'u').\nCommand (m for help): n\nCommand action\n    e   extended\n    p   primary partition (1-4)\np\nPartition number (1-4): 3\nFirst cylinder (2089-6527, default 2089):   // 直接回车，用默认值就可以了\nUsing default value 2089\nLast cylinder, +cylinders or +size{K,M,G} (2089-6527, default 6527):    // 直接回车，用默认值就可以了\nUsing default value 6527\nCommand (m for help): w\nThe partition table has been altered!\n\n# 可以看到新建的分区sda3已结被创建出来了，采取默认值会将剩余所有空间都分到分区中\n[root@tony-play ~]# lsblk\nNAME                           MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nsr0                             11:0    1 1024M  0 rom  \nsda                              8:0    0   50G  0 disk \n├─sda1                           8:1    0  500M  0 part /boot\n├─sda2                           8:2    0 15.5G  0 part \n│ ├─vg_tonyplay-lv_root (dm-0) 253:0    0 13.9G  0 lvm  /\n│ └─vg_tonyplay-lv_swap (dm-1) 253:1    0  1.6G  0 lvm  [SWAP]\n└─sda3                           8:3    0   34G  0 part \n```\n\n## 查看当前文件系统\n\n当前文件系统为`ext4`\n\n```\n[root@tony-play ~]# mount\n/dev/mapper/vg_tonyplay-lv_root on / type ext4 (rw)\nproc on /proc type proc (rw)\nsysfs on /sys type sysfs (rw)\ndevpts on /dev/pts type devpts (rw,gid=5,mode=620)\ntmpfs on /dev/shm type tmpfs (rw,rootcontext=\"system_u:object_r:tmpfs_t:s0\")\n/dev/sda1 on /boot type ext4 (rw)\nnone on /proc/sys/fs/binfmt_misc type binfmt_misc (rw)\n```\n\n## 为新分区创建文件系统\n\n```\n[root@tony-play ~]# sudo mkfs.ext4 /dev/sda3\nmke2fs 1.41.12 (17-May-2010)\nFilesystem label=\nOS type: Linux\nBlock size=4096 (log=2)\nFragment size=4096 (log=2)\nStride=0 blocks, Stripe width=0 blocks\n2228224 inodes, 8912727 blocks\n445636 blocks (5.00%) reserved for the super user\nFirst data block=0\nMaximum filesystem blocks=4294967296\n272 block groups\n32768 blocks per group, 32768 fragments per group\n8192 inodes per group\nSuperblock backups stored on blocks: \n    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632, 2654208, \n    4096000, 7962624\n\nWriting inode tables: done                            \nCreating journal (32768 blocks): done\nWriting superblocks and filesystem accounting information: done\n\nThis filesystem will be automatically checked every 39 mounts or\n180 days, whichever comes first.  Use tune2fs -c or -i to override.\n```\n\n## 查看卷组信息\n\n```\n[root@tony-play ~]# vgdisplay\n  --- Volume group ---\n  VG Name               vg_tonyplay     // 卷组名在下面扩展中会用到\n  System ID             \n  Format                lvm2\n  Metadata Areas        1\n  Metadata Sequence No  3\n  VG Access             read/write\n  VG Status             resizable\n  MAX LV                0\n  Cur LV                2\n  Open LV               2\n  Max PV                0\n  Cur PV                1\n  Act PV                1\n  VG Size               15.51 GiB\n  PE Size               4.00 MiB\n  Total PE              3970\n  Alloc PE / Size       3970 / 15.51 GiB\n  Free  PE / Size       0 / 0   \n  VG UUID               Y9usSM-nDU5-ZAUd-Y3Te-u5Pd-uFBr-gcYHf0\n```\n\n## 创建新物理卷\n\n```\n[root@tony-play ~]# pvcreate /dev/sda3\n  Physical volume \"/dev/sda3\" successfully created\n```\n\n## 扩展到卷组\n\n```\nvgextend vg_tonyplay /dev/sda3  // 卷组名在查看卷组信息中\n  Volume group \"vg_tonyplay\" successfully extended\n```\n\n## 查看逻辑分区\n\n`/dev/vg_tonyplay/lv_root`就是根分区，也是我们要扩展的分区。\n\n```\n[root@tony-play ~]# lvdisplay \n  --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_root   // 根分区\n    LV Name                lv_root\n    VG Name                vg_tonyplay\n    LV UUID                IPd7lm-Sx8g-pe7k-llNL-j1wc-mbA2-2cAdsy\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:58:53 -0400\n    LV Status              available\n    # open                 1\n    LV Size                13.91 GiB\n    Current LE             3561\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:0\n\n    --- Logical volume ---\n    LV Path                /dev/vg_tonyplay/lv_swap\n    LV Name                lv_swap\n    VG Name                vg_tonyplay\n    LV UUID                qX637q-iD6i-8blp-hmmS-MvLy-xZ0y-b4D0BF\n    LV Write Access        read/write\n    LV Creation host, time tony-play, 2017-04-10 17:59:07 -0400\n    LV Status              available\n    # open                 1\n    LV Size                1.60 GiB\n    Current LE             409\n    Segments               1\n    Allocation             inherit\n    Read ahead sectors     auto\n    - currently set to     256\n    Block device           253:1\n```\n\n## 扩展容量到逻辑分区\n\n```\n[root@tony-play ~]# lvextend /dev/vg_tonyplay/lv_root /dev/sda3\n  Size of logical volume vg_tonyplay/lv_root changed from 13.91 GiB (3561 extents) to 47.91 GiB (12264 extents).\n  Logical volume lv_root successfully resized\n```\n\n## 刷新逻辑分区容量使扩展生效\n\n`ext4`用`resize2fs`，`xfs`用`xfs_growfs`\n\n```\n[root@tony-play ~]# resize2fs /dev/vg_tonyplay/lv_root\nresize2fs 1.41.12 (17-May-2010)\nFilesystem at /dev/vg_tonyplay/lv_root is mounted on /; on-line resizing required\nold desc_blocks = 1, new_desc_blocks = 3\nPerforming an on-line resize of /dev/vg_tonyplay/lv_root to 12558336 (4k) blocks.\nThe filesystem on /dev/vg_tonyplay/lv_root is now 12558336 blocks long.\n```\n\n## 查看逻辑分区容量\n\n可以发现`/dev/mapper/vg_tonyplay-lv_root`已经从开始的`14G`扩展到了`48G`。ok，这就说明大功告成了，再也不用通过重装系统这种蹩脚的方式扩容了\n\n```\n[root@tony-play ~]# df -h\nFilesystem            Size  Used Avail Use% Mounted on\n/dev/mapper/vg_tonyplay-lv_root\n                       48G  3.4G   42G   8% /\ntmpfs                 1.9G   72K  1.9G   1% /dev/shm\n/dev/sda1             477M   42M  410M  10% /boot\n```\n\n## 小结\n\n至此，`lvm`扩容工作的过程应该是比较清楚了，之后有机会的话我会再补充一下`LVM`的压缩、删除等操作过程。\n\n[1]: http://cdn.tony-yin.site/work-for-love-600x450.png\n","tags":["LVM"],"categories":["tech"]},{"title":"Ceph 编译（Giant版本）","url":"/2017/11/14/Ceph-Compile/","content":"\n<center>![Ceph Compile][1]</center>\n\n如今入门`Ceph`的时候，大家一般用`ceph-deploy`工具比较多，这个工具的确很强大，很方便，对应的也就是很无脑。如果之后想深入`Ceph`或者想在生产环境中部署`Ceph`的话，就得熟悉`Ceph`源码编译了。这对我们熟悉`Ceph`的`feature`的变化，`component`的相互关系以及围绕`Ceph`各种定制化扩展都大有裨益。本文就笔者`Ceph`源码编译过程和遇到的问题作出分享，希望有人能够得益于此。\n\n<!--more-->\n\n## 环境声明\n\n* OS： Centos 6.8\n* Ceph： Giant\n\n## 源码下载\n\n### 指定 giant 分支\n\n```shell\n git clone -b giant git://github.com/ceph/ceph.git\n ```\n\n### 下载子模块\n\n```shell\ngit submodule update --init --recursive\n```\n\n这一步我始终更新不了，尝试了一些办法未果，所以就去直接手动下载了源码，这个问题以后有时间看下\n\n## 预检\n\n```shell\ncd ceph \n./autogen.sh \n./configure\n```\n\n### Autogen\n\n这一步会频繁的报错一些`m4`文件没有，这需要我们手动创建这些`m4`文件夹即可，\n\n一开始我在`ceph`根目录创建了`m4`文件夹，并生成了包括`acx_pthread.m4`在内的文件，但是还是报错：`acx_pthread.m4 not exist`，后来发现这时候已经**切换目录**了，不止一个地方需要`m4`文件夹，一共有这几个地方需要手动创建目录：`mkdir m4`\n\n```shell\n./src/rocksdb/m4\n./src/gtest/m4\n./src/erasure-code/jerasure/jerasure/m4\n./src/erasure-code/jerasure/gf-complete/m4\n./m4\n```\n\n如果还是报错一些文件不存在，并且通过上述方法不能自行初始化生成的话，可以从网上或者已经编译过的`ceph`环境拷贝过来\n\n```shell\n报错：umdefined macro\n```\n\n下载地址：https://www.gnu.org/software/autoconf-archive/ax_check_classpath.html#ax_check_classpath\n\n### Configure\n\n这一步是编译过程中可能出错的次数最多的，因为可能会因为你的环境缺少相应的包不断报错。不过数量虽多，解决起来还是比较容易的，就根据报错的缺包对应下载安装就好了，下面我先给出一个所有包的安装步骤，然后再针对每个报错环节给出具体的解决方案\n\n#### 总体解决方案\n\n```shell\n# 1. 通过yum安装所有可以安装的包\nyum install -y yasm libuuid-devel libblkid-devel libudev-devel cryptopp-devel fuse-devel libunwind-devel libedit-devel libatomic_ops-devel snappy-devel leveldb-devel libaio-devel xfsprogs-devel boost*\n\n# 2. 部分yum不能安装的可以通过rpm安装\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n#### 具体解决方案\n\n1.yasm\n\n报错：\n\n```shell\nyasm command not found\n```\n\n解决：\n\n```shell\nyum install yasm -y\n```\n\n2.libuuid\n\n报错：\n\n```shell\nconfigure: error: libuuid not found\n```\n\n解决：\n\n```shell\nyum install libuuid-devel -y\n```\n\n3.libblkid\n\n报错：\n\n```shell\nconfigure: error: blkid/blkid.h not found (libblkid-dev, libblkid-devel)\n```\n\n解决：\n\n```shell\nyum install libblkid-devel -y\n```\n\n4.libudev\n\n报错\n\n```shell\nconfigure: error: libudev.h not found (libudev-dev, libudev-devel)\n```\n\n解决：\n\n```shell\nyum install libudev-devel -y\n```\n\n5.crypto\n\n报错：\n\n```shell\n configure: error: no suitable crypto library found\n ```\n\n 解决：\n\n ```shell\n yum install cryptopp-devel -y\n ```\n\n 6.fuse\n\n 报错：\n\n ```shell\n configure: error: no FUSE found (use --without-fuse to disable)\n ```\n\n 解决：\n\n ```shell\n yum install fuse-devel -y\n ```\n\n 7.tcmalloc\n\n 报错：\n\n ```shell\n configure: error: no tcmalloc found (use --without-tcmalloc to disable)\n ```\n\n 解决：\n\n ```shell\n# 1. 需要先安装libunwind-devel，被gperftools依赖\n yum install libunwind-devel -y\n\n# 2. 安装tcmalloc（yum无法安装，需要通过rpm的方式）\nwget https://github.com/gperftools/gperftools/releases/download/gperftools-2.2.1/gperftools-2.2.1.tar.gz\ntar -zxvf gperftools-2.2.1.tar.g\ncd gperftools-2.2.1\n./configure\nmake\nmake install\n```\n\n8.libedit\n\n报错：\n\n```shell\nconfigure: error: No usable version of libedit found.\n```\n\n解决：\n\n```shell\nyum install  libedit-devel -y\n```\n\n9.libatomic-ops\n\n报错：\n\n```shell\nconfigure: error: no libatomic-ops found (use --without-libatomic-ops to disable)\n```\n\n解决：\n\n```shell\nyum install libatomic_ops-devel  -y\n```\n\n10.libsnappy\n\n报错：\n\n```shell\nconfigure: error: libsnappy not found\n```\n\n解决：\n\n```shell\nyum install snappy-devel -y\n\nor \n\nwget ftp://195.220.108.108/linux/centos/6.9/os/x86_64/Packages/snappy-devel-1.1.0-1.el6.x86_64.rpm\nrpm -ivh snappy-devel-1.1.0-1.el6.x86_64.rpm\n```\n\n11.libleveldb\n\n报错：\n\n```shell\nconfigure: error: libleveldb not found\n```\n\n解决：\n\n```shell\nyum install leveldb-devel\n```\n\n12.libaio\n\n报错：\n\n```shell\nconfigure: error: libaio not found\n```\n\n解决：\n\n```shell\nyum install libaio-devel -y\n```\n\n13.libxfs\n\n报错：\n\n```shell\nconfigure: error: xfs/xfs.h not found (--without-libxfs to disable)\n```\n\n解决：\n\n```shell\nyum -y install xfsprogs-devel\n```\n\n14.boost\n\n报错：\n\n```shell\nCan't find boost spirit headers\n```\n\n解决：\n\n```shell\nyum install boost* -y\n```\n\n## 编译安装\n\n机器配置不好的话，编译需要时间比较长。可以使用`make -j`增加并发度，`4`表示同时执行的`make`方法数。\n\n```shell\nmake -j4\nmake install（可选）\n```\n\n## 小结\n\n`ceph`手动源码编译遇到的问题还是蛮多的，如果不自己动手经历一下的话很多东西都不知道，当然这都是经验的积累，不断地锻炼自己解决问题的能力，要学会见招拆招，通过问题发现原理和本质。\n\n[1]: http://cdn.tony-yin.site/ceph-600x450.png\n","tags":["编译"],"categories":["tech"]},{"title":"通过ganesha-nfs导出Ceph为NFS（Luminous）","url":"/2017/11/08/Ceph-NFS-Ganesha/","content":"\n<center>![ganesha][8]</center>\n\n自从`Jewel`版本，`nfs-ganesha`开始支持`ceph`，并且把对接点选择了`rados`。`Ganesha`支持两种方式将`Ceph`导出为`NFS`，一种通过`RGW`，一种通过`CephFS`，通过`FSAL`模块 连接到`RGW`或者`CephFS`， 其中，`FSAL_RGW`调用`librgw2`将`NFS`协议转义为`S3`协议再通过`RGW`存入到`Ceph`中，`FSAL_CEPH` 调用`libcephfs1`将`NFS`转义为`Cephfs`协议再存入到`Ceph` 中。所以需要额外安装这两个包。\n\n本文就`Luminous`版本的`ceph`基于`ganesha`导出`nfs`部署，并且测试一下`rgw`和`cephfs`的性能。[@徐小胖][1]已经就`jewel`版本的过程进行了大致的讲解，我这边主要分享一下我遇到他文章没提到的和`Luminous`场景导致的问题。\n\n<!--more-->\n\n## 参考链接：\n\n* [通过ganesha-nfs将 Ceph 导出为 NFS][1]\n* [RGW+Ganesha环境部署][2]\n\n## 环境声明\n\n* os： centos7\n* ceph： luminous\n* nfs-gnesha： **v2.5 stable**（important）\n\n## 安装依赖\n\n一些编译需要的公共库\n\n```\nyum install gcc git cmake autoconf libtool bison flex doxygen openssl-devel gcc-c++ krb5-libs krb5-devel libuuid-devel nfs-utils -y\n```\n\n`ubuntu`的我也试了一下，主要有以下几个包不同：\n\n* `gcc-c++`   ->   `g++`\n* `libuuid-devel` ->  `uuid-dev`\n* `nfs-utils`   ->  `nfs-kernel-server` \n\n如果要生成`FSAL_RGW`模块，需要安装`librgw2-devel`（我装的`librgw-devel`也可以`work`，看了`so`文件发现就是`2`，只是建立了映射关系，这个看了源码知道了是因为`K`版本对`librgw`编译这块做了升级，加了`2`标识一下）\n\n```\nyum install librgw2-devel -y \n```\n\n如果要生成`FSAL_CEPH`模块，需要安装`libcephfs1-devel`\n\n```\nyum install libcephfs1-devel -y\n```\n\n## 源码下载\n\n上面两篇文章一个下载的是`v2.3 stable`，一个是`v2.4 stable`，两个我都试过，都会在`make`到`80%`左右的时候报错，应该是源码版本和库的版本有冲突导致的，这个问题耽误我挺长时间的，后来猜想可能是版本问题，尝试了一下`v2.5 stable`的源码就可以了\n\n```\ngit clone -b V2.5-stable https://github.com/nfs-ganesha/nfs-ganesha.git --recursive\n```\n\n**注意：（重要）**\n\n最近我同事根据这篇文档部署`nfs-ganesha`的时候，发现之后`cmake`的操作后`USE_FSAL_RGW`始终是`OFF`的状态，一开始检查了一下环境发现`ganesha`是`v2.5`，然后`ceph`也是`L`版本，`centos`也是`7`。\n\n报错如下：\n\n```\nlocal RGW version is 1.1.4 not suitable match, but required latest RGW version is 1.1.6\n```\n\n很好理解，就是我们安装的`RGW`达不到`nfs-ganesha`的要求，首先我们尝试去安装新的版本，但是`yum`的源已经配置了`luminous`，并且网上搜索了一圈并没有找到更新的。\n\nok，那就第二步，那就找原因同样的环境为啥我可以成功？而这位同事却不行？莫非因为我帅？不能这么逆天吧^_^ （明明可以靠脸吃饭，偏偏靠才华，哎……）\n\n言归正传，通过报错信息查看源代码？定位到了报错文件`src/CMakeLists.txt`，然后代码中对`RGW`的版本要求也的确是`1.1.6`，回过头再看我之前下载下来的源代码，怪异的现象发生了，我代码中对`RGW`的版本要求只有`1.1.3`，此时我的第一反应就是应该这个文件被修改过导致两份代码不一样，出于本能直接上了`github`，果不其然，发现了`mattbenjamin`同学在`10`月`17`号在`v2.5`的分支上提交了一个`commit`针对`RGW`版本做出了修改！具体参看[commit详情][3]，而这位贡献者几乎同时也在`ceph`提交了相关代码。这样想想就说得通了，我在搭建`nfs-ganesha`的时候恰好是在他提交之前的，所以我本地`RGW`本地版本是`1.1.4`是比要求版本`1.1.3`要高的，虽然不是完全`match`，但是也基本符合了要求，而我这位同事脸就比较黑了。。。\n\n那该怎么解决这个问题呢？\n\n有两种解决方案：\n\n1. 手动编译`Luminous Ceph`把最新的`RGW`包编译出来，因为现在这个资源还没被公开分享出来；\n2. 克隆这个改动之前的代码\n\n这么看来第二种更为简单一些，我们也是采用的这种方式，`git clone v2.5 stable`后，然后`git checkout`恰当的版本号即可。\n\n**总结：**\n\n其实直接`clone`实时的代码这是不合理的，因为你不知道他什么时候会更新。正确的方式是我们应该找到稳定的`release`版本，`nfs-ganesha`也有提供[一系列的release][4]，而我们通过查看`2.5 tree`上面的`commit`信息，可以知道`RGW`的改动是介于`2.5.3`和`2.5.4`之间的，所以我们下载`2.5.3`较为合适，下载地址在[这里][5]。\n\n这边还需要注意一个问题，这边`release`包下载下来你会发现`libntrirpc`这个文件夹没有内容，出现这个问题是因为`libntrirpc`对应的也是一个`git`仓库，我们需要去[ntrirpc][6]手动克隆，然后通过`nfs-ganesha`的目录发现对`ntrirpc`的版本要求是`fadcbde`（这些都是上`github`可以看到的），我们需要做的就是在下载下来的`ntrirpc`目录下切到`fadcbde`这个`commit`上，并把当前的内容拷贝到`nfs-ganesha`的`libntrirpc`目录下。\n\n实在搞不定的童鞋，我提供一个我克隆的`nfs-ganesha`的代码包，你们可以对比一下，这个我是可以成功部署的\n\n附：我的`nfs-ganesha`[下载地址][7]\n\n## 编译\n\n编译`nfs-ganesha`， **注意打开对应的模块：**\n\n* 如果需要生成`FSAL_RGW`模块，则在编译选项中添加： `-DUSE_FSAL_RGW=ON`\n* 如果需要生成`FSAL_CEPH`模块，则在编译选项中添加： `-DUSE_FSAL_CEPH=ON`\n\n```\ncd src/\nmkdir build\ncd /build/\ncmake -DUSE_FSAL_RGW=ON -DUSE_FSAL_CEPH=ON ../\n```\n\n`cmake`的过程中会有以下输出：\n\n```\n-- Looking for ceph_ll_lookup_root in cephfs - found\n-- Found cephfs libraries: /usr/lib64/libcephfs.so\n-- Found CEPHFS: /usr/include  \n-- Looking for rgw_mount in rgw\n-- Looking for rgw_mount in rgw - found\n-- Found rgw libraries: /usr/lib64/librgw.so\n-- Found RGW: /usr (found suitable version \"1.1\", minimum required is \"1.1\") \n...\n-- USE_FSAL_CEPH = ON\n-- USE_FSAL_CEPH_MKNOD = OFF\n-- USE_FSAL_CEPH_SETLK = OFF\n-- USE_FSAL_CEPH_LL_LOOKUP_ROOT = ON\n-- USE_FSAL_RGW = ON\n```\n\n这一步，很重要，很多时候会因为没有装好的`librgw2-devel`或者`libcephfs1-devel`导致这边的`USE_FSAL_RGW`或者`USE_FSAL_CEPH`状态为`OFF`\n\n**确保，确保，确保：** `-- USE_FSAL_CEPH`为`ON`，以及`-- USE_FSAL_RGW`为`ON`。\n\n如果是`OFF`，请检查下`librgw2-devel`或者`libcephfs1-devel`是否有安装，如果这两个包都已经安装了，还显示为`OFF`， 可以尝试下清空编译目录：`rm -rf build/*`，再进行编译，如果依旧为 `OFF`，可以尝试下删除所有的`Ceph`包，再重新`yum install ceph librgw2-devel libcephfs1-devel -y`。\n\n编译和安装，在`build`目录下\n\n```\nmake\nmake install\n```\n\n> PS:\n> 在`make install`生成的输出中，可以看到:\n>\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/rgw.conf\n> ...\n> -- Up-to-date: /usr/share/doc/ganesha/config_samples/ceph.conf\n>\n> 这两个文件就是配置将`RGW`和`CephFS`配置为`ganesha-nfs`的配置模板。\n \n## 编辑配置文件\n\n注意`Path`后面的路径需要加引号，`rgw`替换成创建`S3`用户生成的用户信息\n\n```\ncat /etc/ganesha/ganesha.conf\nEXPORT\n{\n        Export_ID=1;\n        Path = \"/\";\n        Pseudo = /cephfs;\n        Access_Type = RW;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = CEPH;\n        }\n}\nEXPORT\n{\n        Export_ID=2;\n        Path = \"/\";\n        Pseudo = /rgw;\n        Access_Type = RW;\n        Squash = No_root_squash;\n        NFS_Protocols = 4;\n        Transport_Protocols = TCP;\n        FSAL {\n                Name = RGW;\n                User_Id = \"admin\";\n                Access_Key_Id =\"1MWH3LWM1BS4ZF4HN5IH\";\n                Secret_Access_Key = \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\";\n        }\n}\nRGW {\n    ceph_conf = \"/etc/ceph/ceph.conf\";\n}\n```\n\n## 启动 Ganesha\n\n```\nganesha.nfsd -f /etc/ganesha/ganesha.conf -L /var/log/nfs-ganesha.log -N NIV_DEBUG\n```\n如果一切顺利，你应该可以看到`ganesha.nfsd` 进程在那，如果进程不在，那么查看`Log`，记得在启动进程前，关闭所有`CephX`配置。\n\n**重要**：`librgw init failed (-5)` 解决方法\n\n报错内容如下\n\n```\nRGW-1 : nfs-ganesha-2232083[main] create_export :FSAL :CRIT :RGW module: librgw init failed (-5)\nRGW-1 : nfs-ganesha-2232083[main] mdcache_fsal_create_export :FSAL :MAJ :Failed to call create_export on underlying FSAL\n\n```\n\n经过多次尝试，包括在`ganesha.conf`内添加`init_args`指定秘钥和`Ceph`的用户，`ganesha-nfs` 均无法启动，报的错如标题，解决方法就是关闭`CephX`，将`/etc/ceph/ceph.conf`内的三个 `cephx`改为`none`，然后重启`ceph-mon`，`ceph-osd`，`ceph-radosgw`，`ceph-mds` 进程，再启动`ganesha-nfs`，即可正常运行。\n\n**需要注意的是：**\n\n当你在当前节点上，关闭`cephx`后，你用`ceph -s`查看集群状态时，这时候会报错说明由于没有`auth`认证导致无法连接集群，所以我当时试了很久，尽管`nfs-ganesha`已经运行了，但是`mount`都没有成功`export`，查看`log`才发现原来在`init`配置的时候就失败了，导致这个问题的原因是我只关闭了一个节点的`cephx`，所以需要做的就是将集群内所有节点的`cephx`全部关闭，然后集群就可以正常访问了，并且`nfs-ganesha`也不会因为`cephx`导致加载配置出错\n\n## Check Export\n\n```\n[root@node1 build]# showmount -e  \nExport list for node1:  \n/ (everyone)  \n/ (everyone)  \n```\n\n## 挂载 NFS\n\n```\n[root@node1 mnt]# mount -t nfs4 192.168.1.1:/  /mnt/ceph/  \nroot@node1 mnt]# ls ceph/*  \nceph/cephfs:  \ntest  \n   \nceph/rgw:  \nmy-new-bucket \n```\n\n说明`CephFS`和`RGW`都已经正常对接。\n如果，你所使用的`admin`用户名下有很多的桶，那么这些桶都会以`/mnt/rgw/xxbucket`的结构显示出来，如果你在`/mnt/rgw/`下建立的一个目录，那么就相当于通过`RGW`建立了一个桶，所以，你执行`touch /mnt/rgw/123`是会报错的，因为不符合`S3`的对象必须位于桶内的规定，简单点说，就是把`/mnt/rgw/`和`S3`的根目录一一对应即可。\n\n同样，`CephFS`内的内容都会显示在`/mnt/cephfs/`目录下。可以开始愉快的玩耍了！！\n \n[1]: http://www.xuxiaopang.com/2017/03/27/ganesha-nfs-deploy/#more\n[2]: http://blog.csdn.net/younger_china/article/details/73432726\n[3]: https://github.com/nfs-ganesha/nfs-ganesha/commit/8d039cd139ea1dcfbe316b7e93e96efe755669ef\n[4]: https://github.com/nfs-ganesha/nfs-ganesha/releases\n[5]: https://github.com/nfs-ganesha/nfs-ganesha/releases/tag/V2.5.3\n[6]: https://github.com/nfs-ganesha/nfs-ganesha/tree/next/src\n[7]: http://share.tony-yin.site/nfs-ganesha.tar.gz\n[8]: http://cdn.tony-yin.site/ganesha-600x450.png\n","tags":["Luminous"],"categories":["tech"]},{"title":"RGW 安装和创建","url":"/2017/11/08/Ceph-RGW/","content":"\n<center>![Ceph RGW][2]</center>\n\n本文通过`ceph-deploy`安装和创建`RGW`，然后分别创建`S3`和`Swift`接口并提供了相应的方案。\n\n`Ceph RGW`基于`librados`，是为应用提供`RESTful`类型的对象存储接口。`RGW`提供两种类型的接口：\n\n* S3：兼容`Amazon S3 RESTful API`\n* Swift：兼容`OpenStack Swift API`\n\t\n`S3`和`Swift API`共享同一个命名空间，所以可以使用两种`API`访问相同的数据。\n\n<!--more-->\n\n参考链接: [Ceph：创建RGW][1]\n## 部署 RGW\n\n### 进入 ceph 目录\n\n```\ncd /etc/ceph\n```\n\n### 安装 ceph object gateway\n\n我这边是`node1`，根据`hostname`对号入座就行\n\n```\nceph-deploy install --rgw node1 \n```\n\n### Gather keys\n\n```\nceph-deploy gatherkeys node1\n```\n\n### 创建 rgw 实例\n\n```\nceph-deploy rgw create node1\n```\n\n### Ceph CLI\n\n`Ceph CLI`工具需要在管理员模式下运行，因此需要执行以下命令\n\n```\nceph-deploy admin node1\n```\n\n### 测试是否安装成功\n\n一旦`RGW`开始运行，就可以通过端口`7480`（如果没有修改的话）来访问。如： \n`http://node1:7480`，如果`RGW`运行正常，它应该返回类似的信息： \n\n```\nThis XML file does not appear to have any style information associated with it. The document tree is shown below.\n<ListAllMyBucketsResult xmlns=\"http://s3.amazonaws.com/doc/2006-03-01/\">\n<Owner>\n<ID>anonymous</ID>\n<DisplayName/>\n</Owner>\n<Buckets/>\n</ListAllMyBucketsResult>\n```\n\n## 创建 S3 用户\n\n想正常的访问`RGW`，需要创建相应的`RGW`用户，并赋予相应的权限，`radosgw-admin`命令实现了这些功能。\n\n其中`keys`中的`user`，`access_key`和`secret_key`用于之后的`S3`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin user create --uid=\"admin\" --display-name=\"First user\"\n2017-11-08 16:51:39.883217 7fb6868fac40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 创建 Swift 用户\n\n`Swift`用户是作为子用户`subuser`被创建的，执行以下命令：\n\n其中`swift_keys`中的`user`和`secret_key`用于之后的`swift`接口访问确认\n\n```\n[root@node1 ~]# radosgw-admin subuser create --uid=admin --subuser=admin:swift --access=full\n2017-11-08 16:55:05.371174 7fb4cbfc2c40  0 WARNING: detected a version of libcurl which contains a bug in curl_multi_wait(). enabling a workaround that may degrade performance slightly.\n{\n    \"user_id\": \"admin\",\n    \"display_name\": \"First user\",\n    \"email\": \"\",\n    \"suspended\": 0,\n    \"max_buckets\": 1000,\n    \"auid\": 0,\n    \"subusers\": [\n        {\n            \"id\": \"admin:swift\",\n            \"permissions\": \"full-control\"\n        }\n    ],\n    \"keys\": [\n        {\n            \"user\": \"admin\",\n            \"access_key\": \"1MWH3LWM1BS4ZF4HN5IH\",\n            \"secret_key\": \"cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O\"\n        }\n    ],\n    \"swift_keys\": [\n        {\n            \"user\": \"admin:swift\",\n            \"secret_key\": \"PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P\"\n        }\n    ],\n    \"caps\": [],\n    \"op_mask\": \"read, write, delete\",\n    \"default_placement\": \"\",\n    \"placement_tags\": [],\n    \"bucket_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"user_quota\": {\n        \"enabled\": false,\n        \"check_on_raw\": false,\n        \"max_size\": -1,\n        \"max_size_kb\": 0,\n        \"max_objects\": -1\n    },\n    \"temp_url_keys\": [],\n    \"type\": \"rgw\"\n}\n```\n\n## 测试 S3 接口\n\n需要创建一个`python`测试脚本来测试`S3`访问。该脚本会连接`RGW`，创建一个`bucket`并打印输出所有的`bucket`。其中，变量`access_key`和`secret_access`的值，来自于创建`S3`用户命令时，`radosgw-admin`命令返回的`keys->access_key`和`keys->secret_key`。\n\n### 安装 python-boto库\n\n执行以下步骤，首先安装`python-boto`库，该库用于连接`S3`：\n\n```\nyum install -y python-boto \n```\n\n### 创建脚本\n\n```\n#!/usr/bin/python  \n# -*- coding:utf-8 -*-  \n\nimport boto.s3.connection  \n   \naccess_key = '1MWH3LWM1BS4ZF4HN5IH'     \nsecret_key ='cuObxYgtl1lJgqNxOIpENycVqXfxLxZ8z5IXDM0O'  \nconn = boto.connect_s3(  \n        aws_access_key_id=access_key,  \n        aws_secret_access_key=secret_key,  \n        host='{hostname}',port={port},  \n        is_secure=False,calling_format=boto.s3.connection.OrdinaryCallingFormat(),  \n        )  \n   \nbucket = conn.create_bucket('my-new-bucket')  \nfor bucket in conn.get_all_buckets():  \n    print\"{name} {created}\".format(  \n            name=bucket.name,  \n            created=bucket.creation_date,  \n            )  \n   \n```\n\n需要将上面的`{hostname}`替换成对应的`hostname`或者`IP`,例如`192.168.1.1`；将`{port}`替换成`RGW`运行的端口，默认为`7480`；前者为字符串，后者为数字\n\n### 执行脚本测试\n\n```\n[root@node1 ceph]# python s3test.py \nmy-new-bucket 2017-11-01T10:58:53.670Z\n```\n\n## 测试 Swift 接口\n\n### 安装相关软件包\n\n```\npip installpython-setuptools    \npip installpython-swiftclient  \n```\n\n### 命令行访问\n\n替换`{ip}`，`{port}`和`{swift_secret_key}`，其中`{swift_secret_key}`为创建`Swift`用户时，`radosgw-admin`命令返回的`swift_keys->secret_key`的值\n\n```\nswift -A http://{ip}:{port}/auth/1.0 -Utestuser:swift -K '{swift_secret_key}' list \n```\n\n正常输出应该如下：\n\n```\n[root@node1 ceph]# swift -Ahttp://192.168.1.1:7480/auth/1.0 -U admin:swift -K 'PKRXACd8Ysgx7MCTjd9gHnL3sdpJ2J6wsuy2IS0P' list  \nmy-new-bucket \n```\n\n## 小结\n\n`ceph-deploy`命令，大大地简化了对`Ceph`集群和`RGW的`安装和配置，可以很快速的搭建测试环境，达到测试效果，但如果要在生产环境中应用的话，可能需要手动做一些配置就，不能完全依赖`ceph-deploy`了。\n\n[1]: http://blog.csdn.net/younger_china/article/details/73410918\n[2]: http://cdn.tony-yin.site/gateway-600x450.png\n","tags":["RGW"],"categories":["tech"]},{"title":"使用NFS挂载RBD","url":"/2017/10/31/RBD-Mount-NFS/","content":"\n<center>![RBD][3]</center>\n\n**具体场景**\n\n- 在`Server`端创建`RBD`块设备并挂载到某个目录，然后在`Client`端通过`NFS`将`Server`端之前说的目录再挂载到本地，就可以实现在客户端写文件通过`NFS`到`Server`端的块设备上\n- 这波操作的目的是因为测试`EC`性能时，`Ceph`版本还是`Hammer`，`CephFS`不是很稳定，性能不行，所以想用`RBD`的方式取代，但是`Hammer`版本中`EC`是只支持`RGW`的，并不支持`CephFS`和`RBD`，至于这个场景具体流程之后会在其他文章分享\n- 这波操作在之前`Ceph`的`Hammer`版本做过，这次是要在新版本`Luminous`上实践，由于遇到了很多问题，大部分都是新版本缘故引入的，故与大家分享\n\n> PS：`Luminous`新版本提供了新的`NFS`的处理方式，不再需要用这种老式的`RBD`的手段，之后也会在其他文章分享出来\n\n<!--more-->\n\n## Hammer版本\n\n操作步骤\n\n```\n1. 使用RBD方式导出NFS，步骤如下：\n\n\t1.1 create RBD block device image (size unit: MB)\n\t\t# rbd create --size 1024 ec-pool/test_image\n\t\t\n\t1.2 map RBD device\n\t\t# rbd map ec-pool/test_image\n\t\t\n\t\t[Optional] check mapped device\n\t\t# rbd showmapped\n\t\t\n\t1.3 create filesystem on the device\n\t\t# mkfs.ext4 /dev/rbd0\n\t\t\n\t1.4 mount it,\n\t\t# mkdir -p /vol/test_image\n\t\t# mount /dev/rbd0 /vol/test_image/\n\t\t\n\t1.5 export it, modify the /etc/exports\n\t\t# cat /etc/exports\n\t\t/vol/test_image *(rw,async,no_subtree_check,no_root_squash)\n\t\t# service nfs-kernel-server restart\n\t\t# exportfs -r\n\t\t\n\t1.6 check mount info for NFS server\n\t\t# showmount -e 192.168.1.167\n\t\n2. 在客户端以nfs方式mount上述导出的文件夹\n\t# mount -t nfs 192.168.1.167:/vol/test_image /mnt\n```\n\n## Luminous版本\n\n### Create RBD image\n\n之前`EC Pool`只能为`RGW`服务 ，直到`Luminous`版本，`EC Pool`也可以应用在`RBD`和`CephFS`，并且增加了`data-pool`这样的新特性，具体请参考官网：http://docs.ceph.com/docs/luminous/rados/operations/erasure-code/#erasure-coding-with-overwrites\n\n所以创建`image`的方式也有所改变\n\n#### Enable ec overwrites\n\n```\nceph osd pool set ec_pool allow_ec_overwrites true\n```\n\n#### Create pool and image\n\n由于`EC`不支持`omap`，所以在`CephFS`或者`RBD`场景使用`EC`时，需要将`data`存在`EC Pool`中，将`metadata`存在`Replicated Pool`中\n\n```\n// 创建pool\nceph osd pool create ec-pool 12 12 erasure  // ec pool, store data\nceph osd pool create md-pool 12 12 replicated   // replicated pool, store metadata\nceph osd pool create ssd-pool 12 12 replicated  // replicated pool, used as cache pool\n// 创建image\nrbd create --size 1024 --data-pool ec-pool md-pool/test_image\n```\n\n### Map RBD\n\n这一步之前的操作是`rbd map ec-pool/test_image`，所以当前需要执行\n\n```\n rbd map md-pool/test_image\n```\n\n但是一直`timeout`，通过查看`dmesg`和网上资料发现是因为`linux kernel`版本太低，所以升级了一下内核到`4.3`版本之后就可以`work`了\n\n升级内核版本请参考：\n> * [内核版本过低导致RBD Feature不支持][1]\n> * [RBD Feature][2]\n\n### Check map\n\n```\n rbd showmapped\n```\n\n### Create filesystem on RBD\n\n```\nmkfs.ext4 /dev/rbd0\n```\n\n### Mount\n\n```\n mkdir -p /vol/test_image\n mount /dev/rbd0 /vol/test_image/\n```\n\n### Modify export for nfs on server\n\n```\nvim /etc/exports\n/vol/test_image *(rw,async,no_subtree_check,no_root_squash) // Modify it to /etc/exports\n```\n\n#### Server\n\n##### Install NFS on server and client\n\n`Server`端执行一下操作\n\n##### Install\n\n```\nyum -y install nfs-utils rpcbind\n```\n\n##### Start service\n\n`nfs`依赖`rpcbind`，所以必须先启动`rpcbind`，这很重要\n\n```\nservice rpcbind start\nservice nfs start\nexportfs -r\n```\n\n#### Client\n\n客户端也执行以上操作，假设客户端`ip`为`192.168.1.1`，服务端为`192.168.1.2`\n\n##### Check mount\n\n查看是否可以挂载\n\n```\nshowmount -e 192.168.1.2\n```\n\n##### Mount server to client\n\n```\nmount -t nfs 192.168.1.2:/vol/test_image /mnt\n```\n\n## 总结\n\n遇到的主要问题\n\n* `EC Pool`创建`image`因为新版本改动\n* `rbd map`因为`linux kernel`版本低\n* `nfs`因为`nfs`在`rpcbind`前启动\n\n希望帮助大家尽量少些踩坑 ~~~\n\n作者： Tony\n日期： 2017-10-31 22:33\n\n[1]: http://www.jianshu.com/p/66a724a1f3af\n[2]: http://xiaqunfeng.cc/2017/06/06/ceph-rbd-map-failed/\n[3]: http://share.tony-yin.site/rbd-london-christmas-600x450.jpg\n","tags":["RBD"],"categories":["tech"]},{"title":"Python Profiler","url":"/2017/10/10/Python-Profiler/","content":"\n<center>![Python Profiler][10]</center>\n\n`OSD`启用过程耗时较长，需要进行性能优化。期间通过`python profilers`对代码进行性能分析和数据统计，有坑，有收获，总而言之，这是一个不错的工具\n\n<!--more-->\n\n## Profilers简介\n\n`python profilers`内置的主要有三种`cprofile`, `profile`和`hotshot`,`cprofile`是基于`profile`之上做的扩展，性能要比后者好很多，所以我用的就是`cprofile`\n\n更详细的介绍可以查看官网，`python profilers`的好处在于不用看教程，只要看着官网简短的概述，就能掌握其使用方法\n\n* [Python Profile][1]\n* [关于Python Profilers性能分析器][2]\n\n\n## Cprofile快速使用\n\n### 官网例子\n\n#### 代码\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")')\n```\n\n#### 分析结果\n\n```\n197 function calls (192 primitive calls) in 0.002 seconds\nOrdered by: standard name\nncalls  tottime  percall  cumtime  percall filename:lineno(function)\n     1    0.000    0.000    0.001    0.001 <string>:1(<module>)\n     1    0.000    0.000    0.001    0.001 re.py:212(compile)\n     1    0.000    0.000    0.001    0.001 re.py:268(_compile)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:172(_compile_charset)\n     1    0.000    0.000    0.000    0.000 sre_compile.py:201(_optimize_charset)\n     4    0.000    0.000    0.000    0.000 sre_compile.py:25(_identityfunction)\n   3/1    0.000    0.000    0.000    0.000 sre_compile.py:33(_compile)\n```\n\n#### 图解\n\n<center>![cprofile][3]</center>\n\n## Cprofile深入\n\n上面的基本用法可以在脚本中测试某个语句或者函数，然后打印到控制台。`cprofile`也可以将结果输出到文件中，这是比较常见的做法，因为打印到控制台，第一不能保存结果，第二如果数据量多没法全部浏览，并且影响阅读效果。而放在文件中还可以对结果进行按需排序、筛选等操作\n\n### 输出文件\n\n1. 参数方式\n\n```python\nimport cProfile\nimport re\ncProfile.run('re.compile(\"foo|bar\")', 'restats')\n```\n\n2. CLI方式\n\n```shell\npython -m cProfile [-o output_file] [-s sort_order] myscript.py\n```\n\n## Cprofile优雅使用\n\n上述方式可以使得`cprofile`在一些测试环境中受用，但是在一些复杂的环境中不能很好的`work`。由于`cprofile`是根据`python`在每个事件中存放的`hook`进行性能分析，所以在`cprofile.run()`的时候，要保证他就是最上层，他是调用的源头。\n\n但是实际场景中，我们经常会对一些`API`中的某个方法进行性能分析，如果在被调用处使用`cprofile`，会出现变量或者模块`undefined`的现象，模块不能识别还可以在`run`方法中引入，然后通过分号分隔，例如`cprofile.run(import re, re.compile(\"foo|bar\"))`，具体可以参考这篇文章：[Python Profile 工具性能分析][5]\n\n变量无法识别更是让人头疼，所以为了达到测试效果，你会不得不修改一些并不是很少量的源代码，并且测一个方法就要搞一次很麻烦。还有一些多进程或者跨机器的场景导致代码异步执行，这样`cprofile`更不能达到用户的需求\n\n还好我们可以通过`python`装饰器的机制来做，这样既不用改动源代码，也可以很方便的切换函数分析\n\n### 装饰器接口\n\n这里要注意设置全局变量\n\n```shell\nexport PROFILING=y\n```\n\n接口定义：\n\n```python\nimport cProfile\nimport pstats\nimport os\n# 性能分析装饰器定义\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"tottime\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n### 分析使用\n\n这时候只需要在调用的函数上面加一个装饰器即可\n\n```python\n@do_cprofile('filename')\ndef run():\n    print 'hello world'\n```\n## pstats分析工具\n\n`pstats`可以根据`cprofile`生成的文件进行排序、筛选等处理，呈现更主要的结果\n\n```python\nimport pstats\n \n# 创建Stats对象\np = pstats.Stats(\"result.out\")\n \n# strip_dirs(): 去掉无关的路径信息\n# sort_stats(): 排序，支持的方式和上述的一致\n# print_stats(): 打印分析结果，可以指定打印前几行\n \n# 和直接运行cProfile.run(\"test()\")的结果是一样的\np.strip_dirs().sort_stats(-1).print_stats()\n \n# 按照函数名排序，只打印前3行函数的信息, 参数还可为小数,表示前百分之几的函数信息 \np.strip_dirs().sort_stats(\"name\").print_stats(3)\n \n# 按照运行时间和函数名进行排序\np.strip_dirs().sort_stats(\"cumulative\", \"name\").print_stats(0.5)\n \n# 如果想知道有哪些函数调用了sum_num\np.print_callers(0.5, \"sum_num\")\n \n# 查看test()函数中调用了哪些函数\np.print_callees(\"test\")\n```\n\n上述代码摘自：[使用cProfile分析Python程序性能][4]，原文还提供了`pstats`命令行交互工具方式\n\n## 图形可视化\n\n上面的命令行界面的确是有点反人类，不易一下子清晰地分析性能瓶颈，有很多图形可视化工具可以帮助我们生成简洁明了的图片\n\n工具有：\n\n* gprof2dot\n* vprof\n* RunSnakeRun\n* KCacheGrind & pyprof2calltree\n\n最终我选择了`gprof2dot`，比较符合我的口味\n\n### 安装\n\n我的机器是`ubuntu`，其他类型机器找对应方式，具体参考：[Github gprof2dot][6]\n\n```\napt-get install python graphviz\npip install gprof2dot\n```\n\n注意：\n\n如果`pip`安装软件包报错：'Cannot fetch index base URL  http://pypi.python.org/simple/'\n\n解决办法\n\n```\n1. windows下创建/%user%/pip/pop.ini，并添加以下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n2. linux创建文件~/.pip/pip.conf，并添加一下内容。\n[global]  \nindex-url=http://pypi.douban.com/simple/\n3. 再次使用pip安装相应的包即可。\n```\n\n### 使用\n\n根据`cpofile`输出的文件生成图片，这边输出的文件名为`osd.out`，生成的图片名为`osd.png`\n\n```shell\ngprof2dot -f pstats osd.out | dot -Tpng -o osd.png\n```\n\n具体参考这篇文章：[Python优化第一步: 性能分析实践][7]，写的很好，也很具体\n\n### 效果图\n\n这是我进行性能分析产生的两张图\n\nPicture 1：\n<center>![enable_osd1][8]</center>\n\nPicture 2：\n<center>![enable_osd2][9]</center>\n\n## 继续深入\n\n* `pstats`深入了解\n* `gprof2dot`深入了解\n* 其他的可视化工具\n\n不过最终的目的都是通过性能分析找到性能瓶颈，然后进行优化，适合自己的就好\n\n[1]: https://docs.python.org/2/library/profile.html\n[2]: http://www.cnblogs.com/btchenguang/archive/2012/02/03/2337112.html\n[3]: http://images.cnblogs.com/cnblogs_com/btchenguang/201202/201202031502512968.png\n[4]: http://xianglong.me/article/analysis-python-application-performance-using-cProfile/\n[5]: http://ju.outofmemory.cn/entry/46805\n[6]: https://github.com/jrfonseca/gprof2dot\n[7]: https://zhuanlan.zhihu.com/p/24495603/\n[8]: http://share.tony-yin.site/enable_osd3.png\n[9]: http://share.tony-yin.site/enable_osd3-2.png\n[10]: http://share.tony-yin.site/publicenemyno1-600x400.png\n","tags":["性能优化"],"categories":["tech"]},{"title":"启用OSD性能剖析脚本","url":"/2017/10/10/Enable-OSD-Profile-Script/","content":"\n\n<center>![Profile][1]</center>\n\n针对`OSD`启用过程缓慢进行性能剖析，用到了一系列的脚本，例如批量创建`OSD`,批量启用`OSD`,`Cprofile`装饰器接口等\n\n<!--more-->\n\n`docprofile.py`：提供`cprofiler`的装饰器接口\n\n```python\nimport cProfile\nimport pstats\nimport os\ndef do_cprofile(filename):\n    def wrapper(func):\n        def profiled_func(*args, **kwargs):\n            print filename\n            # Flag for do profiling or not.\n            DO_PROF = os.getenv(\"PROFILING\")\n            if DO_PROF:\n                profile = cProfile.Profile()\n                profile.enable()\n                result = func(*args, **kwargs)\n                profile.disable()\n                # Sort stat by internal time.\n                sortby = \"cum\"\n                ps = pstats.Stats(profile).sort_stats(sortby)\n                ps.dump_stats(filename)\n            else:\n                result = func(*args, **kwargs)\n            return result\n        return profiled_func\n    return wrapper\n```\n\n`pstat.py`：用于处理生成的`cprofiler`文件，做一些排序，截取和打印等工作,默认按`culativetime`排序，打印前`100`条记录，有其他需求的可以适当修改参数\n\n```python\nimport pstats, sys\nfile = sys.argv[1]\np = pstats.Stats(file)\np.strip_dirs().sort_stats('cum').print_stats(100)\n```\n\n[1]: http://share.tony-yin.site/pozza-600x450.png\n","tags":["Profile"],"categories":["tech"]},{"title":"批量创建和删除磁盘分区脚本","url":"/2017/10/02/Batch-Create-And-Delete-Disk-Partition-Script/","content":"\n<center>![Disk Partition][1]</center>\n\n最近在做批量启用`OSD`的性能优化，需要反复的删除磁盘分区，以保证彻底的删除数据，然后就要批量创建分区，保证足够量的分区数量创建足够多的`OSD`，由于数量较多，每次手动搞都要二十几次，所以针对创建和删除写了两个脚本，支持单一、批量和选择大小等功能\n\n<!--more-->\n\n## 清空分区\n\n### 脚本名称 \n\n`clean_disks.py`\n\n### 使用说明\n\n* `python clean_disks.py [disk_name1, disk_name2 ...]` \n* 支持单磁盘和多磁盘清空\n* 支持脚本参数，如果没有参数，默认清空所有磁盘分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会清空系统磁盘，脚本会做过滤工作\n\n### 使用范例\n\n* 清空所有磁盘分区\n\n```\npython clean_disks.py\n```\n\n* 清空`sdb`的分区\n\n```\npython clean_disks.py sdb\n```\n\n* 清空`sdb`和`sdc`的分区\n\n```\npython clean_disks.py sdb sdc\n```\n\n### 脚本代码\n\n> `do_exec`是我自己写一个模块，可以调用`shell`命令，所以用的时候需要转换一下\n\n```\nimport sys\nimport do_exec\n\ndef clean_disk(disk_name):\n    print 'disk: {} clean start ...'.format(disk_name)\n    do_exec('sgdisk -Zog /dev/{}'.format(disk_name))\n    print 'disk: {} clean done ...'.format(disk_name)\n\nmount_info = do_exec('mount')\nsys_disk_name = mount_info[5:8]\n\nif len(sys.argv) > 1:\n    disks = sys.argv[1:]\n    for disk_name in disks:\n        if disk_name == sys_disk_name:\n            print '{} is system disk, can\\'t be clean!'.format(disk_name)\n        else:\n            clean_disk(disk_name)\nelse:\n    all_disks = do_exec('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                clean_disk(disk_name)\n```\n\n## 创建分区\n\n### 脚本名称\n\n`create_partitions.py`\n\n### 使用说明\n\n* `python create_partitions.py size num [disk_name1, disk_name2 ...]`, `size`为分区大小，`num`为分区数\n* 支持单磁盘和多磁盘创建分区\n* 支持脚本参数，如果没有参数，默认为所有磁盘创建分区，加磁盘参数则可清空指定磁盘分区, 多磁盘则累加多位磁盘参数\n* 为了保证安全，脚本不会为系统磁盘创建磁盘分区，脚本会做过滤工作\n\n### 使用范例\n\n* 为`sdb`和`sdc`分别都创建两个大小为`100G`的分区\n\n```\npython create_partitions.py 100G 2 sdb sdc \n```\n\n* 为所有磁盘创建三个大小为`10G`的分区\n\n```\npython create_partitions.py 10G 3\n```\n\n* 为`sdc`创建四个大小为`1T`的分区\n\n```\npython create_partitions.py 1024G 4 sdc\n```\n\n### 脚本代码\n\n```\nimport sys\nimport do_exec\n\nsize = sys.argv[1]\nnum = sys.argv[2]\nmount_info = do_exec('mount')\nsys_disk_name = mount_info[5:8]\n\ndef parted_disks(num, size, disk):\n    for i in range(int(num)):\n        do_exec('sgdisk -n {}:0:+{} /dev/{}'.format(i+1, size, disk))\n        print 'disk {} partition {} done ...'.format(disk, i+1)\n\nif len(sys.argv) > 3:\n    disks = sys.argv[3:]\n    for disk in disks:\n        if disk == sys_disk_name:\n            print '{} is system disk, can\\'t be parted!'.format(disk)\n        else:\n            parted_disks(num, size, disk)\nelse:\n    all_disks = do_exec('lsblk').splitlines()\n    for disk in all_disks:\n        if (disk.startswith('sd')):\n            disk_name = disk.split()[0]\n            if disk_name != sys_disk_name:\n                parted_disks(num, size, disk)\n```\n\n\n[1]: http://cdn.tony-yin.site/floppy_disk_600-450.png\n","tags":["Script"],"categories":["tech"]},{"title":"JS判断对象是否为空","url":"/2017/09/28/JS-Object-Null/","content":"\n<center>![JS][3]</center>\n\n今天在改代码的时候中发现一个`Bug`，通过`debug`发现最终的原因便是判断对象是否为空导致的错误，一个小小的错误反映出`Javascript`这门语言的不严谨性，也提醒我们得熟悉它的一些技巧和原理，避免经常在一些问题上踩坑\n\n<!--more-->\n\n场景还原：通过`if(object)`判断对象内容不为空，\n\n```\nobj = {};   // obj其实是通过其他方式获得，但是它的内容为空\nif (obj) {\n        xxx;    // 判断当obj不为空的时候，执行判断内的代码\n}\n```\n\n那么问题就出现了，即使对象内容为空，这个判断代码块里面的代码还是会被执行\n用`chrome`简单的验证了一下：\n\n<center>![object empty][1]</center>\n\n这和一些其他语言不一样，比如`php`,`python`是可以通过`if(object)`来检查对象是否为空的，但是`JS`不可以，这个问题要不是遇到了肯定想不到\n\n那么`JS`如何判断对象内容是否为空呢？网上搜了下，大概有以下方式：\n\n### 通过JSON.stringify将对象转换成字符串进行判断\n\n```\nvar tt = {};\nif (JSON.stringify(tt) == \"{}\") {\n        console.log('tt is not empty!');\n}\n```\n\n### 通过读取对象元素\n\n```\nvar tt = {};\nvar t = {id: 1};\nif (tt.id) {\n        console.log('tt is not empty!');\n} \nif (t.id) {\n        console.log('t is not empty');\n}\n```\n\n### 总结\n\n方法1相对于方法2来说更加严谨，阅读性高，而方法2的效率却比方法1高\n\n### jQuery提供了判断对象是否为空的API\n\n如果用的是`jquery`的话，可以使用其提供的现有方法：`isEmptyObject`,这是`jQuery1.4`版本后加入的功能，具体用法：\n\n```\nvar tt = {}; \nif ($.isEmptyObject(tt)) {  // jQuery.isEmptyObject(tt)这样调用也可以\n        console.log('tt is empty!');\n}\n```\n\n### 判断对象是否存在\n\n借助上面的问题，顺便了解了一下：[如何判断Javascript对象是否存在][2]\n\n这篇文章里面讲的还是比较全面的，针对不同场景给出了不同的方案\n\n\n\n[1]: http://cdn.tony-yin.site/object_empty.png\n[2]: http://www.ruanyifeng.com/blog/2011/05/how_to_judge_the_existence_of_a_global_object_in_javascript.html\n[3]: http://cdn.tony-yin.site/jsbanana-600-450.png\n","tags":["Javascript"],"categories":["tech"]},{"title":"OSD创建和删除全过程","url":"/2017/09/27/OSD-Create-And-Delete/","content":"\n<center>![create and delete process][1]</center>\n\n`OSD`是`Ceph`中最基本也是最常用的功能，所以经常的创建和删除操作少不了，然后这个两个过程并不是一两个命令那么简单，本文就`OSD`的创建和删除进行内容进行步骤分离和讲解，结尾还有一份一键删除指定`OSD`的脚本\n\n<!--more-->\n\n## 创建过程\n\n### 创建一个OSD\n\n```\nceph osd create [uuid]   #若没有uuid参数，则该命令自动生成一个uuid。该命令产生一个新的osd-number\n```\n\n### 为新的OSD创建默认的工作目录\n\n```\nmkdir -p /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 为新的OSD准备一块磁盘\n\n```\nmkfs.xfs -f /dev/vde \nmount /dev/vde /var/lib/ceph/osd/ceph-{osd-number}\n```\n\n### 初始化新的OSD工作目录\n\n```\nceph-osd -i {osd-number} --mkfs --mkkey\n```\n\n>  注意：在执行上述命令前要求新的OSD工作目录必须为空\n\n### 注册新的OSD认证密钥\n\n```\nceph auth add osd.{osd-number} osd 'allow *' mon 'allow rwx' -i /var/lib/ceph/osd/ceph-{osd-number}/keyring\n```\n\n### 将新的OSD添加到Crush Map中\n\n```\nceph osd crush add osd.{osd-number} {weight} [{bucketype}={bucket-name}......}\n```\n\n此步骤也可以添加buckets后再添加osd，即：\n\n```\nceph osd crush add-bucket node5 host     #创建一个名字为node5的bucket\nceph osd crush move node5 root=default    #将创建出来的bucket放到root下\nceph osd crush create-or-move osd.{osd-number} 1.0 root=default host=node5     #将新的OSD添加到node5下\n```\n\n### 将新的OSD信息填写到ceph.conf中\n\n```\n[osd.{osd-number}]\nhost = {hostname}\ndevs = /dev/vde\n```\n\n### 启动新的OSD\n\n```\n/etc/init.d/ceph start osd.{osd-number}\n```\n\n> 此时通过集群状态查看命令#ceph -s可以看到OSD数量以及up和in的数量都发生了变化，此时再通过命令#ceph -w可以看到ceph经过peering状态后，最终达到active+clean状态\n\n## 一键创建OSD脚本\n\n```\n#! /bin/bash\n\nstart_time=`date +%s`\n\necho \"start time: `date -d @$start_time \"+%Y-%m-%d %H:%M:%S\"`\"\n\ndisk=/dev/$1\n\nosd_id=`ceph osd create`\n\nosd_dir=/data/osd.$osd_id\n\nhost=10.16.100.99\n\nbucket=default_$host\n\necho \"osd $osd_id is created ...\"\n\nmkdir -p $osd_dir\n\necho \"osd directory: /data/osd.$osd_id is created ...\"\n\nmkfs -t ext4 -m 0 $disk\n\necho \"disk $disk is built with ext4 file system ...\"\n\nmount -o noatime,user_xattr $disk $osd_dir\n\necho \"device: $disk is mounted on directory: $osd_dir ...\"\n\nceph mon getmap -o /tmp/monmap\n\nceph-osd -i $osd_id --monmap /tmp/monmap --mkfs --mkjournal\n\necho \"osd $osd_id is initialized ...\"\n\nosd_uuid=`ceph-osd -i $osd_id --get-osd-fsid`\n\ncat >> /etc/ceph/ceph.conf <<EOF\n\n[osd.$osd_id]\nhost = $host\npublic addr = $host\ncluster addr = $host\nosd uuid = $osd_uuid\nEOF\n\necho 'ceph config file is configured ...'\n\nservice ceph start osd.$osd_id\n\necho \"osd $osd_id start ...\"\n\nceph osd crush add $osd_id 0 pool=default host=$bucket\n\necho \"osd $osd_id is added in crush ...\"\n\necho 'all works done ...'\n\nend_time=`date +%s`\n\necho \"end time: `date -d @$end_time \"+%Y-%m-%d %H:%M:%S\"`\"\n\ntime_consuming=$(($end_time - $start_time))\n\necho \"The total time consuming is $time_consuming s\"\n```\n\n## 删除过程\n\n### 将需要删除的OSD在集群中的状态修改为out状态\n\n假定`osd`的`id`为`1`\n\n```\nceph osd out osd.1\n```\n\n### 停止OSD进程\n\n```\n/etc/init.d/ceph stop osd.1\n```\n\n### 将OSD从crush map中删除\n\n```\nceph osd crush remove osd.1    #删除指定的OSD\nceph osd crush remove node1    #删除OSD所在的bucket（此步骤可以不做）\n```\n\n### 清除到OSD的认证密钥\n\n```\nceph auth del osd.1\n```\n\n### 在OSD Map中清除OSD\n\n```\nceph osd rm 1\n```\n\n### 从ceph.conf中清除OSD\n\n```\n[osd.1]\n    host = {hostname}\n    ...\n```\n\n## 一键删除OSD脚本\n\n```\n#! /bin/bash\n\nosd_id=$1\nceph osd out osd.$osd_id\n/etc/init.d/ceph stop osd.$osd_id\nceph osd crush remove osd.$osd_id\nceph auth del osd.$osd_id\nceph osd rm $osd_id\n# 清空 ceph.conf\n```\n\n参考链接：\n> https://my.oschina.net/linuxhunter/blog/539312\n\n[1]: http://cdn.tony-yin.site/5ways-facebook-600-450.png\n","tags":["OSD"],"categories":["tech"]},{"title":"如何打造一个好的产品","url":"/2017/09/21/How-To-Forge-Product/","content":"\n<center>![product][1]</center>\n\n今天`BOSS`带来一个曾经的得力干将。目前是腾讯一家游戏公司的老总，职位为`Producer`，在产品方面分享很多观点，受益匪浅。下面是分享过程中我总结的一些自己觉得正确的观点和心得\n\n> 虽热目前我是一个只负责敲代码的程序猿，产品这种事情好像跟我没多大关系，应该是产品经理的事情，但是其实一个`coder`想要进步，不仅仅是技术水平越来越强，对产品的理解也非常重要。这能理清自己逻辑，能够发散思维，能够帮你做出更好的产品，你开发出一个没人用的产品和一个上线客户量很大的产品对你自己来说无论是成就感还是技术提升都是天壤之别。也许未来我们可能也会创业，那到时候我们的职责可能是老板，可能是产品，可能是研发，更可能是各种职能集一身，对产品拥有良好理解和恰当的打磨技巧对我们来说都是宝贵的财富。\n\n<!--more-->\n\n## 技能心态\n\n### 工作强度 \n\n`BAT`加班不比任何一家中小公司少，腾讯一直把自己当做创业型公司，时刻保持压力和紧张感，这或许是其成功的一个很重要的原因（意料之外，之前没想到`BAT`工作强度这么大）\n\n### 不用过多的Design \n\n因为没有一个产品可以凭借设计文档一步到位的，几乎每个成功的产品都和原来的设计有着天壤之别（仁者见仁，智者见智，把握一个度吧，没必要死扣设计为了最完美化而浪费大量的时间）\n\n### 没有具体的Deadline \n\n每天都是`Deadline`，每天规定的任务做好了再回去\n\n### 谁的声音大听谁的 \n\n不要在内部为了争执一个方案纠结和浪费太多时间，谁的声音大听谁的，因为这绝不是最后一次修改（有性格）\n\n### 产品是每个人的\n\n一个人不应该被他的职位所局限，产品团队中的每个人都应该为这个产品在设计、宣传和包装等方面提意见\n\n### Just Do It \n\n很多事情自己不做一下是不知道的，很多行为看起来很low，不符合你的职位，但是只有真正去做了，才代表走过这条路，你才知道这条路可不可行，还有哪些其他路可以尝试（那位大佬制作某款卡牌游戏时，自己找了同类`50`多种游戏都玩了一遍，而且都玩到了很`6`的地步，用他的话说最后玩到吐。。。）\n\n### 相信自己的潜力 \n\n永远不要觉得自己是做技术的，所以理所当然`UI`不擅长，宣传不擅长，包装不擅长，做一行就要把自己的头埋进去，逼着自己不断尝试，慢慢可能会发现自己做的不错（他们经常会亲自去`QQ`群、贴吧和其他社交平台宣传，我去，托这种事原来真的存在）\n\n### 学会成熟 \n\n对很多事情都要慢慢做到心里有底，比如清楚上线必有`Bug`，设计必会被颠覆（他说上线没问题他会很慌。。。）\n\n### 明确最终目的 \n\n要明确做产品，开发等工作的目的不是按时完成任务，而是要把产品做起来，最终目的是赚钱，这样你做其他类型的事情就可以想明白很多，有时候开发并不一定是最优先最重要的，植物，但是作为一个开发经历了这些事情你的思维会不一样，做事方向也会不一样\n\n### 观察用户习惯\n\n可以尝试花钱请人来使用产品，内部开发人员在背后观察它的使用习惯，不轻易提示\n\n### 用数据说话\n\n当产品上线后，通过数据采集和分析，哪个页面或者操作用户比较感兴趣，哪一步开始用户量变少，出现这种情况，可以尝试做几个版本，然后让用户使用，选择最优的方案（在这个环节不以职位高低说话，完全靠数据评断）\n\n### 要相信自己的技术\n\n相信自己可以实现用户提的所有需求，当你全身心投入一件事的时候，手段很多，解决的方案也很多，比如这位大佬述说他收集一款游戏的同类游戏的经历，收集渠道除了iOS的还有android的，还有韩国市场的，还有模拟器的，还有页游等等（只有想不到，没有做不到）\n\n### 抄袭不丢脸\n\n互联网产品从`0`到`1`很少很少，大家在做一个产品之前都会看看同类产品怎么做的，然后学习每个产品中的精华部分，这个说直接一点就是抄袭，但是不要因为这个觉得丢脸，因为抄袭容易，抄袭得好并不容易（谈到腾讯，大家都会因为**抄袭**而骂声不绝，但是静下想想这的确是个正常套路，一切为了快速出产品）\n\n### 不要害怕被告\n\n等哪天你某些方面抄袭别人的，导致被告，那么说明你已经做大了，产品在某个方面来说已经相当成功了\n\n### 被骂是极好的\n\n很多时候，用户会因为产品一些蹩脚的地方而骂声不断，这时候不要觉得丢脸或者懊恼，相反应该开心，因为很多时候别人愿意骂你说明很大程度上还是很喜欢这个产品的，而真正不喜欢的懒得留下评论\n\n## 产品本身\n\n### 快速出Demo\n\n以上说的不用过多`Design`也好，还是不要过分争执也好，都是为了尽快做出一个产品`Demo`，就是最起码可以一个`work`的东西，这很重要，一旦`work`之后很多想法就会不一样了，他们往往两个星期出一个`Demo`，这对他们`UI`来说很有挑战，因为一般设计就要一个星期，然后原型图又是一个星期，但是两个星期就得弄出来，不管是借鉴别人的也好，还是网上买模板也好\n\n### 不用太完善 \n\n快速做出来不是说实现所有功能和面面俱到，而是可以做到静态页面或者是设计图去找用户，询问他们的观点和建议，这样既可以缩短时间，也可以通过询问客户的意见走向正确的道路，避免不必要的返工\n\n### 尽快让产品上线\n\n产品上线和不上线是天壤之别，因为自己内部永远是在想当然，而客户的反馈才是最重要的，这样才会往正确的方向不断迭代\n\n### 永远不要给用户看到错误信息\n\n能解决的让系统自己解决，不能解决的人工处理，不要太**实诚**，否则用户会感觉到惊恐\n\n### 不要给用户太多选择 \n\n如果一个页面存在两个按钮让用户点击，`80%`的用户会点`A`按钮，`20%`的用户会点`B`按钮，那么页面就显示一个`A`按钮就好了，只要保证`B`按钮能够在页面其他地方能找到就可以了，太多选择会让用户心里没底自己选择的对不对（相信大家脑海里都会浮现某个用户做完一个操作询问对不对的场景吧？）\n\n### 数字必有颜色\n\n这是一个约定俗成的习惯，保证页面的数字有合适的颜色，让用户一眼就能看到最想呈现的元素\n\n### 科技感很重要\n\n要让页面充满科技感，让别人觉得这是一家充满互联网气息的新公司，第一印象好\n\n### 恰当的图标\n\n图标选择要有意义，凸出按钮的作用\n\n### 恰当的名词\n\n用词准备具体，不要让用户去猜，比如下载就是下载，不要说保存\n\n### 易懂高大上的名词\n\n用词不要太专业，这样会让用户不明白，可以高大上一些，比如远程数据复制可以叫云复制\n\n### 抓住用户的期望\n\n产品介绍要突出用户最期望的东西，而不是过分强调技术上面的高档，这不是客户最期望的东西，比如存储你与其说内部实现机制怎么分片，怎么自动控制，不如和用户说我能保证多少数据量的存储，能保证永远不会因为故障丢失数据，能存储多长时间等等\n\n### 调节气氛的话语\n多用网络诙谐的话语进行提示，比如亲，朕等等，这样会舒畅用户的心情\n\n\n作者： Tony\n日期： 2017/09/19\n\n[1]: http://cdn.tony-yin.site/product-600-450.jpg\n","tags":["产品"],"categories":["summary"]},{"title":"PHP-Excel 开发解决方案","url":"/2017/09/18/PHP-Excel/","content":"\n<center>![office][1]</center>\n\n 过去工作中使用`PHPExcel`较多，碰到并解决了各种大大小小的问题，总结出这样一篇文章，一方面记录自己踩过的坑，一方面与大家分享，让大家少走弯路，并不断完善之，欢迎大家去`github`上面`star`和提交`pull request`，不断补充和优化，打造最全面的`PHPExcel`开发解决方案\n\nGithub 地址：https://github.com/tony-yin/PHPExcel_Road\n原文地址：https://tony-yin.github.io/2017/09/18/PHP-Excel/\n\n<!--more-->\n\n## 基础：小试牛刀\n\n### 1. 引用文件\n\n```\nyourpath . /phpexcel/PHPExcel.php\n```\n\n### 2. 实例化phpexcel类\n\n```\n$excel = new PHPExcel();\n```\n\n### 3. 获取当前单sheet（多sheet会在下面讲）\n\n```\n$objexcel = $excel->getActiveSheet();\n```\n\n### 4. 合并单元格\n\n```\n$objexcel->mergeCells('A1:M1');\n```\n\n### 5. 获取一个cell的样式\n\n```\n$objexcel->getStyle('A1');\n```\n\n+ 获取一个cell的字体样式\n\n```\n$cellFont = $objexcel->getStyle('A1')->getFont();\n```\n+ 设置字体大小\n\n```\n$fontStyle->setSize(15);\n```\n+ 设置字体是否加粗\n\n```\n$fontStyle->setBold(true);\n```\n+ 设置字体颜色\n\n```\n$fontStyle->getColor()->setARGB(PHPExcel_Style_Color::COLOR_RED);\n```\n+ 获取一行样式\n\n```\n$rowStyle = $objexcel->getStyle(1)->getRowDimension();\n```\n+ 设置行高度\n\n```\n$rowStyle->setRowHeight(2);\n```\n+ 获取一列样式\n\n```\n$columnStyle = $objexcel->getStyle('A')->getColumnDimension();\n```\n+ 设置列宽度\n\n```\n$columnStyle->setWidth(10);\n```\n+ 获取一列对齐样式\n\n```\n$alignStyle = $objexcel->getStyle('A')->getAlignment();\n```\n+ 设置水平居中：同一水平线上居中，即为左右的中间\n\n```\n$alignStyle>setHorizontal(PHPExcel_Style_Alignment::HORIZONTAL_CENTER);\n```\n+ 设置垂直居中：同一垂直线居中，即为上下的中间\n\n```\n$alignStyle->setVertical(PHPExcel_Style_Alignment::VERTICAL_CENTER);\n```\n+ 自动换行\n\n```\n$$alignStyle->setWrapText(true);\n```\n\n### 6. 获取指定版本excel写对象\n如需更早的版本可将`Excel2007`换成`Excel5`\n```\n$write = PHPExcel_IOFactory::createWriter(\"xcel, 'Excel2007');\n```\n\n## 进阶：一些有用的小知识\n\n### 1.行列数字索引方法\n> `phpexcel`一般获取`cell`或者获取列都是通过`ABC`这样的英文字母获取的，它也可以通过`0、1、2、3`这样的数字表示sheet中的列，从`0`开始，`0`对应`A`，`1`对应`B`，基本上大多数方法都是数字行列索引，例如`getStyleByColumnAndRow($col,$row)`,默认列参数在前，行参数在后，更多的可以参加`phpexcel`源码；\n\n### 2. 单行或单列参数格式\n> 有的时候一个方法需要行列两个参数，例如只需要某一行参数可写成`(null, $row)`,例如只需要获得某一列参数可写成`($col, null)`\n\n### 3. 列的数字索引格式和字母索引格式互转\n\n+ 数字转字符串\n\n```php\nPHPExcel_Cell::columnIndexFromString('A');  // Return 1 not 0;\n```\n+ 字符串转数字\n\n```php\nPHPExcel_Cell::stringFromColumnIndex(0);    // Return 'A';\n```\n\n### 4.PHPExcel读取数字类型 \n> `PHPExcel`读取的`cell`数字，类型都是`double`型，可用`gettyle()`方法检测类型，当初我一直使用`is_int()`方法无果，搞得焦头烂额。。。\n\n### 5. 多cell边框线设置\n`PHPExcel`生成的表格如果你不加处理，是不会帮你生成边框线的，生成边框线的方法如下：\n```php\n$borderArray = array(\n    'borders' => array(\n        'allborders' => array(\n            'style' => PHPExcel_Style_Border::BORDER_THIN\n        )\n    )\n);\n$objexcel->getStyle($col1, $row1,$col2, $row2)->applyFromArray($borderArray);\n```\n>注：\n> 1: `getStyle()`可以看需求改为`getStyleByColumnAndRow()`方法通过数字行列索引读取`style`\n> 2: array中`PHPExcel_Style_Border::`后面有三种格式分别是`BORDER_THIN`和`BORDR_MEDIUM`，表示边框线的粗细；\n> 3: `getStyle()`中的索引可以是静态的，也可以是动态的，一般是在导出`excel`的数据`set`完毕后填写左上角的单元格行列索引和右下角的单元格行列索引；\n\n> 参考资料\n>\n> http://phpexcel.codeplex.com/workitem/22160\n> http://phpexcel.codeplex.com/workitem/20150\n\n### 6. 多cell字体加粗处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(  \n        'bold' => true,                                              \n    ),                                                               \n));\n```\n\n### 7. 多cell字体颜色处理\n\n```\n$objexcel->getStyle($pCoordinate)->applyFromArray(array(\n    'font' => array(\n        'color' => array(\n            'rgb' => 'ff0000',\n            ),\n        ),\n    )\n);\n```\n\n### 8. 多sheet导入\n\n动态为当前`sheet`设置索引，然后获取当前`sheet`，便可循环读取每一个`sheet`内容\n\n```php\n$objexcel->setActiveSheetIndex($index);   //$index = 0 1 2 3\n$objexcel->getActiveSheet();    //return sheet1 sheet2 sheet 3\n```\n\n### 9. 固定格式excel读取在写入\n\n> 当需求是给定一个一个模板`excel`，需要往里面塞数据，我们不一定要通过代码给它设定样式，如果这个模板变化不大，我们完全可以存放一个格式相同的静态文件，然后通过`PHPExcel`读取，再往里面塞数据，最后进行保存操作，可以达到一样的效果，并且可以节省大量的资源。\n\n### 10. 合并单元格导入问题\n\n> 在特殊的表格中，合并单元格普遍存在，而多个单元格合并成的一个单元格，只能`setValue()`一次，而我们如何判断合并单元格的具体行列呢？\n\n```php\n$range = $start_cell->getMergeRange();  // 通过合并单元格的开始单元格比如‘A1’，获取合并范围‘A1:A4’\n$cell->isInRange($range);    // 遍历之后每一个单元格便可通过isInRange()方法判断当前单元格是否在合并范围内\n```\n## 高级：特殊场景特殊手段\n\n### 1. 单元格文本格式数据处理 \n> 一般excel单元格中数据的格式为数据类型，而`PHPExcel`中的`getValue()`方法读取的也是数据类型，当把数据从数据类型改为文本类型后，在`PHPExcel`中读出来的是`PHPExcel_RichText`类型，`getValue()`读取返回`PHPExcel_RichText`是一个`object`类型（`PHPExcel_RichText`数据保存格式）；那如何读取这一类的数据呢？仔细查看读取出来的对象，不难发现有`getPlainText()`这样的方法可以读取文本类型数据，所以我们只要判断当当前数据为文本数据时用`getPlainText()`读取，一般数据用`getValue()`读取\n\n```php\nif ($cell->getValue() instanceof PHPExcel_RichText) {\n    $value = $cell->getValue();\n} else {\n    $value = $cell->getValue();\n}\n```\n> 参考资料\n>\n> http://www.cnblogs.com/DS-CzY/p/4955655.html\n> http://phpexcel.codeplex.com/discussions/34513\n\n### 2. 单元格数据算法处理\n> `excel`拥有强大的算法功能，一般算法格式为`=A3+A4`这类的，复杂的更多，如果使用`PHPExcel`提供的默认读取方法`getValue()`读取出来的结果则为字符串`=A3+A4`,好在`PHPExcel`也足够强大，提供了相应的接口：`getCalculatedValue()`，这个方法专门读取算法数据，但是我们不能将这个方法作为默认读取方法，因为这样可能会将一些本来要读成字符串的读成算法数据，而且`PHPExcel`没有将它作为默认读取方法的另一个重要原因就是算法方式读取很耗时间和性能，一般数据读取根本没有必要这样浪费资源，所以我们可以采用以下这种方式\n\n```php\nif (strstr($cell->getValue(), '=')) {   \n    // 判断如果cell内容以=号开头便默认为算法数据\n    $value = $cell->getCalculatedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 3. 日期数据处理\n\n> 除了以上所说的文本数据和算法数据外，我还遇到过日期类型数据，比如`2016-12-28`输入到`excel`中，它会默认转换成`2016/12/28`，如果采用一般的`getValue()`方式读取也会读取到错误的数据，`PHPExcel`也提供了相应的接口`getFormattedValue()`,并提供了适配的识别方式`PHPExcel_Shared_Date::isDateTime($cell)`,所以代码就很好实现了\n\n```php\nif (PHPExcel_Shared_Date::isDateTime($cell)) {\n    $value = $cell->getFormattedValue(); \n} else {\n    $value = $cell->getValue();\n}\n```\n\n### 4. 读取方法封装\n> 针对`excel`各种数据类型，我们可以写一个函数，将原有的`getValue()`封装一下，这样以后就不用每次都判别一下数据类型了，目前我只遇到上面三种特殊格式，如果有新的，欢迎大家补充，封装函数如下\n\n```php\nfunction get_value_of_cell($cell) {\n    if (strstr($cell->getValue(), '=')) {   \n        $value = $cell->getCalculatedValue(); \n    } else if ($cell->getValue() instanceof PHPExcel_RichText) {\n        $value = $cell->getValue();\n    } else if (PHPExcel_Shared_Date::isDateTime($cell)) {\n        $value = $cell->getFormattedValue(); \n    } else {\n        $value = $cell->getValue();\n    }\n}\n```\n\n### 5. 导出文件在IE、360等浏览器中文件名中文乱码问题\n\n```php\n$filename = 'xxx导出表';\n// 判断如果是IE内核形式的浏览器采用urlencode处理文件名\nif (!preg_match(\"/Firefox/\", $_SERVER[\"HTTP_USER_AGENT\"])) {\n    $filename = urlencode($filename);\n}\n```\n\n> 未完待续。。。\n> 期待你的补充和优化\n\n[1]: http://cdn.tony-yin.site/office-card-game.png\n","tags":["PHPExcel"],"categories":["tech"]},{"title":"磁盘反复拔插记录","url":"/2017/09/10/Disk-pull-and-plug-record/","content":"\n<center>![pull disk][10]</center>\n\n最近在做磁盘拔插后的`OSD`热恢复功能。在反复拔插盘的实践过程中，遇到了一个难题，就是时常会由于拔盘，然后再插盘后盘符发生了改变，`kernel`和`udev`接收不到任何`device`增加或删除的事件消息，\n由于我是基于`udev`监测磁盘的插拔，所以一旦感知不到的话，就无法进行相应的恢复操作，所以在此对拔盘的记录进行一些记录，并在大量操作中得出总结。\n\n<!--more-->\n\n## Operation Record\n\n`normal`: 表示正常情况，没有被拔\n\nenv | udev monitor | UI data | disk alias | device | lsblk | mount info | mount point \n:--:|:----:|:----:|:---:|:---:|:---:|:---:|:---:|\nnormal | - | /dev/sdg1 | osd4-data | /dev/sdg | exist | exist | /data/osd.1\nfirst pull | remove sdg | empty | remove | remove | remove | exist | exist, readerror\nfirst plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\nsecond pull | no remove | empty | osd4-data | /dev/sdd | not change | exist | exist, readerror\nsecond plug | All Not Change | - | - | - | - | - | - \nclear mount | - | - | exist | - | exist | remove | change style\nthird pull | remove sdd | empty | remove | remove | remove | remove | change style\nthird plug | add sdd | /dev/sdd1 | osd4-data | /dev/sdd | exist | exist | exist, readerror\n\n## Before pull\n<center>![before pull][1]</center>\n\n## First pull\n<center>![First pull][2]</center>\n\n## Read error\n<center>![Read error][9]</center>\n\n## First plug\n<center>![First plug][3]</center>\n\n## Second pull\n<center>![Second pull][4]</center>\n\n## Second plug\n<center>![Second plug][5]</center>\n\n## Clear mount info\n<center>![Clear mount info][6]</center>\n\n## Third plug udev monitor\n<center>![Third pull][7]</center>\n\n## Third plug \n<center>![Third plug][8]</center>\n\n## 注意：\n通过上面的表格和图片，如果拔盘前没有卸载`mount`链接，那么可以发现：\n\n* 下次插盘，磁盘名称不再是原来的名称，在当前排序最后的一个磁盘名往后追加，比如当前最后一块盘是`sdf`，那么就改名为`sdg`\n* `lsblk`显示`device`列表不会变化，`alias name`也是不会变化\n* 再次拔盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的删除的事件信息\n* 再次插盘，`device`列表不会变化，`udev monitor`不会接收到任何设备的添加的事件信息\n\n如果清除了`mount`信息，插拔变得正常起来\n\n* `udev`可以正常接收设备添加和删除的信息\n* `lsblk`可以正常显示设备列表，会随着磁盘的拔盘而更新显示，`alias name`也是如此\n* 磁盘名称变回正常，即开机最原始的名称，例如上文的`sdd`\n\n所以针对一块硬盘的拔插只要做好即时清除`mount`就够了，那么这个时候拔另外插槽的一块盘呢？\n结果是：\n\n* 第一次拔盘： `udev`感知到（`remove` 当前磁盘名），`device`列表和`alias name`消失 \n* 第一次插盘： `udev`感知到（`add` **sdg**），`device`增加了`sdg`，`alias name`增加，没有变化\n* 第二次拔盘： 一切都没了反应\n* 清除`mount`信息后，插盘：无反应\n* 删除`alias name`： 无反应\n* 无论怎么样都没反应，也许除了开机或者这快盘插在别的插槽内\n\n## 总结\n\n### 问题1：\n\n目前来看，只要拔盘超过两块，一旦第二块盘的磁盘名称变了跟之前名称不一样，那么之后再怎么清理`mount`信息或者反复拔插都不会被`udev`感知到\n理论上来说，要实现硬盘热替换，那就得保证`udev`每次都能检测到硬盘的插入，拔盘检测倒不是那么重要，\n针对这种情况，目前没发现什么比较好的解决方案，唯一一种也是我当前采用的方案，在检测拔盘事件时会触发一个脚本，做拔的盘的`mount`信息的检查，如果存在`mount`信息，那么就`umount`掉，这是一种比较完美的场景，也就是说要保证每次插盘前，当前盘在系统中都不存在任何`mount`信息\n\n这种方案还是太完美化了，还需要大量的测试，期望能找出更能经得起推敲的方案\n\n### 问题2\n\n测试过程中偶然发现每次硬盘插入如果被`udev`感知到，也会执行大量的`udev rules`，其中包括创建设备，建立连接，建立连接这个事情就包括硬盘分区的`alias`，而我们感知到磁盘设备插入执行脚本在这之前，找了一系列的方案，都没能够使得让脚本在它`alias`之后再执行\n\n这里有一份官方的解释：http://www.reactivated.net/writing_udev_rules.html#external-run\n\n至于为什么脚本依赖这个别名，那是因为系统存的`data path`还是`journal path`都是以别名的形式，如果我们能够在别名之后执行脚本，那么理想情况下系统`db`之前存的别名会自动和插入的盘匹配建立连接，不需要我们再做过多的事情，否则我们得在每次创建`osd`的时候再存原始路径，记录分区信息等等，还有`/etc/fstab`存的都是别名，这样就导致有些操作无法进行，还有一些其他的矛盾暂时还没想到\n[1]: http://share.tony-yin.site/before_pull.png\n[2]: http://share.tony-yin.site/first_pull.png\n[3]: http://share.tony-yin.site/first_plug.png\n[4]: http://share.tony-yin.site/second_pull.png\n[5]: http://share.tony-yin.site/second_pull.png\n[6]: http://share.tony-yin.site/clear_mount.png\n[7]: http://share.tony-yin.site/third_plug.png\n[8]: http://share.tony-yin.site/third_plug_alias.png\n[9]: http://share.tony-yin.site/readerror.png\n[10]: http://cdn.tony-yin.site/open-uri20150611-11-s7rw56.png\n","tags":["Udev"],"categories":["ceph"]},{"title":"Linux Udev","url":"/2017/09/10/Linux-Udev/","content":"<center>![udev][21]</center>\n\n在我入门`udev`，看着各种各样的资料觉得这个东西很高深的时候，突然瞄到一篇文章里面说道： 要树立一个信念：`udev`很简单，当时不以为然，现在深表赞同。看起来很高端很遥远的`Title`，比如`Linux Kernel`，比如软硬件交互，比如热插拔等等，其实掌握几个`rules`，然后再动手实践实践，对比着讲解，就能知道七七八八了，想要再深入，就多看看文档实现和内核源码，其实很多东西都是这样，了解了就很简单，没事了解一点就比别人多知道一些，共勉 \n\n<!--more-->\n## Udev 是什么\n`udev`是`linux kernel`的设备管理器，处理硬件的添加和删除，但它的功能远远不仅于此。比如`U`盘插入，电脑是如何即时的做出响应？耳机插入又是如何做到自动关闭外部音量？这些都跟`udev`有关。虽然`udev`很强大，功能很多，本来就针对`SCSI`硬盘设备的插拔做一些经验的分享。至于为什么我会研究到这个东西，是因为最近在做`ceph`相关的项目，需要实现一个类似热插拔的`feature`，懂的人应该很清楚，就是拔插硬盘后，`osd`自动恢复，主要应用于硬盘寿命即将到期，替换新盘的场景，不明白的同学也不要紧，因为本文主要讨论的是`udev`\n\n## Udev Rules\n`udev`管理各种各样的设备，比如`usb`,`scsi`,`cd`等等，为了区别设备类型，精确匹配，`udev`制定了一套特色的`rules`，为什么说特色呢？因为我觉得它用最简洁的语法满足了方方面面的需求\n\n这里推荐一些有用的`link`：\n\n* [UDEV Primer][3]  `udev`入门必读，可以快速将`udev`了解一番\n* [Writing udev rules][4]  经典文章，看完这个基本就能掌握`udev rules`\n* [Writing udev rules 修订版][5]  国内论坛对格式样式做了一些修改，有洁癖的可以看看\n* [Writing udev rules 译][6]  翻译版本，英文不好的可以中英文对比着看\n* [udev 中文手册][7]  这位金步国先生为开源事业做出了杰出的贡献，早前就看过他翻译的`Apache`的手册\n* [Writing udev rules and kernel examples][8]  看够了上面各种规则，来看看具体的例子吧\n* [Udev 详解][11]  文章如名，真的很详细\n* [Udev 使用方法][20]  一些特殊的技巧  \n\n大家看完上面的文章应该知道`rules`文件的位置和规则,下面我贴一些我实践中的代码和例子\n\n### Rules Example\n这是两条分别针对`SCSI`硬盘设备的添加和删除的`rules`，设备事件分别会触发相应的脚本：\n```\nKERNEL==\"sd[a-z]\", ACTION==\"add\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k add\"\nKERNEL==\"sd[a-z]\", ACTION==\"remove\", SUBSYSTEM==\"block\", RUN+=\"/usr/bin/python /root/test.py %k remove\"\n```\n\n### Notice\n* 执行外部命令必须加上环境变量路径, 上面的这个也可以不加`/usr/bin/python`，但是必须在`py`脚本头部加上`shebang`：`#! /usr/bin/python`\n* `RUN`可以运行程序外的程序，而`PROGRAM`不是做这个事情的，而是为了生成之后需要的变量\n\n### Link\n* [hashtag and shebang][1]\n* [shebang][2]\n\n## Udevadm\n`udevadm`是一个`udev`的管理工具，可以用来获取设备信息(`info`)，监视和控制udev运行时的行为(`control`)，请求内核事件(`trigger`)，管理事件队列(`settle`)，以及提供简单的调试机制(`test`)。很实用，很好用！！！\n\n### 推荐 Link\n\n* [udevadm 中文手册][9]  很全，很基础 ~~\n* [udevadm command examples][10]  很多`udevadm`的命令可以借鉴，说不定以后可以用到呢？\n\n介绍几个我具体使用过的例子：\n### udevadm info\n可以从`udev`数据库中提取设备信息,此外，还可以从`sysfs`中提取设备的属性， 以帮助创建与此设备匹配的`udev`规则\n\n获取`/dev/sdb`的`path`信息\n```\nudevadm info -q path -n /dev/sdb\n```\n### udevadm test\n\n模拟向`udev`发送设备信息，并不会真的触发，**而是将触发后的过程信息打印出来**，对调试很有帮助\n\n``` \nudevadm test --action='add' `udevadm info -q path -n /dev/sdb`\n```\n\n**注意：** `test`不会真正触发事件，而`trigger`可以\n\n### udevadm trigger\n\n可以真正触发`udev`事件，但是不会真正改变硬件，只是触发`kernel`和`udev`的事件，会触发`udev rules`\n\n```\nudevadm trigger -v --action=add --sysname-match=sda\n```\n\n**注意：** \n\n* 不加`sysname-match`那么会触发所有设备，如果是`action=remove`，那么则会移除很多设备，甚至导致`sshd`服务停掉\n* 后面是`sda`而不是`/dev/sda`因为它是根据`pci`路径来定位`name`的，比如`sda`的路径是：\n```\n/sys/devices/pci0000:00/0000:00:10.0/host2/target2:0:0/2:0:0:0/block/sda\n```\n\n### udevadm control\n\n* 重载`udev rules`，对之后触发的新设备有效，对之前已经触发的无效\n```\nudevadm control --relaod-rules\n```\n* 更改`udev`日志等级为`info`，`udev`默认等级为`err`，可以在`/etc/udev/udev.conf`中查看，日志具体信息可以在`syslog`中查看\n```\nudevadm control --log-priority=info\n```\n\n## More about udev\n`udev`还有很多有意思的地方，如果想要更加深入的学习，那么除了熟练地掌握`rules`等技巧，还要对其实现原理有了解，这就得多看文档，多看源码了，建议大家有条件的情况下多看源码，多看原始的东西，因为经过别人的总结就可能会变味，相互对比，不要一味依赖别人总结的产物\n\n* [Linux 自动挂载usb设备 小例子][12]  如何在`usb`设备插入后自动挂载？\n* [udev使用笔记][13]  这篇文章里面那张图片不错\n* [写个程序检测我们的设备插拔][14]  模仿内核源码简单地实现一个设备插拔的检测\n* [linux下热插拔事件的产生是怎样通知到用户空间][15]  事件传递过程，`udev`源码解析\n* [热插拔的工作原理 hotplug机制和udev][16]  `udev`原理讲解\n* [Uevent 上报event事件给上层的详细讲解][17]  `uevent`源码讲解\n* [linux内核空间和用户空间的是怎样区别的，如何交互，如何从用户空间进入内核空间][18]  内核空间和用户空间的一些剖析\n* [Netlink实现热拔插监控 ][19]  消息传输用的是什么？`Netlink`是如何做到的\n\n[1]: http://www.cnblogs.com/daishuguang/p/4192703.html\n[2]: http://smilejay.com/2012/03/linux_shebang/\n[3]: http://webpages.charter.net/decibelshelp/LinuxHelp_UDEVPrimer.html\n[4]: http://www.reactivated.net/writing_udev_rules.html\n[5]: http://www.lai18.com/content/1612267.html\n[6]: http://blog.csdn.net/nhczp/article/details/4099647\n[7]: http://www.jinbuguo.com/systemd/udev.html#\n[8]: http://wiki.dreamrunner.org/public_html/Embedded-System/kernel/udev-rules.html\n[9]: http://www.jinbuguo.com/systemd/udevadm.html\n[10]: http://fibrevillage.com/sysadmin/93-udevadm-usage-examples\n[11]: http://blog.csdn.net/lqrensn/article/details/8097288\n[12]: https://my.oschina.net/bwhite/blog/808995\n[13]: http://www.jianshu.com/p/dd6cecd7755a\n[14]: http://blog.csdn.net/fjb2080/article/details/5009791\n[15]: http://www.itnose.net/detail/6691523.html\n[16]: http://blog.csdn.net/bingqingsuimeng/article/details/7950639\n[17]: http://blog.csdn.net/sunweizhong1024/article/details/7928530\n[18]: http://blog.csdn.net/bingqingsuimeng/article/details/7924756\n[19]: http://blog.chinaunix.net/uid-24943863-id-3223000.html\n[20]: http://www.cnblogs.com/qigaohua/p/6673930.html\n[21]: http://cdn.tony-yin.site/pingu.png\n","tags":["Udev"],"categories":["tech"]},{"title":"Network Down Notification","url":"/2017/09/10/network-down-notification/","content":"\n<center>![network][3]</center>\n\n项目中要做一个`network` `down`了之后给用户发送邮件提醒的功能，这里面涉及到的问题有：\n* 如何监测 network 状态\n* network down之后，如何发送邮件\n* public network 和 storage network的用处和场景\n\n<!--more-->\n\n## get role interfaces\n\n## get interfaces\n\n## get interface info\n\n### 获取 MAC 和 busid \n\n```\nsyspath = os.path.join(\"/sys/class/net/\", interface)  // interface such as 'eth0'\nmac = linecache.getline(os.path.join(syspath, \"address\"), 1)[:-1].upper()\nbuspath = os.path.realpath(syspath)\nbus_id = os.path.basename(os.path.dirname(os.path.dirname(buspath)))\n```\n\n* MAC: 直接在`syspath`目录下读取`address`文件\n* 假设为`eth0`,查看`eth0`目录可见该目录被建立了软连接，并不是真实地址，所以需要取出它的真实地址，这个需要使用`python`中的`os.path.realpath()`处理获得`0000:03:00.0`,作为下一步`lspci`的参数\n\n```\nroot@ceph133:/sys/class/net# ll eth0\nlrwxrwxrwx 1 root root 0  8月  8 01:26 eth0 -> ../../devices/pci0000:00/0000:00:15.0/0000:03:00.0/net/eth0/\n```\n\n### 获取网卡信息\n\n`lspci -s`后面的参数为`pci`设备的地址，也叫`pci寻址`，这个地址由总线编号、插槽编号和功能块编号信息组成\n```\nlspci -m -s xxx // 这边的参数就是上面的bus_id\n```\n\n> 具体`lspci`信息请参照：\n> [lspci命令详解][1]\n> [LSPCI命令详解分析][2]\n\n### 获取网卡状态\n\n通过`ethtool`工具获取网卡状态，就是看`Link detected`是否为`yes`，`yes`表示已激活\n\n```\n/sbin/ethtool eth0 | grep 'Link detected: yes' | wc -l\n```\n\n## bond filter\n\n## Network down notification\n**1.** 目前无论是`storage`还是`public`网卡只要`down`其中一个都无法发送`email`:\n\n* public: `visit foreign network and connect smtp server`\n* storage: `connect ceph rados to get notification info, such as send account and smtp server etc`\n所以只能`monitor`除这两个以外的网卡\n\n**2.** `Python`\n\n* `log`打印的文件取决于当前文件进程的初始化`log level`\n* `logger`: `handler`, `formatter`, `filter`\n* `dictdefault`\n\n\n  [1]: http://coolnull.com/2246.html\n  [2]: http://blog.csdn.net/xiadidi/article/details/18408169\n  [3]: http://cdn.tony-yin.site/it_specialist_kit8-net.png\n","tags":["Notification"],"categories":["tech"]},{"title":"Python操作Oracle","url":"/2017/09/10/Python-Oracle/","content":"<center>![Python][3]</center>\n\n近期在做`Intel`的测试，需要监测各种故障情况下的热恢复或者热迁移等功能，比如拔线测试，拔盘测试，断电测试等，来检验分布式的稳定性和灵活性，以`Oracle`为例，需要一直向数据库中写数据，然后在发生故障时，通过观察插入数据的情况来判断是否能够处理异常情况，所以我需要写一个脚本，能够做到通过`python`自动连接`oracle`数据库并自动插入数据。\n\n<!--more-->\n\n> 当前环境：Linux Centos 7\n\n### 1. 下载安装包cx_Oracle\n\n由于我本地`Python`版本是`2.7`,所以选择是`2.7`版本\n\n```\nwget https://pypi.python.org/packages/e1/18/00987c6a9af9568ee87d1fcba877407684a3f1b87515e5eb82d5d5acb9ff/cx_Oracle-6.0rc1-py27-1.x86_64.rpm#md5=ff8f5cce07bd5ec8ed3ce5c79d451474\n```\n\n参考:\n\n> - [python-cx_Oracle][1]\n> - [安装包列表][2]\n\n\n[1]: https://oracle.github.io/python-cx_Oracle/\n[2]: https://pypi.python.org/pypi/cx_Oracle/5.3\n\n### 2. 安装cx_Oracle\n\n- 安装：\n\n```\nrpm -ivh cx_Oracle-6.0rc1-py27-1.x86_64.rpm\n```\n\n- 查看当前`python`引入模块库的路径\n\n```\n[root ~]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import sys\n>>> print sys.path\n['', '/usr/lib/python2.7/site-packages/AliyunUtil-0.0.1-py2.7.egg', '/usr/lib/python2.7/site-packages/cloud_init-0.7.6-py2.7.egg', '/usr/lib64/python27.zip', '/usr/lib64/python2.7', '/usr/lib64/python2.7/plat-linux2', '/usr/lib64/python2.7/lib-tk', '/usr/lib64/python2.7/lib-old', '/usr/lib64/python2.7/lib-dynload', '/usr/lib64/python2.7/site-packages', '/usr/lib/python2.7/site-packages']\n```\n\n- 检查安装路径，如果`cx_Oracle.so`文件不在以上`python`引入路径内需要将该文件拷贝到以上`python`引入路径,所以我这里不需要进行拷贝操作\n\n```\n[root ~]# rpm -qa | grep cx_Oracle | xargs rpm -ql\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/PKG-INFO\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/SOURCES.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/dependency_links.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle-6.0rc1-py2.7.egg-info/top_level.txt\n/usr/lib64/python2.7/site-packages/cx_Oracle.so\n```\n\n### 3. 下载 Oracle Library\n\n由于`Oracle`官网下载东西存在`Auth`认证，所以简单点就点击选中的`rpm`包保存在`windows`上，然后上传到`linux`上即可\n\n**注意：**\n> - 下载的版本要和连接的`Oracle`基本一致，我需要连接的`oracle`版本是`11.2`，所以我下载的是`oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm`\n> - **操作系统要和之前下载的`cx_Oracle`要一致**，这很重要！！！我在这个上面栽了跟头，花了比较多的时间。。。所以我需要下载`64`位的\n> - 在`Oracle`官网下载东西你得先勾选`Accept License Agreement`，否则你下载不了\n> - 其次你得有`Oracle`的账号\n\n### 4. 安装 Oracle Library\n\n```\nrpm -ivh oracle-instantclient11.2-basic-11.2.0.3.0-1.x86_64.rpm\n```\n\n### 5. 配置环境变量\n\n- **修改`Oracle`安装路径**\n\n将`client64`改为`client`，至于为什么要改暂时还不清楚，只知道不改的话会报错`Oracle Client library cannot be loaded: libclntsh.so: cannot open shared object file: No such file or directory`\n\n```\ncd /usr/lib/oracle/11.2/\nmv client64 client\n```\n\n- 打开`/etc/profile`,追加环境变量路径\n\n```\nvim /etc/profile\n```\n\n- 配置环境变量\n\n```\n// 查看Oracle安装路径\n[root@iZbp180vortqapb7yf8217Z ~]# rpm -qa | grep oracle | xargs rpm -ql\n/usr/lib/oracle/11.2/client64/bin/adrci\n/usr/lib/oracle/11.2/client64/bin/genezi\n/usr/lib/oracle/11.2/client64/lib/libclntsh.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libnnz11.so\n/usr/lib/oracle/11.2/client64/lib/libocci.so.11.1\n/usr/lib/oracle/11.2/client64/lib/libociei.so\n/usr/lib/oracle/11.2/client64/lib/libocijdbc11.so\n/usr/lib/oracle/11.2/client64/lib/ojdbc5.jar\n/usr/lib/oracle/11.2/client64/lib/ojdbc6.jar\n/usr/lib/oracle/11.2/client64/lib/xstreams.jar\n// 追加ORACLE_HOME环境变量路径\nexport ORACLE_HOME=/usr/lib/oracle/11.2/client64\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$ORACLE_HOME/lib\n```\n\n- 生效配置\n\n```\nsource ~/.bashrc\n```\n\n### 6. 检查是否可以引入`cx_Oracle`\n\n`import`没有报错，说明之前的辛苦没有白费\n\n```\n[root client64]# python\nPython 2.7.5 (default, Nov  6 2016, 00:28:07) \n[GCC 4.8.5 20150623 (Red Hat 4.8.5-11)] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import cx_Oracle\n>>>\n```\n\n### 7. 连接 Oracle\n\n一个插入10条数据的Python脚本demo：\n\n```\n#!/usr/bin/python\n# -*- coding: UTF-8 -*-\nimport sys\nimport cx_Oracle\nimport time\n\nrows = int(sys.argv[1])\ncon = cx_Oracle.connect('test/test@sampledb')\ncur = con.cursor()\ncur.execute(\"SELECT MAX(ID) FROM PTTEST\")\nmax_id = cur.fetchone()[0] or 0\n\nstart_id = max_id + 1\nfor i in xrange(rows):\n    cur.execute(\"INSERT INTO PTTEST (ID, NAME, AGE, GENDER, SALARY)VALUES(%d, 'pt', 15, 'male', 50000)\" % int(i + start_id))\n    if i % 100 == 0:\n        time.sleep(3)\n        print 'Insert rows [%d]:%d' % (int(time.time()), i)\n        con.commit()\ncur.close()\ncon.close()\n```\n[3]: http://cdn.tony-yin.site/we-love-python-6000-450.png\n","tags":["Oracle"],"categories":["tech"]},{"title":"Oracle Linux And Oracle Database 11g R2 Intsallation","url":"/2017/09/10/oracle_install/","content":"\n<center>![oralce][9]</center>\n\n最近工作中用到了`Oracle`作为`Intel`测试的数据库，在装`Oracle Linux`和`Oracle Database 11g R2`折腾了很久，这篇文章就这两点对大家分享一些我安装过程中遇到的问题和注意点，`rac`暂时还未配置，稍后找时间补上\n\n<!--more-->\n\n> 友情提醒： 本文参照`link`较多，请细心并且耐心地进行每一步操作\n### Resource Link\n> * [Oracle Linux ISO][1]\n> * [Linux Oracle Database 11g R2][2] \n> * [Windows Oracle Database 11g R2][3] \n\n### Oracle Linux Installation\n**参考列表：**\n\n  * [oracle linux 6.5 安装 oracle 11gR2图文详解][4]\n  * [Oracle 11g R2（自己整理的Word，带详细RAC配置）][5]\n\n> 以下操作步骤基本是按上面两条`link`走的，有一些地方不一样或者是没必要的我会在下面详细讲解，我装的时候以`link1`为主线，然后参照`link2`的`word`文档进行，没有特殊标记的一般都是根据`link1`操作，大家也可以这样操作，可以少遇一些坑，节省一些时间~~~\n\n**注意点：**\n  \n  * `Base server`这一步很关键\n    - 勾选`Oracle Linux Server`, `Resilient Storage`和 `Scalable Filesystem Support`（参照`link2`）\n    - 勾选`Customize Now`，这一步依然非常关键，只有勾选了才可以装一些`Oracle Linux`需要的套件\n    - 需要手动添加安装的套件（系统默认勾选的不要删掉，这边的操作只添加勾选）：\n      * `Base System`：`Compatibility libraries`\n      * `Servers`: `System administration tools`\n      * `Desktops`: `Desktop`、`Desktop Platform`、`Fonts`、`General Purpose Desktop`、`Graphical Administration Tools`、`Input Methods`、`X Window System`\n      * `Development`: `Additional Development`、`Development Tools`\n      * `Appilication`: `Internet Browser`\n  * 安装应该是`1390`个包，装好直接`reboot`\n  * 这时候创建普通用户的时候，千万不要创建`oracle`用户，否则之后会有很多麻烦\n\n### Oracle Linux Configuration（important and complicated）\n**下面讲的主要是和两个文档不同的地方（依然以`link1`为主线）：**\n\n**1.** `selinux`需要`dsiable`\n\n**2.** `/etc/sysctl.conf`参照`link2`，`kernel.shmmax = 536870912`（这行参照`link2`）\n\n**3.** `/etc/security/limits.conf`\n```\n  oracle soft nproc 16384\n  oracle hard nproc 16384\n  oracle soft nofile 4096\n  oracle hard nofile 65536\n  oracle soft stack 10240\n```\n\n**4.** `oracle linux 6.5`光盘下载包（我是通过`yum`安装,没有安装的大部分都是系统已经安装了，少部分是没必要安装）\n* 需要安装：\n  * `ksh`\n  * `libaio-devel.i686`\n  * `libaio-devel.x86_64`\n  * `libaio.i686`\n  * `libstdc++.i686`\n  * `libstdc++-4.4.7-18.el6.i686`\n  * `compat-libstdc++-33.i686`\n* 不需要安装：\n  * `numactl-devel-2*x86_64*`\n\n> 由于我本地的是`libstdc++-4.4.7-4.el6.x86_64`，而安装的是`libstdc++-4.4.7-18.el6.i686`，版本不匹配会造成`yum`报错，而这个库作为众多库的依赖还不能删除，所以只能强行升级：`yum install libstdc++-4.4.7-18.el6.i686`，然后再安装`i686`文件\n\n**5.** 路径、权限与环境变量配置\n这一步参照: [Automating Database Startup and Shutdown on Linux][6]\n\n**附上脚本和`~/.bash_profile`:**\n\n`start_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbstart $ORACLE_HOME\n```\n\n`stop_all.sh`：\n\n```\n#!/bin/bash\n. /home/oracle/scripts/setEnv.sh\n\nexport ORAENV_ASK=NO\n. oraenv\nexport ORAENV_ASK=YES\n\ndbshut $ORACLE_HOME\n```\n\n`setenv.sh`：\n\n```\n# Oracle Settings\nexport TMP=/tmp\nexport TMPDIR=$TMP\n\nexport ORACLE_HOSTNAME=tonydb.localdomain\nexport ORACLE_UNQNAME=sampledb\nexport ORACLE_BASE=/home/oracle/app/oracle\nexport ORACLE_HOME=$ORACLE_BASE/product/11.2.0/dbhome_1\nexport ORACLE_SID=sampledb\n\nexport PATH=/usr/sbin:/usr/local/bin:$PATH\nexport PATH=$ORACLE_HOME/bin:$PATH\n\nexport LD_LIBRARY_PATH=$ORACLE_HOME/lib:/lib:/usr/lib\nexport CLASSPATH=$ORACLE_HOME/jlib:$ORACLE_HOME/rdbms/jlib\n```\n\n`~/.bash_profile`最后一行追加： \n\n```\n. /home/oracle/scripts/setenv.sh\n```\n\n**6.** 安装`FTP`并上传`zip`文件（这一步有需求就执行，我是通过`windows`传输到`linux`上）\n> 安装包在页面最上方`Resource Link`的第二个`link`，有两个`zip`包，都是需要的，这就是`Oracle Database 11g R2`的包，下面就开始安装数据库了，离成功也就不远了\n\n### Oracle Database 11g R2 Installation\n**参照：** [Oracle Database 11g Release 2 Installation on RHEL/CentOS 6.x/5.x/4.x][7]，以这个`link`的步骤为主，`link1`和`link2`为辅\n\n1. 如果出现`next button`闪退的现象，进入`preference`里面调节屏幕分辨率即可\n\n2. `check`时候报错，如果你在上面安装了那么就`ignore`吧，因为这些包都安装了，只不过版本比它`check`的高：\n```\nlibaio-0.3.105\ncompat-libstdc++-33-3.2.3\nlibaio-devel-0.3.105\nlibgcc-3.4.6\nlibstdc++-3.4.6\nunixODBC-2.2.11\nunixODBC-2.2.11\nunixODBC-devel-2.2.11\nunixODBC-devel-2.2.11\npdksh-5.2.14\n```\n\n### Oracle Listener Configuration\n**参照：** [Oracle Listener][8]\n> 最后`test`不成功也没关系，先保存设置进行下一步，这边把`lsnrcetl`起起来就可以了，先不要进入`oracle`命令行，需要等下一步创建了数据库再进行`startup`等操作\n\n### 创建数据库\n> `dbca`进入创建向导，填写正确的`sid`，`sid`不用填`ORCL`,就用同步生成的就好，不同的数据库可以拥有不同的`sid`，按照默认的配置不断地`next`就好\n\n* 安装后的默认数据库为orcl，如需创建新的数据库\n  - 使用辅助工具(Linux UI mode)\n    > root@oracledb: xhost local:oracledb\n    > root@oracledb: su - oracle\n    > oracle@oracledb: dbca\n  - 使用em管理WebUI\n* 创建表空间\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create tablespace test_space datafile '/opt/oracle/oradata/test_data.dbf' size 500M\n  - 使用em管理WebUI\n* 创建用户    \n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > create user test identified by test default tablespace test_space;\n  - 使用em管理WebUI\n* 授权用户权限\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant connect, resource to test;\n  - 使用em管理WebUI\n* 授权DBA权限（RMAN需要dba权限来运行）\n  - 使用sqlplus\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus / as sysdba\n    > grant sysdba to test;\n  - 使用em管理WebUI\n* 数据库启动/关闭\n  * 切换用户\n    > root@oracledb: su - oracle\n  * 启动监听\n    > lsnrctl start\n  * 启动实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > startup\n  * 关闭实例\n    > su - oracle (Windows下此步骤省略)\n    > sqlplus /nolog\n    > conn /as sysdba\n    > shutdown immediate\n    > exit\n  * 关闭监听\n    > lsnrctl stop\n* 归档日志模式启动/关闭\n  * 启动归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database archivelog\n    > alter database open\n  * 关闭归档日志模式\n    > sqlplus /as sysdba \n    > shutdown immediate\n    > startup mount\n    > alter database noarchivelog\n    > alter database open\n\n作者： Tony\n\n日期： 2017/06/26\n\n  [1]: https://pan.baidu.com/s/1pL7ENn9\n  [2]: https://pan.baidu.com/s/1qYufAQo\n  [3]: https://pan.baidu.com/s/1kV3JpCr\n  [4]: https://wenku.baidu.com/view/afd5ca9e5022aaea998f0fa2.html\n  [5]: https://pan.baidu.com/s/1miDQEoC\n  [6]: https://oracle-base.com/articles/linux/automating-database-startup-and-shutdown-on-linux\n  [7]: https://www.tecmint.com/oracle-database-11g-release-2-installation-in-linux/\n  [8]: http://blog.csdn.net/haiross/article/details/13613931\n  [9]: http://cdn.tony-yin.site/oracle-eloqua-campaign-manager.png\n","tags":["Oracle 11g R2"],"categories":["tech"]},{"title":"Moodle Local插件","url":"/2017/09/09/Moodle-Local-Plugin/","content":"\n<center>![moodle][8]</center>\n\n`Local`插件在`Moodle`中相比于其他插件算是比较容易上手的一个插件，`Local`插件中前后台分离，不用遨游在杂糅前后端的代码里。学会`Local`插件是上手其他插件的前提，其实一旦学会`Local`插件也会发现其他插件不过是它的一种变种形式罢了，只要举一反三就能很快掌握。\n\n<!--more-->\n\n## Moodle Local 插件架构\n<center>![image][2]</center>\n\n## 前端\n### [Template][3]\n服务器配置了相应的`url`，所以每个插件的加载首页都是`index.php`，但是并不意味着页面的代码就在`index.php`中，首先在`index.php`中调用`renderer.php` 中相应的页面渲染方法，例如  `render_index_page($page) ` 方法：\n\n```\npublic function render_index_page($page) {\n$data = $page->export_for_template($this);\n    render_from_template('local_xxx/index', $data)\n}\n```\n\n* 在这个方法中，首先调用了`index_page`这个类中的 `export_for_template` 方法，获得页面静态加载需要的数据，例如`mustache`模板需要的数据，`data`数组的索引与模板名称一一对应，例如`mustache`文件中若有 &#123;&#123;`#user`&#125;&#125;`hello!`&#123;&#123;`/user`&#125;&#125;，那么对应 class 一定存在 `$data['user'] = xxxx`；\n* 以 `render_from_template('local_xxx/index', $data)` 为例， `local_xxx/index`，`template`目录下必定有文件名为`index.mustache`的文件，这是`moodle` 封装好的命名规范，而`mustache`文件就是页面展示的代码所在了；\n* 通过&#123;&#123;`#js`&#125;&#125;`require xxx`&#123;&#123;`/js`&#125;&#125;引入对应的`JavaScript`文件；\n\n\n### [CSS][4]\n每个 local 插件中都存在一个`style.css`这样的一个`css`样式文件，系统运行时，会将每个模块的`css`文件内容合并到一起，所以如果有两个名称一样的`css`，则会发生覆盖的现象，所以建议起名字统一在最前面加上当前的插件名称，要注意的是，每次新增、修改或者删除`css`文件内容后，如若需要立即查看效果，必要以`admin`角色进入系统，然后**清空缓存**；\n\n### [JavaScript][5] \n目前系统中的`js`是基于`RequireJS`框架，遵循`AMD`开发规范，利用`Grunt`构建的。`amd`目录下有`src`目录和`build`目录，`src`下的`xxx.js`是供开发时编写调试使用，而`build`目录下的`min.js`则是网站实际运行时所使用的`js`文件，经过压缩可以大幅度提升运行速度。所以每次修改`js`完毕后，提交代码切勿忘记生成对应的`min.js`文件并提供，生成命令为 `grunt uglify`；\n\n## 后端\n### [db][6]\n1. access.php\n主要负责记录权限的配置；\n\n2. service.php\n负责接收`ajax`提交，配置对应的后台`API`的类名、路径 和方法名等信息，并将对应的`ajax`方法存入数据库中；\n\n3. install.xml\n用于安装系统时表结构的对应生成；\n\n4. install.php\n用于安装系统时数据库的缺省数据的提供；\n\n5.  upgrade.php [开发规范][7]\n用于系统表结构变化等版本升级事项的实现，由于生产环境中客户已有大量数据存在于数据库中，所以这时候不能采取重装系统的方式更改表结构或者更改数据库等操作，这时候就得借助`upgrade.php`平滑升级。\n\n### 后台 API\n1. externallib.php\n负责编写`service.php`中`ajax`提交方法对应的后台方法，并调用`lib.php`中的`API`实现；\n\n2. lib.php\n负责方法的具体实现，代码与数据库的数据交互\n\n3. lang/en/xxx.php 和 lang/zh_cn/xxx.php\n\n`en`目录下的表示英文翻译语言文件，`zh_cn`目录下的表示中文翻译语言文件，具体调用方法为： \n\n* `php`中： `get_string(字符串名, 插件名)`\n* `mustache`中： &#123;&#123;`#str`&#125;&#125;`字符串名, 插件名`&#123;&#123;`/str`&#125;&#125;\n\n作者：Tony\n2017年02月28日\n\n[1]: https://www.processon.com/view/link/58b50108e4b02ba7e0885137\n[2]: http://on-img.com/chart_image/58b5173ce4b01d80b64d335e.png \n[3]: https://docs.moodle.org/dev/Templates\n[4]: https://docs.moodle.org/dev/CSS\n[5]: https://docs.moodle.org/dev/Javascript_Modules\n[6]: https://docs.moodle.org/dev/Data_definition_API\n[7]: https://note.youdao.com/share/?token=18D5479D0AFA4286A46EACE741345DF5&gid=29389535\n[8]: http://cdn.tony-yin.site/moodle-logo-concept.png\n","tags":["PHP"],"categories":["tech"]}]