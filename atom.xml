<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tony&#39;s blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tony-yin.github.io/"/>
  <updated>2018-05-22T02:09:31.594Z</updated>
  <id>https://tony-yin.github.io/</id>
  
  <author>
    <name>Tony</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Ctdb Rados（二）：多场景断网高可用</title>
    <link href="https://tony-yin.github.io/2018/05/20/Ctdb-Rados-2/"/>
    <id>https://tony-yin.github.io/2018/05/20/Ctdb-Rados-2/</id>
    <published>2018-05-20T05:07:06.000Z</published>
    <updated>2018-05-22T02:09:31.594Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/ping-600x450.png" alt="ping"></center><p>之前写过一篇文章【<a href="http://www.tony-yin.top/2018/04/20/Ctdb-Rados-All-Banned/" target="_blank" rel="external">Ctdb Rados方式导致All Banned的问题</a>】，谈到了当<code>ctdb</code>将<code>recovery lock</code>设置成<code>rados</code>的方式后，断网<code>master</code>节点会造成所有<code>ctdb</code>节点<code>All Banned</code>，主要原因是<code>master</code>意外断网没有释放锁，其他节点无法获取到锁，当时的解决方案是每<code>5</code>分钟检查一次<code>ctdb</code>状态，如果连续两次发生了<code>All Banned</code>的情况，则手动删除<code>lock</code>，这种做法在最近的测试中遇到了一些问题，本文对这些问题进行剖析并对相应的解决方案进行分享。</p><a id="more"></a><blockquote><p>完整代码地址：<a href="https://github.com/tony-yin/Ctdb-Rados-Monitor" target="_blank" rel="external">https://github.com/tony-yin/Ctdb-Rados-Monitor</a></p></blockquote><h2 id="场景一"><a href="#场景一" class="headerlink" title="场景一"></a>场景一</h2><p>如果基于原来的做法，<code>ctdb</code>发生<code>All Banned</code>的情况，需要十分钟的监控时间加上两分钟左右的<code>recovery</code>时间，也就是说大概需要十二分钟才能恢复<code>ctdb</code>服务，这样看来高可用有点名实其副了，这个也会明显地影响存储业务的正常运行。后来，我们讨论出新的方案：每<code>5s</code>检查一次<code>ctdb</code>的状态，<code>All Banned</code>的次数累计到<code>5</code>次才确定为该故障场景，然后手动删除<code>lock</code>，最终要保证<code>ctdb</code>能够在<code>2min</code>内完成恢复。</p><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p><code>cron tab</code>最短周期只支持分钟级别，所以如何<code>5s</code>检查一次便是一个问题。</p><p>代码是死的，人是活的，虽然<code>cron tab</code>只支持分钟级别，但是我们可以每分钟调用一个脚本，然后在这个脚本中遍历<code>12</code>次，每次调用<code>ctdb monitor</code>脚本，然后<code>sleep 5s</code>，这样就可以达到每<code>5s</code>检查一次<code>ctdb</code>的效果了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"># ctdb_monitor</div><div class="line">* * * * * root /etc/ctdb/cron-seconds</div><div class="line"></div><div class="line"># cron-seconds</div><div class="line">#!/bin/bash</div><div class="line">for((i=1;i&lt;=12;i++));do</div><div class="line">    ../monitor_ctdb</div><div class="line">sleep 5</div><div class="line">done</div></pre></td></tr></table></figure><p>这样检查到<code>ctdb</code>发生<code>All Banned</code>情况，只需要花费<code>25s</code>，剩下的就是<code>recovery</code>的时间了。</p><h2 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h2><p>当<code>ctdb master</code>节点的<code>network</code>服务断掉，其他两个节点（我的开发环境是三节点的虚拟机环境）便会选举一个为<code>master</code>节点，然后去获取<code>lock</code>，因为原<code>master</code>没有释放锁，导致所有节点<code>All Banned</code>，即使我们手动删除了锁，但是这时候其他两个节点仍然处于<code>Banned</code>的情况，需要等到<code>Ban Timeout</code>才会再次尝试获取锁并开始恢复过程，这个<code>timeout</code>的时间是<code>300s</code>，即<code>5min</code>，这显然是我们不能接受的，所以我们要在删除<code>lock</code>后，重启所有节点的<code>ctdb</code>服务。</p><p>不过该如何触发该重启操作呢？</p><p>我们在删除<code>lock</code>后将<code>ctdb</code>所有节点的<code>ip</code>作为对象存进<code>rados</code>中，然后在每<code>5s</code>监控的脚本中，查看<code>rados</code>中是否存在本节点的<code>ip</code>对象，如果有，则尝试重启<code>ctdb</code>操作，重启后便删除该对象。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">function save_nodes_ip() &#123;</div><div class="line">    nodes=$(ctdb listnodes)</div><div class="line">    for node in $nodes; do</div><div class="line">        echo &quot;$node&quot; &gt; $node</div><div class="line">        rados -p rbd put $node $node</div><div class="line">        rm -f $node</div><div class="line">    done</div><div class="line">&#125;</div><div class="line"></div><div class="line">function get_current_node_ips() &#123;</div><div class="line">    ips=$(/usr/sbin/ip addr | grep &quot;inet &quot; | awk &apos;&#123;print $2&#125;&apos;)</div><div class="line">    echo $ips</div><div class="line">&#125;</div><div class="line"></div><div class="line">function monitor_nodes_ip_in_rados() &#123;</div><div class="line">    ips=$(get_current_node_ips)</div><div class="line">    for ipinfo in $ips; do</div><div class="line">        ip=$&#123;ipinfo%/*&#125;</div><div class="line">        if $(timeout 10 rados -p rbd ls | grep &quot;$ip&quot; -qw); then</div><div class="line">            systemctl restart ctdb</div><div class="line">            rados -p rbd rm $ip</div><div class="line">        fi</div><div class="line">    done</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>至于为什么三个节点的<code>ip</code>都要存入<code>rados</code>，这个是因为原<code>master</code>节点恢复网络后，<code>ctdb</code>服务的状态为<code>failed</code>，同样需要重启<code>ctdb</code>服务才能正常恢复原<code>master</code>节点。 </p><p><strong>注意：</strong></p><p>这边有两个问题，当时浪费了我不少时间，问题不是多么深奥，但是不易发现。。。</p><p>第一个问题便是<code>ips=$(/usr/sbin/ip addr | grep &quot;inet &quot; | awk &#39;{print $2}&#39;)</code>这行代码，原来的写法是<code>ips=$(ip addr | grep &quot;inet &quot; | awk &#39;{print $2}&#39;)</code>，当时发现<code>ip</code>总是获取不到，然后无论是命令行还是脚本运行都可以正常获取到，后来还是同事提醒才发现在<code>crontab</code>脚本中，<code>shell</code>命令默认是<code>/usr/bin/</code>下的，而<code>ip</code>命令则是<code>/usr/sbin/</code>下，所以这里的命令我们需要全路径。（这个需要格外注意！！！被坑的不要不要的。。。）</p><p>第二个问题便是<code>rados -p rbd ls | grep &quot;$ip&quot; -qw</code>这行代码，当时没注意写成了<code>rados -p rbd ls | grep &quot;$ip&quot; -w</code>，发现<code>if</code>判断时常有问题，一开始还以为不能<code>grep</code>数字什么的，后来才发现没有加<code>q</code>，<code>q</code>表示安静模式，不打印任何标准输出，如果有匹配的内容则立即返回状态值0。</p><h2 id="场景二"><a href="#场景二" class="headerlink" title="场景二"></a>场景二</h2><p>“断网”这个词不够具体，在实际生产环境中，一个集群中，一般都会有多个网络，就拿本人的<code>ceph</code>集群环境来说（物理机环境，并非前文提及的虚拟机开发环境），<code>ceph</code>有个<code>public network</code>和<code>cluster network</code>，而<code>ctdb</code>也有它的<code>node network</code>和<code>public network</code>，<code>ceph</code>的<code>public</code>和<code>ctdb</code>的<code>public</code>是同一网段，<code>ceph</code>的<code>cluster</code>是单独网段，<code>ctdb</code>的<code>node</code>是单独的网段。所以<code>ctdb master</code>断网可以分为三种情况：</p><ul><li>拔掉<code>ctdb master node</code>网段网线</li><li>拔掉<code>ctdb master public</code>网段网线</li><li>断掉<code>ctdb master network</code>服务</li></ul><p>当拔掉<code>ctdb master public</code>网段网线，这没有什么好说的，<code>ctdb master</code>节点服务还存在，只是<code>master</code>节点上的<code>public address</code>不可用了，会漂移到其他节点上。</p><h3 id="问题1-1"><a href="#问题1-1" class="headerlink" title="问题1"></a>问题1</h3><p>当拔掉<code>ctdb master node</code>网段网线后，<code>master</code>节点仍然有<code>public</code>网卡，（<strong>这里注意</strong>）它仍然可以获取其他<code>ctdb</code>节点的状态，而其他节点却不可以获取它的状态，因为<code>master</code>的<code>node</code>节点<code>ip</code>不存在。所以造成的结果就是原<code>master</code>节点还默认自己是<code>master</code>节点，而其他的节点却又选举出了新的<code>master</code>，我们的脚本因为<code>All Banned</code>手动删除了<code>lock</code>，这时候其他节点可以正常恢复<code>ctdb</code>服务，但是当<code>ctdb master</code>节点断网再恢复后，它还以为自己是<code>master</code>，会不断去获取锁，而原来的锁已经被我们手动删除，这时候新的锁被新的<code>master</code>掌握，所以此时产生脑裂，我们要牺牲原<code>master</code>节点，也就是断网节点，所以需要重启它。这个重启触发机制我们是通过在每次删除<code>lock</code>之后在<code>rados</code>中存入<code>ctdb</code>所有节点的<code>ip</code>作为<code>object</code>（这就是为什么要存入所有节点的<code>ip</code>），然后只要发现有这个<code>object</code>便执行<code>ctdb</code>重启操作，然后便删除这个对象。至于为什么要存所有对象是因为除了原<code>master</code>需要重启之外，另外两个正常节点发生<code>All Banned</code>的情况，默认<code>timeout</code>时间是<code>300s</code>（这个上面也提到过），我们为了减少恢复时间，直接在删除<code>lock</code>后重启<code>ctdb</code>；</p><h2 id="问题2-1"><a href="#问题2-1" class="headerlink" title="问题2"></a>问题2</h2><p>由于现在<code>ctdb</code>的锁是放在<code>rados</code>中，而不是以前的<code>cephfs</code>的方式了。所以当<code>master</code>断网再恢复时，它会不断地去<code>rados</code>获取他原来的锁，这是获取锁的进程越来越多，会阻塞住<code>rados</code>服务，我们可以通过<code>ps -ef | grep rados_helper</code>看到进程不断变多，那么<code>rados</code>服务不能正常读写就影响到我们上一条的机制，不能读<code>rados</code>中是否含有本节点<code>ip</code>的对象，就没办法进行重启操作，那么这样它就会不断地继续获取<code>lock</code>，所以我们在这里又加了一个机制，如果<code>ps -ef | grep rados_helper</code>的数目超过<code>6</code>个，就默认启动重启<code>ctdb</code>服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">function monitor_get_lock_timeout() &#123;</div><div class="line">    count=$(ps -ef | grep rados_helper | wc -l)</div><div class="line">    if [ $count -ge $RADOS_HELPER_PROCESS_MAX ]; then</div><div class="line">        systemctl restart ctdb</div><div class="line">        update_last_ctdb_restart_time</div><div class="line">    fi</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h2><p><code>ctdb</code>目前重启的机制有点多，有自身自带的故障重启，也有我们监控脚本的异常情况，很容易发生重复重启，还有可能<code>rados_helper</code>堆积的进程很多，比如<code>20</code>个，我们的脚本是<code>5s</code>一次，也许<code>20</code>个的时候重启了，过<code>5s</code>，进程释放也需要时间，可能此时还有<code>10</code>个，那么大于我们规定的<code>6</code>个，就会继续重启，这种重复重启没有必要，所以我们要加上<code>ctdb</code>重启的周期限定<code>2min</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">function get_ctdb_restart_interval() &#123;</div><div class="line">    last_time=$(get_ctdb_restart_last_time)</div><div class="line">    if [ -z &quot;$last_time&quot; ]; then</div><div class="line">        interval=$(expr $RESTART_CTDB_INTERVAL_MAX + 1)</div><div class="line">    else</div><div class="line">        current_time=$(date +%s)</div><div class="line">        interval=$(expr $current_time - $last_time)</div><div class="line">    fi</div><div class="line">    echo $interval</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>考虑并解决以上提到的问题，基本上可以覆盖以上三种断网的场景了，在监控和管理<code>ctdb</code>的过程中，一定要小心，不能影响到业务正常运行。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>生产环境网络结构错综复杂，往往在虚拟机上开发的功能当时好好的，到了物理机上面测试会发生各种问题，此时，我们首先要搞清楚网络拓扑结构，熟悉硬件配置，各网段的作用和相互之间的关联，这样遇到问题我们可以顺藤摸瓜，同样<code>ctdb</code>的原理也需要掌握才能了解它各种行为的触发机制，才能更好的定制化监控和管理。之后我会花点时间好好地研究一下<code>ctdb</code>，然后再单独做分享。</p><blockquote><p>完整代码地址：<a href="https://github.com/tony-yin/Ctdb-Rados-Monitor" target="_blank" rel="external">https://github.com/tony-yin/Ctdb-Rados-Monitor</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/ping-600x450.png&quot; alt=&quot;ping&quot;&gt;&lt;/center&gt;

&lt;p&gt;之前写过一篇文章【&lt;a href=&quot;http://www.tony-yin.top/2018/04/20/Ctdb-Rados-All-Banned/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Ctdb Rados方式导致All Banned的问题&lt;/a&gt;】，谈到了当&lt;code&gt;ctdb&lt;/code&gt;将&lt;code&gt;recovery lock&lt;/code&gt;设置成&lt;code&gt;rados&lt;/code&gt;的方式后，断网&lt;code&gt;master&lt;/code&gt;节点会造成所有&lt;code&gt;ctdb&lt;/code&gt;节点&lt;code&gt;All Banned&lt;/code&gt;，主要原因是&lt;code&gt;master&lt;/code&gt;意外断网没有释放锁，其他节点无法获取到锁，当时的解决方案是每&lt;code&gt;5&lt;/code&gt;分钟检查一次&lt;code&gt;ctdb&lt;/code&gt;状态，如果连续两次发生了&lt;code&gt;All Banned&lt;/code&gt;的情况，则手动删除&lt;code&gt;lock&lt;/code&gt;，这种做法在最近的测试中遇到了一些问题，本文对这些问题进行剖析并对相应的解决方案进行分享。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Ctdb" scheme="https://tony-yin.github.io/tags/Ctdb/"/>
    
  </entry>
  
  <entry>
    <title>Megaraid 磁盘定位</title>
    <link href="https://tony-yin.github.io/2018/05/12/Megaraid_Location/"/>
    <id>https://tony-yin.github.io/2018/05/12/Megaraid_Location/</id>
    <published>2018-05-12T05:07:06.000Z</published>
    <updated>2018-05-22T02:17:29.635Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/mega_drive-600x450.png" alt="mega drive"></center><p>早前写过一篇【<a href="http://www.tony-yin.top/2018/01/05/RaidCardToolUtils/" target="_blank" rel="external">利用Raid卡工具获取逻辑盘是否为SSD</a>】的文章，大概讲述了如何通过<code>raid</code>卡工具判断一个逻辑磁盘对应物理磁盘是否为<code>SSD</code>，当时主要提到了<code>megacli</code>和<code>sas3ircu</code>这两种工具，核心是如何通过<code>raid</code>卡工具定位到逻辑磁盘对应的物理磁盘的位置，当时的方式现在看来在有些场景会存在缺陷。</p><a id="more"></a><p>当时的方案主要是先通过<code>lspci</code>获取<code>raid</code>卡型号，然后找到对应的<code>raid</code>卡型号，紧接着通过<code>lsscsi</code>命令获取逻辑磁盘的<code>targetid</code>，再通过<code>raid</code>卡工具根据<code>targetid</code>定位到对应的物理盘。当时的方案在多<code>controller</code>的场景下存在问题，可能会出现重复<code>target id</code>的情况，所以这时候只能再借助<code>controller id</code>来定位唯一的磁盘了。总而言之，想真正定位逻辑磁盘对应的物理磁盘，就必须要获取到磁盘的<code>controller id</code>，<code>enclosure id</code>和<code>slot number</code>，有了这三个参数，便可以获取该磁盘的信息，或者对该物理磁盘进行点灯、响音和做<code>raid</code>等操作。</p><p>那么，具体如何定位逻辑磁盘的物理位置呢？且看下文分析</p><h2 id="获取-raid-卡信息"><a href="#获取-raid-卡信息" class="headerlink" title="获取 raid 卡信息"></a>获取 raid 卡信息</h2><p>通过<code>lspci</code>命令可以获取到操作系统上所有<code>raid</code>卡信息，我们可以看到每个<code>raid</code>卡最前面都有一串数字，比如第一行是<code>02:00.0</code>，第二行是<code>03:00.0</code>，这里的<code>02</code>和<code>03</code>表示的是<code>raid</code>卡的<code>busid</code>，即<code>raid</code>卡控制器在<code>pci</code>总线上的<code>id</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# lspci | grep &quot;LSI Logic&quot;</div><div class="line">02:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3008 [Fury] (rev 02)</div><div class="line">03:00.0 RAID bus controller: LSI Logic / Symbios Logic MegaRAID SAS-3 3108 [Invader] (rev 02)</div></pre></td></tr></table></figure><h2 id="获取磁盘-pcipath"><a href="#获取磁盘-pcipath" class="headerlink" title="获取磁盘 pcipath"></a>获取磁盘 pcipath</h2><p>在<code>linux</code>中，一切皆文件，每个文件都有自己的唯一标识，对于磁盘而言，<code>pcipath</code>就是它的唯一标识，<code>pci</code>总线上面有很多控制器，比如<code>scsi</code>控制器，而磁盘又存在于<code>scsi</code>控制器上，所以我们可以在<code>lsscsi</code>命令获取到的<code>scsi</code>设备列表中查看到操作系统上的磁盘信息。</p><p>以<code>sda</code>为例，我们可以在<code>/dev/disk/by-path</code>目录下查看到磁盘的<code>pcipath</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# ll /dev/disk/by-path/</div><div class="line">total 4</div><div class="line">lrwxrwxrwx 1 root root   9 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0 -&gt; ../../sda</div><div class="line">lrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part1 -&gt; ../../sda1</div><div class="line">lrwxrwxrwx 1 root root  10 May 11 10:30 pci-0000:02:00.0-scsi-0:2:0:0-part2 -&gt; ../../sda2</div><div class="line">lrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:1:0 -&gt; ../../sdb</div><div class="line">lrwxrwxrwx 1 root root   9 May 11 16:22 pci-0000:02:00.0-scsi-0:2:10:0 -&gt; ../../sdk</div></pre></td></tr></table></figure><p>由于在<code>linux</code>中，<code>udev</code>是用户态的设备管理，所以我们也可以通过<code>udev</code>获取。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# udevadm info --query=symlink --name=sda</div><div class="line">disk/by-id/scsi-36509a4c0ac86790022337b9105005435 disk/by-id/wwn-0x6509a4c0ac86790022337b9105005435 disk/by-path/pci-0000:02:00.0-scsi-0:2:0:0</div></pre></td></tr></table></figure><p>这边我们可以得到磁盘<code>sda</code>的<code>pcipath</code>为<code>pci-0000:02:00.0-scsi-0:2:0:0</code>，<code>02</code>就是磁盘的<code>raid</code>卡的<code>bus id</code>，后面的<code>00</code>表示<code>channel id</code>，再后面的<code>0:2:0:0</code>就和<code>lsscsi</code>获取的一样了，其中<code>2</code>就表示<code>target id</code>。</p><p>所以通过<code>bud id</code>，我们可以获取到磁盘对应的<code>raid</code>卡型号，根据对应的<code>raid</code>卡工具操作磁盘。这边我们只讨论<code>megaraid</code>，所以工具也就是<code>megacli</code>了。</p><h2 id="获取-controller-id"><a href="#获取-controller-id" class="headerlink" title="获取 controller id"></a>获取 controller id</h2><p>上面我们获取到了磁盘的<code>target id</code>和对应<code>raid</code>卡的<code>bus id</code>，而对于<code>megacli</code>工具而言，每个<code>raid</code>卡都有一个与之对应的<code>controller</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# /opt/MegaRAID/MegaCli/MegaCli64 -AdpGetPciInfo -aall -NoLog</div><div class="line"></div><div class="line">PCI information for Controller 0</div><div class="line">--------------------------------</div><div class="line">Bus Number      : 2</div><div class="line">Device Number   : 0</div><div class="line">Function Number : 0</div><div class="line"></div><div class="line">PCI information for Controller 1</div><div class="line">--------------------------------</div><div class="line">Bus Number      : 3</div><div class="line">Device Number   : 0</div><div class="line">Function Number : 0</div><div class="line"></div><div class="line"></div><div class="line">Exit Code: 0x00</div></pre></td></tr></table></figure><p>这边我们可以看到<code>megacli</code>获取到了两个<code>controller</code>，也就对应上面<code>lspci</code>获取到的两张<code>raid</code>卡。细心的朋友可以发现这边有一个<code>Bus Number</code>，分别为<code>2</code>和<code>3</code>，而我们上面获取到了<code>raid</code>卡的<code>bus id</code>分别为<code>02</code>和<code>03</code>，没错，这边的<code>Bus Number</code>和<code>bus id</code>是对应的，只是<code>Bus number</code>没有自动填补成两位数，所以我们可以通过<code>bus id</code>得到<code>sda</code>所对应的<code>controller</code>为<code>0</code>。</p><blockquote><p><strong>注意：</strong><br>原本系统中版本<code>8.07.07</code>的<code>megacli</code>工具获取<code>raid</code>卡信息的时候会存在问题，每次<code>Bus Number</code>都会变化，我们只要升级<code>megacli</code>即可，我这边是把<code>megacli</code>升级到了<code>8.07.14</code>版本。</p><p>安装包地址：<a href="https://github.com/tony-yin/Megaraid_location/blob/master/MegaCli-8.07.14-1.noarch.rpm" target="_blank" rel="external">https://github.com/tony-yin/Megaraid_location/blob/master/MegaCli-8.07.14-1.noarch.rpm</a></p></blockquote><h2 id="获取磁盘组"><a href="#获取磁盘组" class="headerlink" title="获取磁盘组"></a>获取磁盘组</h2><p>此时，我们拥有了<code>controller id</code>，可以获取该<code>controller</code>下所有的磁盘组信息。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# /opt/MegaRAID_new/MegaCli/MegaCli64  -LdPdInfo -a0 -NoLog</div><div class="line">Adapter #0</div><div class="line"></div><div class="line">Number of Virtual Disks: 13</div><div class="line">Virtual Drive: 0 (Target Id: 0)</div><div class="line">Name                :</div><div class="line">RAID Level          : Primary-1, Secondary-0, RAID Level Qualifier-0</div><div class="line">Size                : 558.375 GB</div><div class="line">Sector Size         : 512</div><div class="line">Is VD emulated      : No</div><div class="line">Mirror Data         : 558.375 GB</div><div class="line">State               : Optimal</div><div class="line">Strip Size          : 64 KB</div><div class="line">Number Of Drives    : 2</div><div class="line">Span Depth          : 1</div><div class="line">Default Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU</div><div class="line">Current Cache Policy: WriteThrough, ReadAheadNone, Direct, No Write Cache if Bad BBU</div><div class="line">Default Access Policy: Read/Write</div><div class="line">Current Access Policy: Read/Write</div><div class="line">Disk Cache Policy   : Disk&apos;s Default</div><div class="line">Encryption Type     : None</div><div class="line">Default Power Savings Policy: Controller Defined</div><div class="line">Current Power Savings Policy: None</div><div class="line">Can spin up in 1 minute: Yes</div><div class="line">LD has drives that support T10 power conditions: Yes</div><div class="line">LD&apos;s IO profile supports MAX power savings with cached writes: No</div><div class="line">Bad Blocks Exist: No</div><div class="line">Is VD Cached: No</div><div class="line">Number of Spans: 1</div><div class="line">Span: 0 - Number of PDs: 2</div><div class="line"></div><div class="line">PD: 0 Information</div><div class="line">Enclosure Device ID: 32</div><div class="line">Slot Number: 12</div><div class="line">Drive&apos;s position: DiskGroup: 0, Span: 0, Arm: 0</div><div class="line">Enclosure position: 1</div><div class="line">Device Id: 12</div><div class="line">WWN: 50000398181A974C</div><div class="line">Sequence Number: 2</div><div class="line">Media Error Count: 0</div><div class="line">Other Error Count: 0</div><div class="line">Predictive Failure Count: 0</div><div class="line">Last Predictive Failure Event Seq Number: 0</div><div class="line">PD Type: SAS</div><div class="line">...</div><div class="line">...</div></pre></td></tr></table></figure><p>然后我们可以根据<code>target id</code>获取对应的磁盘组信息，<code>target id</code>与上面的<code>Target Id</code>所对应，这样我们可以过滤得到唯一的磁盘组信息。这边我们可以看到<code>sda</code>对应<code>Target Id</code>为<code>0</code>的磁盘组，该<code>raid</code>类型为<code>raid1</code>，虚拟磁盘组中有两块物理盘，然后我们可以获取这两块物理盘的<code>enclosure id</code>和<code>slot number</code>，这样再加上前文的<code>controller id</code>，我们就可以完完全全地定位到具体一块磁盘的物理位置。</p><h2 id="一键定位"><a href="#一键定位" class="headerlink" title="一键定位"></a>一键定位</h2><p>针对这种需求，本人根据以上逻辑写了一个简单的脚本可以一键获取磁盘的定位。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# ./get_disk_location.py sda</div><div class="line">[&apos;0:32:12&apos;, &apos;0:32:13&apos;]</div></pre></td></tr></table></figure><p>这边<code>0:32:12</code>分别表示磁盘的<code>controller id</code>，<code>enclosure id</code>和<code>slot number</code>。</p><blockquote><p>完整代码地址：<a href="https://github.com/tony-yin/Megaraid_location/" target="_blank" rel="external">https://github.com/tony-yin/Megaraid_location/</a></p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>之前的做法大部分场景可行，但是在一些场合偶尔会发现问题，总感觉还是不够靠谱，身边的人还有通过<code>sda</code>，<code>sdb</code>这种排列顺序来查找和<code>megacli</code>中显示磁盘的对应关系的，就更不靠谱了。<code>linux</code>操作系统是可以识别到具体硬件设备的，所以是肯定存在方法识别硬件对应的逻辑设备的，本文通过<code>pcipath</code>获取到设备的唯一标识，然后根据<code>pcipath</code>中的<code>bus id</code>和<code>megacli</code>中的<code>cobtroller</code>建立连接，最后通过<code>target id</code>锁定唯一磁盘组中的磁盘信息。</p><p>通过这种方式，我们不需要肉眼判断，也不需要顾虑部分场景方案不适用，这完全就是操作系统使用的方式，使用这种最基础，最底层的方式实现，真是让人豁然开朗。这跟看源码类似，了解一个功能的背后具体实现，你才知道最正确的姿势，不用去碰，去凑，这种感觉真好。</p><p>给大家推荐一本书《<code>Linux</code>设备驱动程序》，这本书详细讲解了<code>linux</code>中各种设备与驱动的细节，很底层也很枯燥，不过看完后应该会很有收获。希望大家在使用各种已有工具和框架的基础上，多去了解背后的实现机制，这样可以帮助我们更好地实现更深层次的需求。</p><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ol><li><a href="https://wiki.archlinux.org/index.php/Persistent_block_device_naming_(%E7%AE%80%E4%BD%93%E4%B8%AD%E6%96%87)" target="_blank" rel="external">Persistent block device naming</a></li><li><a href="https://github.com/eLvErDe/hwraid" target="_blank" rel="external">hwraid</a></li><li><a href="https://github.com/louwrentius/showtools" target="_blank" rel="external">showtools</a></li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-scsi-subsystem/" target="_blank" rel="external">Linux SCSI 子系统剖析</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/mega_drive-600x450.png&quot; alt=&quot;mega drive&quot;&gt;&lt;/center&gt;

&lt;p&gt;早前写过一篇【&lt;a href=&quot;http://www.tony-yin.top/2018/01/05/RaidCardToolUtils/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;利用Raid卡工具获取逻辑盘是否为SSD&lt;/a&gt;】的文章，大概讲述了如何通过&lt;code&gt;raid&lt;/code&gt;卡工具判断一个逻辑磁盘对应物理磁盘是否为&lt;code&gt;SSD&lt;/code&gt;，当时主要提到了&lt;code&gt;megacli&lt;/code&gt;和&lt;code&gt;sas3ircu&lt;/code&gt;这两种工具，核心是如何通过&lt;code&gt;raid&lt;/code&gt;卡工具定位到逻辑磁盘对应的物理磁盘的位置，当时的方式现在看来在有些场景会存在缺陷。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Megaraid" scheme="https://tony-yin.github.io/tags/Megaraid/"/>
    
  </entry>
  
  <entry>
    <title>Django CAS Token 解决方案</title>
    <link href="https://tony-yin.github.io/2018/05/02/Django_CAS_Token_Solution/"/>
    <id>https://tony-yin.github.io/2018/05/02/Django_CAS_Token_Solution/</id>
    <published>2018-05-02T05:07:06.000Z</published>
    <updated>2018-05-21T02:11:06.462Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/cas-600x450.jpg" alt="cas"></center><p><code>CAS</code>单点登录主要是为了解决主系统和子系统的统一登录问题，能够做到任意一个子系统登录成功后，再登录其他子系统后不再需要认证，让用户不用重复地进行登录认证。<code>CAS</code>单点登录的方案很多，并且大多数都是采用<code>session</code>的方式，而本文结合个人实践，着重讨论<code>django cas token</code>的解决方案。</p><a id="more"></a><p>本方案中，<code>cas</code>客户端和服务端都采用了开源项目，服务端是<a href="https://github.com/jbittel/django-mama-cas" target="_blank" rel="external">django-mama-cas</a>，而客户端是<a href="https://github.com/mingchen/django-cas-ng" target="_blank" rel="external">django-cas-ng</a>。</p><h2 id="CAS-Server"><a href="#CAS-Server" class="headerlink" title="CAS Server"></a>CAS Server</h2><p>服务端相比于客户端要简单地多，根据<code>github</code>步骤一步步下载和配置就好。</p><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install django-mama-cas</div></pre></td></tr></table></figure><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># settings.py</div><div class="line">INSTALLED_APPS = (</div><div class="line">    &apos;mama_cas&apos;,</div><div class="line">)</div><div class="line"></div><div class="line"># 重要！，service是client的IP，是个数组，可以在后面添加SERVICE的HOST:PORT。</div><div class="line">MAMA_CAS_SERVICES = [</div><div class="line">    &#123;</div><div class="line">        &apos;SERVICE&apos;: &apos;http://127.0.1.1:8000&apos;,</div><div class="line">        &apos;CALLBACKS&apos;: [</div><div class="line">            &apos;mama_cas.callbacks.user_model_attributes&apos;,     # 返回除了password的所有Field</div><div class="line">            # &apos;mama_cas.callbacks.user_name_attributes&apos;, # 只返回 username</div><div class="line">        ],</div><div class="line">        &apos;LOGOUT_ALLOW&apos;: True,</div><div class="line">        &apos;LOGOUT_URL&apos;: &apos;http://127.0.1.1:8000/accounts/callback&apos;,</div><div class="line">    &#125;,</div><div class="line">]</div><div class="line"></div><div class="line"># urls.py</div><div class="line">url(r&apos;&apos;, include(&apos;mama_cas.urls&apos;)),</div></pre></td></tr></table></figure><h2 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h2><p>首先是一些基本的客户端配置，比如<code>server ip</code>等，但是<code>django-cas-ng</code>默认是通过<code>session</code>的方式认证的，而我们需要通过<code>token</code>的方式认证，所以如果想继续用<code>django-cas-ng</code>来解决问题，那要么查看它是否有原生支持的接口，要么改源码。改源码可能不大友好，所以我优先研究了一下<code>django-cas-ng</code>的原生支持，无意中发现<a href="https://github.com/mingchen/django-cas-ng#view-wrappers-example" target="_blank" rel="external">view-wrappers-example</a>可以继承它原生的登录接口做一些封装，而我们完全通过继承原生的登录方法，然后加入我们的<code>token</code>相关代码。所以<code>urls.py</code>里面登录的方法我们写的是我们写在<code>view.py</code>中封装的登录方法，而并非默认的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"># settings.py</div><div class="line">INSTALLED_APPS = (</div><div class="line">    # ... other installed apps</div><div class="line">    &apos;django_cas_ng&apos;,</div><div class="line">)</div><div class="line"></div><div class="line">AUTHENTICATION_BACKENDS = (</div><div class="line">    &apos;django_cas_ng.backends.CASBackend&apos;,</div><div class="line">)</div><div class="line"></div><div class="line"># 注意：这是cas server的地址</div><div class="line">CAS_SERVER_URL = &apos;http://127.0.0.1:8000&apos;</div><div class="line"></div><div class="line"># 存入所有CAS 服务端返回的user数据。</div><div class="line">CAS_APPLY_ATTRIBUTES_TO_USER = True</div><div class="line"></div><div class="line"># urls.py</div><div class="line">import view import *</div><div class="line">url(r&apos;^accounts/login$&apos;, cas_login, name=&apos;cas_login&apos;),</div><div class="line"></div><div class="line">#view.py</div><div class="line">from django_cas_ng import views as baseviews</div><div class="line">from django.views.decorators.csrf import csrf_exempt</div><div class="line"></div><div class="line">@csrf_exempt</div><div class="line">def cas_login(request, **kwargs):</div><div class="line">    r = baseviews.login(request, **kwargs)</div><div class="line">    if not request.user.is_anonymous():</div><div class="line">        token = get_token(request)</div><div class="line">        if token:</div><div class="line">            r.set_cookie(&apos;token&apos;, token)</div><div class="line">        else:</div><div class="line">            print &apos;Get token error&apos;</div><div class="line">    else:</div><div class="line">        print(&apos;User is anonymous&apos;)</div><div class="line">    return r</div><div class="line"></div><div class="line">def get_token(request, *args, **kwargs):</div><div class="line">    user = request.user</div><div class="line">    try:</div><div class="line">        request_hash = AuthToken.get_request_hash(request)</div><div class="line">        try:</div><div class="line">            token = generate_token()    # function used to geneate token, this place won&apos;t show more detail codes</div><div class="line">            token.refresh()</div><div class="line">        except IndexError:</div><div class="line">            pass</div><div class="line">    except Exception as e:</div><div class="line">        print e</div><div class="line">        return False</div><div class="line">    return token.key</div></pre></td></tr></table></figure><p>生成<code>token</code>的方法我就不详细描述了，这边主要提供了一个思路，我们将<code>django-cas-ng</code>原生的登录方法进行了继承，然后生成<code>token</code>并放到了<code>session</code>当中。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要为<code>CAS Token</code>方案提供一个思路，如果集成进已有项目中，肯定会遇到很多细节问题，不过万变不离其宗，我们首先要熟悉手中运用的工具，然后要善于在此基础之上根据自己的定制需求进行开发，多看看文档和源码，每一次可能都会有新的发现。</p><h2 id="Refer"><a href="#Refer" class="headerlink" title="Refer"></a>Refer</h2><ol><li><a href="https://www.jianshu.com/p/d97a3d367037" target="_blank" rel="external">使用django-mama-cas快速搭建CAS服务</a></li><li><a href="http://www.voidcn.com/article/p-yvycalqd-brm.html" target="_blank" rel="external">Django实现CAS+OAuth2</a></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/cas-600x450.jpg&quot; alt=&quot;cas&quot;&gt;&lt;/center&gt;

&lt;p&gt;&lt;code&gt;CAS&lt;/code&gt;单点登录主要是为了解决主系统和子系统的统一登录问题，能够做到任意一个子系统登录成功后，再登录其他子系统后不再需要认证，让用户不用重复地进行登录认证。&lt;code&gt;CAS&lt;/code&gt;单点登录的方案很多，并且大多数都是采用&lt;code&gt;session&lt;/code&gt;的方式，而本文结合个人实践，着重讨论&lt;code&gt;django cas token&lt;/code&gt;的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Django" scheme="https://tony-yin.github.io/tags/Django/"/>
    
      <category term="CAS" scheme="https://tony-yin.github.io/tags/CAS/"/>
    
  </entry>
  
  <entry>
    <title>Daily Article Vol 5 - (2018/4/1 ~ 2018/4/30)</title>
    <link href="https://tony-yin.github.io/2018/05/01/Daily-Article-Vol5/"/>
    <id>https://tony-yin.github.io/2018/05/01/Daily-Article-Vol5/</id>
    <published>2018-05-01T02:07:06.000Z</published>
    <updated>2018-05-09T01:48:30.387Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/hello_april_600x450.png" alt="April"></center><p>这是<code>Daily Article</code>系列的第五篇，罗列了<code>2018</code>年<code>4</code>月的阅读清单。</p><a id="more"></a><p>本月是在新公司工作的第一个月，主要做了利用<code>python</code>搭建<code>NAS</code>服务稳定性测试框架、<code>CTDB</code>使用<code>rados object</code>作为<code>lock file</code>、利用<code>django</code>做<code>CAS</code>单点登录。首先<code>python</code>搭建<code>NAS</code>服务稳定性测试框架的工作就是通过<code>python</code>对<code>NAS</code>服务进行连接，并进行读写操作，该框架不需要任何挂载操作便可以对<code>NAS</code>服务进行连接和读写操作，目前支持<code>NFS</code>、<code>CIFS</code>和<code>FTP</code>，具体请参考<a href="http://www.tony-yin.top/2018/04/08/python_nas_stable_test/" target="_blank" rel="external">基于Python的NAS稳定性测试框架</a>；然后就是<code>CTDB</code>使用<code>rados object</code>作为<code>lockfile</code>这种方案的实践，期间也遇到了<code>Nodes All Banned</code>这种问题并解决了；最后就是采用<code>django-mama-cas</code>作为服务端，<code>django-cas-ng</code>作为客户端，搭建了一套<code>CAS</code>环境，并对目前的项目进行集成。</p><p>个人课余时间的话，一方面关注的是<code>python</code>和<code>django</code>有关的知识，因为工作中要用到，其次就是围绕着<code>ceph</code>和存储相关知识的了解和学习了。上周在扫盲区的过程中，看到了很久之前了解的<code>nvme over fabric</code>这个概念，然后又去深入地了解了一下。这一了解又引申出了无数的盲区，比如<code>RDMA</code>、<code>nvme</code>、<code>nvme overip</code>、<code>FC</code>等等，而这些技术也很新，文档不多，并且大多数都是很枯燥的，只能逼着自己慢慢地啃下去。感慨技术变革真是日新月异的同时，也感慨扫盲区的速度远远赶不上盲区生成的速度，每天都会有好多自己不知道或者不是特别了解原理的概念，等待着自己去学习和深入。不过呢，也正如之前一位前辈讲过，很多人都觉得这个技术牛逼，那个技术牛逼，不是那么容易学会的，也正是因为这种心态，很多人都不去学习，而很多学习的人即使没有到达最顶峰，比不学的人多了解一点就强一些。所以人生苦短，没事多看看总没坏处。</p><p>碎片时间也会看极客时间上面的专栏，我已经买了两个专栏。从目前的观看效果来说，首先我觉得还是不错的，但是很多内容都是概念上面的东西，仿佛是为了写文章而写文章，缺少落地的东西，经常围绕着一个概念反复强调，我觉得还不如多讲讲具体的场景更容易让别人接受一些。对于我而言，分布式，服务治理，服务分发这些概念和场景我都有经历过，所以我还蛮能接受的，我想很多没有经历过这些开发的童鞋直接看也许会感觉到云里雾里的。但是还是那句话，多看一点总是有好处的，因为很多东西你看了，你没到一定层次是不会了解的，你提前看了，也许之后某一天就会有一种豁然开朗的感觉。但是我们还是要讲究方法的，时间是有限的，我们要在有限的时间学习更多的知识。我认为看这些文章，目的不是为了一步登天，获得什么实质效应，事实上他也不会给你这种回报，我们要做的就是从上面获取一些技术概念、原理和本质，由此作为一个引子，自己找项目，找场景去实践，去深入，光说不练假把式，只有实践了才能深入，然后最好的就是能够在产品中不断运用和学习相关技术了，这样才是最好的学习和掌握。因为很多时间如果固步自封，很多概念你都不知道，更别谈去学习了。最后我会专门整理一篇文章，把我看过的极客时间的文章分享出来，这样可以让一些童鞋免费了解和学习，我这边用的是极客时间提供的分享的功能，也不算违规，每次分享只有十个名额，所以大家先到先得。emmm，看很多同学买课很积极，很多同学却很消极，我觉得前者起码有学习的想法，后者就不谈了，但是针对前者我想说的是，不是花的钱越多就有用的，而是学到手，记到脑子里才属于自己的，希望大家都能不断学习，越变越好，happy everyday！</p><ol><li><a href="https://www.jianshu.com/p/6f67a4b9dad3" target="_blank" rel="external">django2.0入门教程第一节</a>(4/2) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://www.jianshu.com/p/b8d73d39f184" target="_blank" rel="external">django2.0入门教程第二节</a>(4/2) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://www.jianshu.com/p/2004b8dbebb4" target="_blank" rel="external">django2.0入门教程第三节</a>(4/2) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://www.jianshu.com/p/e2a09d2a4a2f" target="_blank" rel="external">django2.0入门教程第四节</a>(4/2) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://time.geekbang.org/column/article/ed937b37244d4db63f60e5f00be38fce/share" target="_blank" rel="external">【极客时间-左耳听风】：分布式系统关键技术：服务调度</a>(4/3)</li><li><a href="http://www.woshipm.com/data-analysis/872543.html" target="_blank" rel="external">数据分析入门：初识数据埋点（一）</a>(4/7) <i class="fa fa-star"></i></li><li><a href="https://www.jianshu.com/p/5b33bbd61c48" target="_blank" rel="external">docker初体验</a>(4/7)</li><li><a href="https://www.jianshu.com/p/f43659a58d71" target="_blank" rel="external">构建FTP文件传输服务器</a>(4/8)</li><li><a href="https://blog.csdn.net/bear_huangzhen/article/details/41806903?from=singlemessage" target="_blank" rel="external">FTP文件传输协议</a>(4/8)</li><li><a href="https://www.jianshu.com/p/05212313d0e2" target="_blank" rel="external">ftp实现原理以及抓包分析</a>(4/8)</li><li><a href="https://www.jianshu.com/p/e99519739b5e" target="_blank" rel="external">Linux下ftp服务搭建之小试牛刀</a>(4/8)</li><li><a href="https://blog.csdn.net/u014245412/article/details/72286348" target="_blank" rel="external">python 操作samba文件服务器</a>(4/10) <i class="fa fa-star"></i></li><li><a href="https://yq.aliyun.com/articles/578927?utm_content=m_45816" target="_blank" rel="external">佛系程序员的月薪五万指南</a>(4/12) <i class="fa fa-star"></i></li><li><a href="https://lingxiankong.github.io/2013-12-23-python-setup.html" target="_blank" rel="external">关于python中的setup.py</a>(4/13) <i class="fa fa-star"></i></li><li><a href="https://time.geekbang.org/column/article/b307919cc599a82c542ec39e7aa3ddc7/share" target="_blank" rel="external">【极客时间-左耳听风】：分布式系统关键技术：流量与数据调度</a>(4/16) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="http://blog.sina.com.cn/s/blog_8c243ea30102uxaw.html" target="_blank" rel="external">分布式高可用CTDB方案</a>(4/16)</li><li><a href="https://ceph.com/planet/ctdb使用rados-object作为lock-file/" target="_blank" rel="external">CTDB使用rados object作为lock file</a>(4/16) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://blog.csdn.net/naipeng/article/details/75045177" target="_blank" rel="external">关于CTDB</a>(4/17)</li><li><a href="http://www.zphj1987.com/2017/04/20/where-is-cephfs-data-store/" target="_blank" rel="external">Cephfs的文件存到哪里了</a>(4/21)</li><li><a href="http://blog.jobbole.com/56574/" target="_blank" rel="external">最佳日志实践</a>(4/22) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://zhuanlan.zhihu.com/p/27363484" target="_blank" rel="external">最佳日志实践（v2.0）</a>(4/23) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="http://www.zphj1987.com/2017/06/09/use-graylog-get-Ceph-status/" target="_blank" rel="external">使用日志系统graylog获取Ceph集群状态</a>(4/23) <i class="fa fa-star"></i></li><li><a href="http://www.zphj1987.com/2017/07/13/CEPHFS-op-to-graylog/" target="_blank" rel="external">Cephfs 操作输出到日志查询系统</a>(4/23) <i class="fa fa-star"></i></li><li><a href="https://blog.csdn.net/xiaqunfeng123/article/details/56675696?locationNum=1&amp;fps=1" target="_blank" rel="external">bluestore调研</a>(4/23) <i class="fa fa-star"></i></li><li><a href="https://zhuanlan.zhihu.com/p/24312755" target="_blank" rel="external">Python打包时添加非代码文件的坑</a>(4/24)</li><li><a href="https://www.jianshu.com/p/d910a70dfee7" target="_blank" rel="external">cephfs介绍和功能测试</a>(4/24)</li><li><a href="http://xiaqunfeng.cc/2017/01/20/%E5%9D%97%E5%AD%98%E5%82%A8%E7%9A%84%E4%B8%96%E7%95%8C/#more" target="_blank" rel="external">块存储的世界</a>(4/24) <i class="fa fa-star"></i></li><li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000/001432712108300322c61f256c74803b43bfd65c6f8d0d0000#0" target="_blank" rel="external">python virtualenv</a>(4/24) <i class="fa fa-star"></i></li><li><a href="http://stackeye.com/2014/08/rpmbuild-in-action/" target="_blank" rel="external">rpmbuild实战</a>(4/25) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://zhuanlan.zhihu.com/p/28492389" target="_blank" rel="external">使用RPM方式安装Linux软件</a>(4/25)</li><li><a href="https://www.ibm.com/developerworks/cn/linux/l-rpm/index.html" target="_blank" rel="external">RPM 打包技术与典型 SPEC 文件分析</a>(4/25) <i class="fa fa-star"></i></li><li><a href="http://mp.weixin.qq.com/s?src=11&amp;timestamp=1524706491&amp;ver=839&amp;signature=nEMLgYwV4GI9Pd19glqGgJS6DR7sfrZesONMDXvpIWz34-KhzcYGK8h*WE7DjAFpXU9NYztsheG7Doy29A2zEC-V2bw*zFNgXbyy5VK2vRBR83sVVudSgxEsbAN0R8iW&amp;new=1" target="_blank" rel="external">RDMA(远程直接内存访问)技术浅析</a>(4/26) <i class="fa fa-star"></i></li><li><a href="https://blog.csdn.net/chenhaifeng2016/article/details/78072498?locationNum=4&amp;fps=1" target="_blank" rel="external">RDMA技术</a>(4/26) <i class="fa fa-star"></i></li><li><a href="https://www.csdn.net/article/1970-01-01/302809" target="_blank" rel="external">Fabric是否代表网络架构的未来？</a> <i class="fa fa-star-half-full"></i></li><li><a href="http://dy.163.com/v2/article/detail/CDG9CD7G05179LAH.html" target="_blank" rel="external">2017下一代数据中心网络研究报告</a>(4/27)</li><li><a href="http://book.51cto.com/art/201105/266135.htm" target="_blank" rel="external">统一Fabric和互联云</a>(4/27)</li><li><a href="http://net.zol.com.cn/459/4598330.html" target="_blank" rel="external">浅谈数据中心网络架构的发展</a>(4/27) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://blog.csdn.net/memblaze_2011/article/details/51820631" target="_blank" rel="external">为了部落：NVMe over Fabric诞生记</a> <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="http://www.cnblogs.com/rodenpark/p/6220519.html" target="_blank" rel="external">NVMe over Fabrics：概念、应用和实现</a>(4/28)</li><li><a href="https://blog.csdn.net/u010616442/article/details/70773956" target="_blank" rel="external">NVME概述</a>(4/28)</li><li><a href="https://mp.weixin.qq.com/s?src=11&amp;timestamp=1524808998&amp;ver=841&amp;signature=56U4bzWNuWUqlDANFQNBFPkSc2dX-R*HeLL9y7vN*ha-Ph0PrOfhkEPUfB8R*Gy-SLlwFeMqCUKDneUkeDPfpgT-igBpw77SSCr6Hhl9Ul29OsEMtzY1B2-JPQbBd1vH&amp;new=1" target="_blank" rel="external">详谈NVMe over Fabric技术发展简史</a>(4/29) <i class="fa fa-star"></i></li><li><a href="http://www.voidcn.com/article/p-yvycalqd-brm.html" target="_blank" rel="external">Django实现CAS+OAuth2</a> <i class="fa fa-star"></i></li><li><a href="https://weibo.com/p/1001603934517592239583?mod=zwenzhang#_loginLayer_1525517479194" target="_blank" rel="external">NVMe over Fabric</a> <i class="fa fa-star"></i><i class="fa fa-star"></i></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/hello_april_600x450.png&quot; alt=&quot;April&quot;&gt;&lt;/center&gt;

&lt;p&gt;这是&lt;code&gt;Daily Article&lt;/code&gt;系列的第五篇，罗列了&lt;code&gt;2018&lt;/code&gt;年&lt;code&gt;4&lt;/code&gt;月的阅读清单。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
      <category term="Daily-Article" scheme="https://tony-yin.github.io/tags/Daily-Article/"/>
    
  </entry>
  
  <entry>
    <title>Ctdb Rados方式导致All Banned的问题</title>
    <link href="https://tony-yin.github.io/2018/04/20/Ctdb-Rados-All-Banned/"/>
    <id>https://tony-yin.github.io/2018/04/20/Ctdb-Rados-All-Banned/</id>
    <published>2018-04-20T05:07:06.000Z</published>
    <updated>2018-05-09T03:07:35.619Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/lock.jpg" alt="lock object"></center><p><code>ctdb</code>最近专门为<code>ceph</code>提供了一种<code>raods object</code>作为文件锁的方式，<code>lock file</code>可以放在对象存储中，而不是<code>cephfs</code>，从而大大降低了系统宕机的延时。在此方案的实践中，我们发现<code>master</code>节点宕机会导致严重的<code>All Banned</code>的问题，本文则围绕该问题展开讨论和提供本人的解决方案。</p><a id="more"></a><p>很多系统都在用<code>ctdb</code>做<code>HA</code>，今天我们讨论的是基于<code>cephfs</code>的<code>ctdb HA</code>方案。<code>ctdb</code>的作用是在一个共享文件系统中，当所有节点都访问同一个文件时，<code>ctdb</code>会选举出一个<code>master</code>节点获得<code>lock</code>，我们之前的做法是把这个<code>lock file</code>放在<code>cephfs</code>的共享目录中，但是当其中某个节点<code>down</code>了之后，会导致<code>cephfs</code>这个目录卡死，进一步导致<code>lock file</code>在其他节点都获取不到，只有等到锁超时了之后才能获取到，而这个超时时间默认是<code>300s</code>，再加上<code>ctdb</code>的监控检测和恢复的时间，切换的时间少则十几分钟，多则几十分钟，这对于高可用场景来说无疑是灾难级的。</p><h2 id="具体场景"><a href="#具体场景" class="headerlink" title="具体场景"></a>具体场景</h2><p><code>ctdb</code>的编译和安装我就不说了，大家可以参考磨渣的文章：<a href="http://www.zphj1987.com/2018/01/06/CTDB-use-rados-object-as-lock-file/" target="_blank" rel="external">CTDB使用rados object作为lock file</a>。在<code>ceph</code>集群中所有节点安装好<code>ctdb</code>后，起服务后通过<code>systemctl status ctdb</code>可以发现<code>reclock</code>是通过<code>ctdb_mutex_ceph_rados_helper</code>的方式，就说明<code>ctdb rados</code>的方式配置成功了。</p><p>然后我们可以通过<code>rados -p rbd ls</code>也可以看到自己配置的锁存在于<code>rbd pool</code>中。这时我们断电一个<code>slave</code>节点，一分钟左右后可以实现节点切换。但是我们的测试发现当断网<code>master</code>节点的时候，就会造成长时间的卡住，且节点并不会切换。详细查看可以发现断网后，<code>master</code>节点没有释放<code>lock</code>，然后其他的集群节点选举出了<code>master</code>节点后，试图获取锁，但是由于之前的<code>master</code>节点一直没有释放，所以一直获取不到，然后就不停的去获取，<code>ctdb</code>的机制是如果有不断的这种行为，就会让所有节点<code>All Banned</code>。因为<code>slave</code>节点并不拥有锁，所以不存在之前的问题。</p><p>这个问题是比较严重的，因为不存在超时机制，拥有锁的节点断网或者断电，所以不会因为超时就释放锁。所以就会一直就卡着，并且一直实现不了切换节点。这就意味着一旦这种情况发生，客户的业务就会发生中断，这是无法接受的。并且我们也发现了如果使用原来将<code>lock file</code>放在<code>cephfs</code>目录的方式，断网或者断电主节点并不会发生这种情况，后来大概看了下源码大概是因为<code>cephfs</code>自己的机制会强制释放共享目录中文件的锁。</p><p>具体报错如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[root@tony ~]# ctdb status</div><div class="line">Warning: All nodes are banned.</div></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>我们的解决方案没有尝试着修改<code>ctdb</code>的源码，而是通过定时监控<code>ctdb</code>的状态。如果是主节点上面的<code>ctdb</code>，并且如果是<code>rados</code>方式的话，每<code>3</code>分钟查看一下<code>ctdb status</code>的状态，如果有连续两次的状态都是<code>All Banned</code>的话，我们就认为目前主节点发生了不释放锁的问题，我们就主动地删除<code>lock object</code>。部分代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">#! /bin/bash</div><div class="line"></div><div class="line">function check_if_master() &#123;</div><div class="line">    MASTER_PNN=$(ctdb recmaster)</div><div class="line">    CURRENT_PNN=$(ctdb pnn)</div><div class="line">    if [ $MASTER_PNN -eq $CURRENT_PNN ]; then</div><div class="line">        echo true</div><div class="line">    else</div><div class="line">        echo false</div><div class="line">    fi  </div><div class="line">&#125;</div><div class="line"></div><div class="line">function get_lock_name() &#123;</div><div class="line">    LOCK_INFO=$(grep rados $CTDB_CONFIG_FILE | awk &apos;&#123;print $5&#125;&apos;)</div><div class="line">    LOCK_NAME=$&#123;LOCK_INFO:0:-1&#125;</div><div class="line">    echo $LOCK_NAME</div><div class="line">&#125;</div><div class="line"></div><div class="line">function monitor_lock() &#123;</div><div class="line">    STATUS_FILE=/etc/ctdb/status.txt</div><div class="line">    CTDB_STATUS=$(ctdb status 2&gt;&amp;1)</div><div class="line">    ALL_BANNED=&quot;Warning: All nodes are banned.&quot;</div><div class="line"></div><div class="line">    if [ ! -f &quot;$STATUS_FILE&quot; ]; then</div><div class="line">        echo &quot;$CTDB_STATUS&quot; &gt; $STATUS_FILE</div><div class="line">    else</div><div class="line">        if [ &quot;$CTDB_STATUS&quot; = &quot;$ALL_BANNED&quot; ]; then</div><div class="line">            LAST_CTDB_STATUS=$(cat $STATUS_FILE)</div><div class="line">            if [ &quot;$LAST_CTDB_STATUS&quot; = &quot;$ALL_BANNED&quot; ]; then</div><div class="line">                LOCKNAME=$(get_lock_name)</div><div class="line">                echo $(date)&quot; Ctdb all nodes banned: Second time&quot; &gt;&gt; /var/log/monitor_ctdb.log</div><div class="line">                echo $(date)&quot; Remove ctdb rados lock: &quot;$LOCKNAME &gt;&gt; /var/log/monitor_ctdb.log</div><div class="line">                rados -p rbd rm $LOCKNAME </div><div class="line">                echo -n &quot;&quot; &gt; $STATUS_FILE</div><div class="line">            else</div><div class="line">                echo $(date)&quot; Ctdb all nodes banned: First time&quot; &gt;&gt; /var/log/monitor_ctdb.log</div><div class="line">                echo &quot;$ALL_BANNED&quot; &gt; $STATUS_FILE</div><div class="line">            fi</div><div class="line">        else</div><div class="line">            echo -n &quot;&quot; &gt; $STATUS_FILE</div><div class="line">        fi</div><div class="line">    fi</div><div class="line">&#125;</div><div class="line"></div><div class="line">CTDB_CONFIG_FILE=/etc/sysconfig/ctdb</div><div class="line">if $(grep rados $CTDB_CONFIG_FILE -q); then</div><div class="line">    if $(check_if_master); then</div><div class="line">        monitor_lock</div><div class="line">    fi</div><div class="line">fi</div></pre></td></tr></table></figure><p>完整代码地址：<code>https://github.com/tony-yin/Ctdb-Rados-Monitor</code></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>也许我的这种做法不是最优方案，希望遇到同样问题的同学可以一起讨论，拥有更好解决方案的可以一起分享。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/lock.jpg&quot; alt=&quot;lock object&quot;&gt;&lt;/center&gt;

&lt;p&gt;&lt;code&gt;ctdb&lt;/code&gt;最近专门为&lt;code&gt;ceph&lt;/code&gt;提供了一种&lt;code&gt;raods object&lt;/code&gt;作为文件锁的方式，&lt;code&gt;lock file&lt;/code&gt;可以放在对象存储中，而不是&lt;code&gt;cephfs&lt;/code&gt;，从而大大降低了系统宕机的延时。在此方案的实践中，我们发现&lt;code&gt;master&lt;/code&gt;节点宕机会导致严重的&lt;code&gt;All Banned&lt;/code&gt;的问题，本文则围绕该问题展开讨论和提供本人的解决方案。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Ctdb" scheme="https://tony-yin.github.io/tags/Ctdb/"/>
    
      <category term="Rados" scheme="https://tony-yin.github.io/tags/Rados/"/>
    
      <category term="HA" scheme="https://tony-yin.github.io/tags/HA/"/>
    
  </entry>
  
  <entry>
    <title>基于Python的NAS稳定性测试框架</title>
    <link href="https://tony-yin.github.io/2018/04/08/python_nas_stable_test/"/>
    <id>https://tony-yin.github.io/2018/04/08/python_nas_stable_test/</id>
    <published>2018-04-08T05:07:06.000Z</published>
    <updated>2018-04-20T02:16:52.506Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/nas.jpg" alt="NAS Stable Test"></center><p>最近公司有个集群一直在跑着，领导想要测测它上面<code>NAS</code>服务的稳定性，也就是看看正常持续的读写会不会导致<code>NAS</code>服务异常，这个其实通过<code>fio</code>或者<code>cosbench</code>这类的工具测试起来很容易，但是这样一是没有挑战性，二是比较机械，可扩展性低。比如并行测试、进程保护和异常通知等等这些是机械地运用工具测试所做不到的，所以我们尝试做了一套基于<code>NAS</code>稳定性测试的框架。</p><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>整个测试框架打包和发布都是通过RPM的方式，方便测试人员一键部署。部署之后测试工作由<code>supervisor</code>管理，实时监控后台进程的运行状态，发生异常时可以进行重启等自动化操作。所有读写操作都是通过<code>python</code> 连接NAS服务，无需做任何挂载工作。主要测试工作是通过<code>celery</code>实现任务调度，支持并行多个NAS服务的读写测试，<code>broker</code>和<code>backend store</code>都采用了<code>rabbitmq</code>。后端注册了<code>register</code>、<code>nfs</code>、<code>cifs</code>和<code>ftp</code>四个<code>job</code>，定时每<code>10</code>分钟执行一次，设置最大开启<code>worker</code>数为<code>5</code>个。<code>Job</code>注册进消息队列中后，<code>celery worker</code>会自动去消费，针对服务器中不同的<code>NAS</code>服务进行读写操作，每个任务的执行结果最后都会记录在日志中，出了异常通过邮件通知管理员。</p><h2 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h2><p>整个项目的框架图如下：</p><center><img src="http://ow0mgad6r.bkt.clouddn.com/NAS_Stable_Test%E6%A1%86%E6%9E%B6%E5%9B%BE.png" alt="Nas_Stable_Test架构图"></center><h2 id="NAS服务读写流程"><a href="#NAS服务读写流程" class="headerlink" title="NAS服务读写流程"></a>NAS服务读写流程</h2><p>由于每个<code>NAS</code>服务的测试方式是一致的，所以下面就以单个<code>NAS</code>服务的流程来介绍。首先<code>client</code>端向<code>server</code>的<code>NAS</code>服务端口发起连接，<code>server</code>端接收到<code>client</code>端的请求后建立连接。<code>Client</code>在<code>/tmp</code>目录下生成固定大小<code>1G</code>的文件，并且记录该文件的<code>MD5</code>值，然后将该文件上传至远端NAS服务目录（即对<code>NAS</code>服务进行写操作），上传完成后将该文件从本地删除。接着对之前上传至<code>NAS</code>服务目录的文件进行下载（即对<code>NAS</code>服务进行读操作），下载完成后再次记录文件<code>MD5</code>值，并删除掉远端<code>NAS</code>服务目录对应的文件。最后对两次记录的<code>MD5</code>值进行比较，判断上传和下载的文件是否一致，并将比较结果记录在日志中，再次删除本地下载的文件。<code>NAS</code>服务读写流程图如下：</p><center><img src="http://ow0mgad6r.bkt.clouddn.com/NAS_Stable_Test%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B%E5%9B%BE.png" alt="Nas_Stable_Test流程图"></center><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><h3 id="安装libnfs"><a href="#安装libnfs" class="headerlink" title="安装libnfs"></a>安装libnfs</h3><p>通过<code>pip</code>安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install libnfs</div></pre></td></tr></table></figure><p>一般会报这个错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">libnfs/libnfs_wrap.c:2969:25: fatal error: nfsc/libnfs.h: No such file or directory</div></pre></td></tr></table></figure><p>这个错看起来是缺少这个头文件的包，但是通过<code>yum search libnfs</code>是找不到相关的包的，所以我们只能去官网下载<code>rpm</code>包然后在安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">## 下载rpm</div><div class="line">wget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-1.9.8-1.el7.nux.x86_64.rpm</div><div class="line">wget http://li.nux.ro/download/nux/dextop/el7/x86_64//libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm</div><div class="line">## 安装rpm</div><div class="line">yum localinstall libnfs-1.9.8-1.el7.nux.x86_64.rpm</div><div class="line">yum localinstall libnfs-devel-1.9.8-1.el7.nux.x86_64.rpm</div></pre></td></tr></table></figure><h3 id="安装pysmb"><a href="#安装pysmb" class="headerlink" title="安装pysmb"></a>安装pysmb</h3><p>这个比较简单，直接<code>pip</code>安装就可以了，也没遇到什么问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install pysmb</div></pre></td></tr></table></figure><h2 id="NAS服务相关代码实现"><a href="#NAS服务相关代码实现" class="headerlink" title="NAS服务相关代码实现"></a>NAS服务相关代码实现</h2><p>这里只贴出部分<code>python</code>连接或者操作具体<code>Nas</code>服务的代码实现，如果想要了解或者贡献整个项目，请关注：<a href="https://github.com/tony-yin/python_nas" target="_blank" rel="external">Github python_nas项目</a></p><h3 id="NFS"><a href="#NFS" class="headerlink" title="NFS"></a>NFS</h3><h4 id="Connect"><a href="#Connect" class="headerlink" title="Connect"></a>Connect</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">def open(self):</div><div class="line">    self.nfs = libnfs.NFS(&apos;nfs://&#123;&#125;&apos;.format(self.mount_point))        </div><div class="line">    log.info(&apos;nfs connect successfully!&apos;)</div></pre></td></tr></table></figure><h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><p>这里有个关键点就是分段读写文件，避免内存溢出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def read(self):</div><div class="line">log.info(&apos;nfs read start...&apos;)</div><div class="line">a = self.nfs.open(&apos;/&#123;&#125;&apos;.format(self.filename), mode=&apos;r&apos;)          </div><div class="line">with open(self.download_path, &apos;a&apos;) as f:                          </div><div class="line">    while True: </div><div class="line">        content = a.read(1024*1024)                               </div><div class="line">        if content == &apos;&apos;:</div><div class="line">            break</div><div class="line">        f.write(content)                                          </div><div class="line">a.close()</div><div class="line">    log.info(&apos;nfs read end...&apos;)</div></pre></td></tr></table></figure><h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def write(self, content):                                             </div><div class="line">log.info(&apos;nfs write start...&apos;)                                    </div><div class="line">a = self.nfs.open(&apos;/&#123;&#125;&apos;.format(self.filename), mode=&apos;w+&apos;)         </div><div class="line">a.seek(self.file_size)</div><div class="line">a.write(content)</div><div class="line">a.close()</div><div class="line">log.info(&apos;nfs write end...&apos;)</div></pre></td></tr></table></figure><h4 id="Delete"><a href="#Delete" class="headerlink" title="Delete"></a>Delete</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def delete(self):</div><div class="line">    log.info(&apos;nfs file delete start...&apos;)</div><div class="line">self.nfs.unlink(&apos;/&#123;&#125;&apos;.format(self.filename))</div><div class="line">log.info(&apos;nfs file delete end...&apos;)</div></pre></td></tr></table></figure><h3 id="CIFS"><a href="#CIFS" class="headerlink" title="CIFS"></a>CIFS</h3><h4 id="Connect-1"><a href="#Connect-1" class="headerlink" title="Connect"></a>Connect</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def open(self):</div><div class="line">    self.smb = SMBConnection(</div><div class="line">        self.username,</div><div class="line">        self.password,</div><div class="line">        self.my_name.encode(&apos;utf-8&apos;),</div><div class="line">        self.remote_name.encode(&apos;utf-8&apos;),</div><div class="line">        use_ntlm_v2=True</div><div class="line">    )</div><div class="line">    self.smb.connect(self.host, self.port)</div><div class="line">    log.info(&apos;cifs connect successfully!&apos;)</div></pre></td></tr></table></figure><h4 id="Read-1"><a href="#Read-1" class="headerlink" title="Read"></a>Read</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def read(self):</div><div class="line">    log.info(&apos;cifs read start...&apos;)</div><div class="line">    file_obj = open(self.download_path, &apos;wb&apos;)</div><div class="line">    self.smb.retrieveFile(</div><div class="line">        self.directory,</div><div class="line">        self.filename,</div><div class="line">        file_obj</div><div class="line">    )</div><div class="line">    file_obj.close()</div><div class="line">    log.info(&apos;cifs read end...&apos;)</div></pre></td></tr></table></figure><h4 id="Write-1"><a href="#Write-1" class="headerlink" title="Write"></a>Write</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def write(self):</div><div class="line">log.info(&apos;cifs write start...&apos;)</div><div class="line">file_obj = open(self.client_path, &apos;rb&apos;)</div><div class="line">self.smb.storeFile(</div><div class="line">    self.directory,</div><div class="line">    self.filename,</div><div class="line">    file_obj</div><div class="line"> )</div><div class="line">file_obj.close()</div><div class="line">log.info(&apos;cifs write end...&apos;)</div></pre></td></tr></table></figure><h4 id="Delete-1"><a href="#Delete-1" class="headerlink" title="Delete"></a>Delete</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def delete(self):</div><div class="line">log.info(&apos;cifs delete start...&apos;)</div><div class="line">self.smb.deleteFiles(&apos;path3&apos;, self.filename)</div><div class="line">log.info(&apos;cifs delete end...&apos;)</div></pre></td></tr></table></figure><h4 id="Close"><a href="#Close" class="headerlink" title="Close"></a>Close</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def close(self):</div><div class="line">    self.smb.close()</div></pre></td></tr></table></figure><h3 id="FTP"><a href="#FTP" class="headerlink" title="FTP"></a>FTP</h3><h4 id="Connect-2"><a href="#Connect-2" class="headerlink" title="Connect"></a>Connect</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">def open(self):</div><div class="line">    self.ftp = FTP()</div><div class="line">    self.ftp.connect(</div><div class="line">        host=self.host.encode(&apos;utf-8&apos;),</div><div class="line">        port=self.port.encode(&apos;utf-8&apos;)</div><div class="line">    )</div><div class="line">    self.ftp.login(self.username, self.password)</div><div class="line">    log.info(&apos;ftp connect successfully!&apos;)</div></pre></td></tr></table></figure><h4 id="Read-2"><a href="#Read-2" class="headerlink" title="Read"></a>Read</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def read(self):</div><div class="line">log.info(&apos;ftp read start...&apos;)                                     </div><div class="line">buff_size = 1024</div><div class="line">fp = open(self.download_path, &quot;wb&quot;)                               </div><div class="line">self.ftp.retrbinary(</div><div class="line">    &quot;RETR &#123;&#125;&quot;.format(self.filename),</div><div class="line">    fp.write,</div><div class="line">    buff_size                                                     </div><div class="line">)   </div><div class="line">fp.close()</div><div class="line">log.info(&apos;ftp read end...&apos;)</div></pre></td></tr></table></figure><h4 id="Write-2"><a href="#Write-2" class="headerlink" title="Write"></a>Write</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def write(self):</div><div class="line">log.info(&apos;ftp write start...&apos;)                                    </div><div class="line">buff_size = 1024</div><div class="line"> fp = open(self.client_path, &quot;rb&quot;)</div><div class="line">self.ftp.storbinary(</div><div class="line">    &quot;STOR &#123;&#125;&quot;.format(self.filename),</div><div class="line">    fp,</div><div class="line">   buff_size</div><div class="line">)</div><div class="line">fp.close()</div><div class="line">log.info(&apos;ftp write end...&apos;)</div></pre></td></tr></table></figure><h4 id="Delete-2"><a href="#Delete-2" class="headerlink" title="Delete"></a>Delete</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">def delete(self):</div><div class="line">log.info(&apos;ftp delete start...&apos;)</div><div class="line">self.ftp.delete(self.filename)</div><div class="line">log.info(&apos;ftp delete end...&apos;)</div></pre></td></tr></table></figure><h4 id="Close-1"><a href="#Close-1" class="headerlink" title="Close"></a>Close</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">def close(self):</div><div class="line">    self.ftp.quit(self.filename)</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>还有一些其他的<code>API</code>接口就不一一赘述了，具体实现细节大家可以查看<code>github</code>上面的项目代码，具体地址我会在文末贴出。整个项目的核心是通过<code>celery</code>实现任务的调度，还有全程通过<code>python</code>连接和操作<code>nas</code>服务，后续还会不断完善~~~</p><blockquote><p><strong>项目地址：</strong><a href="https://github.com/tony-yin/python_nas" target="_blank" rel="external">https://github.com/tony-yin/python_nas</a></p></blockquote><p><br></p><blockquote><p>参考列表：<br><a href="https://pypi.org/project/libnfs/" target="_blank" rel="external">python 操作samba文件服务器</a><br><a href="https://blog.csdn.net/u014245412/article/details/72286348" target="_blank" rel="external">python libnfs</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/nas.jpg&quot; alt=&quot;NAS Stable Test&quot;&gt;&lt;/center&gt;

&lt;p&gt;最近公司有个集群一直在跑着，领导想要测测它上面&lt;code&gt;NAS&lt;/code&gt;服务的稳定性，也就是看看正常持续的读写会不会导致&lt;code&gt;NAS&lt;/code&gt;服务异常，这个其实通过&lt;code&gt;fio&lt;/code&gt;或者&lt;code&gt;cosbench&lt;/code&gt;这类的工具测试起来很容易，但是这样一是没有挑战性，二是比较机械，可扩展性低。比如并行测试、进程保护和异常通知等等这些是机械地运用工具测试所做不到的，所以我们尝试做了一套基于&lt;code&gt;NAS&lt;/code&gt;稳定性测试的框架。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Python" scheme="https://tony-yin.github.io/tags/Python/"/>
    
      <category term="NAS" scheme="https://tony-yin.github.io/tags/NAS/"/>
    
  </entry>
  
  <entry>
    <title>Daily Article Vol 4 - (2018/3/1 ~ 2018/3/31)</title>
    <link href="https://tony-yin.github.io/2018/04/01/Daily_Article_Vol4/"/>
    <id>https://tony-yin.github.io/2018/04/01/Daily_Article_Vol4/</id>
    <published>2018-04-01T02:13:25.000Z</published>
    <updated>2018-04-20T02:09:42.584Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/2018_march.jpg" alt="Daily Article 3"></center><p>这个月主要因为离职和入职，花费了大量时间和相关的人沟通，还有办理了很多手续，包括找房子和搬家等等。真是忙的焦头烂额，奢侈地给自己放了一个星期的假期。</p><a id="more"></a><p>离开了工作了两年的公司，真是感慨万千。还没毕业就到这家公司实习，然后提前试用、转正。师父耐心的指导，无论是技术上还是生活上都受益匪浅，帮我扎实地掌握了一名软件开发工程师应该具备的技能；钱总作为<code>CTO</code>仿佛身上一直贴着技术控的标签，殊不知竟各项全能，机缘巧合带我做起了<code>Ceph</code>，这一年中不仅从钱总身上学到了很多高新技术，也从每次吃饭饭桌上、下班路上、地铁上倾听他年轻的故事中受益很多，常常听起来热血沸腾。还有很多帮助过我和关心过我的人就不一一点名了。总之，大家都很厉害，也正是因为如此，在我工作的两年中，我能够不断感受到压力，不断进步，并且以后会一直以你们为我的榜样，以后还要多多指教，多交流多切磋。</p><p>入职了新的公司，规模和规范都比原来上了一个档次，正式了许多，上下班都要打卡，一开始不习惯，下班总是忘记。。还有经常开会，要写很多设计和方案，感觉不像以后有师父和钱总这样带我走了，哈哈，但也多了很多参与感，大事小事都可以提出自己的想法，也可以从同事的想法中学习很多。也是巧合，之前网上因博客结缘的一位小伙伴现在和我同组，负责带我熟悉各方面，顿时轻松了很多，他人也很<code>nice</code>，看的出来很热爱技术，身上有很多值得我学习的点，算是一个挺优秀的同龄人了。总体来说，对新公司的印象还是不错的，好好加油吧！</p><p>这个月我买了极客时间上面陈皓老师（左耳朵耗子）的专栏，没买但想看的朋友可以点击我下面分享的链接观看，每个链接都有十个免费的观看圈，先到先到，如果觉得收获很大的话，建议买了看看，受益绝不止这<code>199</code>哦~~~</p><p>新公司任务调度用到了<code>celery</code>框架，感觉还蛮好用的，还在熟悉中…</p><ol><li><a href="http://www.sebastien-han.fr/blog/2015/04/27/ceph-manually-repair-object/" target="_blank" rel="external">Ceph: manually repair object</a>(3/2) <i class="fa fa-star"></i><i class="fa fa-star"></i> </li><li><a href="http://blog.csdn.net/younger_china/article/details/75150261" target="_blank" rel="external">【分析】Ceph数据一致性检查 - Scrub的介绍</a>(3/5)</li><li><a href="http://blog.csdn.net/younger_china/article/details/75149045" target="_blank" rel="external">【分析】Ceph数据一致性检查 - 端到端的数据校验</a>(3/5)</li><li><a href="https://open.weixin.qq.com/connect/oauth2/authorize?appid=wx5d7aad8e7ec33bfd&amp;redirect_uri=https%3A%2F%2Faccount.geekbang.org%2Faccount%2Foauth%2Fcallback%3Ftype%3Dwechatopen%26ident%3D60f4b1%26redirect%3Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare%253Ffailedurl%253Dhttps%253A%252F%252Ftime.geekbang.org%252Fcolumn%252Farticle%252F45c83454a044f89f8eff69b8a7dbeac3%252Fshare&amp;response_type=code&amp;scope=snsapi_userinfo&amp;state=d2b1c68fbd5a953da5f04515e46f9b48&amp;connect_redirect=1#wechat_redirect" target="_blank" rel="external">洞悉技术的本质，享受科技的乐趣</a>(3/17)</li><li><a href="https://time.geekbang.org/column/article/e3b6ea4c823d2c319bcbce9a1dc23501/share" target="_blank" rel="external">程序员如何用技术变现（上）</a>(3/17) <i class="fa fa-star"></i></li><li><a href="https://time.geekbang.org/column/article/c260269c38db5276c17f1be49718f74a/share" target="_blank" rel="external">程序员如何用技术变现（下）</a>(3/17)</li><li><a href="https://time.geekbang.org/column/article/70f255c35f7e58cb008e4410659cb39e/share" target="_blank" rel="external">Equifax信息泄露始末</a>(3/18)</li><li><a href="https://time.geekbang.org/column/article/3aa6350a995faf76c32e61ba6f7db3e9/share" target="_blank" rel="external">从Equifax信息泄露看数据安全</a>(3/18)</li><li><a href="https://time.geekbang.org/column/article/66c547af27e04afd4b7c2f8444d0971f/share" target="_blank" rel="external">何为技术领导力</a>(3/18)</li><li><a href="https://time.geekbang.org/column/article/0294f13512d5b6608115a601eb373287/share" target="_blank" rel="external">如何拥有技术领导力</a>(3/18)</li><li><a href="https://time.geekbang.org/column/article/5df06f61f6d635f5fc71b27ccc39902d/share" target="_blank" rel="external">每个程序员都该知道的事</a>(3/19)</li><li><a href="https://time.geekbang.org/column/article/af44ced83be43d287e728d2eaee10afc/share" target="_blank" rel="external">Go语言，Docker和新技术</a>(3/19)</li><li><a href="https://time.geekbang.org/column/article/946c705caf3299894fcc991bc59992de/share" target="_blank" rel="external">答疑解惑：渴望、热情和选择</a>(3/20)</li><li><a href="https://time.geekbang.org/column/article/ef3daa17d473e583835df8248082f6fc/share" target="_blank" rel="external">如何成为一个大家愿意追随的Leader？</a>(3/20)</li><li><a href="http://blog.51cto.com/hongtengfei/1684809" target="_blank" rel="external">NFS服务详细分析</a>(3/21) <i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/acs713/article/details/7322082" target="_blank" rel="external">理解Linux系统/etc/init.d目录和/etc/rc.local脚本</a>(3/21)</li><li><a href="https://time.geekbang.org/column/article/73253891c16c684d3c9dcfd02dfcb54f/share" target="_blank" rel="external">分布式系统架构的冰与火</a>(3/21) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://time.geekbang.org/column/article/a3b4de20403567fbdf305bdd4e403026/share" target="_blank" rel="external">从亚马逊的实践，谈分布式系统的难点</a>(3/22) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/a18829898663/article/details/71065999" target="_blank" rel="external">访问网络文件共享服务-CIFS</a>(3/23) <i class="fa fa-star"></i></li><li><a href="https://www.liaoxuefeng.com/article/00137760323922531a8582c08814fb09e9930cede45e3cc000" target="_blank" rel="external">任务调度利器：Celery</a>(3/26)</li><li><a href="https://www.jianshu.com/p/1840035cb510" target="_blank" rel="external">异步任务神器 Celery 简明笔记</a>(3/26)</li><li><a href="http://www.open-open.com/lib/view/open1426298834326.html" target="_blank" rel="external">Python 并行分布式框架：Celery</a>(3/26) <i class="fa fa-star"></i></li><li><a href="https://www.jianshu.com/p/9c04890615ba" target="_blank" rel="external">Redis实现简单消息队列</a>(3/27) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://www.cnblogs.com/davidshen/p/8145984.html" target="_blank" rel="external">cifs协议与samba服务</a>(3/28~3/29) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://linuxtoy.org/archives/selinux-introduction.html" target="_blank" rel="external">SELinux 入门</a>(3/29)</li><li><a href="http://www.cnblogs.com/shanyou/archive/2013/02/04/2891300.html" target="_blank" rel="external">Ring Buffer 有什么特别?</a>(3/29) <i class="fa fa-star"></i></li><li><a href="https://time.geekbang.org/column/article/f9126577b469b13317889a99ea70d2f0/share" target="_blank" rel="external">分布式系统的技术栈</a>(3/30) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://time.geekbang.org/column/article/8efeb52c4015735a7ea424e0cff861c1/share" target="_blank" rel="external">分布式系统关键技术：全栈监控</a>(3/31) <i class="fa fa-star"></i><i class="fa fa-star"></i></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/2018_march.jpg&quot; alt=&quot;Daily Article 3&quot;&gt;&lt;/center&gt;

&lt;p&gt;这个月主要因为离职和入职，花费了大量时间和相关的人沟通，还有办理了很多手续，包括找房子和搬家等等。真是忙的焦头烂额，奢侈地给自己放了一个星期的假期。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
      <category term="Daily-Article" scheme="https://tony-yin.github.io/tags/Daily-Article/"/>
    
  </entry>
  
  <entry>
    <title>硬件环境测试环境模拟</title>
    <link href="https://tony-yin.github.io/2018/03/09/Hardware-Test-Tool/"/>
    <id>https://tony-yin.github.io/2018/03/09/Hardware-Test-Tool/</id>
    <published>2018-03-09T06:35:06.000Z</published>
    <updated>2018-03-09T07:19:26.564Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/hardware-600x450.jpg" alt="hardware"></center><p>最近在做一个<code>feature</code>，测试的时候需要硬件环境的支撑。一般我们开发环境都是虚拟机，所以针对这种开发工作的自测无法进行，比如虚拟机上没有物理磁盘，没有<code>raid</code>卡等，为了一个小功能的测试，需要出<code>build</code>，需要硬件环境的部署和安装，这个工作量着实不小。</p><p>往往针对这种情况，作为开发人员可以针对硬件环境的具体需求，尽可能在自己的环境上做模拟，也就是我们俗称的“打桩”。</p><a id="more"></a><p>就拿我这次做的需求来说吧，我想获取磁盘的相关信息，获取方式是通过<code>raid</code>卡工具，可能是<code>megacli</code>，也可能是<code>sas3ircu</code>等等，这个取决于<code>lspci</code>查看<code>raid</code>卡的型号，然后还要通过<code>lsblk</code>和<code>lsscsi</code>工具获取相关信息。</p><p>我的做法是针对这些工具，自己写一个简易的小工具，就比如<code>lsblk</code>我也写一个<code>lsblk</code>的脚本，里面的代码也很简单，先找一个硬件环境，将<code>lsblk</code>读取的内容重定向到文件中，这时候我们自己写的脚本直接去读这个文件就可以了。务必要保证各个软件工具的一致性。</p><p>以<code>lsblk</code>为例，我可能需求两种情况<code>lsblk</code>和<code>lsblk -l</code>：</p><p><code>lsblk</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#! /bin/bash                                                        </div><div class="line"></div><div class="line">lsblk_path=$(dirname $0)</div><div class="line">if [ $# -eq 0 ]; then</div><div class="line">    echo &quot;$(cat $lsblk_path&quot;/lsblk.txt&quot;)&quot;</div><div class="line">fi</div><div class="line"></div><div class="line">while getopts &quot;:l&quot; opt; do</div><div class="line">    case $opt in  </div><div class="line">        l)  </div><div class="line">            echo &quot;$(cat $lsblk_path&quot;/lsblk_list.txt&quot;)&quot;</div><div class="line">            ;;  </div><div class="line">        \?) </div><div class="line">            echo &quot;Invalid option: -$OPTARG&quot;</div><div class="line">            ;;  </div><div class="line">    esac</div><div class="line">done</div></pre></td></tr></table></figure><p><code>lsblk.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">NAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT </div><div class="line">sda        8:0    0   3.7T  0 disk </div><div class="line">├─sda1     8:1    0  30.5M  0 part ar</div><div class="line">├─sda2     8:2    0 488.3M  0 part ar</div><div class="line">├─sda3     8:3    0  93.1G  0 part /rt</div><div class="line">├─sda4     8:4    0   256G  0 part [SWAP]WA</div><div class="line">└─sda5     8:5    0   3.3T  0 part /data/osd.0sd</div><div class="line">sdb        8:16   0 372.1G  0 disk </div><div class="line">└─sdb1     8:17   0 372.1G  0 part ar</div><div class="line">sdc        8:32   0   2.7T  0 disk </div><div class="line">└─sdc1     8:33   0   2.7T  0 part /data/osd.1sd</div><div class="line">sdd        8:48   0   2.7T  0 disk </div><div class="line">└─sdd1     8:49   0   2.7T  0 part /data/osd.2sd</div><div class="line">sde        8:64   0   2.7T  0 disk </div><div class="line">└─sde1     8:65   0   2.7T  0 part /data/osd.3sd</div><div class="line">sdf        8:80   0   2.7T  0 disk </div><div class="line">└─sdf1     8:81   0   2.7T  0 part /data/osd.11d.</div><div class="line">sdg        8:96   0   2.7T  0 disk </div><div class="line">└─sdg1     8:97   0   2.7T  0 part /data/osd.12d.</div><div class="line">sdh        8:112  0   2.7T  0 disk </div><div class="line">└─sdh1     8:113  0   2.7T  0 part ar</div><div class="line">sdi        8:128  0   2.7T  0 disk </div><div class="line">└─sdi1     8:129  0   2.7T  0 part /data/osd.8sd</div><div class="line">sdj        8:144  0   2.7T  0 disk </div><div class="line">└─sdj1     8:145  0   2.7T  0 part /data/osd.9sd</div><div class="line">sdk        8:160  0   2.7T  0 disk </div><div class="line">└─sdk1     8:161  0   2.7T  0 part /data/osd.10d.</div><div class="line">sdl        8:176  0   2.7T  0 disk </div><div class="line">└─sdl1     8:177  0   2.7T  0 part ar</div><div class="line">sdm        8:192  0   2.7T  0 disk </div><div class="line">└─sdm1     8:193  0   2.7T  0 part ar</div><div class="line">sdn        8:208  0   2.7T  0 disk </div><div class="line">└─sdn1     8:209  0   2.7T  0 part ar</div><div class="line">sdo        8:224  0   2.7T  0 disk </div><div class="line">└─sdo1     8:225  0   2.7T  0 part</div><div class="line">sdp        8:240  0   2.7T  0 disk</div><div class="line">└─sdp1     8:241  0   2.7T  0 part</div><div class="line">sdq       65:0    0   2.7T  0 disk</div><div class="line">└─sdq1    65:1    0   2.7T  0 part</div><div class="line">sdr       65:16   0   2.7T  0 disk</div><div class="line">└─sdr1    65:17   0   2.7T  0 part</div><div class="line">sds       65:32   0   2.7T  0 disk</div><div class="line">└─sds1    65:33   0   2.7T  0 part</div><div class="line">sdt       65:48   0   2.7T  0 disk</div><div class="line">└─sdt1    65:49   0   2.7T  0 part</div><div class="line">sdu       65:64   0   2.7T  0 disk</div><div class="line">└─sdu1    65:65   0   2.7T  0 part</div><div class="line">sdv       65:80   0 744.7G  0 disk</div><div class="line">├─sdv1    65:81   0    50G  0 part</div><div class="line">├─sdv2    65:82   0    50G  0 part</div><div class="line">├─sdv3    65:83   0    50G  0 part</div><div class="line">├─sdv4    65:84   0    50G  0 part</div><div class="line">├─sdv5    65:85   0    50G  0 part</div><div class="line">├─sdv6    65:86   0    50G  0 part</div><div class="line">├─sdv7    65:87   0    50G  0 part</div><div class="line">├─sdv8    65:88   0    50G  0 part</div><div class="line">├─sdv9    65:89   0    50G  0 part</div><div class="line">├─sdv10   65:90   0    50G  0 part</div><div class="line">├─sdv11   65:91   0    50G  0 part</div><div class="line">└─sdv12   65:92   0    50G  0 part</div></pre></td></tr></table></figure><p><code>lsblk_list.txt</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div></pre></td><td class="code"><pre><div class="line">NAME   MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT</div><div class="line">sda      8:0    0   3.7T  0 disk </div><div class="line">sda1     8:1    0  30.5M  0 part </div><div class="line">sda2     8:2    0 488.3M  0 part </div><div class="line">sda3     8:3    0  93.1G  0 part /</div><div class="line">sda4     8:4    0   256G  0 part [SWAP]</div><div class="line">sda5     8:5    0   3.3T  0 part /data/osd.0</div><div class="line">sdb      8:16   0 372.1G  0 disk </div><div class="line">sdb1     8:17   0 372.1G  0 part </div><div class="line">sdc      8:32   0   2.7T  0 disk </div><div class="line">sdc1     8:33   0   2.7T  0 part /data/osd.1</div><div class="line">sdd      8:48   0   2.7T  0 disk </div><div class="line">sdd1     8:49   0   2.7T  0 part /data/osd.2</div><div class="line">sde      8:64   0   2.7T  0 disk </div><div class="line">sde1     8:65   0   2.7T  0 part /data/osd.3</div><div class="line">sdf      8:80   0   2.7T  0 disk </div><div class="line">sdf1     8:81   0   2.7T  0 part /data/osd.11</div><div class="line">sdg      8:96   0   2.7T  0 disk </div><div class="line">sdg1     8:97   0   2.7T  0 part /data/osd.12</div><div class="line">sdh      8:112  0   2.7T  0 disk </div><div class="line">sdh1     8:113  0   2.7T  0 part </div><div class="line">sdi      8:128  0   2.7T  0 disk </div><div class="line">sdi1     8:129  0   2.7T  0 part /data/osd.8</div><div class="line">sdj      8:144  0   2.7T  0 disk </div><div class="line">sdj1     8:145  0   2.7T  0 part /data/osd.9</div><div class="line">sdk      8:160  0   2.7T  0 disk </div><div class="line">sdk1     8:161  0   2.7T  0 part /data/osd.10</div><div class="line">sdl      8:176  0   2.7T  0 disk </div><div class="line">sdl1     8:177  0   2.7T  0 part </div><div class="line">sdm      8:192  0   2.7T  0 disk </div><div class="line">sdm1     8:193  0   2.7T  0 part </div><div class="line">sdn      8:208  0   2.7T  0 disk </div><div class="line">sdn1     8:209  0   2.7T  0 part </div><div class="line">sdo      8:224  0   2.7T  0 disk </div><div class="line">sdo1     8:225  0   2.7T  0 part</div><div class="line">sdp      8:240  0   2.7T  0 disk</div><div class="line">sdp1     8:241  0   2.7T  0 part</div><div class="line">sdq     65:0    0   2.7T  0 disk</div><div class="line">sdq1    65:1    0   2.7T  0 part</div><div class="line">sdr     65:16   0   2.7T  0 disk</div><div class="line">sdr1    65:17   0   2.7T  0 part</div><div class="line">sds     65:32   0   2.7T  0 disk</div><div class="line">sds1    65:33   0   2.7T  0 part</div><div class="line">sdt     65:48   0   2.7T  0 disk</div><div class="line">sdt1    65:49   0   2.7T  0 part</div><div class="line">sdu     65:64   0   2.7T  0 disk</div><div class="line">sdu1    65:65   0   2.7T  0 part</div><div class="line">sdv     65:80   0 744.7G  0 disk</div><div class="line">sdv1    65:81   0    50G  0 part</div><div class="line">sdv2    65:82   0    50G  0 part</div><div class="line">sdv3    65:83   0    50G  0 part</div><div class="line">sdv4    65:84   0    50G  0 part</div><div class="line">sdv5    65:85   0    50G  0 part</div><div class="line">sdv6    65:86   0    50G  0 part</div><div class="line">sdv7    65:87   0    50G  0 part</div><div class="line">sdv8    65:88   0    50G  0 part</div><div class="line">sdv9    65:89   0    50G  0 part</div><div class="line">sdv10   65:90   0    50G  0 part</div><div class="line">sdv11   65:91   0    50G  0 part</div><div class="line">sdv12   65:92   0    50G  0 part</div></pre></td></tr></table></figure><p>还有一些其他工具我就不详细贴代码了，具体请去<code>github</code>上查看。</p><p>项目地址：<a href="https://github.com/tony-yin/Hardware_Test_Tool" target="_blank" rel="external">https://github.com/tony-yin/Hardware_Test_Tool</a></p><p>操作步骤：</p><p>1.下载代码</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone git@github.com:tony-yin/Hardware_Test_Tool.git</div></pre></td></tr></table></figure><p>2.安装工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./build install</div></pre></td></tr></table></figure><p>3.卸载工具</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./build uninstall</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/hardware-600x450.jpg&quot; alt=&quot;hardware&quot;&gt;&lt;/center&gt;

&lt;p&gt;最近在做一个&lt;code&gt;feature&lt;/code&gt;，测试的时候需要硬件环境的支撑。一般我们开发环境都是虚拟机，所以针对这种开发工作的自测无法进行，比如虚拟机上没有物理磁盘，没有&lt;code&gt;raid&lt;/code&gt;卡等，为了一个小功能的测试，需要出&lt;code&gt;build&lt;/code&gt;，需要硬件环境的部署和安装，这个工作量着实不小。&lt;/p&gt;
&lt;p&gt;往往针对这种情况，作为开发人员可以针对硬件环境的具体需求，尽可能在自己的环境上做模拟，也就是我们俗称的“打桩”。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Test" scheme="https://tony-yin.github.io/tags/Test/"/>
    
  </entry>
  
  <entry>
    <title>Daily Article Vol 3 - (2018/2/1 ~ 2018/2/28)</title>
    <link href="https://tony-yin.github.io/2018/03/01/Daily-Article-Vol3/"/>
    <id>https://tony-yin.github.io/2018/03/01/Daily-Article-Vol3/</id>
    <published>2018-03-01T02:13:25.000Z</published>
    <updated>2018-03-09T07:18:32.159Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/february-600x450.jpg" alt="Daily Article 3"></center><p>这段期间，一方面是因为过年，另一方面因为换工作，所以诸事缠身，导致每天阅读学习的量严重受影响，不能保持每天都有固定的时间学习和阅读，还是自己的问题，不应该找借口。之后，不管什么事情，必须要保证每天的学习量和阅读量，养成雷打不动的好习惯。</p><p>这是<code>Daily Article</code>系列的第三篇，罗列了<code>2018</code>年<code>2</code>月的阅读清单。</p><a id="more"></a><hr><ol><li><a href="https://my.oschina.net/u/2460844/blog/669769" target="_blank" rel="external">ceph的数据存储之路(10) —–ceph对象存储的ls命令实现及思考</a>(2/1 ~ 2/5) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://python.jobbole.com/82270/" target="_blank" rel="external">八大排序算法的 Python 实现</a>(2/6)</li><li><a href="https://georgezhuo.github.io/georgezhuo.github.io/2015/12/22/ceph-message/" target="_blank" rel="external">Ceph解析-消息处理模块</a>(2/7) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://www.wzxue.com/ceph-network/" target="_blank" rel="external">解析Ceph: 网络层的处理</a>(2/7)</li><li><a href="http://blog.csdn.net/skdkjzz/article/details/41980631" target="_blank" rel="external">ceph存储 ceph集群消息处理</a>(2/8)</li><li><a href="https://my.oschina.net/u/2460844/blog/531646" target="_blank" rel="external">ceph的数据存储之路(1) —rbd设备介绍</a>(2/11) <i class="fa fa-star"></i></li><li><a href="https://my.oschina.net/u/2460844/blog/531686" target="_blank" rel="external">ceph的数据存储之路(2) —– rbd到osd的数据映射</a>(2/11)</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513393&amp;idx=1&amp;sn=c1d6caca8ef9972f1105df982f15bb58&amp;chksm=80d679f2b7a1f0e410fd53691d68a9ada158aac2b8814fe329d6dfed86ee54b68a53eb84cdb0&amp;scene=21#wechat_redirect" target="_blank" rel="external">张大胖学递归</a>(2/16) <i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513387&amp;idx=1&amp;sn=99665948d0b968cf15c5e7a01ffe166c&amp;chksm=80d679e8b7a1f0febad077b57e8ad73bfb4b08de74814c45e1b1bd61ab4017b5041942403afb&amp;scene=21#wechat_redirect" target="_blank" rel="external">张大胖的socket</a>(2/17) <i class="fa fa-star"></i></li><li><a href="https://my.oschina.net/u/2460844/blog/532755?p=2&amp;temp=1519178944837#blog-comments-list" target="_blank" rel="external">ceph的数据存储之路(4) —– rbd client 端的数据请求处理</a>(2/18~2/20) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513353&amp;idx=1&amp;sn=a5dc69542fae6aabf0fef9b5f5881a9d&amp;chksm=80d679cab7a1f0dc530bd1745c2c9552b739afc701ecb2f8e1eba8624d1fefc2c3cc64cd1d30&amp;scene=21#wechat_redirect" target="_blank" rel="external">学习面向对象的令狐冲</a>(2/21) <i class="fa fa-star"></i></li><li><a href="http://mp.weixin.qq.com/s/wyt-0y3lEhewa2cpeILYSQ" target="_blank" rel="external">张大胖学数据库</a>(2/22)</li><li><a href="https://mp.weixin.qq.com/s/tSF_w9xUOj3Q2hmOxJkwLg" target="_blank" rel="external">数据库村的旺财和小强</a>(2/23) <i class="fa fa-star"></i></li><li><a href="https://my.oschina.net/u/2460844/blog/534390?p=1&amp;temp=1519628036891#blog-comments-list" target="_blank" rel="external">ceph的数据存储之路(5) —–osd数据处理</a>(2/24 ~ 2/26) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzI0NDE0NjUxMQ==&amp;mid=2651256389&amp;idx=1&amp;sn=e11edcce5722853f442b9a7b8211787e&amp;chksm=f2901e65c5e79773c7690f29e35dbd1870a5bfdb92c70541979f5d080d6580e3af9ba85fff66&amp;mpshare=1&amp;scene=23&amp;srcid=0502SazrSPsWnszP3xfdEId4#rd" target="_blank" rel="external"> Ceph开发每周谈 Vol 70 | RGW 同步到 AWS S3 | Elastic Search API 整合</a>(2/26)</li><li><a href="http://ceph.com/planet/ceph-osd%E4%BB%8Efilestore-%E8%BD%AC%E6%8D%A2%E5%88%B0-bluestore%E7%9A%84%E6%96%B9%E6%B3%95/" target="_blank" rel="external">Ceph OSD从filestore 转换到 bluestore的方法</a>(2/27)</li><li><a href="https://my.oschina.net/u/2460844/blog/535007" target="_blank" rel="external">ceph的数据存储之路(6) —–pg的创建</a>(2/28)</li><li><a href="https://www.sogou.com/link?url=6IqLFeTuIyjnBL6rnEdhCp_rFXz42FvmEhOk1arvVrbkYalcctcthKCfjFS1STqz" target="_blank" rel="external">XSKY在OpenStack场景下的优势</a>(2/28)</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/february-600x450.jpg&quot; alt=&quot;Daily Article 3&quot;&gt;&lt;/center&gt;

&lt;p&gt;这段期间，一方面是因为过年，另一方面因为换工作，所以诸事缠身，导致每天阅读学习的量严重受影响，不能保持每天都有固定的时间学习和阅读，还是自己的问题，不应该找借口。之后，不管什么事情，必须要保证每天的学习量和阅读量，养成雷打不动的好习惯。&lt;/p&gt;
&lt;p&gt;这是&lt;code&gt;Daily Article&lt;/code&gt;系列的第三篇，罗列了&lt;code&gt;2018&lt;/code&gt;年&lt;code&gt;2&lt;/code&gt;月的阅读清单。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
      <category term="Daily-Article" scheme="https://tony-yin.github.io/tags/Daily-Article/"/>
    
  </entry>
  
  <entry>
    <title>Daily Article Vol 2 - (2018/1/1 ~ 2018/1/31)</title>
    <link href="https://tony-yin.github.io/2018/02/01/Daily-Article-Vol2/"/>
    <id>https://tony-yin.github.io/2018/02/01/Daily-Article-Vol2/</id>
    <published>2018-02-01T02:07:06.000Z</published>
    <updated>2018-02-01T02:02:38.966Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/2018-01-600x450.png" alt="Daily Article Vol2"></center><p>这是<code>Daily Article</code>系列的第二篇，罗列了<code>2018</code>年<code>1</code>月的阅读清单。</p><a id="more"></a><ol><li><a href="http://datawarehouse4u.info/OLTP-vs-OLAP.html" target="_blank" rel="external">OLTP vs. OLAP</a>(1/2)</li><li><a href="http://blog.csdn.net/zhanghaocore/article/details/9820215" target="_blank" rel="external">OLTP和OLAP的区别</a>(1/2)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-31/" target="_blank" rel="external">Ceph开发每周谈 Vol 31｜ZetaScale 开源 | DMClock</a>(1/2)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-32/" target="_blank" rel="external">Ceph开发每周谈 Vol 32｜Ceph-osd on 4.x 内核异常</a>(1/3)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-33/" target="_blank" rel="external">Ceph开发每周谈 Vol 33｜Encode 改进方案</a>(1/3)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-34/" target="_blank" rel="external">Ceph开发每周谈Vol 34 | Ceph Days 亚太路演 | Ceph OSD CLASS 支持 LUA 编程</a>(1/3)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-35/" target="_blank" rel="external">Ceph开发每周谈 Vol 35 | Ceph Developer Month</a>(1/3)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-36/" target="_blank" rel="external">Ceph开发每周谈 Vol 36|Ebay 的 CephFS 使用深度报告</a>(1/4)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-37/" target="_blank" rel="external">Ceph开发每周谈 Vol 37｜NVME Over Fabric｜FAST</a>(1/4)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665514131&amp;idx=1&amp;sn=a11640045e1458c0e3ba866d23541526&amp;chksm=80d67cd0b7a1f5c6c4dee2cb64b6f173524d443dae8be471bb901eebeec4db5d1773818080b7&amp;scene=21#wechat_redirect" target="_blank" rel="external">【码农翻身】 浏览器：一个家族的奋斗</a>(1/6) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665514143&amp;idx=1&amp;sn=28ea209c00309e6b93d8d1f76032d7a4&amp;chksm=80d67cdcb7a1f5ca81d8d454a98af56d58b22f6058f100e21ff30e70867ea6e3e922a4f000bf&amp;scene=21%23wechat_redirect" target="_blank" rel="external">【码农翻身】 浏览器家族的安全反击战</a>(1/6) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s/YvYvL0siJT1UhO0tXnYVNA" target="_blank" rel="external">【码农翻身】 黑客三兄弟</a>(1/6) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513220&amp;idx=1&amp;sn=bb9c4df63cf6994d6aab9d77a10fe628&amp;scene=21#wechat_redirect" target="_blank" rel="external">【码农翻身】 GitHub/Stackoverflow 找工作时有什么用？</a>(1/6)</li><li><a href="https://mp.weixin.qq.com/s/-BMCUuIWYE3O_oC3ZNNJRg" target="_blank" rel="external">【码农翻身】 我是一个线程(修订版)</a>(1/7) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://www.freeoa.net/osuport/botinstal/instal-compile-linux-ipvs-mod_3170.html" target="_blank" rel="external">Linux下安装编译IPVS内核模块</a>(1/8) <i class="fa fa-star"></i></li><li><a href="https://www.tuicool.com/articles/RfmI3u" target="_blank" rel="external">How to Repack Deb Files on Debian and Ubuntu</a> <i class="fa fa-star"></i>(1/8)</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665514224&amp;idx=1&amp;sn=5b802dfab658a626a197635cf56c9bac&amp;chksm=80d67cb3b7a1f5a585f778b28ceb88855b140ff090c0d0b00f10e9e9b889c81c20190ba80a06&amp;scene=38#wechat_redirect" target="_blank" rel="external">什么是DevOps</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/8)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=416915373&amp;idx=1&amp;sn=f80a13b099237534a3ef777d511d831a&amp;scene=21#wechat_redirect" target="_blank" rel="external">我是一个线程</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/8)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513059&amp;idx=1&amp;sn=a2eaf97d9e3000d15a33681d1b720463&amp;scene=21#wechat_redirect" target="_blank" rel="external">Javascript: 一个屌丝的逆袭</a> <i class="fa fa-star"></i><i class="fa fa-star"></i>(1/9)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513094&amp;idx=1&amp;sn=a2accfc41107ac08d74ec3317995955e&amp;scene=21#wechat_redirect" target="_blank" rel="external">TCP/IP 之 大明王朝的邮差</a> <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/9)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513375&amp;idx=1&amp;sn=e11745d5cb28fa1f89465f8d0e5fae1a&amp;chksm=80d679dcb7a1f0cadec0a1db45b3fa3f6eabde4a9e56a77acf551fbc1511e6b8f055ee2c97e3&amp;scene=21#wechat_redirect" target="_blank" rel="external">TCP/IP 之 大明内阁</a> <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/9)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513384&amp;idx=1&amp;sn=82c45e4430618270a744e212d2f57990&amp;chksm=80d679ebb7a1f0fd27fe1814765a0e60b1c293ca4e09403271b4bb3f38f317439956fdca1bc1&amp;scene=21#wechat_redirect" target="_blank" rel="external">TCP/IP 之 蓟辽督师</a> <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/10)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513017&amp;idx=1&amp;sn=5550ee714abd36d0b580713f673e670b&amp;scene=21#wechat_redirect" target="_blank" rel="external">CPU 阿甘</a> <i class="fa fa-star"></i>(1/10)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513254&amp;idx=1&amp;sn=a4d1912b6259c3e65c0e172fb5a10dbb&amp;scene=21#wechat_redirect" target="_blank" rel="external">CPU 阿甘之烦恼</a> <i class="fa fa-star"></i>(1/10)</li><li><a href="https://mp.weixin.qq.com/s?__biz=MjM5NTU2MTQwNA==&amp;mid=2650655754&amp;idx=1&amp;sn=0b03b942ecdb25ef45ea3a9cbff686c3&amp;chksm=beffc6d989884fcf49c79c662162ed7852b17e192b7f40fcda040308eb10b26a9b187f675940&amp;scene=0&amp;key=78733623947167cee9eb37fa8148459e73e455140b258998bf251e0d57524ceaaa6187e820f481781748d004b780efc58804a25dfdf285670ca35369d0e62d2d8e7cd8ca84306d36da7105f9c8042683&amp;ascene=0&amp;uin=MjEzMTAwMzgyNQ%3D%3D&amp;devicetype=iMac+MacBookAir7%2C1+OSX+OSX+10.12.6+build(16G29)&amp;version=12020810&amp;nettype=WIFI&amp;lang=zh_CN&amp;fontScale=100" target="_blank" rel="external">开源万岁！2018 年开源技术的 10 大发展趋势</a>(1/10)</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzIxMTE0ODU5NQ%3D%3D&amp;mid=2650236979&amp;idx=1&amp;sn=71f07d1741a57f8fd429d76d37fd8a07&amp;chksm=8f5a026fb82d8b7931f95e747224b1049d1b0cf72f72babbf6629e183fa15c72570be4fc7253" target="_blank" rel="external">面试过阿里等互联网大公司，我知道了这些套路</a>(1/11)</li><li><a href="https://www.oschina.net/question/2928191_2272289" target="_blank" rel="external">访谈 | 鸟哥惠新宸：程序员应该不断提升自身的不可替代性</a>(1/15)</li><li><a href="https://mp.weixin.qq.com/s/_-7C_ZfFfvNKhBQzSB6j4Q" target="_blank" rel="external">黑客三兄弟（续）</a> <i class="fa fa-star"></i><i class="fa fa-star"></i>(1/19)</li><li><a href="https://mp.weixin.qq.com/s/vyHlB9pem4rv4htJS9ca6Q" target="_blank" rel="external">我是一个网卡</a> <i class="fa fa-star"></i><i class="fa fa-star"></i>(1/20)</li><li><a href="https://mp.weixin.qq.com/s/VyGQ4-Dn4UX2Z0CrCHgUqw" target="_blank" rel="external">我是一个路由器</a> <i class="fa fa-star"></i><i class="fa fa-star"></i>(1/21)</li><li><a href="https://mp.weixin.qq.com/s/KtHxMoc1_3sQd4d_MuKKJA" target="_blank" rel="external">我是一个进程</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i>(1/22)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513289&amp;idx=1&amp;sn=ab19dcad7b1dc217463f155fe106091a&amp;scene=21#wechat_redirect" target="_blank" rel="external">我是一块硬盘（上）</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/22)</li><li><a href="http://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513292&amp;idx=1&amp;sn=5b1c87dc72a20c92883924080174b16b&amp;scene=21#wechat_redirect" target="_blank" rel="external">我是一块硬盘（下）</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/23)</li><li><a href="https://mp.weixin.qq.com/s/gliuPj9tfZkr9oXQn9re2w" target="_blank" rel="external">如何维护一个好的技术博客？</a> <i class="fa fa-star"></i>(1/23)</li><li><a href="https://mp.weixin.qq.com/s/643URs9k_EQIMWGmb35QkQ" target="_blank" rel="external">算法分析神器—时间复杂度</a> <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star-half-full"></i>(1/23)</li><li><a href="https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&amp;mid=2665513299&amp;idx=1&amp;sn=264f4d0891e1b96fb5e356dc7b6c91dc&amp;scene=21#wechat_redirect" target="_blank" rel="external">我是一个键盘</a> <i class="fa fa-star"></i><i class="fa fa-star"></i> (1/24)</li><li><a href="https://mengkang.net/1129.html" target="_blank" rel="external">PHP 面试题 - 如果没有 mb 系列函数，如何切割多字节字符串</a>(1/28)</li><li><a href="http://www.quts.me/ceph-readwrite/" target="_blank" rel="external">ceph读写流程分析</a>(1/28)</li><li><a href="https://segmentfault.com/a/1190000013010835" target="_blank" rel="external">Token 认证的来龙去脉</a>(1/29) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://bean-li.github.io/ceph-read-flow/" target="_blank" rel="external">ceph 读流程(1)</a>(1/29) <i class="fa fa-star"></i></li><li><a href="https://mp.weixin.qq.com/s/gI1TjeAYjqgNo4RKqEqF-Q" target="_blank" rel="external">一个项目做完以后，不能就让它这么“完”了</a>(1/30)</li><li><a href="https://mp.weixin.qq.com/s/hDKQ1ITzTvi20kTA11xa1g" target="_blank" rel="external">科学与星球大战：当科幻遇到现实</a>(1/30) <i class="fa fa-star"></i></li><li><a href="https://bean-li.github.io/ceph-read-2/" target="_blank" rel="external">ceph 读流程(2)</a>(1/31) <i class="fa fa-star"></i></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/2018-01-600x450.png&quot; alt=&quot;Daily Article Vol2&quot;&gt;&lt;/center&gt;

&lt;p&gt;这是&lt;code&gt;Daily Article&lt;/code&gt;系列的第二篇，罗列了&lt;code&gt;2018&lt;/code&gt;年&lt;code&gt;1&lt;/code&gt;月的阅读清单。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
      <category term="Daily-Article" scheme="https://tony-yin.github.io/tags/Daily-Article/"/>
    
  </entry>
  
  <entry>
    <title>实现RBD导出NFS高可用（二）：提供多虚拟IP访问</title>
    <link href="https://tony-yin.github.io/2018/01/28/RBD-HA-2/"/>
    <id>https://tony-yin.github.io/2018/01/28/RBD-HA-2/</id>
    <published>2018-01-28T05:07:06.000Z</published>
    <updated>2018-01-30T12:48:51.245Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/rbd2-600x450.jpg" alt="RBD-HA-2"></center><p>之前分享过一篇<a href="http://www.tony-yin.top/2017/12/07/RBD-HA/" target="_blank" rel="external">【通过 Keepalived 实现 Ceph RBD 的高可用】</a>，主要讲解了将<code>RBD</code>导出为<code>NFS</code>，然后通过<code>keepalived</code>实现高可用，保证当提供虚拟<code>IP</code>节点发生故障时，可以自动切换节点，使得业务不发生中断。</p><p>这样可以基本使用<code>RBD</code>代替<code>CephFS</code>对外提供<code>Ceph</code>服务，至于为什么不用<code>CephFS</code>就不多说了，不清楚的可以去看上一篇。虽然说这样可以保证无单点故障，但是有一点还是不如<code>CephFS</code>，那就是<code>CephFS</code>可以实现多节点同时提供服务，而<code>RBD</code>说白了其实同时只有一个节点能提供服务，当客户端流量高的时候，<code>RBD</code>方式的带宽并不能满足需求。就比如都是三个节点，<code>CephFS</code>可以将客户端流量分流到三个节点，而<code>RBD</code>只能用一个节点，而带宽上限又取决与网卡、磁盘和<code>IO</code>等等原因，所以同样的硬件设施<code>RBD</code>的带宽性能是跟不上的，本文就多虚拟<code>IP</code>暴露访问方式进行分享。</p><a id="more"></a><h2 id="CephFS-amp-RBD"><a href="#CephFS-amp-RBD" class="headerlink" title="CephFS &amp; RBD"></a>CephFS &amp; RBD</h2><p>此前的文章我们<code>Ceph</code>集群只有一个<code>RBD image</code>，并且只通过一个<code>vip</code>暴露这个<code>image</code>让客户端通过<code>NFS</code>访问。这与<code>CephFS</code>的差距就在没有充分利用每个节点的资源，所以我们可以大胆设想一下是否可以通过<code>RBD</code>对外提供多个<code>vip</code>，每个节点都能被<code>NFS</code>访问呢？理想很美好，现实很残酷。如果一个<code>RBD</code>对多个节点同时提供读写的话，会导致不一致的后果，现在<code>RBD</code>并不能做到<code>CephFS</code>那样多个节点同时提供服务且保证读写一致。那怎么办呢？</p><p>虽然一个<code>RBD image</code>不能同时被多客户端访问，但是我们是否可以创建多个<code>RBD image</code>，然后利用多个<code>vip</code>对外提供访问呢？这样听起来貌似可行，但是还是存在诸多问题，比如如何暴露多虚拟<code>IP</code>，如何将<code>IP</code>绑定到具体的<code>RBD image</code>，如何保证多<code>RBD image</code>的高可用等等，下文将就这些技术细节进行详细地分析。</p><h2 id="需求背景"><a href="#需求背景" class="headerlink" title="需求背景"></a>需求背景</h2><p>客户端有多种应用场景，对流量要求较高的情况下，我们可以为每一种应用场景都提供一个<code>vip</code>用于<code>NFS</code>方式访问<code>Ceph</code>存储集群。然后每个<code>vip</code>各自对应集群中的一个<code>RBD image</code>，<code>RBD image</code>尽量均匀的分布到各个节点上，这样才能把性能提升到最高，比如集群有三个节点的话，如果暴露三个<code>vip</code>，那么必须要分布到三个不同的节点上，如果要提供四个<code>vip</code>的话，那么前三个<code>vip</code>均匀地分布到三个节点上，第四个<code>vip</code>就在第一个节点上暴露，以此类推，这边说的第一个节点只是我们自己将三个节点进行逻辑上的排序，我们需要通过一些算法确保<code>vip</code>分布均匀，具体看下文分析。</p><h2 id="整体设计"><a href="#整体设计" class="headerlink" title="整体设计"></a>整体设计</h2><p>一般在完成一个<code>feature</code>之前，我们往往需要做一个<code>design</code>，对要做的事情和流程进行设计和评估，这样不但可以梳理流程，使得之后动手的时候思路清晰，更重要的是可以预见一些问题和难点，尽早与 团队成员进行交流，选择最佳方案，防止真正做的时候走弯路。这边涉及的技术点主要有：</p><h3 id="Keepalived暴露多个VIP"><a href="#Keepalived暴露多个VIP" class="headerlink" title="Keepalived暴露多个VIP"></a>Keepalived暴露多个VIP</h3><p><code>keepalived</code>暴露单个<code>vip</code>很常见，具体格式网上都有，而暴露多个<code>vip</code>就要注意一些细节，比如<code>router_id</code>，<code>ins_name</code>，<code>priority</code>等等，对于一个节点而言，它上面<code>keepalived</code>暴露<code>vip</code>的情况完全是由配置文件<code>keepalived.conf</code>所决定的，而对于<code>keepalived.conf</code>而言，一个<code>vip</code>其实就是<code>ins</code>，而<code>ins_name</code>和<code>router_id</code>要求同一个<code>keepalived</code>组内成员相同，我们这边就默认<code>router_id</code>就是<code>vip</code>隔着小数点的四位整数相加的和，而<code>ins_name</code>则是将<code>vip</code>的小数点换成下划线。</p><h3 id="VIP动态均匀分布"><a href="#VIP动态均匀分布" class="headerlink" title="VIP动态均匀分布"></a>VIP动态均匀分布</h3><p><code>vip</code>均匀分布要保证尽可能的均匀，比如三个节点，如果要提供两个<code>vip</code>的话，那就随意挑选两个节点作为<code>vip</code>绑定，如果四个<code>vip</code>的话，则是三个节点各自绑定一个<code>vip</code>后再任意选择一个节点作为第四个<code>vip</code>绑定。我们这边的做法是先将所有节点进行排序，将两个节点作为一个<code>keepalived</code>组，下两个节点为另外一组，假设有三个节点，我们设为<code>1, 2, 3</code>，那么如果要暴露三个<code>vip</code>，我们就需要三个<code>keepalived</code>组，这边三个组分别是<code>1, 2</code>，<code>3, 1</code>和<code>2, 3</code>，然后组内其中第一个节点为<code>master</code>，第二个节点为<code>backup</code>。这样可以基本保证所有<code>vip</code>的均匀分布，具体算法实现参见下文。</p><h3 id="多RBD高可用"><a href="#多RBD高可用" class="headerlink" title="多RBD高可用"></a>多RBD高可用</h3><p>上一篇文章中只有一个<code>RBD</code>，所以高可用就围绕它一个，发生故障后随意切换节点即可，因为我们每个节点都是一个<code>keepalived</code>组的成员。但是如果有多个<code>RBD</code>的话，我们如果随意切换的话，那么<code>RBD</code>分布就会变得不均匀。上文提及的算法可以保证<code>vip</code>的均匀分布，两两节点作为一个<code>keepalived</code>组，这样我们即使一个节点掉了，切换也只会在当前组内切换，而<code>vip</code>一开始绑定节点的时候就根据相应算法保证了每个<code>RBD</code>的均匀分布，所以这边组内切换不会影响分布的均匀性。</p><p>上一篇文章中提过<code>keepalived</code>的机制，当主节点<code>down</code>了，主节点会触发我们自己写的<code>ChangetoBackup.sh</code>，而副节点则会触发<code>ChangetoMaster.sh</code>。之前由于只有一个<code>RBD</code>，所以当时做的比较无脑，<code>ChangetoMaster.sh</code>直接遍历当前节点上面的所有<code>RBD</code>，然后通过之前记录的<code>RBD</code>和<code>UI</code>上创建的<code>目录</code>的映射关系进行挂载，而<code>ChangetoBackup.sh</code>也是一样的<code>umount</code>所有<code>RBD</code>的挂载点。针对目前的多<code>RBD</code>的情况，这样的做法肯定是不行的，因为现在我们一个节点可能是一个或多个<code>vip</code>的<code>master</code>，也可能是另外一个或多个<code>vip</code>的<code>backup</code>，如果我们还是像之前那样一股脑的全部卸载或者挂载，那么造成的后果显而易见，就是业务中断，暴露服务节点紊乱。所以最合理的应该对号入座，一个<code>vip</code>对应一个<code>RBD image</code>，哪个<code>vip</code>出现了问题，作为该<code>vip</code>的<code>master</code>节点，应该只<code>umount</code>该<code>vip</code>绑定<code>RBD</code>所对应的目录，而<code>backup</code>节点应该只<code>mount</code>对应的目录。其他不相关<code>RBD</code>和其对应的目录，我们都不应该有所操作。那么我们只有在触发<code>ChangetoMaster.sh</code>和<code>ChangetoBackup.sh</code>这两个脚本的时候加上“目录”这个参数，具体实现详见下文分析。</p><h3 id="大容量RBD-image的创建和删除"><a href="#大容量RBD-image的创建和删除" class="headerlink" title="大容量RBD image的创建和删除"></a>大容量RBD image的创建和删除</h3><p>我们系统的实现是<code>UI</code>上创建目录，后端<code>daemon</code>轮询根据目录信息做对应的事情，比如前端<code>UI</code>创建了目录，后端就是在创建<code>RBD image</code>，而生产环境上面的容量的要求都是很高的，往往都是几十<code>T</code>，甚至上百<code>T</code>,但是熟悉<code>RBD</code>的朋友都知道创建如此大的<code>RBD image</code>是需要很长的时间的，那这样就不但会影响当前目录能够提供服务的时间，也会阻塞住代码，影响之后目录的创建。之前我们的做法是一开始我们可以创建一个比较小的<code>image</code>，然后我们后台选择在业务不繁忙的时候进行定时扩容，这也可以算是暂时止血了。但是后来测试发现删除<code>image</code>才是真的慢，这边就不像创建那样有曲线救国的方式了，所以这边无论是创建还是删除<code>RBD image</code>我们都不能做成同步的方式了，我们采取了另起一个线程单独做这个事情，不影响后端业务的正常处理。</p><h3 id="快照保证扩容的安全性"><a href="#快照保证扩容的安全性" class="headerlink" title="快照保证扩容的安全性"></a>快照保证扩容的安全性</h3><p>在我们的测试过程中，发现对<code>RBD image</code>扩容会偶尔发生文件系统出错的情况，这种情况是很危险的，一旦文件系统发生问题，并且用<code>e2fsck</code>等工具修复不了的话，那么数据恢复是很困难的，我们必须要保证客户数据的安全性。所以我们用了<code>RBD</code>的<code>snapshot</code>的功能，在每次扩容之前为<code>RBD image</code>做快照，这样即使发生了问题，我们起码可以做到最小程度的损失。</p><h2 id="具体代码实现"><a href="#具体代码实现" class="headerlink" title="具体代码实现"></a>具体代码实现</h2><h3 id="Keepalived暴露多个VIP-1"><a href="#Keepalived暴露多个VIP-1" class="headerlink" title="Keepalived暴露多个VIP"></a>Keepalived暴露多个VIP</h3><p>当<code>UI</code>创建一个<code>vip</code>的时候，我们就要加一个<code>ins</code>，以下就是我们添加一个<code>ins</code>的<code>API</code>，本文所有代码都是<code>python</code>写的，大家凑合看吧。（部分代码和接口不是很全，文章尾部将会贴出详细代码的地址）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_keepalived_ins</span><span class="params">(self, vip, folder, state)</span>:</span></div><div class="line">    vrrp_ins = <span class="string">"""</span></div><div class="line"><span class="string">vrrp_instance VI_&#123;ins_name&#125; &#123;&#123;</span></div><div class="line"><span class="string">    state &#123;state&#125;</span></div><div class="line"><span class="string">    interface &#123;pubif&#125;</span></div><div class="line"><span class="string">    priority &#123;priority&#125;</span></div><div class="line"><span class="string">    virtual_router_id &#123;router_id&#125;</span></div><div class="line"><span class="string">    advert_int 1</span></div><div class="line"><span class="string">    authentication &#123;&#123;</span></div><div class="line"><span class="string">        auth_type PASS</span></div><div class="line"><span class="string">        auth_pass 1111</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">    track_script &#123;&#123;</span></div><div class="line"><span class="string">        chk_nfs</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">    notify_master "/etc/keepalived/ChangeToMaster.sh &#123;folder&#125;"</span></div><div class="line"><span class="string">    notify_backup "/etc/keepalived/ChangeToBackup.sh &#123;folder&#125;"</span></div><div class="line"><span class="string">    virtual_ipaddress &#123;&#123;</span></div><div class="line"><span class="string">        &#123;vip&#125;</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">&#125;&#125;</span></div><div class="line"><span class="string">"""</span>.format(ins_name = vip.replace(<span class="string">'.'</span>, <span class="string">'_'</span>).replace(<span class="string">'/'</span>, <span class="string">'_'</span>),</div><div class="line">           state = state,</div><div class="line">           priority =  <span class="number">200</span> <span class="keyword">if</span> state == <span class="string">"MASTER"</span> <span class="keyword">else</span> <span class="number">100</span>,</div><div class="line">           router_id = self.get_router_id(vip),</div><div class="line">           pubif = get_public_interface(),</div><div class="line">           folder = folder,</div><div class="line">           vip = vip)</div><div class="line">        <span class="keyword">return</span> vrrp_ins</div></pre></td></tr></table></figure><p>这边我们可以看到<code>ins_name</code>和<code>router_id</code>都是根据<code>vip</code>转换成特定格式，标识<code>ins</code>的唯一性。而<code>priority</code>则是根据<code>state</code>来决定，<code>state</code>为<code>master</code>时，<code>priority</code>为<code>200</code>，而<code>backup</code>的<code>priority</code>为<code>100</code>。至于如何获取<code>state</code>，这个涉及到<code>vip</code>均匀算法，后续会讲。</p><h3 id="VIP动态均匀分布-1"><a href="#VIP动态均匀分布-1" class="headerlink" title="VIP动态均匀分布"></a>VIP动态均匀分布</h3><p>假设三个节点，为<code>1, 2, 3</code>，三个<code>vip</code>，为<code>a, b, c</code>，那么最后<code>a</code>对应的节点为<code>1, 2</code>，<code>b</code>对应的节点为<code>3, 1</code>，<code>c</code>对应的节点为<code>2, 3</code>，具体实现算法是先将所有<code>vip</code>进行排序，获取要操作<code>vip</code>的<code>index</code>，然后获取集群内所有节点，然后将上面获取的<code>index</code>乘以<code>2</code>，再对所有节点的个数做余数，然后可以获得一个整数，这个整数就是<code>vip</code>对应<code>master</code>节点在所有节点数组中的<code>index</code>，这种算法大家应该很容易从规律中推算出来。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_my_state</span><span class="params">(self, vip_idx)</span>:</span></div><div class="line">    nodes = get_all_nodes()</div><div class="line">    nodes.sort()</div><div class="line">    idx = vip_idx * <span class="number">2</span> % len(nodes)</div><div class="line">    my_ip = get_public_ip()</div><div class="line">    <span class="keyword">if</span> my_ip == nodes[idx]:</div><div class="line">        <span class="keyword">return</span> <span class="string">'MASTER'</span></div><div class="line">    <span class="keyword">elif</span> my_ip == nodes[(idx + <span class="number">1</span>) % len(nodes)]:</div><div class="line">        <span class="keyword">return</span> <span class="string">'BACKUP'</span></div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        <span class="keyword">return</span> <span class="keyword">None</span></div></pre></td></tr></table></figure><h3 id="多RBD高可用-1"><a href="#多RBD高可用-1" class="headerlink" title="多RBD高可用"></a>多RBD高可用</h3><p>我们在创建目录的时候，需要获取当前节点是否为<code>master</code>，之前那个只有一个<code>vip</code>，所以当前节点要么是<code>master</code>，要么是<code>backup</code>，但是这边的话，一个节点可能是一个<code>vip</code>的<code>master</code>的同时也可能是另一个<code>vip</code>的<code>backup</code>，所以是否为<code>master</code>是要根据目录而定的。在这边我们在创建目录、删除目录、创建<code>vip</code>和删除<code>vip</code>时，更新一个<code>vip</code>和目录之间的映射关系。这个<code>map</code>我是存在<code>ceph</code>的<code>leveldb</code>中，至于为什么不存在节点本地，是因为这份数据必须要保证所有节点强一致，放在本地节点，可能会因为一些故障原因导致之后内容不一致的情况。</p><p>这边我们要求在创建目录前，必须要存在空闲<code>vip</code>可以提供目录绑定。所以当创建一个<code>vip</code>时，此时应该没有目录需要绑定，我们建立一个<code>key</code>和<code>value</code>都是<code>vip</code>的字典；当创建一个目录的时候，随机找到一个空闲<code>vip</code>进行绑定，建立一个<code>key</code>为<code>vip</code>，<code>value</code>为目录名的字典；当删除<code>vip</code>时，肯定是存在其他空闲<code>vip</code>的，所以在删除原来对应<code>map</code>后，我们要找到其他一个空闲<code>vip</code>与之前删除<code>vip</code>对应的目录进行绑定；当删除目录时，只要将对应关系中的<code>value</code>换成<code>key</code>，也就是对应的<code>vip</code>了。</p><p>有了这个<code>map</code>，我们就可以实时获取目录和<code>vip</code>的信息和之间的对应关系。</p><p><code>vip.py</code></p><p>负责当<code>vip</code>发生变化时，更新<code>ip_folder_map</code>，以及<code>ip_folder_map</code>的读写<code>API</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_ip_folder_map</span><span class="params">()</span>:</span></div><div class="line">    result = &#123;&#125;</div><div class="line">    ip_folder_map = LevelDB(<span class="string">"ip_folder_map"</span>)</div><div class="line">    result = json.loads(ip_folder_map)[<span class="string">"ip_folder_map"</span>]</div><div class="line">    <span class="keyword">return</span> result</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">set_ip_folder_map</span><span class="params">(ip_folder_map)</span>:</span></div><div class="line">    ip_folder_map = LevelDB(<span class="string">"ip_folder_map"</span>)</div><div class="line">    ip_folder_map.set(json.dumps(&#123;<span class="string">"ip_folder_map"</span>: ip_folder_map&#125;))</div><div class="line">    ip_folder_map.save()</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_ip_folder_map_by_ip</span><span class="params">(ips)</span>:</span></div><div class="line">    ip_folder_map = get_ip_folder_map()</div><div class="line">    old_ips = ip_folder_map.keys()</div><div class="line">    <span class="keyword">if</span> len(ips) &gt; len(old_ips):</div><div class="line">        new_ip = list(set(ips) - set(old_ips))[<span class="number">0</span>]</div><div class="line">        ip_folder_map[new_ip] = new_ip</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        del_ip = list(set(old_ips) - set(ips))[<span class="number">0</span>]</div><div class="line">        folder = ip_folder_map[del_ip]</div><div class="line">        <span class="keyword">del</span> ip_folder_map[del_ip]</div><div class="line">        <span class="keyword">if</span> folder != del_ip:</div><div class="line">            <span class="keyword">for</span> k, v <span class="keyword">in</span> ip_folder_map.iteritems():</div><div class="line">                <span class="keyword">if</span> k == v:</div><div class="line">                    ip_folder_map[k] = folder</div><div class="line">                    <span class="keyword">break</span></div><div class="line">    set_ip_folder_map(ip_folder_map)</div></pre></td></tr></table></figure><p><code>folder.py</code></p><p>负责当<code>folder</code>发生变化时，更新<code>ip_folder_map</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> vip</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_ip_folder_map_by_folder</span><span class="params">(folder, type)</span>:</span></div><div class="line">    ip_folder_map = vip.get_ip_folder_map()</div><div class="line">    folder = get_folder_path(folder)</div><div class="line">    <span class="keyword">if</span> type == <span class="string">"add"</span>:</div><div class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> ip_folder_map.iteritems():</div><div class="line">            <span class="keyword">if</span> k == v:</div><div class="line">                ip_folder_map[k] = folder</div><div class="line">                <span class="keyword">break</span></div><div class="line">    <span class="keyword">elif</span> type == <span class="string">"delete"</span>:</div><div class="line">        <span class="keyword">for</span> k,v <span class="keyword">in</span> ip_folder_map.iteritems():</div><div class="line">            <span class="keyword">if</span> v == folder:</div><div class="line">                ip_folder_map[k] = k</div><div class="line">                <span class="keyword">break</span></div><div class="line">    vip.set_ip_folder_map(ip_folder_map)</div></pre></td></tr></table></figure><p>上面说了在切换节点的时候，需要传递目录参数，保证只操作对应目录。而脚本是静态的，目录确是动态的，所以我们需要在目录或者<code>vip</code>发生变化的时候对原来的<code>keepalived.conf</code>进行更新，添加目录参数。也就是说当<code>vip</code>发生变化时，我们根据当前<code>vip</code>选择添加或者减少<code>ins</code>，并且更新每个<code>ins</code>调用脚本后面追加的参数；而<code>folder</code>发生变化时，<code>vip</code>调用脚本后面追加的参数也需要更新，要么是<code>vip</code>，要么是<code>folder</code>。这边也需要用到上面的<code>ip_folder_map</code>，因为每个<code>ins</code>就是一个<code>vip</code>，而每个<code>vip</code>对应一个<code>folder</code>。所以我们这边当目录或者<code>vip</code>发生变化时，会根据<code>ip_folder_map</code>更新<code>keepalived.conf</code>，具体实现代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_keepalived_conf</span><span class="params">(self)</span>:</span></div><div class="line">        kconf = <span class="string">"""global_defs &#123;</span></div><div class="line"><span class="string">    notification_email &#123;</span></div><div class="line"><span class="string">    &#125;</span></div><div class="line"><span class="string">    </span></div><div class="line"><span class="string">    router_id NFS_HA_112</span></div><div class="line"><span class="string">&#125;</span></div><div class="line"><span class="string"></span></div><div class="line"><span class="string">vrrp_script chk_nfs &#123;</span></div><div class="line"><span class="string">    script "/etc/keepalived/check_nfs.sh"</span></div><div class="line"><span class="string">    interval 2</span></div><div class="line"><span class="string">&#125;</span></div><div class="line"><span class="string">"""</span></div><div class="line">        vips = self.ip_folder_map.keys()</div><div class="line">        vips.sort()</div><div class="line">        <span class="keyword">for</span> vip, folder <span class="keyword">in</span> self.ip_folder_map.items():</div><div class="line">            vip_idx = vips.index(vip)</div><div class="line">            state = self.get_my_state(vip_idx)</div><div class="line">            <span class="keyword">if</span> state <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">                kconf += self.add_keepalived_ins(vip, folder, state)</div><div class="line">        <span class="keyword">with</span> open(KEEPALIVED_CONF_PATH, <span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">            f.writelines(kconf)</div><div class="line">        do_shell(<span class="string">'service keepalived reload'</span>)</div></pre></td></tr></table></figure><p>下面是添加一个<code>ins</code>的模板，上面也贴过代码，至于这边再次贴一遍的目的是想侧重展示一下脚本后面参数的动态变化的实现方式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div></pre></td><td class="code"><pre><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_keepalived_ins</span><span class="params">(self, vip, folder, state)</span>:</span></div><div class="line">        vrrp_ins = <span class="string">"""</span></div><div class="line"><span class="string">vrrp_instance VI_&#123;ins_name&#125; &#123;&#123;</span></div><div class="line"><span class="string">    state &#123;state&#125;</span></div><div class="line"><span class="string">    interface &#123;pubif&#125;</span></div><div class="line"><span class="string">    priority &#123;priority&#125;</span></div><div class="line"><span class="string">    virtual_router_id &#123;router_id&#125;</span></div><div class="line"><span class="string">    advert_int 1</span></div><div class="line"><span class="string">    authentication &#123;&#123;</span></div><div class="line"><span class="string">        auth_type PASS</span></div><div class="line"><span class="string">        auth_pass 1111</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">    track_script &#123;&#123;</span></div><div class="line"><span class="string">        chk_nfs</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">    notify_master "/etc/keepalived/ChangeToMaster.sh &#123;folder&#125;"</span></div><div class="line"><span class="string">    notify_backup "/etc/keepalived/ChangeToBackup.sh &#123;folder&#125;"</span></div><div class="line"><span class="string">    virtual_ipaddress &#123;&#123;</span></div><div class="line"><span class="string">        &#123;vip&#125;</span></div><div class="line"><span class="string">    &#125;&#125;</span></div><div class="line"><span class="string">&#125;&#125;</span></div><div class="line"><span class="string">"""</span>.format(ins_name = vip.replace(<span class="string">'.'</span>, <span class="string">'_'</span>).replace(<span class="string">'/'</span>, <span class="string">'_'</span>),</div><div class="line">           state = state,</div><div class="line">           priority =  <span class="number">200</span> <span class="keyword">if</span> state == <span class="string">"MASTER"</span> <span class="keyword">else</span> <span class="number">100</span>,</div><div class="line">           router_id = self.get_router_id(vip),</div><div class="line">           pubif = get_public_interface(),</div><div class="line">           folder = folder,</div><div class="line">           vip = vip)</div><div class="line">        <span class="keyword">return</span> vrrp_ins</div></pre></td></tr></table></figure><p>触发脚本：</p><p><code>ChangetoMaster.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line">folder=<span class="string">"<span class="variable">$(dirname $1)</span>/<span class="variable">$(basename $1)</span>"</span></div><div class="line">fname=$(basename <span class="variable">$folder</span>)</div><div class="line"></div><div class="line"><span class="keyword">if</span> [ -d <span class="variable">$folder</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="keyword">if</span> $(mount | grep -q <span class="string">"<span class="variable">$folder</span> "</span>); <span class="keyword">then</span></div><div class="line">        umount -f <span class="variable">$folder</span> &gt; /dev/null</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    device=$(rbd showmapped | awk <span class="string">'/image_'</span><span class="variable">$fname</span><span class="string">' / &#123;print $5&#125;'</span>)</div><div class="line">    <span class="keyword">if</span> [ -b <span class="string">"<span class="variable">$device</span>"</span> ]; <span class="keyword">then</span></div><div class="line">        mount <span class="variable">$device</span> <span class="variable">$folder</span></div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">service nfs-kernel-server restart</div></pre></td></tr></table></figure><p><code>ChangetoBackup.sh</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line">folder=<span class="variable">$1</span></div><div class="line">service nfs-kernel-server stop</div><div class="line"><span class="keyword">if</span> [ -d <span class="variable">$folder</span> ]; <span class="keyword">then</span></div><div class="line">    <span class="keyword">if</span> $(mount | grep -q <span class="string">"<span class="variable">$folder</span> "</span>); <span class="keyword">then</span></div><div class="line">        umount -f <span class="variable">$folder</span> &gt; /dev/null</div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div><div class="line">service nfs-kernel-server start</div></pre></td></tr></table></figure><h3 id="大容量RBD-image的创建和删除-1"><a href="#大容量RBD-image的创建和删除-1" class="headerlink" title="大容量RBD image的创建和删除"></a>大容量RBD image的创建和删除</h3><p>在另外一个端口另起一个线程，通过异步的方式实现，主要利用<code>python</code>的<code>rpyc</code>模块实现，忧郁项目保密性等原因，只贴上部分关键代码，给大家提供一些思路。</p><p>以删除<code>RBD image</code>为例，调用<code>remove_image</code>方法，进入装饰器，从而在新现成做删除操作，不再阻塞之前进程的流程。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">rbd_background</span><span class="params">()</span>:</span></div><div class="line">    conn = connect(<span class="string">'localhost'</span>, RBD_PORT)</div><div class="line">    module = conn.modules[<span class="string">'rbd_utils'</span>]</div><div class="line">    async_func = rpyc.<span class="keyword">async</span>(getattr(module, func_name))</div><div class="line">    </div><div class="line">    <span class="keyword">return</span> async_func</div><div class="line">    </div><div class="line"><span class="meta">@rbd_utils.rbd_background</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_image</span><span class="params">(pool, image)</span>:</span></div><div class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</div><div class="line">        <span class="keyword">try</span>:</div><div class="line">            logger.info(<span class="string">'rbd &#123;&#125; delete start'</span>.format(image))</div><div class="line">            do_shell(<span class="string">'rbd rm &#123;&#125;/&#123;&#125; &gt;&gt; /var/log/rbd_rm.log'</span>.format(pool, image))</div><div class="line">            logger.info(<span class="string">'rbd &#123;&#125; delete finish'</span>.format(image))</div><div class="line">            <span class="keyword">break</span></div><div class="line">        <span class="keyword">except</span> Exception:</div><div class="line">            logger.error(<span class="string">'rbd &#123;&#125; delete error'</span>.format(image))</div><div class="line">            time.sleep(<span class="number">30</span>)</div></pre></td></tr></table></figure><h3 id="快照保证扩容的安全性-1"><a href="#快照保证扩容的安全性-1" class="headerlink" title="快照保证扩容的安全性"></a>快照保证扩容的安全性</h3><p>首先介绍一下定时扩容的脚本：</p><p><code>monitor_rbd.sh</code>：当<code>RBD image</code>可利用空间小于<code>50%</code>或者小于<code>50T</code>时，扩容<code>50T</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line"><span class="keyword">function</span> convert_to_MB()</div><div class="line">&#123;</div><div class="line">    size=<span class="variable">$1</span></div><div class="line">    unit=<span class="variable">$&#123;size:(-1):1&#125;</span></div><div class="line">    nr=<span class="variable">$&#123;size/$unit/&#125;</span></div><div class="line">    <span class="keyword">case</span> <span class="variable">$unit</span> <span class="keyword">in</span></div><div class="line">        (k|K|\)) <span class="built_in">echo</span> <span class="string">"<span class="variable">$nr</span> / 1024"</span> | bc;;</div><div class="line">        (m|M|\)) <span class="built_in">echo</span> <span class="string">"<span class="variable">$nr</span>"</span>;;</div><div class="line">        (g|G|\)) <span class="built_in">echo</span> <span class="string">"<span class="variable">$nr</span> * 1024"</span> | bc;;</div><div class="line">        (t|T|\)) <span class="built_in">echo</span> <span class="string">"<span class="variable">$nr</span> * 1024 * 1024"</span> | bc;;</div><div class="line">        (p|P|\)) <span class="built_in">echo</span> <span class="string">"<span class="variable">$nr</span> * 1024 * 1024 * 1024"</span> | bc;;</div><div class="line">        *) <span class="built_in">echo</span> <span class="string">"Error: cannot convert to MB: <span class="variable">$size</span>"</span>;;</div><div class="line">    <span class="keyword">esac</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">function</span> get_available_size()</div><div class="line">&#123;</div><div class="line">    disk=<span class="variable">$1</span></div><div class="line">    unit_size=$(convert_to_MB <span class="string">'50T'</span>)</div><div class="line">    </div><div class="line">    disk_size=$(df -h | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $2&#125;'</span>)</div><div class="line">    disk_size=$(convert_to_MB <span class="variable">$disk_size</span>)</div><div class="line">    pool=$(rbd showmapped | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $2&#125;'</span>)</div><div class="line">    available_pool_size=$(ceph df | grep <span class="variable">$pool</span> | awk <span class="string">'&#123;print $5&#125;'</span>)</div><div class="line">    available_pool_size=$(convert_to_MB <span class="variable">$available_pool_size</span>)</div><div class="line">    <span class="keyword">if</span> [ $(<span class="built_in">echo</span> <span class="string">"<span class="variable">$available_pool_size</span> &lt; <span class="variable">$unit_size</span>"</span> | bc) -eq 1 ]; <span class="keyword">then</span></div><div class="line">        new_size=$(<span class="built_in">echo</span> <span class="string">"<span class="variable">$disk_size</span> + <span class="variable">$available_pool_size</span>"</span> | bc)</div><div class="line">    <span class="keyword">else</span></div><div class="line">        new_size=$(<span class="built_in">echo</span> <span class="string">"<span class="variable">$disk_size</span> + <span class="variable">$unit_size</span>"</span> | bc)</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    <span class="built_in">echo</span> <span class="variable">$&#123;new_size%.*&#125;</span></div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="keyword">function</span> check_and_enlarge_disk()</div><div class="line">&#123;</div><div class="line">    disk=<span class="string">"<span class="variable">$1</span>"</span></div><div class="line">    <span class="keyword">if</span> [ <span class="string">"<span class="variable">$disk</span>"</span> = <span class="string">""</span> ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"Error: You must specify the disk name"</span></div><div class="line">        <span class="built_in">return</span> 1</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"Checking the disk [/dev/<span class="variable">$disk</span>] ..."</span></div><div class="line">    <span class="keyword">if</span> ! rbd showmapped | grep -q <span class="variable">$disk</span>; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">"Error: Cannot find the disk [<span class="variable">$disk</span>]"</span></div><div class="line">        <span class="built_in">return</span> 2</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    disk_usage=$(df | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $5&#125;'</span>)</div><div class="line">    available_disk_size=$(df | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $4&#125;'</span>)</div><div class="line">    available_disk_size=$(convert_to_MB <span class="string">"<span class="variable">$&#123;available_disk_size&#125;</span>k"</span>)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"  The disk use% is <span class="variable">$&#123;disk_usage&#125;</span>"</span></div><div class="line">    disk_usage=<span class="variable">$&#123;disk_usage/\%/&#125;</span></div><div class="line">    <span class="keyword">if</span> [ <span class="variable">$disk_usage</span> -lt 50 -a <span class="variable">$available_disk_size</span> -gt 1024 * 1024 * 50 ]; <span class="keyword">then</span></div><div class="line">        <span class="built_in">echo</span> <span class="string">'Less then 50% use and more then 50TB available space left, just quit'</span></div><div class="line">        <span class="built_in">return</span> 0</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">'Enlarging the disk ...'</span></div><div class="line">    new_size=$(get_available_size <span class="variable">$disk</span>)</div><div class="line">    <span class="built_in">echo</span> <span class="string">"  the new size is <span class="variable">$&#123;new_size&#125;</span>MB"</span></div><div class="line">    pool=$(rbd showmapped | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $2&#125;'</span>)</div><div class="line">    image=$(rbd showmapped | grep <span class="variable">$disk</span> | awk <span class="string">'&#123;print $3&#125;'</span>)</div><div class="line">    rbd resize --size <span class="variable">$new_size</span> -p <span class="variable">$pool</span> <span class="variable">$image</span></div><div class="line">    sleep 3</div><div class="line">    resize2fs /dev/<span class="variable">$&#123;disk&#125;</span> <span class="string">"<span class="variable">$&#123;new_size&#125;</span>M"</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"Done"</span></div><div class="line">&#125;</div><div class="line"></div><div class="line">disks=$(lsblk | grep rbd | awk <span class="string">'&#123;print $1&#125;'</span>)</div><div class="line"><span class="keyword">for</span> disk <span class="keyword">in</span> <span class="variable">$disks</span></div><div class="line"><span class="keyword">do</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"=============================================="</span></div><div class="line">check_and_enlarge_disk <span class="string">"<span class="variable">$disk</span>"</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"=============================================="</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure><p>这边我们采用<code>ceph</code>提供的原生<code>python</code>的接口，完成<code>RBD</code>的定时快照的创建和删除</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#!/usr/bin/python</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> time</div><div class="line"><span class="keyword">import</span> rados</div><div class="line"><span class="keyword">import</span> rbd</div><div class="line"><span class="keyword">from</span> folder <span class="keyword">import</span> get_all_folder_info</div><div class="line"><span class="keyword">from</span> vip <span class="keyword">import</span> get_ip_folder_map</div><div class="line"></div><div class="line">CEPH_CONF = <span class="string">'/etc/ceph/ceph.conf'</span></div><div class="line">MAX_SNAP_COUNT = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_snap</span><span class="params">(pool, rbd_image)</span>:</span></div><div class="line">    now = time.localtime()</div><div class="line">    snap = time.strftime(<span class="string">"%Y_%m_%d_%H_%M_%S"</span>, now)</div><div class="line">    <span class="keyword">with</span> rados.Rados(conffile=CEPH_CONF) <span class="keyword">as</span> cluster:</div><div class="line">        <span class="keyword">with</span> cluster.open_ioctx(str(pool)) <span class="keyword">as</span> ioctx:</div><div class="line">            <span class="keyword">with</span> rbd.Image(ioctx, rbd_image) <span class="keyword">as</span> image:</div><div class="line">                image.create_snap(snap)</div><div class="line">                </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_images</span><span class="params">()</span>:</span></div><div class="line">    pubif = get_public_interface()</div><div class="line">    pub_ips = do_cmd(<span class="string">"ip addr show &#123;&#125; | awk '/inet/ &#123;&#123;print $2&#125;&#125;'"</span>.format(pubif)).split()</div><div class="line">    vip_folders = get_ip_folder_map(gwgroup)</div><div class="line">    my_folders = []</div><div class="line">    <span class="keyword">for</span> pip <span class="keyword">in</span> pub_ips:</div><div class="line">        <span class="keyword">if</span> pip <span class="keyword">in</span> vip_folders <span class="keyword">and</span> pip != vip_folders[pip]:</div><div class="line">            my_folders.append(os.path.basename(vip_folders[pip]))</div><div class="line">    folders = get_all_folder_info()</div><div class="line">    images = []</div><div class="line">    <span class="keyword">for</span> folder <span class="keyword">in</span> folders:</div><div class="line">        <span class="keyword">if</span> folder <span class="keyword">in</span> my_folders:</div><div class="line">            images.append(&#123;</div><div class="line">                <span class="string">'image'</span>: <span class="string">'image_&#123;&#125;'</span>.format(folder),</div><div class="line">                <span class="string">'pool'</span>: folders[folder][<span class="string">'pool'</span>]</div><div class="line">            &#125;)</div><div class="line">    <span class="keyword">return</span> images</div><div class="line">    </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">remove_old_snap</span><span class="params">(pool, rbd_image)</span>:</span></div><div class="line">    <span class="keyword">with</span> rados.Rados(conffile=CEPH_CONF) <span class="keyword">as</span> cluster:</div><div class="line">        <span class="keyword">with</span> cluster.open_ioctx(str(pool)) <span class="keyword">as</span> ioctx:</div><div class="line">            <span class="keyword">with</span> rbd.Image(ioctx, rbd_image) <span class="keyword">as</span> image:</div><div class="line">                snaps = sorted(image.list_snaps(), key=<span class="keyword">lambda</span> snap: snap[<span class="string">'name'</span>])</div><div class="line">                <span class="keyword">if</span> len(snaps) &gt; MAX_SNAP_COUNT:</div><div class="line">                    <span class="keyword">for</span> snap <span class="keyword">in</span> snaps[<span class="number">0</span>:len(snaps)-MAX_SNAP_COUNT]:</div><div class="line">                        image.remove_snap(snap[<span class="string">'name'</span>])</div><div class="line">                        </div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></div><div class="line">    images = get_images()</div><div class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> images:</div><div class="line">        create_snap(image[<span class="string">'pool'</span>], image[<span class="string">'image'</span>])</div><div class="line">        remove_old_snap(image[<span class="string">'pool'</span>], image[<span class="string">'image'</span>])</div><div class="line">        device = do_shell(<span class="string">"rbd showmapped | awk '/&#123;&#125;[ \t]*&#123;&#125;/ &#123;&#123;print $5&#125;&#125;'"</span>.format(image[<span class="string">'pool'</span>], image[<span class="string">'image'</span>]))</div><div class="line">        do_shell(<span class="string">'/usr/local/bin/monitor_rbd.sh &#123;&#125;'</span>.format(os.path.basename(device)))</div><div class="line">        </div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    main()</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>内容和代码都比较多，其实每一个技术点都可以单独拿出来写一篇，但是我觉得这是一个完整<code>feature</code>，想让大家能够代入，了解到完成这样一个<code>feature</code>周边需要支持的各种技术点和注意点，一个<code>feature</code>往往是经过不断迭代和维护，很多实现方法也随着时间和应用场景不断发生变化。</p><p>完成这样一个<code>feature</code>，我也是反复修改，就比如异步实现<code>RBD image</code>的创建和删除，很多场景在生产环境和测试环境中的 情况是完全不一样的，比如我开发的时候创建的<code>image</code>都是<code>1G</code>的，当然很快，也不能存在什么阻塞的问题，也遇到很多问题和想不通的地方，感谢我的同事和前辈提供的帮助和启发。</p><p>最后，衷心希望<code>ceph</code>能够早日将<code>CephFS</code>完善，保证其在生产环境中的稳定性和性能。这样我们也就不用绞尽脑汁这般曲线救国了，哈哈。</p><p>最后的最后，贴上部分代码地址，由于项目保密性等原因，我只能贴出比较关键的代码，大家请见谅，我觉得这些代码应该足够了，足够给大家提供一个思路了，其实往往思路比代码更重要，相信很多人的实现方式要比我更加优秀呢！</p><blockquote><p>代码地址：<a href="https://github.com/tony-yin/Multi_RBD_HA" target="_blank" rel="external">https://github.com/tony-yin/Multi_RBD_HA</a></p></blockquote><p>如果大家觉得有帮助的话，欢迎<code>Star</code>哦 ~(≧▽≦)/~</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/rbd2-600x450.jpg&quot; alt=&quot;RBD-HA-2&quot;&gt;&lt;/center&gt;

&lt;p&gt;之前分享过一篇&lt;a href=&quot;http://www.tony-yin.top/2017/12/07/RBD-HA/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;【通过 Keepalived 实现 Ceph RBD 的高可用】&lt;/a&gt;，主要讲解了将&lt;code&gt;RBD&lt;/code&gt;导出为&lt;code&gt;NFS&lt;/code&gt;，然后通过&lt;code&gt;keepalived&lt;/code&gt;实现高可用，保证当提供虚拟&lt;code&gt;IP&lt;/code&gt;节点发生故障时，可以自动切换节点，使得业务不发生中断。&lt;/p&gt;
&lt;p&gt;这样可以基本使用&lt;code&gt;RBD&lt;/code&gt;代替&lt;code&gt;CephFS&lt;/code&gt;对外提供&lt;code&gt;Ceph&lt;/code&gt;服务，至于为什么不用&lt;code&gt;CephFS&lt;/code&gt;就不多说了，不清楚的可以去看上一篇。虽然说这样可以保证无单点故障，但是有一点还是不如&lt;code&gt;CephFS&lt;/code&gt;，那就是&lt;code&gt;CephFS&lt;/code&gt;可以实现多节点同时提供服务，而&lt;code&gt;RBD&lt;/code&gt;说白了其实同时只有一个节点能提供服务，当客户端流量高的时候，&lt;code&gt;RBD&lt;/code&gt;方式的带宽并不能满足需求。就比如都是三个节点，&lt;code&gt;CephFS&lt;/code&gt;可以将客户端流量分流到三个节点，而&lt;code&gt;RBD&lt;/code&gt;只能用一个节点，而带宽上限又取决与网卡、磁盘和&lt;code&gt;IO&lt;/code&gt;等等原因，所以同样的硬件设施&lt;code&gt;RBD&lt;/code&gt;的带宽性能是跟不上的，本文就多虚拟&lt;code&gt;IP&lt;/code&gt;暴露访问方式进行分享。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="NFS" scheme="https://tony-yin.github.io/tags/NFS/"/>
    
      <category term="HA" scheme="https://tony-yin.github.io/tags/HA/"/>
    
      <category term="RBD" scheme="https://tony-yin.github.io/tags/RBD/"/>
    
      <category term="Keepalived" scheme="https://tony-yin.github.io/tags/Keepalived/"/>
    
  </entry>
  
  <entry>
    <title>scp 免交互式和 ssh 免交互式脚本</title>
    <link href="https://tony-yin.github.io/2018/01/10/scp-and-ssh/"/>
    <id>https://tony-yin.github.io/2018/01/10/scp-and-ssh/</id>
    <published>2018-01-10T15:40:46.000Z</published>
    <updated>2018-01-30T12:51:35.223Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/open_ssh-600x450.png" alt="Scp-And-Ssh"></center><p>简单实现了<code>scp</code>的免交互式脚本和<code>ssh</code>免交互式脚本。</p><a id="more"></a><h2 id="scp"><a href="#scp" class="headerlink" title="scp"></a>scp</h2><p>场景：</p><p>需要将以下源主机上的三个文件拷贝到以下目的主机的对应目录下</p><blockquote><p><code>src host</code>: 192.168.1.1<br><code>dist host</code>: 192.168.1.2<br><code>files</code>: <code>/root/1/1.txt</code>, <code>/root/2/2.txt</code>, <code>/root/3/3.txt</code></p></blockquote><p>这时候如果手动做的话，将会很繁琐，所以这时候需要一个脚本能够实现文件的自动复制，并且脚本需要自动把密码验证的步骤也覆盖。</p><p>这里的关键就是如何实现<code>shell</code>交互式命令行的自动化，这边可以用分界符<code>EOF</code>，<code>EOF</code>范围中的字符将会被作为命令输入到交互式命令行中，具体脚本如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">SOURCEHOST=192.168.1.1</div><div class="line">DISTHOST=$1</div><div class="line">FILE1=/root/1/1.txt</div><div class="line">FILE2=/root/2/2.txt</div><div class="line">FILE3=/root/3/3.txt</div><div class="line">FOLDER1=/root/1/</div><div class="line">FOLDER2=/root/2/</div><div class="line">FOLDER3=/root/3/</div><div class="line">PASSWORD=123456</div><div class="line">scp FILE1 $&#123;DISTHOST&#125;$&#123;Folder1&#125; &lt;&lt; EOF</div><div class="line">$PASSWORD</div><div class="line">EOF</div><div class="line">scp FILE2 $&#123;DISTHOST&#125;$&#123;Folder2&#125; &lt;&lt; EOF</div><div class="line">$PASSWORD</div><div class="line">EOF</div><div class="line">scp FILE3 $&#123;DISTHOST&#125;$&#123;Folder3&#125; &lt;&lt; EOF</div><div class="line">$PASSWORD</div><div class="line">EOF</div></pre></td></tr></table></figure><p>调用方式：<code>sh scp.sh 192.168.1.2</code></p><h2 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h2><p>场景：</p><p>上面的场景是建立在登陆<code>192.168.1</code>主机的基础上，现在我想我在任意主机上都可以实现上面将<code>192.168.1.1</code>的上述文件拷贝到<code>192.168.1.2</code>的对应目录下</p><p>这里的关键是实现<code>ssh</code>的免密登陆，这时我们需要用到<code>sshpass</code>，具体介绍可以自行搜索下，用法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">#! /bin/bash</div><div class="line"></div><div class="line">SOURCEHOST=192.168.1.1</div><div class="line">DISTHOST=$1</div><div class="line">FILE1=/root/1/1.txt</div><div class="line">FILE2=/root/2/2.txt</div><div class="line">FILE3=/root/3/3.txt</div><div class="line">FOLDER1=/root/1/</div><div class="line">FOLDER2=/root/2/</div><div class="line">FOLDER3=/root/3/</div><div class="line">PASSWORD=123456</div><div class="line"></div><div class="line">sshpass -p $PASSWORD ssh $SOURCEHOST \</div><div class="line">    sshpass -p $PASSWORD scp $&#123;FILE1&#125; $&#123;DISTHOST&#125;$&#123;FOLDER1&#125; &amp;&amp; \</div><div class="line">    sshpass -p $PASSWORD scp $&#123;FILE2&#125; $&#123;DISTHOST&#125;$&#123;FOLDER2&#125; &amp;&amp; \</div><div class="line">    sshpass -p $PASSWORD scp $&#123;FILE3&#125; $&#123;DISTHOST&#125;$&#123;FOLDER3&#125;</div></pre></td></tr></table></figure><p>调用方式：<code>sh scp.sh 192.168.1.2</code></p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/open_ssh-600x450.png&quot; alt=&quot;Scp-And-Ssh&quot;&gt;&lt;/center&gt;

&lt;p&gt;简单实现了&lt;code&gt;scp&lt;/code&gt;的免交互式脚本和&lt;code&gt;ssh&lt;/code&gt;免交互式脚本。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Script" scheme="https://tony-yin.github.io/tags/Script/"/>
    
      <category term="Linux" scheme="https://tony-yin.github.io/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>利用Raid卡工具获取逻辑盘是否为SSD</title>
    <link href="https://tony-yin.github.io/2018/01/05/RaidCardToolUtils/"/>
    <id>https://tony-yin.github.io/2018/01/05/RaidCardToolUtils/</id>
    <published>2018-01-05T05:07:06.000Z</published>
    <updated>2018-01-30T12:50:22.464Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/raid-600x450.png" alt="RaidCardToolUtils"></center><p>网上很多获取一块盘是否为<code>SSD</code>的方式都是不靠谱的，不能覆盖到所有情况。一般我们在操作系统上的硬盘都是虚拟出来的逻辑盘，比如<code>/dev/sda</code>这种，它可能对应一块单独的物理硬盘，也有可能对应的是几块盘组成的<code>raid</code>。我们有时候想获取一块盘的具体信息，比如磁盘类型、插槽号、序列号等等，这时候我们就得借助对应的<code>raid</code>卡工具了，最常见的如<code>Megacli</code>，通过逻辑盘找到对应的物理盘，然后读取信息。</p><a id="more"></a><h2 id="Raid卡简介"><a href="#Raid卡简介" class="headerlink" title="Raid卡简介"></a>Raid卡简介</h2><p>所谓<code>raid</code>卡，就是为了更好的统一管理物理硬盘而存在的，在出现单独的<code>raid</code>卡之前，对硬盘做<code>raid</code>操作，需要<code>cpu</code>完成其中的计算操作，这个会很影响其他依赖<code>cpu</code>的应用或进程的性能，后来就将<code>raid</code>卡单独提取出来，并且在其之上存在一个小型<code>cpu</code>供来完成<code>raid</code>相关操作的计算，这其中最常见的<code>raid</code>工具应该非<code>Megacli</code>莫属了。</p><p>为什么说最常见的呢？因为<code>raid</code>卡工具对应不同型号的<code>raid</code>卡是不一样，<code>LSI</code>只是一个半导体厂商，负责提供<code>raid</code>芯片，最后还需要集成到服务器厂商的机器上，所以最后的工具还是由厂商决定和提供，也可以理解为特定型号的<code>raid</code>对应各自的工具。</p><h2 id="HBA卡简介"><a href="#HBA卡简介" class="headerlink" title="HBA卡简介"></a>HBA卡简介</h2><p>近来，又出现了一种<code>HBA</code>卡，只从<code>HBA</code>的英文解释<code>HOST BUS ADAPTER</code>（主机总线适配器）就能看出来，他肯定是给主机用的，一般<code>HBA</code>就是给主机插上后，给主机扩展出更多的接口，来连接外部的设备。大多数讲到<code>HBA</code>卡都是指光纤的<code>HBA</code>卡，给主机提供光纤接口的。也有<code>ISCSI</code>的<code>HBA</code>卡，链接<code>ISCSI</code>设备的，从这种功能上说，我们也可以把独立网卡称为<code>HBA</code>卡，通过独立网卡扩展出网口来连接外部网络设备或主机。不过习惯上大部分<code>HBA</code>只是称光纤卡或者<code>iscsi</code>卡。</p><p>简而言之，这种<code>HBA</code>卡本身是为了扩展外部连接设备而存在的，但是它具有部分<code>raid</code>功能，与<code>raid</code>卡相比它的优势在于它价格便宜，性价比高；劣势在于虽然具有<code>raid</code>功能，但是都是基础的功能，没有<code>raid</code>卡那么完善。</p><blockquote><p>这篇文章讲<code>raid</code>卡和<code>HBA</code>卡讲的挺好的：<a href="http://www.cnblogs.com/weikunzz/p/6707395.html" target="_blank" rel="external">HBA卡 和 RAID卡</a></p></blockquote><h2 id="需求和背景"><a href="#需求和背景" class="headerlink" title="需求和背景"></a>需求和背景</h2><p>据我所知，这类工具往往是运维人员用的居多，但是往往开发中也会需要用到。本文通过获取逻辑盘对应盘的类型展开描述，并借此讲解获取逻辑盘的一类信息或通过逻辑盘操作对应物理盘。因为这其中的关键就是找到逻辑盘和物理盘之间的对应关系。无论是<code>raid</code>卡工具还是<code>HBA</code>卡工具都是罗列所有硬盘的信息，所以你要从中找到你选择的逻辑盘所对应的便是重中之重。</p><p>逻辑盘对应的物理盘可能为单独的硬盘，也可能是<code>raid</code>，单独的可以直接读取硬盘类型，<code>raid</code>的话我们认为只会将同样类型的盘做<code>raid</code>，混合的情况不考虑。</p><p><code>raid</code>卡工具的话，我只对<code>Megacli</code>和<code>Sas3ircu</code>进行讲解，所以阅读本文前最好有使用以上两个工具的相关经验。首先我会根据目前存在的<code>raid</code>卡类型建立一个<code>map</code>关系，然后通过<code>raid</code>卡类型自动获取对应<code>raid</code>卡工具，每个<code>raid</code>卡都是一个类，然后里面的方法都是为该工具定制化的操作。</p><h2 id="获取raid卡工具"><a href="#获取raid卡工具" class="headerlink" title="获取raid卡工具"></a>获取raid卡工具</h2><p>目前就考虑两种型号的<code>raid</code>卡，以后有新的再往<code>map</code>里面填充就好了。<code>NotSupport</code>指的是其他不支持型号的<code>raid</code>卡和虚拟机。</p><blockquote><p><code>do_shell</code>是本人封装的一个在<code>python</code>中执行<code>shell</code>命令的方法，大家可以根据自己的情况对该方法进行转换</p></blockquote><p>通过获取的<code>card mode</code>，根据<code>map</code>找到对应的<code>tool</code>，然后实例化对应的工具类</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">RaidCardToolFactory</span><span class="params">()</span>:</span></div><div class="line">    RaidCardMap = &#123;</div><div class="line">        <span class="string">'SAS2208'</span>: MegaraidTool,</div><div class="line">        <span class="string">'SAS3008'</span>: HBATool,</div><div class="line">        <span class="string">'NotSupport'</span>: NotSupport</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">getTool</span><span class="params">(self)</span>:</span></div><div class="line">        card_model = self.get_raidcard_model()</div><div class="line">        tool = self.RaidCardMap[card_model]()</div><div class="line">        <span class="keyword">return</span> tool</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_raidcard_model</span><span class="params">(self)</span>:</span></div><div class="line">        card_model = <span class="string">'NotSupport'</span></div><div class="line">        card_info = do_shell(<span class="string">"lspci | grep 'LSI Logic'"</span>)</div><div class="line">        <span class="keyword">if</span> card_info == <span class="string">''</span>:</div><div class="line">            <span class="keyword">return</span> card_model</div><div class="line">        card = card_info.strip().splitlines()[<span class="number">0</span>].split()</div><div class="line">        <span class="keyword">if</span> <span class="string">'RAID bus controller'</span> <span class="keyword">in</span> card_info:</div><div class="line">            card_model = card[<span class="number">10</span>] + card[<span class="number">11</span>]</div><div class="line">        <span class="keyword">elif</span> <span class="string">'Attached SCSI controller'</span> <span class="keyword">in</span> card_info:</div><div class="line">            card_model = card[<span class="number">10</span>]</div><div class="line">        <span class="keyword">return</span> card_model</div></pre></td></tr></table></figure><h2 id="Megaraid工具类"><a href="#Megaraid工具类" class="headerlink" title="Megaraid工具类"></a>Megaraid工具类</h2><ol><li>先通过<code>lsscsi</code>命令获取逻辑盘是否为<code>raid</code>；</li><li>如果是<code>raid</code>，那么直接根据<code>lsscsi</code>获取当前逻辑盘的<code>target id</code>，也就是第三个号，然后通过<code>megacli cfgdsply -aALL</code>获取所有<code>raid</code>信息，根据逻辑盘的<code>target id</code>对应物理盘中的<code>Target Id</code>找到对应<code>raid</code>，然后只要获取<code>raid</code>中第一块物理盘的硬盘类型即可，也就是<code>Media Type</code>，具体参见下方<code>API</code>: <code>get_ld_type</code></li><li>如果不是<code>raid</code>，那么直接根据<code>lsscsi</code>获取当前逻辑盘的<code>target id</code>，也就是第三个号，这边的<code>target id</code>直接对应<code>megacli</code>中每一块单盘中的<code>Device Id</code>字段，所以根据<code>target id</code>匹配<code>megacli pdlist aAll</code>获取磁盘列表的每一项的<code>Device Id</code>便可以找到对应的物理盘，具体参见下方<code>API</code>: <code>get_pd_type</code>。</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">MegaraidTool</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_disk_type</span><span class="params">(self, disk_name)</span>:</span></div><div class="line">        scsi_info = do_shell(<span class="string">"lsscsi | grep &#123;&#125; -w"</span>.format(disk_name))</div><div class="line">        target_id = scsi_info.split()[<span class="number">0</span>].split(<span class="string">":"</span>)[<span class="number">2</span>]</div><div class="line">        serial_nu = scsi_info.split()[<span class="number">3</span>].strip()[<span class="number">2</span>:]</div><div class="line">        <span class="keyword">if</span> <span class="string">"LSI"</span> <span class="keyword">in</span> scsi_info:</div><div class="line">            disk_type = self.get_ld_type(target_id, serial_nu)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            disk_type = self.get_pd_type(target_id)</div><div class="line">        <span class="keyword">return</span> disk_type</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_ld_type</span><span class="params">(self, target_id, serial_nu)</span>:</span></div><div class="line">        disk_type = <span class="string">''</span></div><div class="line">        cmd = MEGACLI + <span class="string">' cfgdsply -aALL -NoLog|grep -E "Product Name|Target Id|Media Type"'</span></div><div class="line">        output = do_shell(cmd)</div><div class="line">        adapters = output.split(<span class="string">'Product Name'</span>)</div><div class="line">        <span class="keyword">for</span> adapter <span class="keyword">in</span> adapters:</div><div class="line">            <span class="keyword">if</span> serial_nu <span class="keyword">not</span> <span class="keyword">in</span> adapter:</div><div class="line">                <span class="keyword">continue</span></div><div class="line">            lines = adapter.split(<span class="string">'\n'</span>)</div><div class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> lines:</div><div class="line">                <span class="keyword">if</span> <span class="string">"Target Id: &#123;&#125;"</span>.format(target_id) <span class="keyword">in</span> line:</div><div class="line">                    index = lines.index(line)</div><div class="line">                    <span class="keyword">if</span> <span class="string">'Solid State Device'</span> <span class="keyword">in</span> lines[index + <span class="number">1</span>]:</div><div class="line">                        disk_type = <span class="string">"SSD"</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        disk_type = <span class="string">"HDD"</span></div><div class="line">                    <span class="keyword">break</span></div><div class="line">            <span class="keyword">if</span> disk_type != <span class="string">''</span>:</div><div class="line">                <span class="keyword">break</span></div><div class="line">        <span class="keyword">return</span> disk_type</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_pd_type</span><span class="params">(self, target_id)</span>:</span></div><div class="line">        disk_type = <span class="string">''</span></div><div class="line">        cmd = MEGACLI + <span class="string">' pdlist aAll | grep -E "Device Id|Media Type"'</span></div><div class="line">        output = do_shell(cmd)</div><div class="line">        lines = output.split(<span class="string">'\n'</span>)</div><div class="line">        <span class="keyword">if</span> <span class="string">'Device Id: &#123;&#125;'</span>.format(target_id) <span class="keyword">not</span> <span class="keyword">in</span> lines:</div><div class="line">            <span class="keyword">return</span> <span class="string">''</span></div><div class="line">        index = lines.index(<span class="string">'Device Id: &#123;&#125;'</span>.format(target_id))</div><div class="line">        <span class="keyword">if</span> <span class="string">'Solid State Device'</span> <span class="keyword">in</span> lines[index + <span class="number">1</span>]:</div><div class="line">            disk_type = <span class="string">"SSD"</span></div><div class="line">        <span class="keyword">else</span> :</div><div class="line">            disk_type = <span class="string">"HDD"</span></div><div class="line">        <span class="keyword">return</span> disk_type</div></pre></td></tr></table></figure><h2 id="HBA工具类"><a href="#HBA工具类" class="headerlink" title="HBA工具类"></a>HBA工具类</h2><ol><li><code>HBA</code>类用的工具是<code>sas3ircu</code>，首先我们需要根据命令<code>sas3ircu list</code>获取所有的<code>controller</code>，然后每次获取信息都需要遍历所有<code>controller</code>；</li><li>第一步依旧是判断逻辑盘是否为<code>raid</code>；</li><li>如果是<code>raid</code>，获取逻辑盘的<code>target id</code>，与之匹配的是<code>sas3ircu</code>中的<code>Initiator at ID</code>字段，找到对应的<code>raid</code>，然后通过获取其下第一个物理盘的类型，这边类型字段变成了<code>Drive Type</code>，具体参考下方<code>API</code>: <code>get_ld_type</code>；</li><li>如果非<code>raid</code>，我匹配的是<code>sas3ircu</code>中的<code>Sas Address</code>字段，那么逻辑盘的<code>Sas Address</code>如何获取呢？这边我用的方式是通过<code>udev</code>获取逻辑盘的<code>symlink</code>，这里面有很多<code>address</code>，而我们需要的是<code>by-path</code>，我这边就简单做了，看<code>sas3ircu</code>每个盘的<code>Sas Address</code>是否被<code>udev</code>获取的<code>symlink</code>包含，如果包含了，那么也就匹配到了，然后直接获取<code>Drive Type</code>字段就可以得到磁盘类型类；具体参考下方<code>API</code>: <code>get_pd_type</code></li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">HBATool</span><span class="params">()</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_disk_type</span><span class="params">(self, disk_name)</span>:</span></div><div class="line">        scsi_info = do_shell(<span class="string">"lsscsi | grep &#123;&#125; -w"</span>.format(disk_name))</div><div class="line">        <span class="keyword">if</span> <span class="string">"LSI"</span> <span class="keyword">in</span> scsi_info:</div><div class="line">            target_id = scsi_info.split()[<span class="number">0</span>].split(<span class="string">":"</span>)[<span class="number">2</span>]</div><div class="line">            disk_type = self.get_ld_type(target_id)</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            sas_address = do_cmd(<span class="string">'udevadm info --query=symlink --name=&#123;&#125;'</span>.format(disk_name))</div><div class="line">            disk_type = self.get_pd_type(sas_address)</div><div class="line">        <span class="keyword">return</span> disk_type</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_ld_type</span><span class="params">(self, target_id)</span>:</span></div><div class="line">        disk_type = <span class="string">''</span></div><div class="line">        controllers = self.get_controllers()</div><div class="line">        <span class="keyword">for</span> controller <span class="keyword">in</span> controllers:</div><div class="line">            cmd = <span class="string">'sas3ircu &#123;&#125; display|grep -E "Initiator at ID|Drive Type"'</span>.format(controller)</div><div class="line">            output = do_shell(cmd)</div><div class="line">            <span class="keyword">if</span> <span class="string">'Initiator at ID #&#123;&#125;'</span>.format(target_id) <span class="keyword">in</span> output:</div><div class="line">                lines = output.splitlines()</div><div class="line">                index = lines.index(<span class="string">'Initiator at ID #&#123;&#125;'</span>.format(target_id))</div><div class="line">                <span class="keyword">if</span> <span class="string">'HDD'</span> <span class="keyword">in</span> lines[index + <span class="number">1</span>]:</div><div class="line">                    disk_type = <span class="string">'HDD'</span></div><div class="line">                <span class="keyword">else</span>:</div><div class="line">                    disk_type = <span class="string">'SSD'</span></div><div class="line">                <span class="keyword">break</span></div><div class="line">        <span class="keyword">return</span> disk_type</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_pd_type</span><span class="params">(self, sas_address)</span>:</span></div><div class="line">        disk_type = <span class="string">''</span></div><div class="line">        controllers = self.get_controllers()</div><div class="line">        <span class="keyword">for</span> controller <span class="keyword">in</span> controllers:</div><div class="line">            cmd = <span class="string">'sas3ircu &#123;&#125; display|grep -E "SAS Address|Drive Type"'</span>.format(controller)</div><div class="line">            output = do_shell(cmd)</div><div class="line">            lines = output.splitlines()</div><div class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">0</span>, len(lines), <span class="number">2</span>):</div><div class="line">                address = lines[i].split()[<span class="number">-1</span>].replace(<span class="string">'-'</span>, <span class="string">''</span>)</div><div class="line">                <span class="keyword">if</span> address <span class="keyword">in</span> sas_address:</div><div class="line">                    <span class="keyword">if</span> <span class="string">'HDD'</span> <span class="keyword">in</span> lines[i + <span class="number">1</span>]:</div><div class="line">                        disk_type = <span class="string">'HDD'</span></div><div class="line">                    <span class="keyword">else</span>:</div><div class="line">                        disk_type = <span class="string">'SSD'</span></div><div class="line">                    <span class="keyword">break</span></div><div class="line">            <span class="keyword">if</span> disk_type != <span class="string">''</span>:</div><div class="line">                <span class="keyword">break</span></div><div class="line">        <span class="keyword">return</span> disk_type</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_controllers</span><span class="params">(self)</span>:</span></div><div class="line">        cmd = <span class="string">'sas3ircu list | awk \'&#123;print $1&#125;\''</span></div><div class="line">        list = do_shell(cmd).splitlines()</div><div class="line">        index = list.index(<span class="string">'Index'</span>) + <span class="number">2</span></div><div class="line">        controllers = []</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(index, len(list) - <span class="number">1</span>):</div><div class="line">            controllers.append(list[i])</div><div class="line">        <span class="keyword">return</span> controllers</div></pre></td></tr></table></figure><h2 id="调用方式"><a href="#调用方式" class="headerlink" title="调用方式"></a>调用方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> mcs3.raidcardutils <span class="keyword">import</span> RaidCardToolFactory</div><div class="line"></div><div class="line">tool = RaidCardToolFactory().getTool()</div><div class="line">disk_type = tool.get_disk_type(disk_name)</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>其实这其中的关键就是先找到每一块物理盘的唯一标识，然后我们根据工具获取列表中的唯一标识字段，获取逻辑盘对应的信息，就比如上面的<code>Device Id</code>，对应的是逻辑盘的<code>target id</code>。</p><blockquote><p>完整代码地址：<a href="https://github.com/tony-yin/RaidCardTool/" target="_blank" rel="external">https://github.com/tony-yin/RaidCardTool/</a><br>如果有所帮助的话，帮忙<code>star</code>一下哦 ^_^</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/raid-600x450.png&quot; alt=&quot;RaidCardToolUtils&quot;&gt;&lt;/center&gt;

&lt;p&gt;网上很多获取一块盘是否为&lt;code&gt;SSD&lt;/code&gt;的方式都是不靠谱的，不能覆盖到所有情况。一般我们在操作系统上的硬盘都是虚拟出来的逻辑盘，比如&lt;code&gt;/dev/sda&lt;/code&gt;这种，它可能对应一块单独的物理硬盘，也有可能对应的是几块盘组成的&lt;code&gt;raid&lt;/code&gt;。我们有时候想获取一块盘的具体信息，比如磁盘类型、插槽号、序列号等等，这时候我们就得借助对应的&lt;code&gt;raid&lt;/code&gt;卡工具了，最常见的如&lt;code&gt;Megacli&lt;/code&gt;，通过逻辑盘找到对应的物理盘，然后读取信息。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Raidcard" scheme="https://tony-yin.github.io/tags/Raidcard/"/>
    
  </entry>
  
  <entry>
    <title>Daily Article Vol 1 - (2017/11/1 ~ 2017/12/31)</title>
    <link href="https://tony-yin.github.io/2018/01/01/Daily-Article-2017/"/>
    <id>https://tony-yin.github.io/2018/01/01/Daily-Article-2017/</id>
    <published>2018-01-01T02:13:25.000Z</published>
    <updated>2018-03-09T07:16:20.311Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/2018-1b.png" alt="Daily Article 2"></center><p>书籍可以系统的学习一些知识，并且需要比较长的时间集中注意力学习。而现在网络越来越发达，各种社区的流行，还有开源分享精神的传播，导致现在互联网上很多优秀文章、博客、微信公众号等出现，这些文章贴近热点，往往都很新，并且篇幅有长有短，我们可以利用一些碎片时间来吸收这些知识。优秀文章很多，所以每天读个一两篇文章可以作为一个习惯养成，这样日积月累相信会获益良多。</p><p><code>Daily Article</code>系列就是为了记录我每天的阅读历程，以月为单位，每个月出一篇大的总结，一是为了约束自己每天按时按量阅读，也给自己打气，二是给自己每个月的阅读内容做一个总结，用于回头阅读，三是将其中有质量的内容分享给有需要的人。由于我是从<code>2017</code>九月才可以记录，所以趁着年底索性把十一和十二月的记录一次性发出来。</p><p>目前我采取星星的方式给文章评级，最高三颗星，最低没有星星，一般我放上来的都不会是太水的文章，所以如果是一般的工具类或者没有很大特色的文章我不会进行标记。一颗星表示<code>good</code>，即这篇文章有特色，对自己有帮助；两颗星表示<code>verygood</code>，说明这篇文章内容很好，有深度有广度，是一篇很有质量的文章；三颗星表示<code>excellent</code>，说明这篇文章不仅内容技术讲的很到位，文章文笔也很出色，实践结合理论，让人很容易理解，看完后收获很大或者是顿悟，总而言之是一片很优秀的文章。当然我还会在这三个等级中结合半个星星进行调节，反正就是对文章的一个个人看法而已，仁者见仁，智者见智吧。</p><a id="more"></a><h2 id="十一月"><a href="#十一月" class="headerlink" title="十一月"></a>十一月</h2><ol><li><a href="http://www.xuxiaopang.com/08/23/easy-ceph-CephX/" target="_blank" rel="external">大话 Ceph – CephX 那点事儿</a>(11/15-11/17) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://mp.weixin.qq.com/s/UiDd-1zwrqIsk3-KEcAQaA" target="_blank" rel="external">互联网架构为什么要做服务化？</a>(11/18) <i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/liuaigui/article/details/7163482" target="_blank" rel="external">基于开源软件构建高性能集群NAS系统</a>(11/20)</li><li><a href="http://blog.csdn.net/whycold/article/details/11898249" target="_blank" rel="external">虚拟IP原理</a>(11/21)</li><li><a href="http://mp.weixin.qq.com/s/8aI9jS0SXJl5NdcM3TPYuQ" target="_blank" rel="external">究竟为什么要引入数据库中间件</a>(11/27) <i class="fa fa-star"></i><i class="fa fa-star-half-full"></i></li><li><a href="https://time.geekbang.org/column/article/183" target="_blank" rel="external">程序员如何用技术变现（上）</a>(11/28)</li><li><a href="https://time.geekbang.org/column/article/294" target="_blank" rel="external">Go语言，Docker和新技术</a>(11/28)</li><li><a href="https://h5.ele.me/hongbao/#hardware_id=&amp;is_lucky_group=True&amp;lucky_number=10&amp;track_id=&amp;platform=0&amp;sn=29dad164ef30a0c9&amp;theme_id=1745&amp;device_id=" target="_blank" rel="external">秒杀系统架构优化思路</a>(11/29) <i class="fa fa-star"></i></li></ol><h2 id="十二月"><a href="#十二月" class="headerlink" title="十二月"></a>十二月</h2><ol><li><a href="https://time.geekbang.org/column/article/294" target="_blank" rel="external">Ceph开发每周谈首发</a>(12/1)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-1/" target="_blank" rel="external">Ceph开发每周谈Vol2</a>(12/4)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-1/" target="_blank" rel="external">Ceph开发每周谈Vol3</a>(12/5)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-1/" target="_blank" rel="external">Ceph开发每周谈Vol4</a>(12/5)</li><li><a href="https://www.thomas-krenn.com/en/wiki/Ext4_Filesystem" target="_blank" rel="external">Ext4 Filesystem</a>(12/11)</li><li><a href="https://www.hecticgeek.com/2015/01/ext4-external-hard-disk-busy-at-idle-fix/" target="_blank" rel="external">Formatted ‘Ext4’ External Hard Disk is Busy</a>(12/12)</li><li><a href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=bfff68738f1cb5c93dab1114634cea02aae9e7ba" target="_blank" rel="external">Ext4lazyinit detail</a>(12/12)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-5/" target="_blank" rel="external">Ceph开发每周谈Vol5</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-6/" target="_blank" rel="external">Ceph开发每周谈Vol6</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-7/" target="_blank" rel="external">Ceph开发每周谈Vol7</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-8/" target="_blank" rel="external">Ceph开发每周谈Vol8—社区加快开发节奏, CDS 变更为 CDM, Firefly 结束版本支持</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-9/" target="_blank" rel="external">Ceph开发每周谈Vol9—Ceph开发每周谈 Vol 9—加密、压缩、EC 全场景支持</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-10/" target="_blank" rel="external">Ceph开发每周谈 Vol 10—NFS 已经被 RadosGW 支持|用户态</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-11/" target="_blank" rel="external">Ceph开发每周谈 Vol 11—RadosGW 支持 KeyStone V3, AWS v4, 多站点多活</a>(12/13)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-12/" target="_blank" rel="external">Ceph开发每周谈 Vol 12—Scrub 增强，Jewel 小结</a>(12/13)</li><li><a href="http://link.zhihu.com/?target=https%3A//opensource.com/life/16/10/introduction-linux-filesystems" target="_blank" rel="external">An introduction to Linux filesystems</a>(12/15 ~ 12/17) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://www.cnblogs.com/ggjucheng/archive/2012/01/08/2316599.html" target="_blank" rel="external">linux lsof命令详解</a>(12/18)</li><li><a href="http://blog.csdn.net/stpeace/article/details/47069215" target="_blank" rel="external">linux中的ldd命令简介</a>(12/18)</li><li><a href="http://blog.csdn.net/qq_26819733/article/details/50610129" target="_blank" rel="external">linux–&gt;ldd命令的介绍</a>(12/18)</li><li><a href="http://www.cnblogs.com/lyongde/p/4190588.html" target="_blank" rel="external">【Linux笔记】ldconfig、ldd</a>(12/18)</li><li><a href="https://zhuanlan.zhihu.com/p/27875337" target="_blank" rel="external">Linux 的 EXT4 文件系统的历史、特性以及最佳实践</a>(12/18) <i class="fa fa-star"></i><i class="fa fa-star"></i> </li><li><a href="http://www.ruanyifeng.com/blog/2011/12/inode.html" target="_blank" rel="external">理解inode</a>(12/18)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-13/" target="_blank" rel="external">Ceph开发每周谈 Vol 13 — Cache on SPDK / bufferlist</a>(12/19)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-14/" target="_blank" rel="external">Ceph开发每周谈 Vol 14 — LDAP/ BlueStore SMR</a>(12/19)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-15/" target="_blank" rel="external">Ceph开发每周谈 Vol 15—Unix Socket / BlueStore 压缩和 Checksum</a>(12/19)</li><li><a href="https://opensource.com/life/15/9/everything-is-a-file" target="_blank" rel="external">Everything is a file</a>(12/21) <i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://www.tldp.org/LDP/sag/html/dev-fs.html" target="_blank" rel="external">Overview of the Directory Tree</a>(12/21)</li><li><a href="https://www.xsky.com/news/20171218/" target="_blank" rel="external">没错，它就是存储界的“大胃王”</a>(12/22)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-16/" target="_blank" rel="external">Ceph开发每周谈 Vol 16—Jewel RC Release!</a>(12/22)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-17/" target="_blank" rel="external">Ceph开发每周谈 Vol 17 — ARM Status | RBD 一致性组合 | 内核模块 转至元数据结尾</a>(12/22)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-18/" target="_blank" rel="external">Ceph开发每周谈 Vol 18 — EXT4 废弃论战? | 去重支持</a>(12/22)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-19/" target="_blank" rel="external">Ceph开发每周谈Vol19 | Ceph Next 2016 闭门会议资讯独家大放送</a>(12/26)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-20/" target="_blank" rel="external">Ceph开发每周谈Vol 20 | NVMe Over Fabric/Kernel Multi Queue</a>(12/26)</li><li><a href="http://mp.weixin.qq.com/s/h9mz5gWN8tKRz8by2-Sn6w" target="_blank" rel="external">Ceph开发每周谈Vol 104 | NFS Ganesha VS Kernel Client</a>(12/27)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-21/" target="_blank" rel="external">Ceph开发每周谈Vol 21 | ZetaScale | CMP/WriteSame</a>(12/27)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-22/" target="_blank" rel="external">Ceph开发每周谈Vol 22 | 全球最大Ceph集群到底有多大？</a>(12/28)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-23/" target="_blank" rel="external">Ceph开发每周谈vol23｜BlueStore新动向</a>(12/28)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-24/" target="_blank" rel="external">Ceph开发每周谈 Vol 24｜Jewel 10.2.1 第一个 Bug 修复版本释出</a>(12/28)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-24-2/" target="_blank" rel="external">Ceph开发每周谈 Vol 25 | Ceph &amp; DPDK 网络插件开源</a>(12/29)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-27/" target="_blank" rel="external">Ceph开发每周谈Vol 27｜主线分支默认启用 AsyncMessenger</a>(12/29)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-28/" target="_blank" rel="external">Ceph开发每周谈 Vol 28 | OSD 心跳 | Jewel RBD 测试</a>(12/30)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-29/" target="_blank" rel="external">Ceph开发每周谈 Vol 29 — RBD Cache 警告: 数据不一致风险</a>(12/30)</li><li><a href="https://www.xsky.com/tec/ceph-weekly-vol-30/" target="_blank" rel="external">Ceph开发每周谈 Vol 30 — ISA-L 和 BlueStore 性能有哪些进展？</a>(12/30)</li><li><a href="http://blog.csdn.net/haoel/article/details/2879" target="_blank" rel="external">用GDB调试程序（一）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2880" target="_blank" rel="external">用GDB调试程序（二）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2881" target="_blank" rel="external">用GDB调试程序（三）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2882" target="_blank" rel="external">用GDB调试程序（四）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2883" target="_blank" rel="external">用GDB调试程序（五）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2884" target="_blank" rel="external">用GDB调试程序（六）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://blog.csdn.net/haoel/article/details/2885" target="_blank" rel="external">用GDB调试程序（七）</a>(12/31) <i class="fa fa-star"></i><i class="fa fa-star"></i><i class="fa fa-star"></i></li><li><a href="http://www.zphj1987.com/2017/04/26/rados-put-strip-debug/" target="_blank" rel="external">rados put striper功能的调试</a>(12/31) <i class="fa fa-star"></i></li><li><a href="https://ivanjobs.github.io/2016/05/11/prepare-ceph-dev-env.html" target="_blank" rel="external">准备Ceph开发环境</a>(12/31) <i class="fa fa-star"></i></li><li><a href="https://my.oschina.net/u/2460844/blog/515353" target="_blank" rel="external">ceph编译源码、单机搭建调试环境</a>(12/31) <i class="fa fa-star"></i></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/2018-1b.png&quot; alt=&quot;Daily Article 2&quot;&gt;&lt;/center&gt;

&lt;p&gt;书籍可以系统的学习一些知识，并且需要比较长的时间集中注意力学习。而现在网络越来越发达，各种社区的流行，还有开源分享精神的传播，导致现在互联网上很多优秀文章、博客、微信公众号等出现，这些文章贴近热点，往往都很新，并且篇幅有长有短，我们可以利用一些碎片时间来吸收这些知识。优秀文章很多，所以每天读个一两篇文章可以作为一个习惯养成，这样日积月累相信会获益良多。&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Daily Article&lt;/code&gt;系列就是为了记录我每天的阅读历程，以月为单位，每个月出一篇大的总结，一是为了约束自己每天按时按量阅读，也给自己打气，二是给自己每个月的阅读内容做一个总结，用于回头阅读，三是将其中有质量的内容分享给有需要的人。由于我是从&lt;code&gt;2017&lt;/code&gt;九月才可以记录，所以趁着年底索性把十一和十二月的记录一次性发出来。&lt;/p&gt;
&lt;p&gt;目前我采取星星的方式给文章评级，最高三颗星，最低没有星星，一般我放上来的都不会是太水的文章，所以如果是一般的工具类或者没有很大特色的文章我不会进行标记。一颗星表示&lt;code&gt;good&lt;/code&gt;，即这篇文章有特色，对自己有帮助；两颗星表示&lt;code&gt;very
good&lt;/code&gt;，说明这篇文章内容很好，有深度有广度，是一篇很有质量的文章；三颗星表示&lt;code&gt;excellent&lt;/code&gt;，说明这篇文章不仅内容技术讲的很到位，文章文笔也很出色，实践结合理论，让人很容易理解，看完后收获很大或者是顿悟，总而言之是一片很优秀的文章。当然我还会在这三个等级中结合半个星星进行调节，反正就是对文章的一个个人看法而已，仁者见仁，智者见智吧。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
      <category term="Daily-Article" scheme="https://tony-yin.github.io/tags/Daily-Article/"/>
    
  </entry>
  
  <entry>
    <title>译：一切皆文件</title>
    <link href="https://tony-yin.github.io/2017/12/21/Everything-is-a-file/"/>
    <id>https://tony-yin.github.io/2017/12/21/Everything-is-a-file/</id>
    <published>2017-12-21T07:34:32.000Z</published>
    <updated>2017-12-21T07:36:22.516Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/file-600x450.png" alt="Everything is a file"></center><p>这里先提一个技巧性的问题:以下哪一个是文件?</p><ul><li>目录</li><li><code>Shell</code>脚本</li><li><code>Office</code>文档</li><li>串行端口（<code>Serial ports</code>）</li><li>内核数据结构</li><li>内核调优参数</li><li>硬盘驱动器</li><li>分区</li><li>逻辑卷（<code>LVM</code>）</li><li>打印机</li><li>套接字（<code>Sockets</code>）</li></ul><p>也许你不会相信，但是对于<code>Unix</code>和<code>Linux</code>，它们都是文件。这是最令人惊奇的概念之一——这样做使得许多管理任务可以被一些非常简单但功能强大的方法执行，否则这些任务实现起来可能非常困难甚至不可能。</p><a id="more"></a><h2 id="备份主引导记录"><a href="#备份主引导记录" class="headerlink" title="备份主引导记录"></a>备份主引导记录</h2><p>举个简单任务的例子，考虑一下为你的硬盘驱动器地主引导记录（<code>MBR</code>）做一个备份工作。有时候我需要恢复或重新创建我的<code>MBR</code>，尤其是分区表。从头开始重新创建它是非常困难的。但是从保存好的文件中恢复出来这是非常容易的。<code>Linux</code>有一个很强大的<code>GNU</code>工具 — <code>dd</code>，它可以实现这个和其他很多功能。</p><p><code>dd</code>表示<code>disk dump</code>的缩写，意为“磁盘转储”，但是我们很多资深管理员一直认为它是<code>disk destroyer</code>的缩写，因为如果你不是很小心的话，这个工具会准确无误地执行你告诉它要做的事情，包括将硬盘上或者分区上所有的数据都破坏掉。</p><p>以下命令将会备份你的<code>MBR</code>，它必须要是<code>root</code>用户执行，因为非<code>root</code>用户没有访问<code>/dev</code>目录下硬盘驱动器<a href="https://en.wikipedia.org/wiki/Device_file" target="_blank" rel="external">设备文件</a>的权限。<code>BS</code>是<code>Block Size</code>缩写，表示块大小，<code>count</code>表示从源文件读取的块的个数。这个命令将在<code>/tmp</code>目录创建一个<code>myMBR.bak</code>的文件。这个文件的大小将为<code>512</code>字节，包含了<code>MBR</code>的内容，包括引导代码和分区表等。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd if=/dev/sda of=/tmp/myMBR.bak bs=512 count=1</div></pre></td></tr></table></figure><p>如果<code>MBR</code>被损坏了，就需要引导到一个修复盘并执行下面的命令，这个命令本质上就是上面的反向操作。值得注意的是这条命令没有必要指定块大小和块个数这两个参数，因为<code>dd</code>命令将会把备份文件简单地拷贝到硬盘的第一个扇区，并且当它执行到源文件末尾后停止。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dd if=/tmp/myMBR.bak of=/dev/sda</div></pre></td></tr></table></figure><h2 id="都是文件系统的一部分"><a href="#都是文件系统的一部分" class="headerlink" title="都是文件系统的一部分"></a>都是文件系统的一部分</h2><p><code>Linux</code>计算机上的所有内容都可以作为文件系统空间的文件被访问。这是非常重要的，这使得我们 可以<a href="http://yarchive.net/comp/linux/everything_is_file.html" target="_blank" rel="external">使用通用的工具访问不同的东西</a>。</p><p><code>dd</code>命令可用于将硬盘的整个分区拷贝到一个文件或者如下所示的其他硬盘。在这里<code>dd</code>命令再次将数据拷贝到输入设备的末尾并停止。请确保输出设备的容量要大于输入设备。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">dd if=/dev/sdf2 of=/dev/sdg3</div><div class="line"></div><div class="line">dd if=/dev/sda of=/dev/sdg</div></pre></td></tr></table></figure><p>此外文件系统还有其他工具可以达到此作用。比如，<code>cat</code>命令可以用来将任意文件的内容发送到标准输出，这包括分区和整个硬盘。然后，输出还可以被重定向到一个文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat /dev/sda1 &gt; partition1.backup</div></pre></td></tr></table></figure><p>但是，<code>cat</code>命令没有<code>dd</code>命令的控制功能。例如，不能指定从源设备或者源文件读取的数据量。</p><p>下面是一个有趣的实验，它将正面一切皆文件的事实。大多数<code>Linux</code>发行版都有多个虚拟控制台，其中<code>1</code>到<code>7</code>可以用来登录到一个带有<code>shell</code>接口的本地控制台会话。可以通过一些组合键访问它们，比如<code>Ctrl-Alt-F1</code>是控制台<code>1</code>，<code>Ctrl-Alt-F2</code>是控制台2，以此类推。</p><p>按<code>Ctrl-Alt-F2</code>切换到控制台2。在一些发行版中，登录信息包括与此控制台相关的<code>tty</code>（<code>Teletype</code>）设备，但是也有很多发行版不包括。页面应该显示<code>tty2</code>的信息，因为你当前在控制台<code>2</code>。</p><p>用一个非<code>root</code>登录，你可以通过<code>who am i</code>这个命令来确定哪一个<code>tty</code>设备连接到当前控制台。</p><p>在我们实际执行这个实验之前，请看一下<code>/dev</code>目录下的<code>tty2</code>和<code>tty3</code>设备的列表清单。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ls -l /dev tty[23]</div></pre></td></tr></table></figure><p>有大量的定义过的<code>tty</code>设备，但是它们其中的大多数我们并不关心，我们只关系<code>tty2</code>和<code>tty3</code>设备。作为设备文件，它们没有什么特殊之处；它们只是简单的字符类型的设备。我们将用这些设备做这个实验。<code>tty2</code>设备连接到虚拟控制台<code>2</code>，<code>tty3</code>设备连接到虚拟控制台<code>3</code>。</p><p>按<code>Ctrl-Alt-F3</code>组合键切换到控制台<code>3</code>，再次以同样的非<code>root</code>用户登录。</p><p>现在在控制台<code>3</code>输入以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">echo &quot;Hello world&quot; &gt; /dev/tty2</div></pre></td></tr></table></figure><p>按<code>Ctrl-Alt-F2</code>组合键返回控制台<code>2</code>。字符串“Hello world”（没有引号）将显示在控制台<code>2</code>上。</p><p>这个实验也可以在<code>GUI</code>桌面的终端模拟器上进行。桌面上的终端会话在<code>/dev</code>树中使用伪终端设备，比如<code>/dev/pts/1</code>。通过<code>Konsole</code>或者<code>Xterm</code>开启两个终端会话，确定它们连接到哪个伪终端后，使用其中一个发送消息给另一个。</p><p>现在继续试验，使用<code>cat</code>命令在不同的终端显示<code>/etc/fstab</code>文件。</p><p>另一个有趣的实验是使用<code>cat</code>命令直接将文件打印到打印机上。假设你的打印机设备是<code>/dev/usb/lp0</code>，并且你的打印机可以直接打印<code>PDF</code>文件，下面的命令将会在你的打印机上打印一个<code>PDF</code>文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">cat test.pdf &gt; /dev/usb/lp0</div></pre></td></tr></table></figure><p><code>dd</code>命令也可以用来打印一个准备打印的文件。不过，我认为<code>cat</code>命令实际上更适合这个任务。</p><h2 id="“一切皆文件”的含义"><a href="#“一切皆文件”的含义" class="headerlink" title="“一切皆文件”的含义"></a>“一切皆文件”的含义</h2><p>“一切都是文件”的含义是深远的，远远超过了像这篇文章所列举的那样。你们已经在前面的实验中看到过一些例子，但这里有一个包含这些和更多的简短列表。</p><ol><li>克隆硬盘。</li><li>备份分区。</li><li>备份主引导记录(<code>MBR</code>)。</li><li>在<code>u</code>盘上安装<code>ISO</code>镜像。</li><li>与其他终端用户沟通。</li><li>将文件打印到打印机。</li><li>更改<code>/proc pseudo</code>文件系统中的某些文件的内容，以修改运行内核的配置参数。</li><li>用随机数据或零覆盖文件、分区或整个硬盘驱动器。</li><li>将不需要的输出重定向到<code>/dev/null</code>设备，它将永远不会显示。</li><li>等等，等等，等等。。。</li></ol><p>这里有太多的例子，任何一个列表都只是表面的一部分。我相信，你肯定会想出或指出许多比我这里提到更有创造性的方式，来使用<code>Linux</code>的这个特性。我很乐意看到你对如何使用“一切都是文件”的评论。</p><h2 id="附加信息"><a href="#附加信息" class="headerlink" title="附加信息"></a>附加信息</h2><p>有关<code>/dev/</code>目录和你可能在那里找到的设备的更多信息，请参阅<code>Linux Journal</code>上的<a href="http://www.linuxjournal.com/article/2597" target="_blank" rel="external">这篇文章</a>。有关单个设备的更详细信息，<a href="http://www.tldp.org/" target="_blank" rel="external">Linux文档项目</a>中的<a href="http://www.tldp.org/LDP/sag/html/dev-fs.html" target="_blank" rel="external">这篇文章</a>和<a href="http://www.tldp.org/LDP/Linux-Filesystem-Hierarchy/html/dev.html" target="_blank" rel="external">这篇文章</a>会有所帮助。</p><blockquote><p>原文地址：<a href="https://opensource.com/life/15/9/everything-is-a-file" target="_blank" rel="external">https://opensource.com/life/15/9/everything-is-a-file</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/file-600x450.png&quot; alt=&quot;Everything is a file&quot;&gt;&lt;/center&gt;

&lt;p&gt;这里先提一个技巧性的问题:以下哪一个是文件?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;目录&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Shell&lt;/code&gt;脚本&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Office&lt;/code&gt;文档&lt;/li&gt;
&lt;li&gt;串行端口（&lt;code&gt;Serial ports&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;内核数据结构&lt;/li&gt;
&lt;li&gt;内核调优参数&lt;/li&gt;
&lt;li&gt;硬盘驱动器&lt;/li&gt;
&lt;li&gt;分区&lt;/li&gt;
&lt;li&gt;逻辑卷（&lt;code&gt;LVM&lt;/code&gt;）&lt;/li&gt;
&lt;li&gt;打印机&lt;/li&gt;
&lt;li&gt;套接字（&lt;code&gt;Sockets&lt;/code&gt;）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;也许你不会相信，但是对于&lt;code&gt;Unix&lt;/code&gt;和&lt;code&gt;Linux&lt;/code&gt;，它们都是文件。这是最令人惊奇的概念之一——这样做使得许多管理任务可以被一些非常简单但功能强大的方法执行，否则这些任务实现起来可能非常困难甚至不可能。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Linux" scheme="https://tony-yin.github.io/tags/Linux/"/>
    
      <category term="Filesystem" scheme="https://tony-yin.github.io/tags/Filesystem/"/>
    
  </entry>
  
  <entry>
    <title>译：Linux 文件系统介绍</title>
    <link href="https://tony-yin.github.io/2017/12/17/Linux-Filesystem/"/>
    <id>https://tony-yin.github.io/2017/12/17/Linux-Filesystem/</id>
    <published>2017-12-17T14:01:15.000Z</published>
    <updated>2017-12-21T07:42:57.680Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/filesystem-600x450.png" alt="Linux Filesystem"></center><p>本文旨在对<code>Linux</code>文件系统概念进行深层次的讨论。本文既不准备对某个特定类型的文件系统（比如<code>ext4</code>）进行基础性的描述，也不打算作为一个讲解文件系统命令的教程。</p><a id="more"></a><p>每台通用的计算机都需要把各种类型的数据存储在硬盘驱动器(<code>HDD</code>)或者一些同样功能的设备上，比如<code>USB</code>。存储在这些设备上有几个原因，首先,<code>RAM</code>会在计算机电源关闭时丢失内容，虽然也有非易失性类型的内存，可以在电源关闭后维持数据存储不丢失(如<code>flash</code>内存也就是闪存使用的<code>USB</code>和固态硬盘)，但<code>flash</code>内存要比一些标准的、挥发性的内存比如<code>DDR3</code>和其他类似的类型昂贵的多。</p><p>数据需要存储在硬盘驱动器上的第二个原因是，即使是标准的<code>RAM</code>也要比磁盘空间更昂贵。<code>RAM</code>和磁盘成本都在迅速下降，但如果按每字节的成本来算的话还是<code>RAM</code>更高。我们就基于<code>16GB RAM</code>和<code>2TB</code>硬盘的成本，快速计算其每个字节的成本，结果显示  <code>RAM</code>比硬盘驱动器的价格高约<code>71</code>倍。目前，<code>RAM</code>的典型成本大约每字节<code>0.0000000043743750</code>。</p><p>更加直截了当说的话，在计算机的早期，一种内存是基于<code>CRT</code>屏幕上的点的，每一比特大约<code>1</code>美元，这是非常非常昂贵的!</p><h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>你也许会听到人们经常以不同的或者混淆的方式谈论文件系统这个词。这个词本身可能有多重含义，你可能需要从讨论或文档的语境中辨别真正的意思。</p><p>我将尝试根据我在不同情况下使用它的方式来定义“文件系统”的各种含义。注意，在试图遵循标准的“official”含义时，我的意图是根据它的各种用法定义术语。在本文后面的小节中，将更详细地讨论这些含义。</p><ol><li>整个<code>Linux</code>目录结构从顶部(/)根目录开始。</li><li>各种特定类型的数据存储格式，如<code>EXT3</code>、<code>EXT4</code>、<code>BTRFS</code>、<code>XFS</code>等。<code>Linux</code>支持近<code>100</code>种类型的文件系统，包括一些非常古老的文件系统，以及一些最新的文件系统。每个文件系统类型都使用自己的元数据结构来定义如何存储和访问数据。</li><li>一个分区或被格式化为特定类型文件系统的逻辑卷，可以被挂载到<code>Linux</code>文件系统上的指定挂载点上。</li></ol><h2 id="基本的文件系统功能"><a href="#基本的文件系统功能" class="headerlink" title="基本的文件系统功能"></a>基本的文件系统功能</h2><p>磁盘存储是必需的，它带来了一些有趣且不可避免的细节。显然，文件系统的设计目的是为数据的非易失性存储提供空间，这是它的最根本的功能。但是它还有许多其他重要的功能满足不同的需求。</p><p>所有文件系统都需要提供一个命名空间（<code>namespace</code>）——即一个命名和组织的方法。它定义了如何命名文件，具体来说是文件名的长度和可用于文件名的字符的子集，这些字符可以从全部字符集中获取。它还定义了磁盘上数据的逻辑结构，例如使用目录来组织文件，而不是将它们集中在一个单一的、巨大的文件集中。</p><p>一旦定义了名称空间，就需要一个元数据结构来为该名称空间提供逻辑基础。其中元数据包括支持分层目录结构所需的数据结构；用于确定磁盘上哪些块空间已经被使用和哪些可用的结构；允许维护文件和目录名称的结构；文件相关的信息，比如它们的大小和时间，比如创建时间、修改时间或最后访问时间等等；以及属于文件的数据在磁盘上面的位置。还有一些其他元数据用于存储关于磁盘划分的高级信息，如逻辑卷和分区。这个更高级别的元数据和它所代表的结构包含了描述存储在驱动器或分区上的文件系统的信息，这些元数据独立于上面提到的一般文件系统元数据。</p><p>文件系统还需要<code>API</code>接口为系统函数调用提供访问，这些系统函数调用操作文件和目录等文件系统对象。<code>APIs</code>提供诸如创建、移动和删除文件之类的接口。它还提供了一些算法来确定文件放置在文件系统上的位置。这些算法还有确定速度或最小化磁盘碎片等作用。</p><p>现代文件系统还提供了一个安全模式，它是一个为文件和目录定义访问权限的方案。<code>Linux</code>文件系统安全模式有助于确保用户只能访问他们自己的文件，而不是其他人或操作系统本身的文件。</p><p>最后的构建块是实现所有这些功能所需的软件。为了改提高系统和程序员效率，<code>Linux</code>使用了一种<code>two-part</code>的软件实现方式。</p><center><img src="http://ow0mgad6r.bkt.clouddn.com/filesystem_diagram.png" alt="filesystem diagram"></center><br><center>图1：Linux two-part 文件系统软件实现方式</center><p>这两部分实现的第一部分是<code>Linux</code>虚拟文件系统。这个虚拟文件系统为内核和开发人员提供了访问<strong>所有类型</strong>文件系统的一组命令。虚拟文件系统软件调用特定的设备驱动程序来连接到各种类型的文件系统。文件系统特定的设备驱动程序是实现的第二部分。设备驱动程序将文件系统命令的标准集根据特定分区或逻辑卷上的文件系统类型做转换和解释。</p><h2 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h2><p>作为一个通常很有条理的处女座，我喜欢把东西放在那些小而有组织的地方，而不是一个大的桶。使用目录可以帮助我存储和定位我想找的文件。目录也被称为文件夹，因为它们可以被看作实际生活中办公桌上保存文件的文件夹。</p><p>在<code>Linux</code>和许多其他操作系统中，目录可以以树状的层次结构来构造。<code>Linux</code>目录结构在<a href="http://www.pathname.com/fhs/" target="_blank" rel="external">Linux文件系统层次标准(FHS)</a>中得到了很好的定义和记录。在访问它们时引用这些目录是通过使用由前斜杠(<code>/</code>)连接的顺序较深的目录名称(<code>/</code>)来实现的，例如<code>/var/log</code>和<code>/var/spool/mail</code>。这些被称为路径。</p><p>下表提供了一个非常简短的标准、众所周知的和定义在顶层上的<code>Linux</code>目录及其用途的列表。</p><center><img src="http://ow0mgad6r.bkt.clouddn.com/linux-filesystaem.png" alt="top level directory"></center><br><center>表1：Linux文件系统层次结构的顶层</center><p>表<code>1</code>中显示的目录及其子目录及其子目录的子目录，其中背景色为蓝色的目录被认为是根文件系统中不能缺少的组成部分。也就是说，它们不能作为单独的文件系统创建，并且在启动时安装。这是因为它们(特别是它们的内容)必须在引导时出现，以便系统正确引导。</p><p><code>/media</code>和<code>/mnt</code>目录是根文件系统的一部分，但它们不应该包含任何数据。相反，它们只是临时的挂载点。<br>其余的目录，在表<code>1</code>中没有背景颜色的目录不需要在引导序列中出现，但是会在以后安装，在启动序列中准备主机来执行有用的工作。</p><p>可以通过参考官方的<a href="http://www.pathname.com/fhs/" target="_blank" rel="external">Linux文件系统层次标准</a><code>(FHS)web</code>页面，以了解这些目录及其许多子目录的详细信息。维基百科对<code>FHS</code>的描述也很好。应尽可能密切地遵循这一标准，以确保业务和职能的一致性。不管主机上使用的文件系统类型是什么，这个分层目录结构都是相同的。</p><h2 id="Linux-统一目录结构"><a href="#Linux-统一目录结构" class="headerlink" title="Linux 统一目录结构"></a>Linux 统一目录结构</h2><p>在一些非<code>linux PC</code>操作系统中，如果有多个物理硬盘或多个分区，每个磁盘或分区都被分配一个驱动器号。想定位到文件或程序所在的硬盘的位置，驱动器号是必需的，比如<code>C:</code>或<code>D:</code>。然后，以命令的形式发出驱动器字母<code>D:</code>例如，要更改到<code>D:</code>驱动器，然后使用<code>cd</code>命令更改到正确的目录来定位所需的文件。每个硬盘都有自己独立的和完整的目录树。</p><p><code>Linux</code>文件系统将所有物理硬盘和分区统一到一个目录结构中。这一切都是从顶部(<code>/</code>)目录开始的。所有其他目录及其子目录都位于<code>Linux</code>根目录下。这意味着只有一个单独的目录树来搜索文件和程序。</p><p>它们之所以能工作,都是因为文件系统,如<code>/home</code>、<code>/tmp</code>、<code>/var</code>、<code>/opt</code>、<code>/usr</code>可以被创建在单独的物理硬盘上一个不同的分区,或一个不同的逻辑卷<code>/</code>(根)文件系统,然后安装在一个挂载点(目录)作为根文件系统树的一部分。即使是可移动的驱动器，如闪存盘或外部<code>USB</code>或<code>ESATA</code>硬盘驱动器也将安装到根文件系统中，并成为该目录树的一个不可分割的部分。</p><p>在从一个版本的<code>Linux</code>发行版升级到另一个版本，或者从一个发行版切换成到另一个发行版时，文件系统有一个很好的理由可以做到这一点。一般来说，除了<code>Fedora</code>的<code>dnf</code>升级之类的升级工具之外，偶尔重新格式化包含操作系统的硬盘驱动器是明智的，因为在升级过程中，硬盘驱动器会清除任何随时间积累的东西。如果<code>/home</code>是根文件系统的一部分，它将被重新格式化，然后必须从备份中恢复。通过将<code>/home</code>格式化为一个单独的文件系统，那么在根文件系统格式化时它将识别成一个单独的文件系统，并且可以跳过当前步骤。这也适用于数据库、电子邮件收件箱、网站和其他可变用户和系统数据存储的目录<code>/var</code>。</p><p>维护<code>Linux</code>目录树的某些部分作为单独的文件系统还有其他原因。例如，很久以前，当我还没有意识到围绕着所有需要的<code>Linux</code>目录都作为<code>/(root)</code>文件系统的一部分的潜在问题时，我曾用大量非常大的文件填充了我的主目录。由于<code>/home</code>目录和<code>/tmp</code>目录都不是独立的文件系统，而只是根文件系统的子目录，所以整个根文件系统都被填满了。操作系统没有空间创建临时文件或扩展现有的数据文件。起初，应用程序开始抱怨没有空间保存文件，然后操作系统本身开始变得非常奇怪。引导到单用户模式，并清除我的主目录中的问题文件，这让我可以重新开始。然后，我使用一个相当标准的多文件系统设置重新安装了<code>Linux</code>，并能够防止完全的系统崩溃再次发生。</p><p>我曾经还遇到过一个情况，<code>Linux</code>主机继续运行，但是阻止用户使用<code>GUI</code>桌面登录。我能够使用一个<a href="https://en.wikipedia.org/wiki/Virtual_console" target="_blank" rel="external">虚拟控制台</a>本地使用命令行接口(<code>CLI</code>)，并远程使用<code>SSH</code>。问题是，<code>/tmp</code>文件系统已经填满了，而<code>GUI</code>桌面所需的一些临时文件在登录时无法创建。由于<code>CLI</code>登录不需要在<code>/tmp</code>中创建文件，因此缺少空间并没有阻止我使用<code>CLI</code>进行登录。在这种情况下，<code>/tmp</code>目录是一个单独的文件系统，在卷组中有大量可用的空间，<code>/tmp</code>逻辑卷是其中的一部分。我只是将<code>/tmp</code><a href="https://opensource.com/business/16/9/linux-users-guide-lvm" target="_blank" rel="external">逻辑卷扩展</a>到一个够大的容量（其实就是<code>LVM</code>扩容），以适应我对该主机所需要的临时文件空间数量的新需求，并解决了问题。请注意，此解决方案不需要重新启动，并且当<code>/tmp</code>文件系统被放大后，用户可以登录到桌面。</p><blockquote><p>逻辑卷扩展也可以参考我之前总结的一篇文章，简洁明了：<a href="http://www.tony-yin.top/2017/11/14/LVM-Space-Expansion/" target="_blank" rel="external">LVM动态扩展</a></p></blockquote><p>另一种情况发生在我在一家大型科技公司做实验室管理员的时候。我们的一个开发人员在错误的位置（<code>/var</code>）安装了应用程序（我个人认为不能说是装在错误的位置，只能说装的位置的可用空间不合适）。应用程序崩溃是因为<code>/var</code>文件系统已经满了，而存储在<code>/var/log/</code>上的日志文件由于缺少空间，不能添加新的消息。但是，由于关键的<code>/(root)</code>和<code>/tmp</code>文件系统没有填充，系统仍然保持运行。删除违规应用程序并将其重新安装到<code>/opt</code>文件系统中解决了这个问题。（其实通过<code>LVM</code>动态扩容也是可以解决这个问题，要么扩展空间大小，要么换大空间的文件系统）</p><h2 id="文件系统类型"><a href="#文件系统类型" class="headerlink" title="文件系统类型"></a>文件系统类型</h2><p><code>Linux</code>支持读取大约<code>100</code>个分区类型，它只可以在其中的几个而不是所有的分区上创建或写文件。但是，在同一个根文件系统上的不同类型的挂载文件系统是可以做到的，也是非常常见的。在此上下文中，我们讨论的是在硬盘或逻辑卷的分区上存储和管理用户数据所需的结构和元数据。这里提供了<code>Linux fdisk</code>命令识别的文件系统分区类型的完整列表，这样您就可以了解<code>Linux</code>与许多类型的系统之间的高度兼容性。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">0 Empty 24 NEC DOS 81 Minix / old Lin bf Solaris 1 FAT12 27 Hidden NTFS Win 82 Linux swap / So c1 DRDOS/sec (FAT- 2 XENIX root 39 Plan 9 83 Linux c4 DRDOS/sec (FAT- 3 XENIX usr 3c PartitionMagic 84 OS/2 hidden or c6 DRDOS/sec (FAT- 4 FAT16 &lt;32M 40 Venix 80286 85 Linux extended c7 Syrinx 5 Extended 41 PPC PReP Boot 86 NTFS volume set da Non-FS data 6 FAT16 42 SFS 87 NTFS volume set db CP/M / CTOS / . 7 HPFS/NTFS/exFAT 4d QNX4.x 88 Linux plaintext de Dell Utility 8 AIX 4e QNX4.x 2nd part 8e Linux LVM df BootIt 9 AIX bootable 4f QNX4.x 3rd part 93 Amoeba e1 DOS access a OS/2 Boot Manag 50 OnTrack DM 94 Amoeba BBT e3 DOS R/O b W95 FAT32 51 OnTrack DM6 Aux 9f BSD/OS e4 SpeedStor c W95 FAT32 (LBA) 52 CP/M a0 IBM Thinkpad hi ea Rufus alignment e W95 FAT16 (LBA) 53 OnTrack DM6 Aux a5 FreeBSD eb BeOS fs f W95 Ext&apos;d (LBA) 54 OnTrackDM6 a6 OpenBSD ee GPT 10 OPUS 55 EZ-Drive a7 NeXTSTEP ef EFI (FAT-12/16/ 11 Hidden FAT12 56 Golden Bow a8 Darwin UFS f0 Linux/PA-RISC b 12 Compaq diagnost 5c Priam Edisk a9 NetBSD f1 SpeedStor 14 Hidden FAT16 &lt;3 61 SpeedStor ab Darwin boot f4 SpeedStor 16 Hidden FAT16 63 GNU HURD or Sys af HFS / HFS+ f2 DOS secondary 17 Hidden HPFS/NTF 64 Novell Netware b7 BSDI fs fb VMware VMFS 18 AST SmartSleep 65 Novell Netware b8 BSDI swap fc VMware VMKCORE 1b Hidden W95 FAT3 70 DiskSecure Mult bb Boot Wizard hid fd Linux raid auto 1c Hidden W95 FAT3 75 PC/IX bc Acronis FAT32 L fe LANstep 1e Hidden W95 FAT1 80 Old Minix be Solaris boot ff BBT</div></pre></td></tr></table></figure><p>拥有支持读取这么多分区类型的能力的主要目的是允许兼容性和与其他计算机系统的文件系统的某些互操作性。使用<code>Fedora</code>创建新文件系统时可用的选项如下所示。</p><ul><li>btrfs</li><li><strong>cramfs</strong></li><li><strong>ext2</strong></li><li><strong>ext3</strong></li><li><strong>ext4</strong></li><li>fat</li><li>gfs2</li><li>hfsplus</li><li>minix</li><li><strong>msdos</strong></li><li>ntfs</li><li>reiserfs</li><li><strong>vfat</strong></li><li>xfs</li></ul><p>其他发行版支持创建不同的文件系统类型。例如，<code>CentOS 6</code>只支持创建上面列表中粗体显示的文件系统。</p><h2 id="挂载"><a href="#挂载" class="headerlink" title="挂载"></a>挂载</h2><p>在<code>Linux</code>中，<code>to mount</code>一词指的是早期的计算机中，当一个磁带或可移动的磁盘包需要在适当的驱动器上进行物理安装时。在物理上放置磁盘之后，磁盘包上的文件系统将由操作系统逻辑上挂载，以使操作系统、应用程序和用户能够访问这些内容。</p><p>挂载点仅仅是一个目录，就像任何其他的目录一样，它是作为根文件系统的一部分创建的。例如，<code>home</code>文件系统安装在目录<code>/home</code>上。文件系统可以安装在其他非根文件系统上的挂载点上，但这并不常见。<br><code>Linux</code>根文件系统安装在根目录上(<code>/</code>)非常早的引导序列。其他文件系统会被安装在后面，由<code>Linux</code>启动程序，无论是在<code>SystemV</code>下的<code>rc</code>，还是在新的<code>Linux</code>版本中的<code>systemd</code>，在启动过程中挂载文件系统是由<code>/ etc/fstab</code>配置文件管理的。一个容易记住的方法是<code>fstab</code>表示“文件系统表”，它是要挂载的文件系统的列表，还有它们指定的挂载点，以及特定文件系统可能需要的任何选项。</p><p>文件系统安装在现有的目录（挂载点）上，使用<code>mount</code>命令。一般来说，任何被用作挂载点的目录都应该是空的，并且没有包含其中的任何其他文件。<code>Linux</code>不会阻止用户将一个文件系统安装到一个已经存在的文件系统上，或者在一个包含文件的目录上。如果你在现有的目录或文件系统上安装了一个文件系统，那么原始的内容将会被隐藏，只有新挂载的文件系统的内容才会可见。</p><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>我希望本文能够把围绕“文件系统”这个术语的一些可能的混淆给理清楚。我花了很长时间才真正理解并理解<code>Linux</code>文件系统的复杂性、优雅和功能。</p><p>如果你有问题，请把它们加到下面的评论中，我会尽量回答。</p><h2 id="下个月"><a href="#下个月" class="headerlink" title="下个月"></a>下个月</h2><p>另一个重要的概念是，对于<code>Linux</code>，一切都是文件。这个概念为用户和系统管理员提供了一些有趣且重要的实际应用程序。我之所以提到这一点，是因为您可能想在我下个月的<code>/dev</code>目录下的文章前阅读我的<a href="https://opensource.com/life/15/9/everything-is-a-file" target="_blank" rel="external">一切皆文件</a>的文章。</p><blockquote><p>原文地址：<a href="https://opensource.com/life/16/10/introduction-linux-filesystems" target="_blank" rel="external">https://opensource.com/life/16/10/introduction-linux-filesystems</a></p></blockquote><p>“一切皆文件”这篇文章我也进行了翻译</p><blockquote><p>翻译地址：<a href="http://www.tony-yin.top/2017/12/21/Everything-is-a-file" target="_blank" rel="external">http://www.tony-yin.top/2017/12/21/Everything-is-a-file</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/filesystem-600x450.png&quot; alt=&quot;Linux Filesystem&quot;&gt;&lt;/center&gt;

&lt;p&gt;本文旨在对&lt;code&gt;Linux&lt;/code&gt;文件系统概念进行深层次的讨论。本文既不准备对某个特定类型的文件系统（比如&lt;code&gt;ext4&lt;/code&gt;）进行基础性的描述，也不打算作为一个讲解文件系统命令的教程。&lt;/p&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Linux" scheme="https://tony-yin.github.io/tags/Linux/"/>
    
      <category term="Filesystem" scheme="https://tony-yin.github.io/tags/Filesystem/"/>
    
  </entry>
  
  <entry>
    <title>阅读感悟：《白夜行》</title>
    <link href="https://tony-yin.github.io/2017/12/10/Into-Withe-Night/"/>
    <id>https://tony-yin.github.io/2017/12/10/Into-Withe-Night/</id>
    <published>2017-12-10T11:57:34.000Z</published>
    <updated>2017-12-11T11:59:53.743Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/white_night.jpg" alt="Into White Night"></center><p>阅读周期：2017/12/08 ~ 2017/12/10<br>阅读评分：4.9<br>阅读人群：还是成年的人看着好一点，不大适合未成年人</p><blockquote><p>我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借着这份光，我便能把黑夜当成白天。我从来就没有太阳，所以不怕失去。</p></blockquote><a id="more"></a><p>之前一直在看技术书，这周末想稍微放松一下自己，翻了翻买了许久却没阅读的《白夜行》，之前听身边很多人称赞过这本书，果然看了就没停的下来，花了两天直接拉满读完。。。</p><p>坊间传言这是东野圭吾写的最好的一本书，我只看过《嫌疑人x的献身》，对比那个的话我觉得这一本更加细腻和深远。</p><p>看完了这本书，我去看了很多的书评，各有各的见解，并且都很有道理，我想这就是东野圭吾的厉害支持了，真正厉害的推理小说，不是作者在推理，而是最后是读者在推理。</p><p>对我个人而言，这本书最大的亮眼之处，就是作者重来不主动把某件事情说明，而是留下种种线索，相关关联，埋下伏笔，这样就导致永远没有一个确定性的答案，作者在意的也许不是具体的场景和手段，他着重刻画的是人性的本质、作恶的动机和灵魂的交融。</p><p>回过头一看，全书亮司与雪穗的生活宛如两条平行线，男女主角之间竟然没有任何对白，也没有交待任何他们见面的情节。书最后亮司突然死了，故事也很突然就这样结束了。让人反应不过来，因为她的太阳失去了。亮司承担了所有的罪恶，为了想让她生活在阳光下。</p><p>很多人都在讨论桐源和雪穗之间的关系，大多数人都认为他们是深爱着的。我个人觉得他们或许有爱，但是更多的是相互依靠和相互依附，也就是双方都是彼此的灵魂，活下去的羁绊，我觉得这是和单单的爱情是不一样的，否则他们大可以在一起，他们有的是钱，完全可以隐性瞒名的私奔。</p><p>我觉得桐源应该更爱雪穗一些，比如一直在他身边守护着她，最后关头为了保住她选择自杀，还有就是只有和她ML的时候才不会迟泄（这个在夕子那个宾馆的章节可以看出），但是桐源尊重雪穗，他只是一直在她身边守护着，永远不会过分打扰她的生活，即使她和别的男人恋爱，即使结婚，即使离婚再结婚，在他心里，雪穗永远是最重要的，满足她想要的和想做的是他一生的追求。桐源一直是作恶的执行者，雪穗背后的指使者，很多读者都为桐源不值，觉得他为了雪穗付出了那么多，最后得到确是冷漠的不回头，其实他并不后悔，看到《嫌疑人x的献身》的同学都知道最后时刻男主多希望女主也能像这边的女主雪穗一样头也不回地离开，这样他的付出才是值得的。桐源对典子应该也是有感情的，从他愿意带她去大阪，跟她讲童年时光，不过这种感情是建立在先利用之后感情培养出来的，但是在桐源心中雪穗永远是最重要的，即使典子获取桐源所有的爱情，当要做出选择的时候，我相信桐源还是选择雪穗吧，因为我觉得在他心中爱情远没有他和雪穗之间的羁绊重要，他们之间的关系关乎到灵魂和生死，他们是对方相互的太阳。</p><blockquote><p>曾经拥有的东西被夺走，并不代表就会回到原来没有那种东西的时候。                    —典子</p></blockquote><p>而雪穗难道对桐源没感情吗？我想肯定有，但是他们俩成为互相羁绊的时间太小了，才十一二岁，在太早的时间双方互相守护秘密，更像是相互保护，少了爱情的起源，所以她们也不知道双方之间是不是爱，雪穗说过她不知道如何去爱一个人，这个人说的可能是高工城，可能是一成，也有可能是桐源，桐源和雪穗经常就是在实行某个行动需要对方配合帮忙的时候才会聚头，比如夕子的那件事，雪穗肯定是通过手或者嘴帮助桐源完成了SJ，但是雪穗从来没有为高工城做过这样的事情，桐源在雪穗的心目中的位置可见一斑。但是也许她自己都不确定这是不是爱情，就像高工城内心的疑问一样，总感觉这份感情里面掺杂了很多其他东西。而雪穗对一诚或许在感情上面和别的人有一些不同，但是这也仅仅是有一点点不同，除了感情我觉得还有一个原因，雪穗不喜欢被别人无视，当大学社团一诚喜欢江利子忽略雪穗时，这让她感觉到嫉妒了，她一直认为自己是完美的，是所有人的焦点所在。</p><p>很多人都在愤怒雪穗最后的冷漠，上面我也说了，除了桐源希望用自己的生命保全雪穗之外，雪穗的行为是头也不回的离去，要知道雪穗在此之前无论是生母、养母、前夫等等，都表现出自己的伤心，唯独这次雪穗没有表现正常，也算是她的失误了，我想她不仅因为是失去了她的太阳觉得无助，最重要的是他也不敢面对这个现实，她面对谁都可以表演的很好，但是除了桐源，她做不到，也许她过不了多久也会选择离开这个世界，要么就永远在白夜中度过。。。</p><p>店已打烊女主确上楼而不是离开，是否会意味着要与亮一同结束在梦魇开始的地方？</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/white_night.jpg&quot; alt=&quot;Into White Night&quot;&gt;&lt;/center&gt;

&lt;p&gt;阅读周期：2017/12/08 ~ 2017/12/10&lt;br&gt;阅读评分：4.9&lt;br&gt;阅读人群：还是成年的人看着好一点，不大适合未成年人&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;我的天空里没有太阳，总是黑夜，但并不暗，因为有东西代替了太阳。虽然没有太阳那么明亮，但对我来说已经足够。凭借着这份光，我便能把黑夜当成白天。我从来就没有太阳，所以不怕失去。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="read" scheme="https://tony-yin.github.io/categories/read/"/>
    
    
      <category term="Read" scheme="https://tony-yin.github.io/tags/Read/"/>
    
  </entry>
  
  <entry>
    <title>故障修复：文件系统导致OSD启动失败</title>
    <link href="https://tony-yin.github.io/2017/12/08/Filesystem-Bug-Fix/"/>
    <id>https://tony-yin.github.io/2017/12/08/Filesystem-Bug-Fix/</id>
    <published>2017-12-08T12:54:16.000Z</published>
    <updated>2017-12-10T12:59:19.652Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/bugfix.png" alt="Bug fix"></center><p>记一次因为文件系统导致<code>OSD</code>无法启动的故障修复。</p><a id="more"></a><h2 id="集群状况："><a href="#集群状况：" class="headerlink" title="集群状况："></a>集群状况：</h2><p><code>Cluster</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">root@gigabyte:~# ceph -s</div><div class="line">    cluster de3627bb-c748-4623-8cb1-b88c646ff5d5</div><div class="line">     health HEALTH_WARN 42 pgs degraded; 1 pgs down; 1 pgs peering; 17 pgs recovering; 25 pgs recovery_wait; 359 pgs stale; 42 pgs stuck degraded; 1 pgs stuck inactive; 359 pgs stuck stale; 244 pgs stuck unclean; recovery 6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%); 2/23 in osds are down</div><div class="line">     monmap e1: 1 mons at &#123;erxdl=10.16.180.28:6789/0&#125;, election epoch 2, quorum 0 erxdl</div><div class="line">     mdsmap e654: 1/1/1 up &#123;0=irlhy=up:active&#125;</div><div class="line">     osdmap e218: 23 osds: 21 up, 23 in</div><div class="line">      pgmap v8943: 7936 pgs, 16 pools, 639 GB data, 159 kobjects</div><div class="line">            105 GB used, 76827 GB / 76933 GB avail</div><div class="line">            6665/163785 objects degraded (4.069%); 6665/163785 unfound (4.069%)</div><div class="line">                 359 stale+active+clean</div><div class="line">                 201 active+remapped</div><div class="line">                7333 active+clean</div><div class="line">                  25 active+recovery_wait+degraded</div><div class="line">                  17 active+recovering+degraded</div><div class="line">                   1 down+peering</div></pre></td></tr></table></figure><p><code>OSD</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">root@test:/data# ceph osd tree</div><div class="line"># idweighttype nameup/downreweight</div><div class="line">-382.29pool 4T</div><div class="line">-482.29host test1</div><div class="line">00osd.0up    1</div><div class="line">83.578osd.8up    1</div><div class="line">173.578osd.17up    1</div><div class="line">53.578osd.5up    1    </div><div class="line">...</div><div class="line">...</div><div class="line">163.578osd.16up    1</div><div class="line">103.578osd.10up    1</div><div class="line">153.578osd.15up    1</div><div class="line">130osd.13down1</div></pre></td></tr></table></figure><p>由上可知：<code>osd.0</code>和<code>osd.13</code>已经被集群剔除，并且权重变为了<code>0</code></p><h2 id="报错日志："><a href="#报错日志：" class="headerlink" title="报错日志："></a>报错日志：</h2><h3 id="报错1："><a href="#报错1：" class="headerlink" title="报错1："></a>报错1：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2017-12-06 10:18:05.180802 7f4809ae07c0 -1 osd.13 234 set_disk_tp_priority(22) Invalid argument: osd_disk_thread_ioprio_class is  but only the following values are allowed: idle, be or rt</div><div class="line"></div><div class="line">2017-12-06 10:10:44.974634 7f4cbcfff700 -1 os/FileStore.cc: In function &apos;virtual int FileStore::read(coll_t, const ghobject_t&amp;, uint64_t, size_t, ceph::bufferlist&amp;, bool)&apos; thread 7f4cbcfff700</div><div class="line"></div><div class="line">2017-12-06 10:10:44.972299 os/FileStore.cc: 2851: FAILED assert(allow_eio || !m_filestore_fail_eio || got != -5)</div></pre></td></tr></table></figure><h3 id="报错2"><a href="#报错2" class="headerlink" title="报错2:"></a>报错2:</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">2017-12-06 10:32:56.602065 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is supported and appears to work</div><div class="line"></div><div class="line">2017-12-06 10:32:56.602095 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: FIEMAP ioctl is disabled via &apos;filestore fiemap&apos; config option</div><div class="line"> </div><div class="line">2017-12-06 10:32:56.620337 7f576071b7c0  0 genericfilestorebackend(/ceph/osd.13) detect_features: syncfs(2) syscall fully supported (by glibc and kernel)</div><div class="line"></div><div class="line">2017-12-06 10:32:56.739219 7f576071b7c0  0 filestore(/ceph/osd.13) limited size xattrs</div></pre></td></tr></table></figure><h3 id="报错3："><a href="#报错3：" class="headerlink" title="报错3："></a>报错3：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2017-12-06 10:16:32.442815 7f06cddf2700 -1 journal aio to 1398235136~434176 got (5) Input/output error</div><div class="line"></div><div class="line">2017-12-06 10:16:32.443787 7f06cddf2700 -1 os/FileJournal.cc: In function &apos;void FileJournal::write_finish_thread_entry()&apos; thread 7f06cddf2700</div><div class="line"></div><div class="line">2017-12-06 10:16:32.442867 os/FileJournal.cc: 1383: FAILED assert(0 == &quot;unexpected aio error&quot;)</div></pre></td></tr></table></figure><h3 id="报错4："><a href="#报错4：" class="headerlink" title="报错4："></a>报错4：</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2017-12-06 00:52:33.814785 7fb24be877c0 -1 filestore(/ceph/osd.13) FileStore::mount: unable to access basedir &apos;/ceph/osd.0&apos;: (30) Read-only file system</div><div class="line"></div><div class="line">2017-12-06 00:52:33.814801 7fb24be877c0 -1 osd.13 0 OSD:init: unable to mount object store</div><div class="line"></div><div class="line">2017-12-06 00:52:33.814806 7fb24be877c0 -1 ^[[0;31m ** ERROR: osd init failed: (30) Read-only file system</div></pre></td></tr></table></figure><h2 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h2><p>首先我们肯定是要尝试把<code>OSD</code>起来嘛，所以要做的就是先给<code>osd</code>加权重，接着加入集群，最后再启动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">ceph osd crush add osd.0 3.578 host=test1</div><div class="line">ceph osd in osd.0</div><div class="line">service ceph start osd.0</div><div class="line"></div><div class="line">ceph osd crush add osd.13 3.578 host=test1</div><div class="line">ceph osd in osd.13</div><div class="line">service ceph start osd.13</div></pre></td></tr></table></figure><p>这时候发现<code>OSD</code>还是起不来，我们就去看<code>osd.0</code>和<code>osd.13</code>的<code>log</code>，也就会发现以上一系列的报错日志，错误较多；</p><p><code>filesystem</code>出现较多<code>limited size xattrs</code>这一行引起了我的注意，由于此环境<code>osd</code>用的文件系统是<code>ext4</code>，而<code>ext4</code>对存储<code>xattr</code>的大小有限制，使得OSD信息不能安全的保存。</p><p>所以在<code>ceph</code>中如果<code>osd</code>采用<code>ext4</code>文件系统时，需要在配置项里面加入相关配置实现用<code>omap</code>来存储<code>xattr</code>，而<code>xfs</code>文件系统由于对<code>xattr</code>的存储是足够的，所以不存在这个问题。</p><p>所以解决这个问题有三个方案：</p><h3 id="方案1"><a href="#方案1" class="headerlink" title="方案1"></a>方案1</h3><p>更改文件系统，将<code>ext4</code>改成<code>xfs</code></p><h3 id="方案2"><a href="#方案2" class="headerlink" title="方案2"></a>方案2</h3><p>文件系统还是采用<code>ext4</code>，配置让<code>Ceph filestore</code>中的<code>omap</code>存储<code>xattr</code>，在<code>/etc/ceph/ceph.conf</code>中<code>global section</code>或<code>osd section</code>中插入一行以下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">filestore xattr use omap = true</div></pre></td></tr></table></figure><h3 id="方案3"><a href="#方案3" class="headerlink" title="方案3"></a>方案3</h3><p>限制对象的长度大小，同样是修改<code>ceph.conf</code>，在<code>global section</code>或者<code>osd section</code>中加入以下配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">osd max object name len = 256 </div><div class="line">osd max object namespace len = 64</div></pre></td></tr></table></figure><p>然后再次重启以下对应的<code>osd</code>服务就OK了！</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在<code>Ceph</code>中如果用<code>ext4</code>文件系统的话，一定要注意配置将<code>xattr</code>存在<code>omap</code>中。</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/bugfix.png&quot; alt=&quot;Bug fix&quot;&gt;&lt;/center&gt;

&lt;p&gt;记一次因为文件系统导致&lt;code&gt;OSD&lt;/code&gt;无法启动的故障修复。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Bugfix" scheme="https://tony-yin.github.io/tags/Bugfix/"/>
    
  </entry>
  
  <entry>
    <title>通过 Keepalived 实现 Ceph RBD 的高可用</title>
    <link href="https://tony-yin.github.io/2017/12/07/RBD-HA/"/>
    <id>https://tony-yin.github.io/2017/12/07/RBD-HA/</id>
    <published>2017-12-06T16:25:31.000Z</published>
    <updated>2017-12-07T06:47:35.572Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/HA.jpg" alt="HA"></center><p>由于<code>Cephfs</code>很不稳定，并且性能差，很难达到用户在性能上的需求，所以<code>Cephfs</code>也难以应用于生产环境之上。而<code>RBD</code>可以说是一直非常稳定的存储接口方案，用户可以将<code>image</code>挂载到客户端进行访问读写，然而很多用户不愿意在本地安装<code>Ceph</code>客户端，所以我们常常需要自己封装一层，给客户端暴露出一个通用的接口进行访问，现在一般都是默认用<code>NFS</code>，所以本文就<code>Ceph RBD</code>如何实现高可用暴露<code>NFS</code>给客户端访问进行分享。</p><a id="more"></a><h2 id="环境："><a href="#环境：" class="headerlink" title="环境："></a>环境：</h2><blockquote><p><code>Linux Distribution</code> : <code>ubuntu</code><br><code>Ceph</code> : <code>Giant</code><br><code>Keepalived</code> : <code>v1.2.2</code><br>集群信息 ：三节点，<code>IP</code>分别为<code>192.168.1.111</code>，<code>192.168.1.112</code>，<code>192.168.1.113</code></p></blockquote><h2 id="Keepalived-简介"><a href="#Keepalived-简介" class="headerlink" title="Keepalived 简介"></a>Keepalived 简介</h2><p>建议先简单了解一些<code>keepalived</code>的机制再看下面的内容~</p><blockquote><p><code>Keepalived</code>的作用是检测集群中服务器的状态，如果有一台服务器死机，或工作出现故障，<code>Keepalived</code>将检测到，并将有故障的服务器从集群中剔除，当服务器工作正常后<code>Keepalived</code>自动将服务器加入到服务器群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的服务器。</p></blockquote><p>下面从网上找了几张图片，方便大家理解一下其原理和机制：</p><h3 id="Keepalived-内部结构："><a href="#Keepalived-内部结构：" class="headerlink" title="Keepalived 内部结构："></a>Keepalived 内部结构：</h3><center><img src="http://ow0mgad6r.bkt.clouddn.com/keepalived_space.png" alt="keepalive layer"></center><h3 id="双机热备："><a href="#双机热备：" class="headerlink" title="双机热备："></a>双机热备：</h3><center><img src="http://ow0mgad6r.bkt.clouddn.com/keepalived_two_host.png" alt="keepalive two host master and backup"></center><h3 id="负载均衡、应用分层："><a href="#负载均衡、应用分层：" class="headerlink" title="负载均衡、应用分层："></a>负载均衡、应用分层：</h3><center><img src="http://ow0mgad6r.bkt.clouddn.com/keepalived_layer.png" alt="keepalive layer"></center><h3 id="配置文件解析"><a href="#配置文件解析" class="headerlink" title="配置文件解析"></a>配置文件解析</h3><p>已下摘录自：<a href="http://blog.csdn.net/love_is_simple/article/details/47903527" target="_blank" rel="external">http://blog.csdn.net/love_is_simple/article/details/47903527</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">global_defs &#123;</div><div class="line">   notification_email &#123;</div><div class="line">     sai@localhost # 报警邮件接收人的地址</div><div class="line">   &#125;</div><div class="line">   notification_email_from root@localhost # 发送报警邮件发件人地址</div><div class="line">   smtp_server 127.0.0.1# 发送邮件的服务器地址</div><div class="line">   smtp_connect_timeout 30# 邮件超时时间(可以根据自己的需求进行设定)</div><div class="line">   router_id LVS_DEVEL# 一个实例的标识地址(可以有多个实例但不能相同)</div><div class="line">&#125;</div><div class="line">vrrp_script monitor_nginx &#123;</div><div class="line">  script “/root/scripts/monitor_nginx.sh”#根据自己的实际路径放置脚本文件</div><div class="line">  interval 1# 脚本执行间隔</div><div class="line">Weight -5#脚本结果导致的优先级变更:5表示优先级加5；-5表示优先级减5</div><div class="line">&#125;</div><div class="line">vrrp_instance VI_1 &#123;# 虚拟路由器自己的名字</div><div class="line">    state MASTER# 设置服务器模式，当前为主节点,master端</div><div class="line">    interface eth0# 实例网卡,也就是提供服务的网卡，来发送vrrp通告</div><div class="line">    virtual_router_id 51# 设置vrid,这里非常重要,相同的vrid为一个组,他决定,它将决定多播的MAC地址.（建议不要使用默认地址,以免发生冲突）</div><div class="line">    priority 100#  设置本节点的优先级,优先级高的为master</div><div class="line">    advert_int 1# 检查间隔,默认为1秒</div><div class="line">    authentication &#123;</div><div class="line">        auth_type PASS# 认证方式,可以是pass或者AH两种认证方式</div><div class="line">        auth_pass 1111# 认证密码</div><div class="line">    &#125;</div><div class="line">    virtual_ipaddress &#123;# 设置vip,虚拟ip地址(实现高可用,转移的vip地址)</div><div class="line">        10.0.1.230# 此地址并不存在,当成为主节点时,此ip地址将会自动生成</div><div class="line">&#125;</div><div class="line">script_track &#123;</div><div class="line">monitor_nginx  #跟踪这个monitor_nginx脚本;就是不断去检查这个脚本</div><div class="line">&#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="原理分析"><a href="#原理分析" class="headerlink" title="原理分析"></a>原理分析</h2><h3 id="RBD-导出-NFS"><a href="#RBD-导出-NFS" class="headerlink" title="RBD 导出 NFS"></a>RBD 导出 NFS</h3><p>首先我们要实现<code>RBD</code>导出<code>NFS</code>功能，毕竟只有先能让客户端通过<code>NFS</code>访问后端存储，然后才有必要谈后端存储集群的高可用方案。</p><p>我们需要在<code>Ceph Server</code>集群创建<code>RBD image</code>；然后在三个节点上都建立<code>RBD Map</code>关系，最终只会有一个块设备被<code>mount</code>，其余两个既用于占位（防止多<code>image</code>的情况下造成节点间块设备名称不一致），又是为了作为备机在主机发生故障时转换角色，挂载当前节点的块设备。</p><p>接着在三个节点上分别在指定目录下创建目录，本文是在<code>/vol/</code>目录下创建目录，比如创建目录<code>/vol/ec</code>，这个目录就是块设备对应的挂载目录。</p><p>如果有童鞋对<code>rbd</code>导出<code>nfs</code>过程有兴趣的话，具体请参考：<a href="http://www.tony-yin.top/2017/10/31/RBD-Mount-NFS/" target="_blank" rel="external">使用NFS挂载RBD</a>。</p><h3 id="Keepalived-实现-HA"><a href="#Keepalived-实现-HA" class="headerlink" title="Keepalived 实现 HA"></a>Keepalived 实现 HA</h3><p>我们后端存储集群最终只会暴露出一个接口或者说是一个<code>IP</code>，<code>keepalived</code>中有<code>VIP</code>这种配置可以支持，所以我们需要在三个节点上配置<code>keepalived.conf</code>文件，然后启动<code>keepalived</code>所有节点会选举一个<code>master</code>节点并暴露虚拟<code>IP</code>。</p><p>然后我们在<code>master</code>节点上将块设备挂载到之前创建的目录<code>/vol/ec</code>，同步信息至<code>/ect/exports</code>，可以通过<code>showmount -e vip</code>可以发现<code>/vol/ec</code>已经暴露到了<code>vip</code>上，客户端便可以通过<code>NFS</code>将上一步创建的目录<code>/vol/ec</code>挂载到本地目录，比如<code>/client_ec</code>；</p><p>这时候客户端已经可以通过虚拟<code>IP</code>对<code>RBD image</code>进行读写了，但是如果这时候<code>master</code>节点<code>down</code>了咋办呢？</p><p>为了防止集群中主节点不能给<code>client</code>提供访问，我们需要实现高可用，也就是当主节点<code>down</code>了后，集群自动切换主机，并且针对<code>RBD</code>做自动相应挂载操作，让用户无感知访问存储后端。</p><p>我们需要配置<code>keepalived.conf</code>，当节点角色转为<code>backup</code>时，触发停止<code>NFS</code>并卸载暴露目录等操作；当节点角色转为<code>master</code>时，触发挂载<code>RBD image</code>并启动<code>NFS</code>等操作；定时检查当前<code>NFS</code>，一旦<code>NFS</code>服务停止了，尝试重启，如果重启失败，停止<code>keepalived</code>服务触发节点角色切换等等。</p><p>这些操作对用户来说是无感知的，我们还可以针对<code>keepalived</code>做相关邮件配置提醒服务器发生故障等等。</p><h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><h3 id="准备环境"><a href="#准备环境" class="headerlink" title="准备环境"></a>准备环境</h3><h4 id="搭建-Ceph-集群"><a href="#搭建-Ceph-集群" class="headerlink" title="搭建 Ceph 集群"></a>搭建 Ceph 集群</h4><p>这个就不多说了，基本操作。</p><h4 id="安装-Keepalived"><a href="#安装-Keepalived" class="headerlink" title="安装 Keepalived"></a>安装 Keepalived</h4><p>本位基于<code>ubuntu</code>，<code>redhat</code>派可以转换成对应的命令再操作</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">apt-get install libpopt-dev     // 安装依赖</div><div class="line">apt-get install keepalived</div></pre></td></tr></table></figure><h3 id="RBD-导出"><a href="#RBD-导出" class="headerlink" title="RBD 导出"></a>RBD 导出</h3><h4 id="创建-image"><a href="#创建-image" class="headerlink" title="创建 image"></a>创建 image</h4><p>这里默认在<code>test1 pool</code>中创建<code>1G</code>的<code>image</code>，请根据自己的场景转换大小，生产环境一般都要几十<code>T</code>，甚至上百<code>T</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">rbd create --size 1024 -p test1 image_ec</div></pre></td></tr></table></figure><h4 id="建立-map-关系"><a href="#建立-map-关系" class="headerlink" title="建立 map 关系"></a>建立 map 关系</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">rbd map -p test1 image_ec</div><div class="line"># 输出块设备名称： /dev/rbd0</div></pre></td></tr></table></figure><h4 id="为块设备格式化文件系统"><a href="#为块设备格式化文件系统" class="headerlink" title="为块设备格式化文件系统"></a>为块设备格式化文件系统</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mkfs.ext4 -i 400000 /dev/rbd0</div></pre></td></tr></table></figure><h4 id="将块设备挂载到本地目录"><a href="#将块设备挂载到本地目录" class="headerlink" title="将块设备挂载到本地目录"></a>将块设备挂载到本地目录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">mount -o noatime,user_xattr /dev/rbd0 /vol/ec</div></pre></td></tr></table></figure><h3 id="配置-Keepalived"><a href="#配置-Keepalived" class="headerlink" title="配置 Keepalived"></a>配置 Keepalived</h3><h4 id="配置-VIP"><a href="#配置-VIP" class="headerlink" title="配置 VIP"></a>配置 VIP</h4><p><code>VIP</code>必须是当前集群不存在的<code>ip</code>，通过将配置个节点上<code>keepalived.conf</code>，为<code>virtual_ipaddress</code>选项添加<code>IP</code>，我这边用的是<code>192.168.1.13/24</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">virtual_ipaddress &#123;</div><div class="line">    192.168.1.13/24</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="配置优先级和网卡"><a href="#配置优先级和网卡" class="headerlink" title="配置优先级和网卡"></a>配置优先级和网卡</h4><p>三个节点的角色都配置为<code>BACKUP</code>，并且配置<code>nopreempt</code>，这样就可以实现不抢占模式，当主节点<code>down</code>恢复后不会抢占成为主节点，对我而言哪个是主节点并不重要，频繁切换反而会造成客户端延时。我这边的对外网卡是<code>eth0</code>，<code>priority</code>是真正决定一开始初始化选举<code>master</code>的因素，最大值的节点是<code>master</code>节点，一旦切换角色，这个值并不会改变。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">vrrp_instance VI_1 &#123;</div><div class="line">    state BACKUP</div><div class="line">    interface eth0</div><div class="line">    priority 100</div><div class="line">    nopreempt</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="配置角色切换后触发相关操作"><a href="#配置角色切换后触发相关操作" class="headerlink" title="配置角色切换后触发相关操作"></a>配置角色切换后触发相关操作</h4><p>当主机<code>down</code>之后，如果没有关机，角色转换为<code>backup</code>后需要做卸载相关操作；而之前的备机如今成为了主机，也要做挂载等相关操作，这些需求我们可以通过配置<code>keepalived</code>，当角色转换时触发相关脚本，这里的配置就表示当节点角色切换为了<code>master</code>时则需要执行<code>/etc/keepalived/ChangeToMaster.sh</code>，角色切换为<code>backup</code>则会执行<code>/etc/keepalived/ChangeToBackup.sh</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">vrrp_instance VI_1 &#123;</div><div class="line">    notify_master &quot;/etc/keepalived/ChangeToMaster.sh&quot; </div><div class="line">    notify_backup &quot;/etc/keepalived/ChangeToBackup.sh&quot;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="配置定时检测-NFS-状态"><a href="#配置定时检测-NFS-状态" class="headerlink" title="配置定时检测 NFS 状态"></a>配置定时检测 NFS 状态</h4><p>如果一旦<code>NFS</code>服务断了，我们不及时处理的话，客户端就可以明显地感知到无法读写了。所以我们需要定时不断检测<code>NFS</code>的状态，这个也可以通过配置<code>track_script</code>选项执行某个脚本并指定间隔时间：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">vrrp_script chk_nfs &#123;</div><div class="line">    script &quot;/etc/keepalived/check_nfs.sh&quot;       # 调用脚本</div><div class="line">    interval 2      # 设置间隔时间为 2s</div><div class="line">&#125;</div><div class="line">vrrp_instance VI_1 &#123;</div><div class="line">    track_script &#123;</div><div class="line">        chk_nfs     # 调用上面的chk_nfs函数</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="整个配置文件内容"><a href="#整个配置文件内容" class="headerlink" title="整个配置文件内容"></a>整个配置文件内容</h4><p>暂时还是比较精简的，邮件什么的都没配置，<code>keepalived</code>还是可以做很多事情的，有兴趣的童鞋可以深入研究</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line">global_defs &#123;</div><div class="line">    notification_email &#123;</div><div class="line">    &#125;</div><div class="line"></div><div class="line">    router_id NFS_HA_222</div><div class="line">&#125;</div><div class="line"></div><div class="line">vrrp_script chk_nfs &#123;</div><div class="line">    script &quot;/etc/keepalived/check_nfs.sh&quot;</div><div class="line">    interval 2</div><div class="line">&#125;</div><div class="line"></div><div class="line">vrrp_instance VI_1 &#123;</div><div class="line">    #state MASTER</div><div class="line">    state BACKUP</div><div class="line">    interface eth0</div><div class="line">    priority 100</div><div class="line">    virtual_router_id 100</div><div class="line">    advert_int 1</div><div class="line">    authentication &#123;</div><div class="line">        auth_type PASS</div><div class="line">        auth_pass 1111</div><div class="line">    &#125;</div><div class="line">    track_script &#123;</div><div class="line">        chk_nfs</div><div class="line">    &#125;</div><div class="line">    nopreempt</div><div class="line">    notify_master &quot;/etc/keepalived/ChangeToMaster.sh&quot;</div><div class="line">    notify_backup &quot;/etc/keepalived/ChangeToBackup.sh&quot;</div><div class="line">    virtual_ipaddress &#123;</div><div class="line">        192.168.1.13/24</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h4 id="相关脚本"><a href="#相关脚本" class="headerlink" title="相关脚本"></a>相关脚本</h4><blockquote><p>大家可以手动从下面复制，也可以去我的<code>github</code>上面获取，欢迎点赞！</p><p>地址： <a href="https://github.com/tony-yin/ceph_scripts#keepalived" target="_blank" rel="external">https://github.com/tony-yin/ceph_scripts#keepalived</a></p></blockquote><p>这些脚本都是针对我当前环境的，需要针对自己的环境和需求进行相应更改</p><p><code>ChangeToBackup.sh</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">#!/bin/bash</div><div class="line"></div><div class="line">service nfs-kernel-server stop</div><div class="line">for folder in $(ls /vol)</div><div class="line">do</div><div class="line">    if $(mount | grep $folder -q); then</div><div class="line">        umount -f /vol/$folder</div><div class="line">    fi</div><div class="line">done</div></pre></td></tr></table></figure><p><code>ChangeToMaster.sh</code>：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"><span class="keyword">for</span> folder <span class="keyword">in</span> $(ls /vol)</div><div class="line"><span class="keyword">do</span></div><div class="line">    <span class="keyword">if</span> $(mount | grep <span class="variable">$folder</span> -q); <span class="keyword">then</span></div><div class="line">        umount /vol/<span class="variable">$folder</span> &gt; /dev/null</div><div class="line">    <span class="keyword">fi</span></div><div class="line">    device=$(grep <span class="variable">$folder</span> /etc/block_map -w | awk <span class="string">'&#123;print $1&#125;'</span>)</div><div class="line">    mount <span class="variable">$device</span> /vol/<span class="variable">$folder</span></div><div class="line"><span class="keyword">done</span></div><div class="line">service nfs-kernel-server start</div></pre></td></tr></table></figure><p><code>check_nfs.sh</code>：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/sh</span></div><div class="line">vip=$(grep -A 1 virtual_ipaddress /etc/keepalived/keepalived.conf | grep -v virtual_ipaddress | tr -d [:blank:] | cut -d <span class="string">'/'</span> -f 1)</div><div class="line"><span class="keyword">if</span> ! /sbin/ip addr | grep -q <span class="variable">$vip</span>; <span class="keyword">then</span></div><div class="line">    <span class="built_in">exit</span></div><div class="line"><span class="keyword">fi</span></div><div class="line"></div><div class="line"><span class="comment"># check nfs service</span></div><div class="line">/sbin/service nfs-kernel-server status &gt;/dev/null</div><div class="line"><span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></div><div class="line">    <span class="comment"># abnormal, try to restart the nfs service</span></div><div class="line">    /sbin/service nfs-kernel-server restart</div><div class="line">    /sbin/service nfs-kernel-server status &gt;/dev/null</div><div class="line">    <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></div><div class="line">        <span class="comment"># still abnormal</span></div><div class="line">        <span class="keyword">for</span> folder <span class="keyword">in</span> $(ls /vol)</div><div class="line">        <span class="keyword">do</span></div><div class="line">            <span class="keyword">if</span> $(mount | grep <span class="variable">$folder</span> -q); <span class="keyword">then</span></div><div class="line">                umount -f /vol/<span class="variable">$folder</span></div><div class="line">            <span class="keyword">fi</span></div><div class="line">        <span class="keyword">done</span></div><div class="line">        <span class="comment"># stop keepalived service</span></div><div class="line">        /sbin/service keepalived stop</div><div class="line">    <span class="keyword">fi</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><p>配置完后，分别在三个节点上执行<code>service keepalived restart</code>重启服务，然后分别在三个节点上执行<code>ip addr</code>查看<code>IP</code>情况，可以发现<code>VIP</code>暴露在了<code>node2</code>上，说明我这里<code>node2</code>在<code>keepalived.conf</code>里面配置<code>priority</code>的值是最大的</p><p><code>node1</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">root@node1:/etc/keepalived# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure><p><code>node2</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">root@node2:/etc# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet 192.168.1.13/24 scope global secondary eth0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure><p><code>node3</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">root@node3:~# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure><h3 id="客户端通过-NFS-访问-RBD"><a href="#客户端通过-NFS-访问-RBD" class="headerlink" title="客户端通过 NFS 访问 RBD"></a>客户端通过 NFS 访问 RBD</h3><p>客户端检查<code>VIP</code>对外暴露接口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@tony /]# showmount -e 192.168.1.13</div><div class="line">Export list for 192.168.1.13:</div><div class="line">/vol/ec1 *</div></pre></td></tr></table></figure><p>将<code>server</code>端挂载块设备的目录<code>/vol/ec1</code>再次挂载到客户端上的<code>ec1</code>目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@tony /]# mkdir ec1</div><div class="line">[root@tony /]# mount -o rw,hard,intr -t nfs 192.168.1.13:/vol/ec1 /ec1</div><div class="line">[root@tony /]# cd ec1</div><div class="line">[root@tony ec1]# ls</div><div class="line">lost+found      # 此时是没有数据的</div></pre></td></tr></table></figure><p>我们可以测试一下读写，先看下读，比如我们在<code>node2</code>的<code>/vol/ec1</code>目录下写一个文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">root@node2:/vol/ec1# ls</div><div class="line">lost+found</div><div class="line">root@node2:/vol/ec1# echo &apos;hello&apos; &gt; hello.txt</div></pre></td></tr></table></figure><p>然后客户端查看<code>/ec1</code>目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@tony ec1]# ls</div><div class="line">hello.txt  lost+found</div><div class="line">[root@tony ec1]# cat hello.txt </div><div class="line">hello</div></pre></td></tr></table></figure><p>接下来测写，我们可以在客户端写一个文件，然后到服务端查看</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">[root@tony ec1]# echo &apos;i am client&apos; &gt; client.txt</div><div class="line">[root@tony ec1]# ls</div><div class="line">client.txt  hello.txt  lost+found</div></pre></td></tr></table></figure><p>服务端查看：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@node2:/vol/ec1# ls</div><div class="line">client.txt  hello.txt  lost+found</div><div class="line">root@node2:/vol/ec1# cat client.txt </div><div class="line">i am client</div></pre></td></tr></table></figure><p>ok，读写正常，目前为止客户端访问后端存储集群一切顺利！</p><h3 id="测试高可用"><a href="#测试高可用" class="headerlink" title="测试高可用"></a>测试高可用</h3><p>分三个测试：</p><ul><li>手动停止主机<code>NFS</code></li><li>手动停止主机<code>Keepalived</code></li><li>手动关机主机</li></ul><h4 id="手动停止主机-NFS"><a href="#手动停止主机-NFS" class="headerlink" title="手动停止主机 NFS"></a>手动停止主机 NFS</h4><p>这个主要是测试<code>check_nfs.sh</code>这个脚本是否在实时监控<code>NFS</code>状态，可以看到刚<code>stop</code>再次查看状态已经是<code>running</code>了，本测试通过~</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">root@node2:/vol/ec1# service nfs-kernel-server stop</div><div class="line"> * Stopping NFS kernel daemon                                                                                                                                                         [ OK ] </div><div class="line"> * Unexporting directories for NFS kernel daemon...                                                                                                                                   [ OK ] </div><div class="line">root@node2:/vol/ec1# service nfs-kernel-server status</div><div class="line">nfsd running</div></pre></td></tr></table></figure><h4 id="手动停止主机-Keepalived"><a href="#手动停止主机-Keepalived" class="headerlink" title="手动停止主机 Keepalived"></a>手动停止主机 Keepalived</h4><p>手动停止主机<code>node2</code>的<code>keepalived</code>服务，发现<code>VIP</code>已经在<code>node2</code>上面消失不见</p><p><code>node2</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">root@node2:/vol/ec1# service keepalived stop</div><div class="line"> * Stopping keepalived keepalived                                                                                                                                                     [ OK ] </div><div class="line">root@node2:/vol/ec1# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:61:26 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.112/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure><p>我们可以在<code>node1</code>发现了上面消失不见得<code>VIP</code>，可知如今角色发生了改变，<code>node1</code>已经成为了新的<code>master</code>节点</p><p><code>node1</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"># 出现了 VIP：192.168.1.13/24</div><div class="line">root@node1:/etc/keepalived# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:70:4e brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.111/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet 192.168.1.13/24 scope global secondary eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line"># 查看node1的/vol/ec1目录     </div><div class="line">root@node1:/etc/keepalived# ls /vol/ec1</div><div class="line">client.txt  hello.txt  lost+found</div><div class="line"># 查看mount信息</div><div class="line">root@node1:/etc/keepalived# mount</div><div class="line">/dev/sda3 on / type ext4 (rw,errors=remount-ro)</div><div class="line">/dev/sdb2 on /data/osd.0 type ext4 (rw,noatime,user_xattr)</div><div class="line">nfsd on /proc/fs/nfsd type nfsd (rw)</div><div class="line">...</div><div class="line">/dev/rbd0 on /vol/ec1 type ext4 (rw)</div></pre></td></tr></table></figure><p>此时我们可以再次测试一下读写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 新的主机读</div><div class="line">root@node1:/etc/keepalived# ls /vol/ec1</div><div class="line">client.txt  hello.txt  lost+found</div><div class="line"></div><div class="line"># 新的主机写</div><div class="line">root@node1:/etc/keepalived# echo &apos;new service 111&apos; &gt; /vol/ec1/new_server.txt</div><div class="line">root@node1:/etc/keepalived# ls /vol/ec1</div><div class="line">client.txt  hello.txt  lost+found  new_server.txt</div><div class="line"></div><div class="line"># 客户端读</div><div class="line">[root@tony ec1]# ls</div><div class="line">client.txt  hello.txt  lost+found  new_server.txt</div><div class="line">[root@tony ec1]# cat new_server.txt</div><div class="line">new service 111</div><div class="line"></div><div class="line"># 客户端写</div><div class="line">[root@tony ec1]# echo &apos;hello new server&apos; &gt; hello_new_server.txt</div><div class="line"># 可以看到刚刚客户端写的文件</div><div class="line">root@node1:/vol/ec1# ls</div><div class="line">client.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt</div></pre></td></tr></table></figure><p>ok，本测试通过~</p><h4 id="手动关机主机"><a href="#手动关机主机" class="headerlink" title="手动关机主机"></a>手动关机主机</h4><p>关闭主机<code>node1</code>，稍等片刻，确定完全关闭再测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">root@node1:/vol# reboot</div><div class="line"></div><div class="line">Broadcast message from root@node1</div><div class="line">(/dev/pts/3) at 9:16 ...</div><div class="line"></div><div class="line">The system is going down for reboot NOW!</div></pre></td></tr></table></figure><p>等待完全关闭，我们在<code>node3</code>上看到了<code>VIP</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">root@node3:~# ip addr</div><div class="line">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN </div><div class="line">    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00</div><div class="line">    inet 127.0.0.1/8 scope host lo</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP qlen 1000</div><div class="line">    link/ether 00:50:56:aa:a9:13 brd ff:ff:ff:ff:ff:ff</div><div class="line">    inet 192.168.1.113/24 brd 192.168.1.255 scope global eth0</div><div class="line">       valid_lft forever preferred_lft forever</div><div class="line">    inet 192.168.1.13/24 scope global secondary eth0</div><div class="line">       valid_lft forever preferred_lft forever</div></pre></td></tr></table></figure><p>客户端读写测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 客户端读</div><div class="line">[root@tony ec1]# ls</div><div class="line">client.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt</div><div class="line"># 客户端写</div><div class="line">[root@tony ec1]# echo &apos;reboot&apos; &gt;  reboot.txt</div><div class="line">[root@tony ec1]# ls</div><div class="line">client.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt</div></pre></td></tr></table></figure><p><code>node3</code>:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">root@node3:~# ls /vol/ec1</div><div class="line">client.txt  hello_new_server.txt  hello.txt  lost+found  new_server.txt  reboot.txt</div><div class="line">root@node3:~# cat /vol/ec1/reboot.txt </div><div class="line">reboot</div></pre></td></tr></table></figure><p>ok，本测试通过~</p><h3 id="测试小结"><a href="#测试小结" class="headerlink" title="测试小结"></a>测试小结</h3><p>通过上面三个测试，我们已经基本确保了<code>keepalived</code>会保证集群中主机发生异常时还是可以很好地对外提供服务，并且真正地做到了高可用，低延时，高可靠。</p><h2 id="QA-环节"><a href="#QA-环节" class="headerlink" title="QA 环节"></a>QA 环节</h2><p>因为我实现这个<code>RBD</code>高可用是在我们项目中做的，我们项目中<code>UI</code>上可以创建共享目录，但是之前都是用的<code>cephfs</code>实现的，而我做的就是将<code>cephfs</code>方式使用<code>RBD</code>替代，大家应该都清楚作为<code>POSIX</code>文件接口的<code>cephfs</code>内部已经做好了很多事情，它可以将所有节点挂载的目录做到真正的共享，也就是共享目录三个目录都有，改一个其他两个都会随之而改变，而不是像我们<code>RBD</code>同时只会针对某一个主机访问。</p><p>而<code>RBD</code>替换的话必然存在很多困难和问题，在这里我就以<code>QA</code>问答的方式分享一下我实现过程中遇到的种种问题和别人提出的需求。</p><h3 id="问题1"><a href="#问题1" class="headerlink" title="问题1"></a>问题1</h3><p>问：如何通过代码实现三个节点都做相关操作的，比如创建<code>image</code>和目录等等？</p><p>答：我们项目是这样实现的，将前端<code>UI</code>的增删改查（比如创建或删除目录等）和后端具体实现共享目录业务分离，也就是说前端只负责做这些信息的增删改查，对应的后端也只是负责这些信息的增删改查，所以用户会即时收到反馈。而真正做事的是在共享业务后端，这个和<code>UI</code>对应的后端做事有所区别，这边共享业务后端是做成了一个<code>Daemon</code>每隔几秒就会去扫描<code>UI</code>后端存储数据是否变化，发生了变化就会做相关操作，比如多了一个文件夹就要创建<code>image</code>等，删除一个文件夹也要做一系列的事情。</p><h3 id="问题2"><a href="#问题2" class="headerlink" title="问题2"></a>问题2</h3><p>问：如何判断哪个节点是<code>master</code>？</p><p>答：这个很简单，就通过<code>ip addr</code>命令查找<code>VIP</code>就好了，不能通过配置文件中的<code>priority</code>来判断，因为即使角色切换，那个值也不会变化的，也就是说即使<code>priority</code>是最大值也有可能当前节点不是主节点，这里要注意的是不能仅仅是包含<code>VIP</code>，而是要精确匹配才行，比如<code>VIP</code>是<code>192.168.1.12</code>，如果此时还有个<code>192.168.1.123</code>，如果只是字符串包含的话，那这个也会被匹配，所以要精确匹配。</p><h3 id="问题3"><a href="#问题3" class="headerlink" title="问题3"></a>问题3</h3><p>问：创建文件夹后端实现的逻辑是什么样的？</p><p>答：后端<code>Daemon</code>当扫描存储的目录信息相比于上一次扫描时新增的话，那么后端就会做事情了。首先我们要判断是否为主节点，如果是主节点，那么就创建<code>image</code>，然后做<code>Map</code>，接着就要<code>format</code>文件系统，创建目录，然后再做挂载。这时候要注意其他两个备节点也要做<code>rbd map</code>操作，这样做的原因，一是为了占位，比如当创建的项目多了之后，<code>backup</code>节点再<code>map</code>的时候顺序会乱掉，二是为了当主机<code>down</code>，备机转换为<code>master</code>后要找同样的块设备挂载，比如都是<code>/dev/rbd0</code></p><h3 id="问题4"><a href="#问题4" class="headerlink" title="问题4"></a>问题4</h3><p>问：删除文件夹后端实现的逻辑是什么样的？</p><p>答：扫描当前目录少于上一次扫描的目录，那么就针对这些目录，主节点要先<code>umount</code>，再<code>umap</code>，然后<code>rm image</code>，最后删除目录，对于备节点的话就<code>umap</code>，然后再删除目录就好</p><h3 id="问题5"><a href="#问题5" class="headerlink" title="问题5"></a>问题5</h3><p>问：三个节点的<code>Daemon</code>可能执行的顺序不一样，不一定是主节点先执行，那么这个时候备节点将无法<code>map</code>，同理很有可能在删除<code>image</code>的时候，别的节点的<code>image</code>都还没<code>unmap</code>，这样的话<code>image</code>是会删除失败的，这里怎么处理节点间的冲突呢？</p><p>答：</p><p>首先是创建目录，这时候主节点我们已经做得比较好了，主要担心备节点<code>map</code>的时候<code>image</code>还没有创建，那么我们这边就要判断一下，如果<code>image</code>还不在指定<code>pool</code>中，那么就要设置当前目录情况还为上一次的目录信息，这样下一次扫描代码就会又以为有新目录了，那么该段代码就会又执行一次，此时应该成功了，反正只要成功的时候才会把当前目录信息更改为最新的目录信息。</p><p>再来说删除目录，这个和创建<code>map</code>不同的地方在于，我要删除<code>image</code>的时候，我无法知道这个<code>image</code>还有没有和其他节点有<code>map</code>关系，所以我们只有尝试去删除，这边加一个异常捕获，因为<code>rm image</code>报错我们不处理的话会造成代码出错，所以外面包一层异常，这样就可以和上面类似的操作了。这边要注意的是一旦发生异常，我们还必须要在<code>map</code>回去，否则我们无法获取<code>pool</code>等信息了，因为我们是通过<code>rbd showmapped</code>来获取相关信息的。</p><h3 id="问题6"><a href="#问题6" class="headerlink" title="问题6"></a>问题6</h3><p>问：请问<code>keepalived</code>如何做自动化的？</p><p>答：由于<code>keepalived</code>也比较简单，三个配置文件相关配置信息都配一样的，我们要做的就是网卡和<code>VIP</code>，网卡的话我们就从项目中获取<code>public ip</code>，而<code>VIP</code>就是<code>UI</code>上面配置的，然后读写文件就好了。</p><h3 id="问题7"><a href="#问题7" class="headerlink" title="问题7"></a>问题7</h3><p>问：一旦节点关机的话，下次开机后块设备就会没了，我们该如何做呢？</p><p>答：这个问题的确存在，所以我们要提前将对应关系存在文件中，下次开机的时候根据文件然后对应做<code>map</code>工作</p><h3 id="问题8"><a href="#问题8" class="headerlink" title="问题8"></a>问题8</h3><p>问：多个块设备的时候，<code>keepalived</code>触发的脚本如何做？</p><p>答：其实这个的做法我已经暴露在上面我分享的三个脚本里面了，要做的就是遍历<code>/vol</code>目录下的所有目录或者所有<code>rbd*</code>，这边要注意的就是<code>/vol/</code>或者指定目录下存在的必须只有创建的共享目录。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文通过<code>keepalived</code>初步实现了<code>RBD</code>的高可用，简单地替代了<code>cephfs</code>导出<code>NFS</code>，针对<code>cephfs</code>性能不行的问题，应该有很多小伙伴有这种需求，希望这篇文章能给大家带来一些思路和帮助</p><p>之后，我会尝试研究<code>CTDB</code>做高可用，因为<code>keepalived</code>由于比较简单，所以功能也就比较局限了。然后本文只有<code>NFS</code>，后续应该还会有<code>smb</code>，<code>iscsi</code>等等</p><p>通过本文，我对<code>RBD</code>和高可用的认识又深入了一些，其实本文涉及到的技术还是比较简单的，还有很多更复杂、更牛逼的高可用方案，这里不禁要说一句，后端还是有意思呀！（相比于前端而言），起码对我来说是这样的，以后会不断精进这些技术，加油！</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/HA.jpg&quot; alt=&quot;HA&quot;&gt;&lt;/center&gt;

&lt;p&gt;由于&lt;code&gt;Cephfs&lt;/code&gt;很不稳定，并且性能差，很难达到用户在性能上的需求，所以&lt;code&gt;Cephfs&lt;/code&gt;也难以应用于生产环境之上。而&lt;code&gt;RBD&lt;/code&gt;可以说是一直非常稳定的存储接口方案，用户可以将&lt;code&gt;image&lt;/code&gt;挂载到客户端进行访问读写，然而很多用户不愿意在本地安装&lt;code&gt;Ceph&lt;/code&gt;客户端，所以我们常常需要自己封装一层，给客户端暴露出一个通用的接口进行访问，现在一般都是默认用&lt;code&gt;NFS&lt;/code&gt;，所以本文就&lt;code&gt;Ceph RBD&lt;/code&gt;如何实现高可用暴露&lt;code&gt;NFS&lt;/code&gt;给客户端访问进行分享。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="HA" scheme="https://tony-yin.github.io/tags/HA/"/>
    
      <category term="RBD" scheme="https://tony-yin.github.io/tags/RBD/"/>
    
      <category term="Keepalived" scheme="https://tony-yin.github.io/tags/Keepalived/"/>
    
  </entry>
  
  <entry>
    <title>Cephx 实战演练</title>
    <link href="https://tony-yin.github.io/2017/11/30/Cephx-practice/"/>
    <id>https://tony-yin.github.io/2017/11/30/Cephx-practice/</id>
    <published>2017-11-30T03:04:04.000Z</published>
    <updated>2017-11-30T03:10:00.441Z</updated>
    
    <content type="html"><![CDATA[<center><img src="http://ow0mgad6r.bkt.clouddn.com/cephx-600x450.jpg" alt="cephx"></center><p>本文就阅读完<a href="http://www.xuxiaopang.com/2017/08/23/easy-ceph-CephX/" target="_blank" rel="external">徐小胖的大话Cephx</a>后，针对一些猜测和疑惑进行了实战演练，对原文的一些说法和结论进行了验证，并进行了一系列的扩展的思考猜想和总结。最后收获满满，不仅对原文的一些结论进行了验证，也发现了其中的一些问题，更多的是自己动手后一些奇妙的场景和发现。</p><a id="more"></a><p>本文实战任务和完成情况如下：</p><ul><li style="list-style: none"><input type="checkbox" checked> 删除<code>client.admin.keyring</code></li><li style="list-style: none"><input type="checkbox" checked> 修改<code>cephx</code>配置</li><li style="list-style: none"><input type="checkbox" checked> 修改<code>Monitor keyring</code></li><li style="list-style: none"><input type="checkbox" checked> 修改<code>OSD keyring</code></li><li style="list-style: none"><input type="checkbox" checked> 修改<code>client.admin.keyring</code>，通过<code>Mon</code>找回正确的<code>keyring</code></li><li style="list-style: none"><input type="checkbox" checked> <code>Mon Cap</code></li><li style="list-style: none"><input type="checkbox" checked> <code>OSD Cap</code></li><li style="list-style: none"><input type="checkbox" checked> 删除所有<code>keyring</code>文件再恢复</li><li style="list-style: none"><input type="checkbox" checked> 删除<code>ceph.conf</code>再恢复</li><li style="list-style: none"><input type="checkbox"> 关闭<code>CephX</code>后不重启<code>OSD</code></li><li style="list-style: none"><input type="checkbox" checked> 通过<code>osd.keyring</code>访问集群</li><li style="list-style: none"><input type="checkbox"> 配置只能访问一个<code>RBD</code>的用户权限</li></ul><h2 id="删除-client-admin-keyring"><a href="#删除-client-admin-keyring" class="headerlink" title="删除 client.admin.keyring"></a>删除 client.admin.keyring</h2><p>主节点开始存在<code>keyring</code>，可以正常访问集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph]# ls</div><div class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap</div><div class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring</div><div class="line">[root@node1 ceph]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_WARN</div><div class="line">            no active mgr</div><div class="line">            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete</div><div class="line">            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized</div><div class="line">            application not enabled on 3 pool(s)</div><div class="line">            clock skew detected on mon.node2, mon.node3</div><div class="line"> </div><div class="line">  services:</div><div class="line">    mon:     3 daemons, quorum node1,node2,node3</div><div class="line">    mgr:     no daemons active</div><div class="line">    osd:     6 osds: 5 up, 5 in</div><div class="line">    rgw:     1 daemon active</div><div class="line">    rgw-nfs: 1 daemon active</div><div class="line"> </div><div class="line">  data:</div><div class="line">    pools:   10 pools, 444 pgs</div><div class="line">    objects: 257 objects, 36140 kB</div><div class="line">    usage:   6256 MB used, 40645 MB / 46901 MB avail</div><div class="line">    pgs:     63.288% pgs not active</div><div class="line">             311/771 objects degraded (40.337%)</div><div class="line">             158 undersized+degraded+peered</div><div class="line">             158 active+undersized+degraded</div><div class="line">             65  down</div><div class="line">             58  incomplete</div><div class="line">             5   active+clean+remapped</div></pre></td></tr></table></figure><p>将<code>keyring</code>文件移动到其他地方，相当于删除了<code>keyring</code>，这时访问集群报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph]# mv ceph.client.admin.keyring /tmp/</div><div class="line">[root@node1 ceph]# ls</div><div class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-mgr.keyring  ceph.bootstrap-osd.keyring  ceph.bootstrap-rgw.keyring  ceph.conf  ceph-deploy-ceph.log  ceph.mon.keyring  rbdmap</div><div class="line">[root@node1 ceph]# ceph -s</div><div class="line">2017-11-23 18:07:48.685028 7f63f6935700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory</div><div class="line">2017-11-23 18:07:48.685094 7f63f6935700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</div><div class="line">2017-11-23 18:07:48.685098 7f63f6935700  0 librados: client.admin initialization error (2) No such file or directory</div><div class="line">[errno 2] error connecting to the cluster</div></pre></td></tr></table></figure><p>再拷贝回来又可以访问集群了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph]# mv /tmp/ceph.client.admin.keyring ./</div><div class="line">[root@node1 ceph]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_WARN</div><div class="line">            no active mgr</div><div class="line">            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete</div><div class="line">            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized</div><div class="line">            application not enabled on 3 pool(s)</div><div class="line">            clock skew detected on mon.node2, mon.node3</div></pre></td></tr></table></figure><p><code>node3</code>由于<code>/etc/ceph/</code>目录下没有<code>keyring</code>文件，所以也无法连接集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@node3 ceph]# ls</div><div class="line">ceph.conf  ceph-deploy-ceph.log  rbdmap</div><div class="line">[root@node3 ceph]# ceph -s</div><div class="line">2017-11-23 17:59:16.659034 7fbe34678700 -1 auth: unable to find a keyring on /etc/ceph/ceph.client.admin.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,: (2) No such file or directory</div><div class="line">2017-11-23 17:59:16.659085 7fbe34678700 -1 monclient: ERROR: missing keyring, cannot use cephx for authentication</div><div class="line">2017-11-23 17:59:16.659089 7fbe34678700  0 librados: client.admin initialization error (2) No such file or directory</div><div class="line">[errno 2] error connecting to the cluster</div></pre></td></tr></table></figure><p><strong>结论：</strong></p><blockquote><p>当<code>ceph.conf</code>中的<code>auth</code>配置为<code>cephx</code>的时候，访问集群是需要秘钥文件的</p></blockquote><h2 id="修改-cephx-配置"><a href="#修改-cephx-配置" class="headerlink" title="修改 cephx 配置"></a>修改 cephx 配置</h2><p>在<code>node3</code>节点上的<code>/etc/ceph/</code>目录下操作，首先将<code>ceph.client.admin.keyring</code>文件删除，然后将<code>auth</code>配置从<code>cephx</code>改为<code>none</code>，然后先重启<code>monitor</code>，再重启<code>osd</code>，这时候依然不可以访问集群，因为<code>cephx</code>是面向整个集群的，而不是某个节点，接下来需要在其他节点做一样的操作，更改<code>cephx</code>为<code>none</code>，然后重启<code>monitor</code>和<code>osd</code>，这时候便可以在没有<code>keyring</code>文件的情况下访问集群了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"># 删除keyring文件</div><div class="line">[root@node3 ~]# cd /etc/ceph/</div><div class="line">[root@node3 ceph]# ls</div><div class="line">ceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap</div><div class="line">[root@node3 ceph]# mv ceph.client.admin.keyring /tmp/</div><div class="line"># 更改cephx配置</div><div class="line">[root@node3 ceph]# cat ceph.conf </div><div class="line">[global]</div><div class="line">fsid = 99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.1.58,192.168.1.61,192.168.1.62</div><div class="line">auth_cluster_required = cephx</div><div class="line">auth_service_required = cephx</div><div class="line">auth_client_required = cephx</div><div class="line"></div><div class="line">public network = 192.168.1.0/24</div><div class="line">mon clock drift allowed = 2</div><div class="line">mon clock drift warn backoff = 30</div><div class="line">[root@node3 ceph]# vim ceph.conf </div><div class="line">[root@node3 ceph]# cat ceph.conf </div><div class="line">[global]</div><div class="line">fsid = 99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.1.58,192.168.1.61,192.168.1.62</div><div class="line">auth_cluster_required = none</div><div class="line">auth_service_required = none</div><div class="line">auth_client_required = none</div><div class="line"></div><div class="line">public network = 192.168.1.0/24</div><div class="line">mon clock drift allowed = 2</div><div class="line">mon clock drift warn backoff = 30</div><div class="line">[root@node3 ceph]# systemctl restart ceph-mon</div><div class="line">ceph-mon@               ceph-mon@node3.service  ceph-mon.target         </div><div class="line">[root@node3 ceph]# systemctl restart ceph-mon</div><div class="line">ceph-mon@               ceph-mon@node3.service  ceph-mon.target         </div><div class="line">[root@node3 ceph]# systemctl restart ceph-mon.target</div><div class="line">[root@node3 ceph]# systemctl restart ceph-osd.target</div><div class="line"># 更改单个节点配置后依然不可以访问集群</div><div class="line">[root@node3 ceph]# ceph -s</div><div class="line">2017-11-27 23:05:23.022571 7f5200c2f700  0 librados: client.admin authentication error (95) Operation not supported</div><div class="line">[errno 95] error connecting to the cluster</div><div class="line"># 相应的更改其他几个节点并重启，便又可以正常访问集群了</div><div class="line">[root@node3 ceph]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_WARN</div><div class="line">    ...</div></pre></td></tr></table></figure><p><strong>结论：</strong></p><blockquote><p>当<code>auth</code>配置为<code>cephx</code>的时候访问集群必须要借助秘钥文件，而当<code>auth</code>配置为<code>none</code>的时候，不再需要秘钥文件就可以访问集群了。（<strong>更改配置需要集群所有节点都做才可以生效，而不是单一节点</strong>）</p></blockquote><h2 id="删除monitor秘钥"><a href="#删除monitor秘钥" class="headerlink" title="删除monitor秘钥"></a>删除monitor秘钥</h2><p><code>/etc/ceph</code>和<code>/var/lib//ceph/mon/ceph-node1</code>各有一个<code>mon keyring</code><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# cd /etc/ceph/</div><div class="line">[root@node1 ceph]# ls</div><div class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap</div><div class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring</div><div class="line">[root@node1 ceph]# cd /var/lib/ceph/mon/ceph-node1/</div><div class="line">[root@node1 ceph-node1]# ls</div><div class="line">done  keyring  kv_backend  store.db  systemd</div></pre></td></tr></table></figure></p><p>先删除<code>/etc/ceph/ceph-mon.keyring</code>，还是可以访问集群</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph]# rm ceph.mon.keyring </div><div class="line">rm: remove regular file ‘ceph.mon.keyring’? y</div><div class="line">[root@node1 ceph]# systemctl restart ceph-mon@node1.service </div><div class="line">[root@node1 ceph]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_WARN</div><div class="line">            no active mgr</div><div class="line">            Reduced data availability: 281 pgs inactive, 65 pgs down, 58 pgs incomplete</div><div class="line">            Degraded data redundancy: 311/771 objects degraded (40.337%), 439 pgs unclean, 316 pgs degraded, 316 pgs undersized</div><div class="line">            application not enabled on 3 pool(s)</div><div class="line">            clock skew detected on mon.node2</div><div class="line">...</div><div class="line">...</div></pre></td></tr></table></figure><p>再删除<code>/var/lib/ceph/mon/ceph-node1/keyring</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# rm keyring </div><div class="line">rm: remove regular file ‘keyring’? y</div><div class="line">[root@node1 ceph-node1]# systemctl restart ceph-mon@node1.service </div><div class="line">[root@node1 ceph-node1]# ceph -s</div></pre></td></tr></table></figure><p>访问集群一直<code>timeount</code>，查看<code>log</code>文件发现<code>Mon</code>初始化失败</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2017-11-24 00:33:55.812955 7fa16f995e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can&apos;t open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory</div><div class="line">2017-11-24 00:33:55.812991 7fa16f995e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,</div><div class="line">2017-11-24 00:33:55.812999 7fa16f995e40 -1 failed to initialize</div></pre></td></tr></table></figure><p>ok，那我们再试试将<code>/var/lib/ceph/mon/ceph-node1/keyring</code>删除，将<code>etc/ceph/ceph.mon.keyring</code>拷贝回来，这时候意外发生了，居然<code>mon</code>初始化失败</p><p><strong>结论：</strong></p><blockquote><p><code>Monitor</code>启动是需要<code>keyring</code>文件进行秘钥认证的，并且这个文件必须是<code>/var/lib/ceph/mon/ceph-node1/</code>目录下的，<code>/etc/ceph/</code>目录下的<code>ceph.mon.keyring</code>并不起作用</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# rm keyring </div><div class="line">rm: remove regular file ‘keyring’? y</div><div class="line">[root@node1 ceph]# ls</div><div class="line">ceph.bootstrap-mds.keyring  ceph.bootstrap-osd.keyring  ceph.client.admin.keyring  ceph-deploy-ceph.log  rbdmap</div><div class="line">ceph.bootstrap-mgr.keyring  ceph.bootstrap-rgw.keyring  ceph.conf                  ceph.mon.keyring  </div><div class="line">[root@node1 ceph]# ceph -s</div><div class="line">// timeout</div><div class="line">...</div></pre></td></tr></table></figure><p><code>mon.log</code>中的现象：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2017-11-24 00:44:26.534865 7ffaf5117e40 -1 auth: error reading file: /var/lib/ceph/mon/ceph-node1/keyring: can&apos;t open /var/lib/ceph/mon/ceph-node1/keyring: (2) No such file or directory</div><div class="line">2017-11-24 00:44:26.534901 7ffaf5117e40 -1 mon.node1@-1(probing) e1 unable to load initial keyring /etc/ceph/ceph.mon.node1.keyring,/etc/ceph/ceph.keyring,/etc/ceph/keyring,/etc/ceph/keyring.bin,</div><div class="line">2017-11-24 00:44:26.534916 7ffaf5117e40 -1 failed to initialize</div></pre></td></tr></table></figure><p>至此，我们可以得出结论<code>monitor</code>初始化的时候依赖的文件是<code>/var/lib/ceph/mon/ceph-node1/keyring</code>而不是<code>/etc/ceph/ceph.mon.keyring</code></p><h2 id="修改-Mon-keyring"><a href="#修改-Mon-keyring" class="headerlink" title="修改 Mon keyring"></a>修改 Mon keyring</h2><h3 id="原始的-keyring"><a href="#原始的-keyring" class="headerlink" title="原始的 keyring"></a>原始的 keyring</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# cat keyring </div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph-node1]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div></pre></td></tr></table></figure><h3 id="将中间的五个A替换成了五个C"><a href="#将中间的五个A替换成了五个C" class="headerlink" title="将中间的五个A替换成了五个C"></a>将中间的五个A替换成了五个C</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# vim keyring </div><div class="line">[root@node1 ceph-node1]# cat keyring </div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div></pre></td></tr></table></figure><h3 id="重启查看-Mon-keyring"><a href="#重启查看-Mon-keyring" class="headerlink" title="重启查看 Mon keyring"></a>重启查看 Mon keyring</h3><p>理想结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# systemctl restart ceph-mon.target</div><div class="line">[root@node1 ceph-node1]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div></pre></td></tr></table></figure><p>令人疑惑的现实：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZCCCCCBAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">[root@node1 ceph]# ceph auth get mon.</div><div class="line">exported keyring for mon.</div><div class="line">[mon.]</div><div class="line">key = AQCo7fdZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div></pre></td></tr></table></figure><p>可以看到一会是修改之前的<code>keyring</code>，一会是修改之后的<code>keyring</code>，那遇到这种问题，我们就通过<code>log</code>观察如何获取<code>keyring</code>的</p><p><code>node1</code>的<code>mon.log</code>中日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">2017-11-24 09:30:08.697047 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:08.697106 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/1169357136&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:10.020571 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:10.020641 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/2455152702&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:11.393391 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:11.393452 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/1704778092&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:12.669987 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:12.670049 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/275069695&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:14.113077 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:14.113147 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/3800873459&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:15.742038 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:15.742106 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/1908944728&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:30:17.629681 7f9b73e09700  0 mon.node1@0(leader) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:30:17.629729 7f9b73e09700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/2193002591&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div></pre></td></tr></table></figure><p><code>node2</code>的<code>mon.log</code>中日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">2017-11-24 09:29:23.799402 7fdb3c0ae700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/4284881078&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:29:26.030516 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:29:26.030588 7fdb3c0ae700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/4157525590&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div><div class="line">2017-11-24 09:29:38.637677 7fdb3c0ae700  0 mon.node2@1(peon) e1 handle_command mon_command(&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125; v 0) v1</div><div class="line">2017-11-24 09:29:38.637748 7fdb3c0ae700  0 log_channel(audit) log [INF] : from=&apos;client.? 192.168.1.58:0/4028820259&apos; entity=&apos;client.admin&apos; cmd=[&#123;&quot;prefix&quot;: &quot;auth get&quot;, &quot;entity&quot;: &quot;mon.&quot;&#125;]: dispatch</div></pre></td></tr></table></figure><p><strong>结论：</strong></p><ul><li><code>Monitor</code>的秘钥哪怕被修改过了，也不会影响<code>Monitor</code>的启动，也就是说<code>Monitor</code>启动时只要存在秘钥文件就好，内容忽略并不重要</li><li><code>Monitor</code>启动的时候读取秘钥文件是随机的，并不一定是当前节点的，具体选择机制需要后期去看源代码了</li></ul><h2 id="修改OSD-keyring和修复"><a href="#修改OSD-keyring和修复" class="headerlink" title="修改OSD keyring和修复"></a>修改OSD keyring和修复</h2><p><code>OSD</code>启动的时候需要秘钥才可以登录集群，这个秘钥会存在<code>Monitor</code>的数据库中，所以登录的时候就会拿本地的<code>keyring</code>和存在<code>Monitor</code>中的<code>keyring</code>相匹配，正确的话才可以启动成功。</p><p>下面我们将本地的<code>OSD keyring</code>故意改错，然后重启<code>OSD</code>查看效果</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"># 更改秘钥文件</div><div class="line">[root@node3 ceph]# cd /var/lib/ceph/osd/ceph-2</div><div class="line">[root@node3 ceph-2]# ls</div><div class="line">activate.monmap  active  block  bluefs  ceph_fsid  fsid  keyring  kv_backend  magic  mkfs_done  ready  systemd  type  whoami</div><div class="line">[root@node3 ceph-2]# cat keyring </div><div class="line">[osd.2]</div><div class="line">key = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==</div><div class="line">[root@node3 ceph-2]# vim keyring </div><div class="line">[root@node3 ceph-2]# cat keyring </div><div class="line">[osd.2]</div><div class="line">key = BBBp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==</div><div class="line">[root@node3 ceph-2]# systemctl restart ceph-osd</div><div class="line">ceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     </div><div class="line">[root@node3 ceph-2]# systemctl restart ceph-osd</div><div class="line">ceph-osd@           ceph-osd@2.service  ceph-osd@5.service  ceph-osd.target     </div><div class="line">[root@node3 ceph-2]# systemctl restart ceph-osd@2.service</div><div class="line"># 重启后发现OSD的状态时down</div><div class="line">[root@node3 ceph-2]# ceph osd tree | grep osd.2</div><div class="line"> 2   hdd 0.00980         osd.2    down  1.00000 1.00000</div></pre></td></tr></table></figure><p>查看日志，发现<code>init</code>失败，原因是<code>auth</code>认证出错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">2017-11-27 23:52:18.069207 7fae1e8d2d00 -1 auth: error parsing file /var/lib/ceph/osd/ceph-2/keyring</div><div class="line">2017-11-27 23:52:18.069285 7fae1e8d2d00 -1 auth: failed to load /var/lib/ceph/osd/ceph-2/keyring: (5) Input/output error</div><div class="line">...</div><div class="line">2017-11-27 23:52:41.232803 7f58d15ded00 -1  ** ERROR: osd init failed: (5) Input/output error</div></pre></td></tr></table></figure><p>我们可以通过查询<code>Monitor</code>数据库获取正确的<code>keyring</code>，将错误的<code>keyring</code>修正过来再重启<code>OSD</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 查询Monitor数据库中的osd keyring</div><div class="line">[root@node3 ceph-2]# ceph auth get osd.2</div><div class="line">exported keyring for osd.2</div><div class="line">[osd.2]</div><div class="line">key = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==</div><div class="line">caps mgr = &quot;allow profile osd&quot;</div><div class="line">caps mon = &quot;allow profile osd&quot;</div><div class="line">caps osd = &quot;allow *&quot;</div><div class="line"># 修正keyring</div><div class="line">[root@node3 ceph-2]# vim keyring </div><div class="line">[root@node3 ceph-2]# cat keyring </div><div class="line">[osd.2]</div><div class="line">key = AQCp8/dZ4BHbHxAA/GXihrjCOB+7kZJfgnSy+Q==</div><div class="line">[root@node3 ceph-2]# systemctl restart ceph-osd@2.service </div><div class="line"># 重启OSD后可以发现osd.2状态已经变为up</div><div class="line">[root@node3 ceph-2]# ceph osd tree | grep osd.2</div><div class="line"> 2   hdd 0.00980         osd.2      up  1.00000 1.00000</div></pre></td></tr></table></figure><p><strong>结论：</strong></p><blockquote><p><code>OSD</code>启动需要正确的<code>keyring</code>，错误的话则无法启动成功，正确的<code>keyring</code>会被存在<code>Monitor</code>的数据库中</p></blockquote><h2 id="修改Client-keyring和修复"><a href="#修改Client-keyring和修复" class="headerlink" title="修改Client keyring和修复"></a>修改Client keyring和修复</h2><p>之前我们通过删除<code>client keyring</code>验证了当<code>auth=cephx</code>的时候，客户端需要<code>keyring</code>才可以访问集群，那么它是像<code>Monitor</code>一样内容不被<code>care</code>还是和<code>OSD</code>一样需要精确匹配<code>keyring</code>呢？</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 修改ceph.client.admin.keyring</div><div class="line">[root@node3 ceph-2]# cd /etc/ceph/</div><div class="line">[root@node3 ceph]# ls</div><div class="line">ceph.client.admin.keyring  ceph.conf  ceph-deploy-ceph.log  rbdmap</div><div class="line">[root@node3 ceph]# cat ceph.client.admin.keyring </div><div class="line">[client.admin]</div><div class="line">key = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line">[root@node3 ceph]# vim ceph.client.admin.keyring </div><div class="line">[root@node3 ceph]# cat ceph.client.admin.keyring </div><div class="line">[client.admin]</div><div class="line">key = BBBB7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line"># 访问集群出错</div><div class="line">[root@node3 ceph]# ceph -s</div><div class="line">2017-11-28 00:06:05.771604 7f3a69ccf700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring</div><div class="line">2017-11-28 00:06:05.771622 7f3a69ccf700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error</div><div class="line">2017-11-28 00:06:05.771634 7f3a69ccf700  0 librados: client.admin initialization error (5) Input/output error</div><div class="line">[errno 5] error connecting to the cluster</div></pre></td></tr></table></figure><p>可以看出访问集群需要正确的<code>keyring</code>，这时候如何修复呢？大家应该能够猜到，它和<code>OSD</code>的原理是一样的，正确的<code>keyring</code>也存在与<code>Monitor</code>的数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># 直接获取client.admin出错</div><div class="line">[root@node3 ceph]# ceph auth get client.admin</div><div class="line">2017-11-28 00:08:19.159073 7fcabb297700 -1 auth: error parsing file /etc/ceph/ceph.client.admin.keyring</div><div class="line">2017-11-28 00:08:19.159079 7fcabb297700 -1 auth: failed to load /etc/ceph/ceph.client.admin.keyring: (5) Input/output error</div><div class="line">2017-11-28 00:08:19.159090 7fcabb297700  0 librados: client.admin initialization error (5) Input/output error</div><div class="line">[errno 5] error connecting to the cluster</div><div class="line"># 需要加上monitor的keyring文件才可以获取client.admin.keyring</div><div class="line">[root@node3 ceph]# ceph auth get client.admin --name mon. --keyring /var/lib/ceph/mon/ceph-node3/keyring</div><div class="line">exported keyring for client.admin</div><div class="line">[client.admin]</div><div class="line">key = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line">caps mds = &quot;allow *&quot;</div><div class="line">caps mgr = &quot;allow *&quot;</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">caps osd = &quot;allow *&quot;</div><div class="line"># 修正keyring</div><div class="line">[root@node3 ceph]# vim ceph</div><div class="line">ceph.client.admin.keyring  ceph.conf                  ceph-deploy-ceph.log       </div><div class="line">[root@node3 ceph]# vim ceph.client.admin.keyring </div><div class="line">[root@node3 ceph]# cat ceph.client.admin.keyring </div><div class="line">[client.admin]</div><div class="line">key = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line"># 访问集群成功</div><div class="line">[root@node3 ceph]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_WARN</div><div class="line">    ...</div></pre></td></tr></table></figure><p>出现了令人惊奇的一幕，就是上面通过<code>ceph auth</code>获取<code>OSD</code>的<code>keyring</code>可以正常获取，而获取<code>client.admin.keyring</code>却要加上<code>monitor.keyring</code>，原因可以从报错信息看出，<code>ceph auth</code>需要以客户端连接集群为前提。</p><p>结论：</p><blockquote><p><code>Client</code>访问集群和<code>OSD</code>一样，需要正确的<code>keyring</code>与存在<code>Monitor</code>数据库中对应的<code>keyring</code>相匹配，并且当<code>client.admin.keyring</code><br>不正确时，通过<code>ceph auth</code>读取<code>keyring</code>的时候需要加上<code>monitor keyring</code>的选项</p></blockquote><h2 id="Mon-Caps"><a href="#Mon-Caps" class="headerlink" title="Mon Caps"></a>Mon Caps</h2><h3 id="r-权限"><a href="#r-权限" class="headerlink" title="r 权限"></a>r 权限</h3><p><code>Monior</code>的<code>r</code>权限就是拥有读权限，对应的读权限都有哪些操作？在这里的读权限其实就是拥有读取<code>Monitor</code>数据库中信息的权限，<code>MON</code>作为集群的状态维护者，其数据库(<code>/var/lib/ceph/mon/ceph-$hostname/store.db</code>)内保存着集群这一系列状态图(<code>Cluster Map</code>)，这些<code>Map</code>包含但不限于：</p><ul><li><code>CRUSH Map</code></li><li><code>OSD Map</code></li><li><code>MON Map</code></li><li><code>MDS Map</code></li><li><code>PG Map</code></li></ul><p>所以接下来我们可以创建一个新的只拥有读权限的用户，进行相关操作验证读权限具体拥有哪些权限</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">ceph auth get-or-create client.mon_r mon &apos;allow r&apos; &gt;&gt; /root/key</div><div class="line">[root@node3 ceph]# ceph auth get client.mon_r</div><div class="line">exported keyring for client.mon_r</div><div class="line">[client.mon_r]</div><div class="line">key = AQABvRxaBS6BBhAAz9uwjYCT4xKavJhobIK3ig==</div><div class="line">caps mon = &quot;allow r&quot;</div><div class="line"></div><div class="line">ceph --name client.mon_r --keyring /root/key -s      // ok</div><div class="line"></div><div class="line">ceph --name client.mon_r --keyring /root/key osd crush dump     // ok</div><div class="line">ceph --name client.mon_r --keyring /root/key osd getcrushmap -o crushmap.map        // ok</div><div class="line"></div><div class="line">ceph --name client.mon_r --keyring /root/key osd dump       // ok</div><div class="line">ceph --name client.mon_r --keyring /root/key osd tree       // ok</div><div class="line">ceph --name client.mon_r --keyring /root/key osd stat       // ok</div><div class="line"></div><div class="line">ceph --name client.mon_r --keyring /root/key pg dump        // ok</div><div class="line">ceph --name client.mon_r --keyring /root/key pg stat        // ok</div></pre></td></tr></table></figure><p>尝试了下两个写操作，都显示报错权限拒绝</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">[root@node3 ceph]# rados --name client.mon_r --keyring /root/key -p testpool put crush crushmap.map</div><div class="line">error putting testpool/crush: (1) Operation not permitted</div><div class="line"></div><div class="line">[root@node3 ceph]# ceph --name client.mon_r --keyring /root/key osd out osd.0</div><div class="line">Error EACCES: access denied</div></pre></td></tr></table></figure><p><strong>注意：</strong></p><p>虽然上面有<code>osd</code>和<code>pg</code>等信息，但是这些都隶属于<code>crush map</code>的范畴中，所以这些状态数据都是从<code>Monitor</code>获取的</p><p><strong>结论：</strong></p><blockquote><p><code>Monitor</code>的读权限对应的是从<code>Monitor</code>数据库获取一系列的<code>Map</code>信息，具体的上面也都讲的很详细了，并且该权限只能读取状态信息，不能获取具体数据信息，且不能进行<code>OSD</code>等守护进程写操作</p></blockquote><h3 id="w-权限"><a href="#w-权限" class="headerlink" title="w 权限"></a>w 权限</h3><p><code>w</code>权限必须配合<code>r</code>权限才会有效果，否则，单独<code>w</code>权限执行指令时，是会一直<code>access denied</code>的。所以我们在测试<code>w</code>权限时，需要附加上<code>r</code>权限才行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ceph auth get-or-create client.mon_rw mon &apos;allow rw&apos; &gt;&gt; /root/key</div></pre></td></tr></table></figure><p>而<code>w</code>权限就可以做一些对组件的非读操作了，比如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># 踢出OSD</div><div class="line">ceph osd out</div><div class="line"># 删除OSD</div><div class="line">ceph osd rm </div><div class="line"># 修复PG</div><div class="line">ceph pg repair</div><div class="line"># 替换CRUSH</div><div class="line">ceph osd setcrushmap</div><div class="line"># 删除MON</div><div class="line">ceph mon rm</div><div class="line">...</div><div class="line"># 还有很多操作，就不一一赘述</div></pre></td></tr></table></figure><p><strong>结论：</strong></p><blockquote><p><code>Mon</code>的<code>r</code>权限可以读取集群各个组件的状态，但是不能修改状态，而<code>w</code>权限是可以做到的</p></blockquote><p><strong>注意：</strong></p><blockquote><p>这里的<code>w</code>权限能做到的写权限也只是修改组件的状态，但是并不包括对集群对象的读写权限，因为这些组件状态信息是存在<code>Mon</code>，而对象信息是存在<code>OSD</code>里面的，而这里的<code>w</code>权限也只是<code>Mon</code>的写权限，所以也很好理解了。</p></blockquote><h3 id="x-权限"><a href="#x-权限" class="headerlink" title="x 权限"></a>x 权限</h3><p><code>MON</code>的<code>x</code>权限很局限，因为这个权限仅仅和<code>auth</code>相关，比如<code>ceph auth list</code>，<code>ceph auth get</code> 之类的指令，和<code>w</code>权限类似，<code>x</code>权限也需要<code>r</code>权限组合在一起才能有效力：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"># 用上面创建拥有rw权限的用户访问auth list后auth报错</div><div class="line">[root@node3 ~]# ceph --name client.mon_rw --keyring /root/key auth list</div><div class="line">2017-11-28 21:28:10.620537 7f0d15967700  0 librados: client.mon_rw authentication error (22) Invalid argument</div><div class="line">InvalidArgumentError does not take keyword arguments</div><div class="line"># 创建rw权限的用户访问auth list成功</div><div class="line">[root@node3 ~]# ceph --name client.mon_rx --keyring /root/key auth list</div><div class="line">installed auth entries:</div><div class="line"></div><div class="line">osd.0</div><div class="line">key: AQDaTgBav2MgDBAALE1GEEfbQN73xh8V7ISvFA==</div><div class="line">caps: [mgr] allow profile osd</div><div class="line">caps: [mon] allow profile osd</div><div class="line">caps: [osd] allow *</div><div class="line">...</div><div class="line">...</div></pre></td></tr></table></figure><p>这边需要注意的是徐小胖的原文应该是笔误，他是用的<code>client.mon.rw</code>访问的，所以说实践可以发现很多光看发现不了的东西</p><p><strong>结论：</strong></p><blockquote><p><code>x</code>权限也需要和<code>r</code>权限搭配才有效果，该权限只能处理与<code>auth</code>相关的操作</p></blockquote><h3 id="权限"><a href="#权限" class="headerlink" title="* 权限"></a>* 权限</h3><p>这没什么好说的，猜也能猜到了，就是拥有<code>rwx</code>所有权限</p><h2 id="OSD-Caps"><a href="#OSD-Caps" class="headerlink" title="OSD Caps"></a>OSD Caps</h2><p>这一章需要研究一波再发出来</p><h2 id="丢失所有秘钥的再恢复"><a href="#丢失所有秘钥的再恢复" class="headerlink" title="丢失所有秘钥的再恢复"></a>丢失所有秘钥的再恢复</h2><p>如果所有秘钥全部删除，是否真的能恢复？所有秘钥包括</p><ul><li><code>MON</code> ： <code>/var/lib/ceph/mon/ceph-$hostname/keyring</code></li><li><code>OSD</code> ： <code>/var/lib/ceph/osd/ceph-$hostname/keyring</code></li><li><code>Client</code> ：<code>/etc/ceph/ceph.client.admin.keyring</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line"># 删除 mon keyring</div><div class="line">[root@node1 ceph-node1]# mv keyring /root/</div><div class="line"># 删除 ceph.conf</div><div class="line">[root@node1 ceph-node1]# mv /etc/ceph/ceph.conf /root/</div><div class="line"># 删除 client.admin.keyring</div><div class="line">[root@node1 ceph-node1]# mv /etc/ceph/ceph.client.admin.keyring /root</div><div class="line"># 尝试访问集群报错</div><div class="line">[root@node1 ceph-node1]# ceph -s</div><div class="line">2017-11-29 23:57:14.195467 7f25dc4cc700 -1 Errors while parsing config file!</div><div class="line">2017-11-29 23:57:14.195571 7f25dc4cc700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory</div><div class="line">2017-11-29 23:57:14.195579 7f25dc4cc700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory</div><div class="line">2017-11-29 23:57:14.195580 7f25dc4cc700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory</div><div class="line">Error initializing cluster client: ObjectNotFound(&apos;error calling conf_read_file&apos;,)</div><div class="line"># 尝试获取auth list报错</div><div class="line">[root@node1 ceph-node1]# ceph auth list</div><div class="line">2017-11-29 23:57:27.037435 7f162c5a7700 -1 Errors while parsing config file!</div><div class="line">2017-11-29 23:57:27.037450 7f162c5a7700 -1 parse_file: cannot open /etc/ceph/ceph.conf: (2) No such file or directory</div><div class="line">2017-11-29 23:57:27.037452 7f162c5a7700 -1 parse_file: cannot open ~/.ceph/ceph.conf: (2) No such file or directory</div><div class="line">2017-11-29 23:57:27.037453 7f162c5a7700 -1 parse_file: cannot open ceph.conf: (2) No such file or directory</div><div class="line">Error initializing cluster client: ObjectNotFound(&apos;error calling conf_read_file&apos;,)</div></pre></td></tr></table></figure><p>ok，下面开始修复：</p><h3 id="伪造-Mon-keyring"><a href="#伪造-Mon-keyring" class="headerlink" title="伪造 Mon keyring"></a>伪造 Mon keyring</h3><p>在<code>ceph</code>中除了<code>mon.</code>用户以外的的账户密码都保存在<code>Mon</code>的数据库<code>leveldb</code>中，但是<code>mon.</code> 用户的信息并没有保存在数据库里，而是在<code>MON</code>启动时读取<code>Mon</code>目录下的<code>keyring</code> 文件得到的，这也是我们之前验证后得到的结论。所以，我们可以随便伪造一个<code>keyring</code>，放到<code>Mon</code> 目录下去。然后同步到各个<code>Mon</code>节点，然后重启三个<code>Mon</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">[root@node1 ceph-node1]# cd /var/lib/ceph/mon/ceph-node1/</div><div class="line">[root@node1 ceph-node1]# ls</div><div class="line">done  kv_backend  store.db  systemd</div><div class="line">[root@node1 ceph-node1]# vim keyring</div><div class="line"># 伪造 keyring，可以看到里面还有tony的字样，可以看出明显是伪造的</div><div class="line">[root@node1 ceph-node1]# cat keyring </div><div class="line">[mon.]</div><div class="line">key = AQCtonyZAAAAABAAQOysx+Yxbno/2N8W1huZFA==</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line"># 重启 mon</div><div class="line">[root@node1 ceph-node1]# service ceph-mon@node1 restart</div><div class="line">Redirecting to /bin/systemctl restart  ceph-mon@node1.service</div></pre></td></tr></table></figure><p>可以看到效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"># monitor log显示mon.node1@0初始化成功，并被选举成了monitor leader</div><div class="line">2017-11-30 00:15:04.042157 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election</div><div class="line">2017-11-30 00:15:04.042299 7f8c4e28a700  1 mon.node1@0(electing).elector(934) init, last seen epoch 934</div><div class="line">2017-11-30 00:15:04.048498 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1 calling new monitor election</div><div class="line">2017-11-30 00:15:04.048605 7f8c4e28a700  1 mon.node1@0(electing).elector(937) init, last seen epoch 937, mid-election, bumping</div><div class="line">2017-11-30 00:15:04.078454 7f8c4e28a700  0 log_channel(cluster) log [INF] : mon.node1@0 won leader election with quorum 0,1,2</div></pre></td></tr></table></figure><p><strong>注意（很重要）：</strong></p><blockquote><p>虽然说<code>mon</code>在启动的时候读取对应的<code>keyring</code>，不在乎内容的正确性，但是不代表这个<code>keyring</code>可以胡乱修改。也就是说这个<code>keyring</code>是要<strong>符合某种规范和格式的</strong>，在实践过程我发现<code>keyring</code>前三位必须为大写的<code>AQC</code>，当然还有其他的格式要求，比如结尾是否必须要是<code>==</code>？长度是否是固定的？这个格式要求可能很多，我没有时间一个一个手动无脑验证，这个可以日后查看源码了解实现思路，有兴趣的童鞋可以试试，说不定可以发现很有趣的现象。当然说了这么多是否意味着很难伪造呢？这个我们也不必担心，最好的做法是从别的集群的<code>Mon keyring</code>拷贝一份过来就可以了，自己胡乱伪造启动会报错如下：</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">2017-11-29 23:49:50.134137 7fcab3e23700 -1 cephx: cephx_build_service_ticket_blob failed with error invalid key</div><div class="line">2017-11-29 23:49:50.134140 7fcab3e23700  0 mon.node1@0(probing) e1 ms_get_authorizer failed to build service ticket</div><div class="line">2017-11-29 23:49:50.134393 7fcab3e23700  0 -- 192.168.1.58:6789/0 &gt;&gt; 192.168.1.61:6789/0 conn(0x7fcacd15d800 :-1 s=STATE_CONNECTING_WAIT_CONNECT_REPLY_AUTH pgs=0 cs=0 l=0).handle_connect_reply connect got BADAUTHORIZER</div></pre></td></tr></table></figure><p>###　还原 ceph.conf</p><p>没有<code>/etc/ceph/ceph.conf</code>这个文件，我们是没法执行<code>ceph</code>相关指令的，所以我们需要尽可能的还原它。首先<code>fsid</code>可以通过去任意<code>osd</code>目录（<code>/var/lib/ceph/osd/ceph-$num/</code>）读取<code>ceph-fsid</code>文件获得，然后<code>mon_initial_members</code>和<code>mon_host</code>代表着集群每个节点的<code>hostname</code>和<code>ip</code>，这些都是我们知道的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"># 还原 ceph.conf</div><div class="line">[root@node1 ceph-node1]# cat /var/lib/ceph/osd/ceph-0/ceph_fsid </div><div class="line">99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">[root@node1 ceph-node1]# vim /etc/ceph/ceph.conf</div><div class="line">[root@node1 ceph-node1]# cat /etc/ceph/ceph.conf</div><div class="line">[global]</div><div class="line">fsid = 99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">mon_initial_members = node1, node2, node3</div><div class="line">mon_host = 192.168.1.58,192.168.1.61,192.168.1.62</div><div class="line">auth_cluster_required = cephx</div><div class="line">auth_service_required = cephx</div><div class="line">auth_client_required = cephx</div><div class="line"></div><div class="line">public network = 192.168.1.0/24</div><div class="line"></div><div class="line"># 通过 mon keyring 访问集群状态成功</div><div class="line">[root@node1 ceph-node1]# ceph -s --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_OK</div><div class="line"> </div><div class="line">  services:</div><div class="line">    mon: 3 daemons, quorum node1,node2,node3</div><div class="line">    mgr: node1_mgr(active)</div><div class="line">    osd: 6 osds: 6 up, 6 in</div></pre></td></tr></table></figure><h3 id="恢复-ceph-client-keyring"><a href="#恢复-ceph-client-keyring" class="headerlink" title="恢复 ceph.client.keyring"></a>恢复 ceph.client.keyring</h3><p>有了<code>Mon keyring</code>，并且可以执行<code>ceph</code>指令，那么我们就可以通过<code>ceph auth get</code>去<code>Monitor leveldb</code>获取任意<code>keyring</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"># 通过 Mon 获取 client.admin.keyring</div><div class="line">[root@node1 ceph-node1]# ceph --name mon. --keyring /var/lib/ceph/mon/ceph-node1/keyring auth get client.admin</div><div class="line">exported keyring for client.admin</div><div class="line">[client.admin]</div><div class="line">key = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line">caps mds = &quot;allow *&quot;</div><div class="line">caps mgr = &quot;allow *&quot;</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">caps osd = &quot;allow *&quot;</div><div class="line"># 创建 /etc/ceph/ceph.client.admin.keyring，并将上面内容更新到该文件</div><div class="line">[root@node1 ceph-node1]# vim /etc/ceph/ceph.client.admin.keyring</div><div class="line">[root@node1 ceph-node1]# cat /etc/ceph/ceph.client.admin.keyring</div><div class="line">[client.admin]</div><div class="line">key = AQDL7fdZWaQkIBAAsFhvFVQYqSeM/FVSY6o8TQ==</div><div class="line">caps mds = &quot;allow *&quot;</div><div class="line">caps mgr = &quot;allow *&quot;</div><div class="line">caps mon = &quot;allow *&quot;</div><div class="line">caps osd = &quot;allow *&quot;</div><div class="line"></div><div class="line"># 用默认 ceph -s 测试一下，发现可以正常访问了</div><div class="line"></div><div class="line">[root@node1 ceph-node1]# ceph -s</div><div class="line">  cluster:</div><div class="line">    id:     99480db2-f92f-481f-b958-c03c261918c6</div><div class="line">    health: HEALTH_OK</div><div class="line"> </div><div class="line">  services:</div><div class="line">    mon: 3 daemons, quorum node1,node2,node3</div><div class="line">    mgr: node1_mgr(active)</div><div class="line">    osd: 6 osds: 6 up, 6 in</div></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>首先感谢徐小胖给我提供了<code>cephx</code>方面的思路，希望日后多出好文，我也在不断地拜读这些优质文章。这篇文章花了我很长时间，大家从日志的时间可以看出来，跨度已经有好几天了，很多实践真的不是一蹴而就的，需要反复的尝试和思考才能得到最后的成功。<code>Ceph</code>还是要多动手，看别人文章是好事，但是记得要加以实践，否则再好的文章也只是想当然，作者说什么你就跟着他的思路走，你永远不知道别人一句简短的话语和结论的背后花了多少时间去推敲和实践，你看起来一条命令执行成功或者在某一步执行某个命令那也许是别人失败了无数次总结出来的。所以我们要自己实践去验证，除了可以验证原文的观点正确与否，往往可以发现一些其他有用的知识。</p><p>经历这次总结，收获满满，我对<code>cephx</code>的理解又上了一个层次。本文就<code>cephx</code>在不同组件中的角色扮演和依赖关系进行梳理，然后再对各组件的<code>cap</code>进行了研究，最后针对各个<code>keyring</code>的恢复给出了详细的指南和步骤。然后还剩两项任务没有完成，等有空进行完善！</p>]]></content>
    
    <summary type="html">
    
      &lt;center&gt;&lt;img src=&quot;http://ow0mgad6r.bkt.clouddn.com/cephx-600x450.jpg&quot; alt=&quot;cephx&quot;&gt;&lt;/center&gt;

&lt;p&gt;本文就阅读完&lt;a href=&quot;http://www.xuxiaopang.com/2017/08/23/easy-ceph-CephX/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;徐小胖的大话Cephx&lt;/a&gt;后，针对一些猜测和疑惑进行了实战演练，对原文的一些说法和结论进行了验证，并进行了一系列的扩展的思考猜想和总结。最后收获满满，不仅对原文的一些结论进行了验证，也发现了其中的一些问题，更多的是自己动手后一些奇妙的场景和发现。&lt;/p&gt;
    
    </summary>
    
      <category term="tech" scheme="https://tony-yin.github.io/categories/tech/"/>
    
    
      <category term="Ceph" scheme="https://tony-yin.github.io/tags/Ceph/"/>
    
      <category term="Cephx" scheme="https://tony-yin.github.io/tags/Cephx/"/>
    
  </entry>
  
</feed>
